{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import regex as re\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_queries(section):\n",
    "    prompt = f\"\"\"You are an expert in particle physics and a member of the LHCb collaboration. You have been tasked with generating high quality queries for a Q&A dataset of particle physics papers. A high quality query is something conceptually simple to ask but difficult to answer. A high quality query is answerable only by digging deep into the contents of the paper, with the answer scattered across multiple different sections.\n",
    "\n",
    "    Example high quality queries:\n",
    "    ['What is the dominant systematic uncertainty?',\n",
    "    'What decay is used in this measurement?',\n",
    "    'How large is the statistical uncertainty relative to the nominal value?',\n",
    "    'Is this result compatible with the SM?',\n",
    "    'How much data was analysed?',\n",
    "    'Which decays and final states enter the analysis?',\n",
    "    'What are the dangerous backgrounds in this analysis?',\n",
    "    'What are all the correcting weights?',\n",
    "    'Why is this result different to the previous result?',\n",
    "    'How many resonances does the model contain?',\n",
    "    'How is the efficiency modelled?',\n",
    "    'How does this measurement relate to other results?',\n",
    "    'Why is it so difficult to work with photons at LHCb?',\n",
    "    'What are the challenges when dealing with amplitude fits?',\n",
    "    'What are the considered backgrounds and how are they addressed?',\n",
    "    'How is semileptonic background treated?',\n",
    "    'How does this analysis deal with the correlation between the three-body mass and the two-body masses?',\n",
    "    'How does the selection strategy remove background from charm decays or other three-body radiative decays?',\n",
    "    'Why does this analysis not simply do an angular fit?',\n",
    "    'How large is the signal purity in the analysis?']\n",
    "\n",
    "    Please begin by processing the below section of a paper. You may re-use the example high quality queries, if appropriate. Provide your high quality queries as a list of strings: [\"query one\", \"query two\", ...]. Do not provide anything else.\n",
    "\n",
    "    {section}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tex_file_by_linenumber(file_path, start_line=0, end_line=1000000):\n",
    "    document_by_linenumber = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            if line_number < start_line:\n",
    "                continue\n",
    "            if line_number > end_line:\n",
    "                break\n",
    "            document_by_linenumber[line_number] = line\n",
    "    return document_by_linenumber\n",
    "\n",
    "async def queries_from_chunk(chunk):\n",
    "    # Change model here #\n",
    "    llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    prompt = get_prompt_queries(chunk)\n",
    "    answer = await llm.acomplete(prompt)\n",
    "    # Check if the answer contains a JSON block\n",
    "    pattern = r\"(?<=```json)([\\s\\S]*?)(?=```)\"\n",
    "    match = re.search(pattern, answer.text)\n",
    "    if match:\n",
    "        answer_text = match.group(1).replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        answer_text = answer.text.replace(\"\\n\", \" \")\n",
    "    return answer_text\n",
    "\n",
    "async def generate_queries_async(tex_path, lines_per_chunk = 40):\n",
    "    lines = read_tex_file_by_linenumber(tex_path)\n",
    "    total_lines = len(lines)\n",
    "    num_chunks = (total_lines + lines_per_chunk - 1) // lines_per_chunk\n",
    "    all_answers = []\n",
    "\n",
    "    calls = []\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * lines_per_chunk + 1\n",
    "        end_idx = min((chunk_idx + 1) * lines_per_chunk, total_lines)\n",
    "        chunk = \"\".join([lines[line_num] for line_num in range(start_idx, end_idx + 1) if line_num in lines])\n",
    "        calls.append(queries_from_chunk(chunk))\n",
    "\n",
    "    queries = await asyncio.gather(*calls)\n",
    "    queries = sum([eval(q) for q in queries], [])\n",
    "    return queries\n",
    "\n",
    "def generate_queries(tex_path, lines_per_chunk = 40):\n",
    "    return asyncio.run(generate_queries_async(tex_path, lines_per_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_best_queries(candidate_queries):\n",
    "    prompt = f\"\"\"You are an expert in particle physics and a member of the LHCb collaboration. You have been tasked with generating high quality queries for a Q&A dataset of particle physics papers. Provide your queries as a list of strings: [\"query one\", \"query two\", ...]. Do not provide anything else.\n",
    "\n",
    "    Example high quality queries:\n",
    "    ['What is the dominant systematic uncertainty?',\n",
    "    'What decay is used in this measurement?',\n",
    "    'How large is the statistical uncertainty relative to the nominal value?',\n",
    "    'Is this result compatible with the SM?',\n",
    "    'How much data was analysed?',\n",
    "    'Which decays and final states enter the analysis?',\n",
    "    'What are the dangerous backgrounds in this analysis?',\n",
    "    'What are all the correcting weights?',\n",
    "    'Why is this result different to the previous result?',\n",
    "    'How many resonances does the model contain?',\n",
    "    'How is the efficiency modelled?',\n",
    "    'How does this measurement relate to other results?',\n",
    "    'Why is it so difficult to work with photons at LHCb?',\n",
    "    'What are the challenges when dealing with amplitude fits?',\n",
    "    'What are the considered backgrounds and how are they addressed?',\n",
    "    'How is semileptonic background treated?',\n",
    "    'How does this analysis deal with the correlation between the three-body mass and the two-body masses?',\n",
    "    'How does the selection strategy remove background from charm decays or other three-body radiative decays?',\n",
    "    'Why does this analysis not simply do an angular fit?',\n",
    "    'How large is the signal purity in the analysis?']\n",
    "\n",
    "    Please consider the below list of potential queries. Based off of the description and examples above, your task is to consider the below candidate queries and output only the 15 highest quality ones. A high quality query is something conceptually simple to ask but difficult to answer. A high quality query is answerable only by digging deep into the contents of the paper, with the answer scattered across multiple different sections. Once you have selected 15 high quality queries, you should reword each of them slightly. Output a list, in the exact same format as provided, with only the 15 reworded high quality queries kept.\n",
    "\n",
    "    Candidate queries:\n",
    "    {candidate_queries}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_best_queries(candidate_queries):\n",
    "    prompt = get_prompt_best_queries(candidate_queries)\n",
    "    # Change model here\n",
    "    llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    answer = llm.complete(prompt)\n",
    "\n",
    "    # Check if the answer contains a JSON block\n",
    "    pattern = r\"(?<=```json)([\\s\\S]*?)(?=```)\"\n",
    "    match = re.search(pattern, answer.text)\n",
    "    if match:\n",
    "        answer_text = match.group(1).replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        answer_text = answer.text.replace(\"\\n\", \" \")\n",
    "    answer = eval(answer_text)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query answering with multiple queries per chunk (much cheaper API calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "def get_multi_query_prompt(document, queries):\n",
    "    formatted_queries = \"\\n\".join([f\"{i+1}. {query}\" for i, query in enumerate(queries)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a particle physics expert, enthusiastically answering queries about their paper. Because you have written the paper, you have an extremely detailed understanding of the contents of every single section of the paper.\n",
    "    \n",
    "    You will be given:\n",
    "    \n",
    "    - queries: A list of questions related to the contents of your paper. The answers to these queries may be scattered across multiple different sections.\n",
    "    \n",
    "    Your answer should be a JSON array where each element corresponds to a query and includes the field:\n",
    "\n",
    "    - relevant_passages: A list of line numbers that directly answer a part of the query. If the query is not answered anywhere in text, provide an empty list. Make sure to consider the full document, do not just provide the first partial answer you see. However, do not provide any more line numbers than is needed or provide line numbers which are ultimately not relevant to the query.\n",
    "\n",
    "    Structure:\n",
    "    - You will be given an excerpt from your paper in the following format, with the line number given before the first ':'\n",
    "        1: text for line 1...\n",
    "        2: text for line 2...\n",
    "        3: text for line 3...\n",
    "    - Your output should be a valid JSON array with one object per query, like this:\n",
    "        [\n",
    "            {{\n",
    "                \"query\": \"What is the dominant systematic uncertainty?\",\n",
    "                \"relevant_passages\": [relevant_line_1, relevant_line_2, ...],\n",
    "            }},\n",
    "            {{\n",
    "                \"query\": \"What detector was used?\",\n",
    "                \"relevant_passages\": [relevant_line_1, relevant_line_2, ...],\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "\n",
    "    The excerpt is given below:\n",
    "    {document}\n",
    "\n",
    "    The queries are given below:\n",
    "    {formatted_queries}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def read_tex_file_by_linenumber(file_path, start_line=0, end_line=1000000):\n",
    "    document_by_linenumber = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            if line_number < start_line:\n",
    "                continue\n",
    "            if line_number > end_line:\n",
    "                break\n",
    "            document_by_linenumber[line_number] = line\n",
    "    return document_by_linenumber\n",
    "\n",
    "def collapse_line_numbers(line_numbers, max_gap=2):\n",
    "    if not line_numbers:\n",
    "        return []\n",
    "    \n",
    "    # Sort the line numbers first and remove duplicates\n",
    "    sorted_lines = sorted(set(line_numbers))\n",
    "    \n",
    "    # Initialize with the first range\n",
    "    ranges = [[sorted_lines[0], sorted_lines[0]]]\n",
    "    \n",
    "    # Process remaining line numbers\n",
    "    for line in sorted_lines[1:]:\n",
    "        # If this line is within the allowed gap of the end of the current range\n",
    "        if line <= ranges[-1][1] + max_gap + 1:\n",
    "            # Extend the current range\n",
    "            ranges[-1][1] = max(ranges[-1][1], line)\n",
    "        else:\n",
    "            # Start a new range\n",
    "            ranges.append([line, line])\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "def get_text_for_range(range_start, range_end, full_lines):\n",
    "    range_text = \"\"\n",
    "    for line_num in range(range_start, range_end + 1):\n",
    "        if line_num in full_lines:\n",
    "            range_text += full_lines[line_num]\n",
    "        else:\n",
    "            range_text += f\"Line {line_num} not in document\\n\"\n",
    "    return range_text\n",
    "\n",
    "async def find_answers_to_queries_async(lines, queries, model=\"gpt-4o-mini\"):\n",
    "    document = \"\".join([f\"{key}: {value}\" for key, value in lines.items()])\n",
    "    prompt = get_multi_query_prompt(document, queries)\n",
    "    \n",
    "    llm = OpenAI(temperature=0, model=model)\n",
    "    answer = await llm.acomplete(prompt)\n",
    "\n",
    "    answer_text = answer.text\n",
    "    answer_text = re.sub(r'\\n', ' ', answer_text)\n",
    "    match = re.search(r\"\\[(?:[^\\[\\]]|(?R))*\\]\", answer_text)\n",
    "    \n",
    "    if match:\n",
    "        try:\n",
    "            answers = eval(match.group(0))\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating response: {e}\")\n",
    "            # Create a fallback answer\n",
    "            answers = [{\"query\": query, \"relevant_passages\": []} for query in queries]\n",
    "    else:\n",
    "        print(f\"Could not extract answer list from response: {answer_text}\")\n",
    "        answers = [{\"query\": query, \"relevant_passages\": []} for query in queries]\n",
    "    \n",
    "    # Return just the raw line numbers from each chunk\n",
    "    return answers\n",
    "\n",
    "async def process_chunk(chunk_idx, total_chunks, chunk, queries, model):\n",
    "    print(f\"Processing chunk {chunk_idx+1}/{total_chunks}: lines {min(chunk.keys())}-{max(chunk.keys())}\")\n",
    "    \n",
    "    try:\n",
    "        chunk_answers = await find_answers_to_queries_async(chunk, queries, model)\n",
    "        return chunk_idx, chunk_answers\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_idx}: {e}\")\n",
    "        # Return empty answers in case of error\n",
    "        return chunk_idx, [{\"query\": q, \"relevant_passages\": []} for q in queries]\n",
    "\n",
    "async def process_chunks_for_queries(file_path, queries, lines_per_chunk=50, overlap=5, model=\"gpt-4o-mini\", max_concurrent=5):\n",
    "    # Read all lines from the file\n",
    "    all_lines = read_tex_file_by_linenumber(file_path)\n",
    "    total_lines = max(all_lines.keys()) if all_lines else 0\n",
    "    \n",
    "    # Dictionary to store all line numbers for each query\n",
    "    query_line_numbers = {query: [] for query in queries}\n",
    "    \n",
    "    # Calculate the number of chunks with overlap\n",
    "    effective_chunk_size = lines_per_chunk - overlap\n",
    "    num_chunks = (total_lines + effective_chunk_size - 1) // effective_chunk_size\n",
    "    \n",
    "    # Create a list of chunk specifications\n",
    "    chunks = []\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        # Calculate chunk boundaries with overlap\n",
    "        if chunk_idx == 0:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = chunk_idx * effective_chunk_size - overlap + 1\n",
    "        \n",
    "        # End index is start + chunk size, but not beyond total lines\n",
    "        end_idx = min(start_idx + lines_per_chunk - 1, total_lines)\n",
    "        \n",
    "        # Extract the chunk\n",
    "        chunk = {line_num: all_lines[line_num] for line_num in range(start_idx, end_idx + 1) if line_num in all_lines}\n",
    "        \n",
    "        if chunk:  # Only add non-empty chunks\n",
    "            chunks.append((chunk_idx, chunk))\n",
    "    \n",
    "    # Process chunks in batches to limit concurrency\n",
    "    results = []\n",
    "    for i in range(0, len(chunks), max_concurrent):\n",
    "        batch = chunks[i:i+max_concurrent]\n",
    "        batch_tasks = [process_chunk(idx, num_chunks, chunk, queries, model) for idx, chunk in batch]\n",
    "        \n",
    "        # Wait for all tasks in this batch to complete\n",
    "        batch_results = await asyncio.gather(*batch_tasks)\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Optional rate limiting between batches (not between individual API calls within a batch)\n",
    "        if i + max_concurrent < len(chunks):\n",
    "            print(f\"Completed batch {i//max_concurrent + 1}/{(len(chunks) + max_concurrent - 1)//max_concurrent}. Waiting before starting next batch...\")\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    # Process all results to collect line numbers for each query\n",
    "    for _, chunk_answers in sorted(results):  # Sort by chunk index to maintain order\n",
    "        for i, answer in enumerate(chunk_answers):\n",
    "            if i < len(queries):\n",
    "                query = queries[i]\n",
    "                query_line_numbers[query].extend(answer.get('relevant_passages', []))\n",
    "    \n",
    "    # Now process all collected line numbers for each query\n",
    "    consolidated_answers = []\n",
    "    for query in queries:\n",
    "        # Remove duplicates and collapse line numbers across all chunks\n",
    "        collapsed_ranges = collapse_line_numbers(query_line_numbers[query])\n",
    "        \n",
    "        # Generate text for each range\n",
    "        range_texts = [get_text_for_range(start, end, all_lines) for start, end in collapsed_ranges]\n",
    "        \n",
    "        consolidated_answers.append({\n",
    "            \"query\": query,\n",
    "            \"relevant_passages\": collapsed_ranges,\n",
    "            \"relevant_passages_text\": range_texts\n",
    "        })\n",
    "    \n",
    "    return consolidated_answers\n",
    "\n",
    "def find_answers_in_paper(file_path, queries, lines_per_chunk=30, overlap=5, model=\"gpt-4o-mini\", max_concurrent=10):\n",
    "    return asyncio.run(process_chunks_for_queries(file_path, queries, lines_per_chunk, overlap, model, max_concurrent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_format(input_data):\n",
    "    result = {}\n",
    "    \n",
    "    for item in input_data:\n",
    "        query = item.get('query')\n",
    "        passages = item.get('relevant_passages', [])\n",
    "        \n",
    "        if query:\n",
    "            result[query] = passages\n",
    "    \n",
    "    return result\n",
    "\n",
    "def write_to_json(answers, header):\n",
    "    json_dict = convert_dict_format(answers)\n",
    "    import json\n",
    "    with open(f\"/work/submit/mcgreivy/beauty-in-stats/tree_rag/validation_data/autogenerated/{header}.json\", \"w\") as file:\n",
    "        json.dump(convert_dict_format(answers), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/7: lines 1-30\n",
      "Processing chunk 2/7: lines 21-50\n",
      "Processing chunk 3/7: lines 46-75\n",
      "Processing chunk 4/7: lines 71-100\n",
      "Processing chunk 5/7: lines 96-125\n",
      "Processing chunk 6/7: lines 121-150\n",
      "Processing chunk 7/7: lines 146-159\n"
     ]
    }
   ],
   "source": [
    "for header in [\"1508.00788\"]:\n",
    "    tex_path = f\"/work/submit/mcgreivy/paper_trees_cache/split_tex/{header}.tex\"\n",
    "    candidate_queries = generate_queries(tex_path)\n",
    "    best_queries = get_best_queries(candidate_queries)\n",
    "    answers = find_answers_in_paper(tex_path, best_queries)\n",
    "    write_to_json(answers, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1102.0206', '1109.0963', '1301.7084', '1402.6852', '1403.1339', '1404.0275', '1405.3219', '1406.2624', '1407.2222', '1408.5373', '1508.00788']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "headers = []\n",
    "validation_data_dir = \"/work/submit/mcgreivy/beauty-in-stats/tree_rag/validation_data/autogenerated/\"\n",
    "split_tex_dir = \"/work/submit/mcgreivy/paper_trees_cache/split_tex/\"\n",
    "\n",
    "for file in os.listdir(validation_data_dir):\n",
    "    pattern = r\"(.*).json\"\n",
    "    header = re.findall(pattern, file)[0]\n",
    "    headers.append(header)\n",
    "\n",
    "print(headers)\n",
    "\n",
    "questions_to_answers = {}\n",
    "questions_to_linenumbers = {}\n",
    "for header in headers:\n",
    "    questions_to_answers[header] = {}\n",
    "    questions_to_linenumbers[header] = {}\n",
    "    \n",
    "    with open(split_tex_dir + f\"{header}.tex\", 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(validation_data_dir + f'{header}.json', 'r') as file:\n",
    "        q_to_line = json.load(file)\n",
    "        \n",
    "    for question in q_to_line:\n",
    "        answers = []\n",
    "        for start, end in q_to_line[question]:\n",
    "            answer = re.sub(\"\\s+\", \" \", \"\".join(lines[start - 1 : end]))\n",
    "            answers.append(answer)\n",
    "        questions_to_answers[header][question] = answers\n",
    "        questions_to_linenumbers[header][question] = q_to_line[question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "for key in questions_to_answers:\n",
    "    key = \"1508.00788\"\n",
    "    for question in questions_to_answers[key]:\n",
    "        all_questions.append(question)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the main challenges in measuring the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ branching fraction using the LHCb Run 1 dataset?',\n",
       " 'How does the multivariate algorithm contribute to the signal selection process for the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ decay?',\n",
       " 'What theoretical predictions exist for the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ branching fraction, and how do they align with experimental findings?',\n",
       " 'How does the selection strategy balance background reduction and signal efficiency for the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ decay?',\n",
       " 'What role does the $\\\\Bz \\rightarrow \\\\phi \\\\Kstar(892)^0$ decay play in the analysis of the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ decay?',\n",
       " 'What are the implications of the $\\\\Bs \\rightarrow \\\\phi \\\\phi$ decay for theories beyond the Standard Model?',\n",
       " 'How is the combinatorial background from hadrons originating at the primary vertex minimized?',\n",
       " 'What is the function of the ring-imaging Cherenkov detectors in this analysis?',\n",
       " 'How are the $\\\\phi$ and $K^*$ meson candidates identified and selected?',\n",
       " 'What are the invariant mass criteria for the $K^+K^-$ and $K^+\\\\pi^-$ pairs?',\n",
       " 'How is the background from specific quark-hadron decays eliminated?',\n",
       " 'What method is employed to reject background from $\\\\Bz \\to \\\\phi \\\\Kstar(892)^0$ decays?',\n",
       " 'How are open charm decays suppressed in this analysis?',\n",
       " 'What is the efficiency of the vetoes in retaining simulated $\\\\Bs \\rightarrow \\\\phi \\\\phi$ decays?',\n",
       " 'How is the Boosted Decision Tree (BDT) utilized to suppress combinatorial background?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BeautyInStats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
