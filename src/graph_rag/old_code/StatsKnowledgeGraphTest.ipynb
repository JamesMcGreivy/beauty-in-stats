{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import regex as re \n",
    "import subprocess\n",
    "import importlib\n",
    "from collections import deque\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "os.chdir(\"/work/submit/mcgreivy/beauty-in-stats/graph_rag\")\n",
    "\n",
    "import StatsKnowledgeGraph\n",
    "importlib.reload(StatsKnowledgeGraph)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunked Stats Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16530 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "\n",
      "Phase 1: Extracting entities from all chunks\n",
      "Processing chunks 0 to 39\n",
      "Sleeping for 10 seconds...\n",
      "Processing chunks 40 to 79\n",
      "Sleeping for 10 seconds...\n",
      "Processing chunks 80 to 119\n",
      "Sleeping for 10 seconds...\n",
      "Processing chunks 120 to 159\n",
      "Sleeping for 10 seconds...\n",
      "Processing chunks 160 to 177\n",
      "\n",
      "Phase 2: Deduplicating 554 extracted entities\n",
      "After deduplication: 122 unique entities\n",
      "\n",
      "Phase 3: Extracting relationships using merged entities\n",
      "Processing relationships for chunks 0 to 39\n",
      "Sleeping for 10 seconds...\n",
      "Processing relationships for chunks 40 to 79\n",
      "Sleeping for 10 seconds...\n",
      "Processing relationships for chunks 80 to 119\n",
      "Sleeping for 10 seconds...\n",
      "Processing relationships for chunks 120 to 159\n",
      "Sleeping for 10 seconds...\n",
      "Processing relationships for chunks 160 to 177\n",
      "\n",
      "Phase 4: Deduplicating 862 extracted relationships\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199662, Requested 1196. Please try again in 257ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199647, Requested 1229. Please try again in 262ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199648, Requested 1244. Please try again in 267ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199638, Requested 1205. Please try again in 252ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199668, Requested 1206. Please try again in 262ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199614, Requested 1229. Please try again in 252ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199585, Requested 1212. Please try again in 239ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199580, Requested 1217. Please try again in 239ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199577, Requested 1223. Please try again in 240ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199527, Requested 1457. Please try again in 295ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199510, Requested 1524. Please try again in 310ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199422, Requested 1213. Please try again in 190ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199333, Requested 1696. Please try again in 308ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199299, Requested 1526. Please try again in 247ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-vbCBUfLxtSLrNODFqTZaltoI on tokens per min (TPM): Limit 200000, Used 199257, Requested 1766. Please try again in 306ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 454 unique relationships\n"
     ]
    }
   ],
   "source": [
    "def clean_latex(text):\n",
    "    environments = [\n",
    "        r'\\\\begin\\{figure\\*?\\}.*?\\\\end\\{figure\\*?\\}',\n",
    "        r'\\\\begin\\{wrapfigure\\*?\\}.*?\\\\end\\{wrapfigure\\*?\\}',\n",
    "        r'\\\\begin\\{thebibliography\\}.*?\\\\end\\{thebibliography\\}',\n",
    "        r'\\\\label\\{.*?\\}',\n",
    "        r'.*\\\\begin\\{document\\}',\n",
    "    ]\n",
    "    \n",
    "    for pattern in environments:\n",
    "        text = re.sub(pattern, ' ', text, flags=re.DOTALL)\n",
    "\n",
    "    marker = \"\\\\section{Introduction}\"\n",
    "    position = text.find(marker)\n",
    "    if position > 0:\n",
    "        text = text[position:]\n",
    "\n",
    "    text = re.sub(r'\\%.*\\n', \"\", text)\n",
    "    text = re.sub(r\"\\n\\s*\", \"\\n\", text)\n",
    "    text = re.sub(r'([^\\S\\n])+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def split_sections(text, depth=0, title=\"\", max_tokens=6000):\n",
    "    pattern = r\"(\\\\\" + \"sub\" * depth + r\"section[\\*\\s]*(?:\\[[^\\]]*\\])?\\s*({(?:[^{}]*+|(?2))*}))\"\n",
    "    matches = re.finditer(pattern, text)\n",
    "\n",
    "    if not matches or depth > 3:\n",
    "        return [text]\n",
    "\n",
    "    sections = []\n",
    "    start = 0\n",
    "    section_title = \"\"\n",
    "    for match in [(match.start(), match.end()) for match in matches] + [(-1, -1)]:\n",
    "        end = match[0]\n",
    "        section_text = text[start:end]\n",
    "        if len(re.sub(\"\\s\", \"\", section_text)) > 0:\n",
    "            new_title = f\"{title}\\n{section_title}\" if len(title) > 0 else section_title\n",
    "            num_tokens = len(StatsKnowledgeGraph.embedding_model.tokenizer.tokenize(section_text))\n",
    "            if num_tokens > max_tokens:\n",
    "                sections.extend(split_sections(section_text, depth=depth+1, title=new_title))\n",
    "            else:\n",
    "                sections.append(new_title + section_text)\n",
    "\n",
    "        start = match[1]\n",
    "        section_title = text[end:start]\n",
    "\n",
    "    return sections\n",
    "\n",
    "chunks = []\n",
    "chunk_to_paper = {}\n",
    "source_directory = \"/work/submit/mcgreivy/beauty-in-stats/graph_rag/data/arXiv\"\n",
    "for root, dirs, files in os.walk(source_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tex\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                text = clean_latex(f.read())\n",
    "                sections = split_sections(text)\n",
    "                chunks.extend(sections)\n",
    "                for chunk in sections:\n",
    "                    chunk_to_paper[chunk] = root.split(\"/\")[-1]\n",
    "                \n",
    "chunks = list(filter(lambda x: len(x) > 5, chunks))\n",
    "print(len(chunks))\n",
    "\n",
    "stats_entities, stats_relationships = StatsKnowledgeGraph.process_all_sections(chunks, chunk_size=40, sleep_time=10)\n",
    "np.save(\"./data/saved_kg/stats_entities.npy\", stats_entities, allow_pickle=True)\n",
    "np.save(\"./data/saved_kg/stats_relationships.npy\", stats_relationships, allow_pickle=True)\n",
    "# stats_entities, stats_relationships = np.load(\"./data/saved_kg/stats_entities.npy\", allow_pickle=True), np.load(\"./data/saved_kg/stats_relationships.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_entities = []\n",
    "# for paper in set(chunk_to_paper.values()):\n",
    "#     paper_entity = {\"entity_name\": paper, \"entity_type\": \"stats_review_paper\", \"description\": \"One of the stats review papers\"}\n",
    "#     paper_entities.append(paper_entity)\n",
    "# paper_entities = np.array(paper_entities)\n",
    "\n",
    "# paper_relationships = []\n",
    "# for entity in stats_entities:\n",
    "#     entity_name_a = entity[\"entity_name\"]\n",
    "#     entity_type_a = entity[\"entity_type\"]\n",
    "#     for passage in entity[\"relevant_passages\"]:\n",
    "#         entity_name_b = chunk_to_paper[passage]\n",
    "#         entity_type_b = \"stats_review_paper\"\n",
    "#         paper_relationship = {\"entity_name_a\": entity_name_a, \"entity_type_a\": entity_type_a, \"entity_name_b\": entity_name_b, \"entity_type_b\": entity_type_b, \"relationship_name\": \"is described in\"}\n",
    "#         paper_relationships.append(paper_relationship)\n",
    "# paper_relationships = np.array(paper_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = []\n",
    "# chunk_to_paper = {}\n",
    "\n",
    "# i = 0\n",
    "# corpus_path = \"/work/submit/mcgreivy/beauty-in-stats/src/scraper/data/cleaned_tex\"\n",
    "# for file in os.listdir(corpus_path):\n",
    "#     if file.endswith(\".tex\"):\n",
    "#         file_path = os.path.join(corpus_path, file)\n",
    "#         with open(file_path) as f:\n",
    "#             text = clean_latex(f.read())\n",
    "#             sections = split_sections(text)\n",
    "#             chunks.extend(sections)\n",
    "#             for chunk in sections:\n",
    "#                 chunk_to_paper[chunk] = file\n",
    "#             i += 1\n",
    "#             if i > 20:\n",
    "#                 break\n",
    "\n",
    "# print(len(chunks))\n",
    "\n",
    "# #relevant_entities = [e for e in stats_entities if e[\"entity_type\"] not in  \"statistics_concept\"]\n",
    "# #lhcb_entities, lhcb_relationships = StatsKnowledgeGraph.lhcb_kg_extension(chunks, chunk_to_paper, relevant_entities, chunk_size=40, sleep_time=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_entities = np.hstack((stats_entities, paper_entities, lhcb_entities))\n",
    "# # all_relationships = np.hstack((stats_relationships, paper_relationships, lhcb_relationships))\n",
    "# # np.save(\"./data/saved_kg/all_entities.npy\", all_entities, allow_pickle=True)\n",
    "# # np.save(\"./data/saved_kg/all_relationships.npy\", all_relationships, allow_pickle=True)\n",
    "# all_entities = np.load(\"./data/saved_kg/all_entities.npy\", allow_pickle=True)\n",
    "# all_relationships = np.load(\"./data/saved_kg/all_relationships.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "uri=\"neo4j+s://2d257b33.databases.neo4j.io\"\n",
    "username=\"neo4j\"\n",
    "password=\"LrVuuzEjpH3gmxLAFlOwgZoKnDCnX5AU3rRqS0PW97g\"\n",
    "\n",
    "graph = Graph(uri, auth=(username, password))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.delete_all()\n",
    "\n",
    "entity_nodes = {}\n",
    "\n",
    "for entity in stats_entities:\n",
    "    node = Node(\n",
    "        entity[\"entity_type\"],\n",
    "        name=entity[\"entity_name\"],\n",
    "        description=entity[\"description\"],\n",
    "    )\n",
    "\n",
    "    graph.create(node)\n",
    "    entity_nodes[entity[\"entity_name\"]] = node\n",
    "\n",
    "for rel in stats_relationships:\n",
    "    try:\n",
    "        rel = rel.copy()\n",
    "        source_node = entity_nodes[rel[\"entity_name_a\"]]\n",
    "        target_node = entity_nodes[rel[\"entity_name_b\"]]\n",
    "        relationship = rel[\"relationship_name\"]\n",
    "        rel.pop(\"entity_name_a\")\n",
    "        rel.pop(\"entity_name_b\")\n",
    "        rel.pop(\"relationship_name\")\n",
    "        rel.pop(\"relevant_passages\", None)\n",
    "\n",
    "        relationship = Relationship(\n",
    "            source_node,\n",
    "            relationship,\n",
    "            target_node,\n",
    "            **rel\n",
    "        )\n",
    "\n",
    "        graph.create(relationship)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'entity_name': 'likelihood function and related concepts', 'entity_type': 'statistics_concept', 'description': 'A fundamental statistical function that measures the probability of observing given data under various parameter values, extensively used in statistical inference, hypothesis testing, and parameter estimation. This encompasses related concepts such as likelihood ratios, maximum likelihood estimation, and various probability distributions like Poisson, Gaussian, and binomial distributions, which are essential in modeling and analyzing data in fields such as particle physics.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Importance Sampling}\\n[The following section has been adapted from text written primarily by Sven Kreiss, Alex Read, and myself for the ATLAS Higgs combination. It is reproduced here for convenience. ]\\nTo claim a discovery, it is necessary to populate a small tail of a test statistic distribution. Toy Monte-Carlo techniques use the model $\\\\F_{\\\\rm tot}$ to generate toy data $\\\\data_{toy}$. For every pseudo-experiment (toy), the test statistic is calculated and added to the test statistic distribution. Building this distribution from toys is independent of the assumptions that go into the asymptotic calculation that describes this distribution with an analytic expression. Recently progress has been made using Importance Sampling to populate the extreme tails of the test statistic distribution, which is much more computationally intensive with standard methods. The presented algorithms are implemented in \\\\roostats\\\\ \\\\texttt{ToyMCSampler}.\\n\\\\subsubsection{Naive Importance Sampling}\\nAn ensemble of \"standard toys\" is generated from a model representing the Null hypothesis with $\\\\mu=0$ and the nuisance parameters $\\\\vec\\\\theta$ fixed at their profiled values to the observed data $\\\\nuisObs$, written\\\\\\\\ \\\\mbox{$\\\\F_{\\\\rm tot}(\\\\datasim,\\\\globs|\\\\mu=0,\\\\nuisObs)$}. With importance sampling however, the underlying idea is to generate toys from a different model, called the importance density. A valid importance density is for example the same model with a non-zero value of $\\\\mu$. The simple Likelihood ratio is calculated for each toy and used as a weight.\\n\\\\[\\n{\\\\rm weight} = \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)} {\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)}\\n\\\\]\\nThe weighted distribution is equal to a distribution of unweighted toys generated from the Null. The choice of the importance density is a delicate issue. Michael Woodroofe presented a prescription for creating a well behaved importance density~\\\\cite{Woodroofe}. Unfortunately, this method is impractical for models as large as the combined Higgs models. An alternative approach is shown below.\\n\\\\subsubsection{Phase Space Slicing}\\nThe first improvement from naive importance sampling is the idea of taking toys from both, the null density and the importance density. There are various ways to do that. Simply stitching two test statistic distributions together at an arbitrary point has the disadvantage that the normalizations of both distributions have to be known.\\nInstead, it is possible to select toys according to their weights. First, toys are generated from the Null and the simple Likelihood ratio is calculated. If it is larger than one, the toy is kept and otherwise rejected. Next, toys from the importance density are generated. Here again, the simple Likelihood ratio is calculated but this time the toy is rejected when the Likelihood ratio is larger than one and kept when it is smaller than one. If kept, the toy\\'s weight is the simple Likelihood ratio which is smaller than one by this prescription.\\nIn the following section, this idea is restated such that it generalizes to multiple importance densities.\\n\\\\subsubsection{Multiple Importance Densities}\\nThe above procedure for selecting and reweighting toys that were generated from both densities can be phrased in the following way:\\n\\\\begin{itemize}\\n\\\\item A toy is generated from a density with $\\\\mu=\\\\mu\\'$ and the Likelihoods $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)$ and $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)$ are calculated.\\n\\\\item The toy is veto-ed when the Likelihood with $\\\\mu=\\\\mu\\'$ is not the largest. Otherwise, the toy is used with a weight that is the ratio of the Likelihoods.\\n\\\\end{itemize}\\nThis can be generalized to any number of densities with $\\\\mu_i=\\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}$. For the toys generated from model $i$:\\n\\\\begin{align}\\n\\\\textrm{veto:}& \\\\textrm{ if } \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs) \\\\neq \\\\max\\\\left\\\\{ \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_j,\\\\nuisObs) : \\\\mu_j = \\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}\\\\right\\\\} \\\\\\\\\\n\\\\textrm{weight} &= \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)}{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs)}\\n\\\\end{align}\\nThe number of importance densities has to be known when applying the vetos. It should not be too small to cover the parameter space appropriately and it should not be too large, because too many importance densities lead to too many vetoed toys which decreases overall efficiency. The value and error of $\\\\hat{\\\\mu}$ from a fit to data can be used to estimate the required number of importance densities for a given target overlap of the distributions.\\nThe sampling efficiency in the tail can be further improved by generating a larger number of toys for densities with larger values of $\\\\mu$. For example, for $n$ densities, one can generate $2^k / 2^n = 2^{k-n}$ of the overall toys per density $k$ with $k=0, \\\\ldots, n-1$. The toys have to be re-weighted for example by $2^{n-1} / 2^k$ resulting in a minimum re-weight factor of one. The current implementation of the error calculation for the p-value is independent of an overall scale in the weights.\\nThe method using multiple importance densities is similar to Michael Woodroofe\\'s \\\\cite{Woodroofe} prescription of creating a suitable importance density with an integral over $\\\\mu$. In the method presented here, the integral is approximated by a sum over discrete values of $\\\\mu$. Instead of taking the sum, a mechanism that allows for multiple importance densities is introduced.\\n', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\", '\\\\section{Appendix A: Poisson cumulative distribution}\\nThe confidence level $\\\\alpha$ associated with a specific signal strength $\\\\mu$ is determined by the cumulative distribution function (CDF). There is a relation that links the CDF of the Poisson distribution to that of the $\\\\chi^{2}(x; k=2(n+1))$ distribution. By applying the definition of the confidence level, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha = 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) = \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{k/2+1} e^{-x^{2}/2}}{2^{k/2} \\\\Gamma(k/2)} = \\n\\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{2^{1+n}n!} dx.\\n\\\\end{equation}\\nBy applying integration by parts, where $u=\\\\frac{x^{n}}{n !}$, $du=\\\\frac{x^{n-1}}{(n-1)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n+1}}$, we obtained the following result:\\n\\\\begin{equation}\\n1-\\\\alpha=- \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{n! 2^{n}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{2^{n}(n-1)!} dx.\\n\\\\end{equation}\\nBy once again applying integration by parts, with $u=\\\\frac{x^{n-1}}{(n-1) !}$, $du=\\\\frac{x^{n-2}}{(n-2)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-1)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, the following expression is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} -\\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{(n-1)! 2^{(n-1)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{2^{(n-1)}(n-2)!} dx.\\n\\\\end{equation}\\nFinally, by integrating once more by parts, with $u=\\\\frac{x^{n-2}}{(n-2) !}$, $du=\\\\frac{x^{n-3}}{(n-3)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-2)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n-1}}$, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} -\\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{(n-2)! 2^{(n-2)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} + \\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx.\\n\\\\end{equation}\\nAt this stage, a pattern can be discerned in the results obtained after repeatedly applying integration by parts. Specifically, the evaluation of the integral as $x \\\\to \\\\infty$ equal zero, while for $x = 2\\\\lambda$, a term of the following form is obtained:\\n\\\\begin{equation}\\n\\\\frac{(\\\\lambda)^{n-i}e^{-\\\\lambda}}{(n-i)!}.\\n\\\\end{equation}\\nThus, after performing $n-1$ integration by parts, the following result is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+ \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{e^{-\\\\frac{x}{2}}}{2} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+e^{-\\\\lambda}\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}.\\n\\\\end{equation}\\nThis result corresponds to the cumulative distribution function (CDF) of the Poisson distribution. This distribution would then be related to the cumulative $\\\\chi^{2}$ distribution ($F_{\\\\chi^{2}}$) as follows:\\n\\\\begin{equation}\\n\\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}= 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)).\\n\\\\end{equation}\\n', \"\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Bayesian upper limits}\\nSince the upper limits obtained through the frequentist approach can lead to non-physical statistical boundaries, alternative approaches can improve the results. One of the most promising strategies is based on Bayes' theorem:\\n\\\\begin{equation}\\nP(\\\\bm{\\\\theta}/x) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\bm{\\\\theta})\\\\Pi(\\\\bm{\\\\theta})}{P(\\\\bm{x})}.\\n\\\\end{equation}\\nWhere $P(\\\\bm{\\\\theta}|\\\\bm{x})$ represents the probability that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true given the set of observations $\\\\bm{x}$, and is known as the \\\\textit{posterior distribution}. $\\\\mathcal{L}(\\\\bm{x}|\\\\bm{\\\\theta})$, known as the \\\\textit{likelihood function}, describes the probability of observing $\\\\bm{x}$ given that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true. On the other hand, $\\\\Pi(\\\\bm{\\\\theta})$ is the \\\\textit{prior distribution}, which reflects the probability that the hypothesis $\\\\bm{\\\\theta}$ is true before the observations are made, and $P(\\\\bm{x})$ is the \\\\textit{total probability} of observing $\\\\bm{x}$ across all hypotheses. In optimization processes, this latter distribution is considered a normalization factor for the posterior distribution~\\\\cite{wang2023recent}. From a parameter estimation perspective, sampling from the posterior distribution generally requires robust methods, such as the Metropolis-Hastings algorithm~\\\\cite{chib1995understanding}.\\nBy incorporating an appropriate prior distribution, such as one with a minimum at $\\\\mu=0$, the coverage problem present in the frequentist approach is corrected. This implies that, for no value of the parameter of interest $\\\\mu$, is the null hypothesis excluded. An unbiased distribution is given by:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nThe value of $\\\\mu^{max}$ is chosen to obtain p-values consistent with the critical region (i.e., $\\\\alpha = 0.05$). However, the degree of subjectivity in selecting the prior distribution introduces ambiguity in the calculation of credible confidence intervals, as well as in the upper limits. In general, it has been observed that Bayesian parameter estimation leads to less restrictive upper limits, which are dependent on the choice of the prior distribution. This characteristic has limited the use of Bayesian upper limits in physics analyses with real observations~\\\\cite{cms2012observation,atlas2012observation}.\\nReturning to the simplified model with no observation, $b \\\\approx n = 0$, and $s=1$, the Bayesian upper limit for this new model is $\\\\mu_{up} = 2.999$ at $95\\\\\\n\\\\begin{equation}\\nP(\\\\bm{x}) = \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu)\\\\Pi(\\\\mu) d \\\\mu.\\n\\\\end{equation}\\nAlthough both methods seem to lead to the same results for the estimation of upper limits, varying the background component and the number of observed events reveals an adjustment in the upper limits that corrects the coverage issue in the estimation. Figure~[\\\\ref{fig:6}] shows the behavior of the upper limit as a function of the background component and the number of observations. Note how the null hypothesis is not excluded when the prior distribution is correctly chosen. However, manipulating the prior distribution can result in a significant shift in the upper limit value. For this reason, the Bayesian method is not widely used in the analysis of real data.\\nAdditionally, it is possible to estimate the upper limit using the Markov Chain Monte Carlo (MCMC) technique~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Bayesian/MetropolisSampling.ipynb}{Source Code}}. This method allows sampling from the posterior distribution based on Markov processes, as described in various sources~\\\\cite{chib1995understanding, raftery1992practical}. In general, this approach enables the construction of the marginal posterior function for the calculation of upper limits, the estimation of standard errors ($2\\\\sigma$), and the necessary parameter estimation in methods such as the profile of maximum likelihood, discussed later. Figure~[\\\\ref{fig:7}] shows the sampling of the posterior distribution for the toy model, leading to an approximation of the upper limit using the $P_{95}$ percentile.\\nThe MCMC algorithm is widely used for estimation in real multichannel experiments and incorporates systematic effects. The development of these ideas requires non-Bayesian approaches that do not depend on the choice of the prior distribution. An initial non-Bayesian approach that protects the null hypothesis is known as the modified frequentist method~\\\\cite{read2002presentation, cms2022portrait}.\\n\", \"\\\\section{Upper Limits including systematic uncertainties, Bayesian approach}\\nThe inclusion of systematic effects in the calculation of upper limits, experimental sensitivity, and observation requires a Bayesian approach. This strategy extends the likelihood function by including typically Gaussian distributions to model effects such as efficiency, luminosity, Monte Carlo event estimation, and others~\\\\cite{lista2016practical,cranmer2015practical,junk1999confidence}. In particular, to establish the reconstruction efficiency of background events, a nuisance parameter (\\\\( \\\\epsilon \\\\)) centered on the expected value of \\\\( b \\\\) can be included. Thus, the parameter of the Poisson distribution is defined by:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu,\\\\epsilon) = \\\\mu s + \\\\epsilon b,\\n\\\\end{equation}\\nwhere the efficiency follows a binomial distribution \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\). The standard deviation of the likelihood adjusts the uncertainty of the number of background events across the observable spectrum, typically ranging from 5 to 20\\\\\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon) = \\\\prod_{i=1}^{N-channels}\\n\\\\frac{ e^{-(\\\\mu s_{i} + \\\\epsilon b_{i})} (\\\\mu s_{i} + \\\\epsilon b_{i})^{n_{i}} }{n_{i}!}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^{2}}} e^{ -\\\\frac{ (1-\\\\epsilon)^{2} } {2\\\\sigma^{2}} }.\\n\\\\end{equation}\\nThe non-informative prior distribution naturally extends to:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu,\\\\epsilon) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\ \\\\text{and} \\\\ 0 < \\\\epsilon < \\\\epsilon^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nUsing Bayes' theorem, the posterior distribution is obtained.\\n\\\\begin{equation}\\nP(\\\\mu, \\\\epsilon / \\\\bm{x}) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon)}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThis means that to establish the upper limits of \\\\( \\\\mu \\\\) or the experimental sensitivity, the posterior must be marginalized to find the profile \\\\( P(\\\\mu | \\\\bm{x}) \\\\). This is a standard probability calculation and requires numerical integration or sampling of the posterior distribution using, for example, the Markov Chain Monte Carlo (MCMC) algorithm~\\\\cite{raftery1992practical, wang2023recent}. In any case, the probability profile is given by:\\n\\\\begin{equation}\\nP(\\\\mu,\\\\bm{x}) = \\\\int_{0}^{\\\\infty} P(\\\\mu, \\\\epsilon / \\\\bm{x}) d\\\\epsilon = \\\\frac{ \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\epsilon}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThe marginalization process correctly propagates the effect of systematic uncertainty in the upper limits. In general, the correlation shifts the limit values to higher values, thereby restricting the sensitivity of a model in the experiment or the exclusion power in an experimental study~\\\\cite{conway2005calculation}. As mentioned previously, the expected and observed upper limits are defined over the marginal distribution:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = \\\\int_{0}^{\\\\mu_{up}} P(\\\\mu,x) d\\\\mu. = 0.95\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:19}] shows the posterior distribution as a function of the signal strength \\\\( \\\\mu \\\\) and the efficiency in estimating background events (\\\\( b \\\\)), for the channel with \\\\( n=105 \\\\), \\\\( b=100 \\\\), \\\\( s=10 \\\\), and a systematic uncertainty of \\\\( \\\\sigma=0.1 \\\\) for the background events~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/UpperLimitSystematic.ipynb}{Source code}}. Similarly, Figure~[\\\\ref{fig:20}] shows the marginal distribution obtained using the double Gaussian quadrature method~\\\\cite{golub1969calculation}.\\nBy varying the systematic uncertainty, it is possible to find the observed upper limits and the correlation effect between parameters. Table~[\\\\ref{tb:3}] shows the behavior of the observed upper limit as a function of \\\\( \\\\sigma \\\\) for the numerical approximation (Gaussian quadrature) and for the sampling generated by the Metropolis algorithm, as well as the correlation coefficient indicating how uncertainty limits the exclusion power of the model. It is important to note that for high-dimensional posterior distributions, there are no quadrature rules that allow for accurately estimating the marginal distribution. In such cases, the Metropolis algorithms and optimization methods have been widely applied~\\\\cite{atlas2012observation,cms2012observation}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & Gaussian quadrature & MCMC algorithm & Correlation coef\\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 & -0.32\\\\\\\\\\n0.10 & 3.34 & 3.31 & -0.54\\\\\\\\\\n0.15 & 4.09 & 4.13 & -0.68\\\\\\\\\\n0.20 & 4.91 & 4.66 & -0.77\\\\\\\\\\n0.25 & 5.79 & 5.80 & -0.88\\\\\\\\ \\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nIn particular, the \\\\texttt{emcee} package was used for sampling the extended posterior distribution shown above~\\\\cite{foreman2013emcee, Bocklund2019ESPEI}. Figure~[\\\\ref{fig:21}] shows the corner plot of the posterior distribution along with the marginal distributions associated with the signal strength \\\\( \\\\mu \\\\) and the reconstruction efficiency \\\\( \\\\epsilon \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/MetropolisSamplingBayes.ipynb}{Source code}}. Additionally, the maximum likelihood estimators of the posterior are shown; these parameters are required for the profile likelihood method presented in the following section.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\nIn scientific research, experiments are designed to collect data, and theories or models are developed to explain those observations. In general, the falsification of theories is based on hypothesis testing. Hypothesis tests determine, with a given confidence level ($CL$), whether the observed data provide sufficient evidence to reject an initial hypothesis, called the null hypothesis ($H_{0}$), in favor of an alternative hypothesis ($H_{1}$). The null hypothesis ($H_{0}$) is considered true until observations indicate otherwise; in such a case, the initial explanation is rejected, and the new theory ($H_{1}$) is accepted~\\\\cite{sinervo2002signal}. Both the frequentist and Bayesian approaches applied here yield robust upper limit estimations, adaptable to various experimental setups, making them vital tools for model testing and exclusion.\\nIn high-energy physics (HEP), the null hypothesis ($H_{0}$) refers to all known physical processes, which are summarized in what is known as the Standard Model. The alternative hypothesis ($H_{1}$) represents potential models that could explain new observations that the accepted model cannot account for, such as supersymmetry, extra dimensions, among others~\\\\cite{cowan2011asymptotic,florez2016probing}.\\nAdditionally, hypothesis testing requires the selection of a confidence level in terms of statistical significance ($\\\\alpha$).\\n\\\\begin{equation}\\nCL = 1 - \\\\alpha.\\n\\\\end{equation}\\nWhere $\\\\alpha$ (type I error) is the probability of rejecting the null hypothesis when it is true. By convention, model exclusion in particle physics is done for a value of $\\\\alpha = 0.05$, which corresponds to a confidence level ($CL$) of $95\\\\\\n\\\\begin{equation}\\n\\\\alpha = \\\\int_{P}^{\\\\infty} \\\\frac{1}{2\\\\pi} e^{-x^{2}/2} dx.\\n\\\\end{equation}\\nWhere $P$ is the percentile of the distribution, for which the type I error is $\\\\alpha = 0.05$. Figure~[\\\\ref{fig:2}] shows the standard normal distribution; the shaded area represents the type I error for the percentile $P_{95} \\\\approx 1.645$, which corresponds to the model exclusion condition at the $3\\\\sigma$ level.\\nThe confidence level for an observation is significantly higher. In general, the discovery threshold is set at $5\\\\sigma$, where the type I error is $\\\\alpha = 2.86 \\\\times 10^{-7}$. Table~[\\\\ref{tb:1}] summarizes different confidence levels and their interpretation in high-energy physics (HEP)~\\\\cite{lista2016practical,cranmer2015practical,cowan2011asymptotic}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccl}\\n\\\\hline\\n$\\\\alpha$ & $CL [\\\\ \\\\hline\\n$0.1586$ & $84.13$ & $1\\\\sigma$ & $H_{1}$ no excluded \\\\\\\\\\n$0.05$ & $95.00$ & $1.645\\\\sigma$ & $H_{1}$ excluded (Model exclusion) \\\\\\\\\\n$0.0227$ & $97.72$ & $2\\\\sigma$ & $H_{1}$ excluded \\\\\\\\ \\n$1.349 \\\\times 10^{-3}$ & $99.86$ & $3\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$3.167 \\\\times 10^{-5}$ & $99.99$ & $4\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$2.8665 \\\\times 10^{-7}$ & $99.99997$ & $5\\\\sigma$ & $H_{0}$ excluded (Observation) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Significance at different observation points. The exclusion of the alternative hypothesis requires a result statistical consistent with the background-only hypothesis ($H_{0}$), while confirmation of the observation requires compatibility with the signal + background hypothesis ($H_{1}$).}\\n\\\\end{center}\\n\\\\end{table}\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The modified frequentist approach}\\nA {\\\\it modified frequentist approach}~\\\\cite{CLs} was proposed for the first time for the\\ncombination of the results of searches for the Higgs boson by the four LEP experiments, ALEPH, DELPHI, L3 and OPAL~\\\\cite{Higgs_at_LEP}.\\nGiven a test statistic $\\\\lambda(x)$ that depends on some observation $x$, its distribution should be determined\\nunder the two hypotheses\\n$H_1$ (signal plus background) and $H_0$ (background only). The following $p$-values can be used,\\nwhere we assume that the test statistic $\\\\lambda$ tends to have small values for $H_1$ and\\nlarger values for $H_0$:\\n\\\\begin{eqnarray}\\np_{s+b}& = & P(\\\\lambda(x | H_1) \\\\ge \\\\lambda^{\\\\mathrm{obs}} )\\\\,, \\\\\\\\\\np_b & = & P(\\\\lambda(x | H_0) \\\\le \\\\lambda^{\\\\mathrm{obs}} )\\\\,. \\n\\\\end{eqnarray}\\n$p_{s+b}$ and $p_b$ can be interpreted as follows:\\n\\\\begin{itemize}\\n\\\\item $p_{s+b}$ is the probability to obtain a result which is less compatible with the signal than the observed result, assuming the signal hypothesis;\\n\\\\item $p_b$ is the probability to obtain a result less compatible with the background-only hypothesis than the observed one, assuming background only.\\n\\\\end{itemize}\\nInstead of requiring, as for a frequentist upper limit, $p_{s+b} \\\\le \\\\alpha$,\\nthe modified approach introduces a new quantity, $\\\\mathrm{CL}_s$, defined as:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b}\\\\,,\\n}\\n\\\\end{equation}\\nand the upper limit is set by requiring $\\\\mathrm{CL}_s \\\\le \\\\alpha$.\\nFor this reason, the modified frequentist approach is also called {\\\\it ``$\\\\mathrm{CL}_s$ method''}.\\nIn practice, in most of the realistic cases, $p_b$ and $p_{s+b}$ are computed from\\nsimulated pseudoexperiments ({\\\\it toy Monte Carlo}) by approximating the probabilities\\ndefined in Eq.~(\\\\ref{eq:CLSpsb},~\\\\ref{eq:CLSpb}) with the fraction of the total number of pseudoexperiments\\nsatisfying their respective condition:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b} = \\\\frac{N(\\\\lambda_{s+b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}{N(\\\\lambda_{b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}\\\\,.\\n\\\\end{equation}\\nSince $1-p_b \\\\le 1$, then $\\\\mathrm{CL}s \\\\ge p_{s+b}$, hence upper limits computed with the $\\\\mathrm{CL}_s$ method are\\nalways {\\\\it conservative}.\\nIn case the distributions of the test statistic $\\\\lambda$ (or equivalently $-2\\\\ln\\\\lambda$) for the two hypotheses $H_0$ and $H_1$\\nare well separated (Fig.~\\\\ref{fig:CLs12}, left),\\nif $H_1$ is true, than $p_b$ will have a very high chance to be very small, hence $1-p_b \\\\simeq 1$ and $\\\\mathrm{CL}_s \\\\simeq p_{s+b}$. In this case\\n$\\\\mathrm{CL}_s$ and the purely frequentist upper limits coincide.\\nIf the two distributions instead largely overlap (Fig.~\\\\ref{fig:CLs12}, right), indicating that the experiment has poor sensitivity on the\\nsignal, in case $p_b$ is large, because of a statistical fluctuation, then $1 - p_b$ becomes small.\\nThis prevents $\\\\mathrm{CL}_s$ to become too small,\\ni.e.: it prevents to reject cases where the experiment has little sensitivity.\\nIf we apply the $\\\\mathrm{CL}_s$ method to the previous counting experiment, using \\nthe observed number of events $n^{\\\\mathrm{obs}}$ as test statistic,\\nthen $\\\\mathrm{CL}s$ can be written, considering that $n$ tends to be large in case of\\n$H_1$, for this case, as:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{P(n\\\\le n^{\\\\mathrm{obs}} | s+b)}{P(n \\\\le n^{\\\\mathrm{obs}} |b)}\\\\,.\\n\\\\end{equation}\\nExplicitating the Poisson distribution, the computation gives the same result as for the Bayesian case with a uniform prior\\n(Eq.~(\\\\ref{eq:Helene})). In many cases, the $\\\\mathrm{CL}_s$ upper \\nlimits give results that are very close, numerically, to Bayesian\\ncomputations performed assuming a uniform prior.\\nOf course, this does not allow to interpret $\\\\mathrm{CL}_s$ upper limits\\nas Bayesian upper limits.\\nConcerning the interpretation of $\\\\mathrm{CL}_s$, it's worth reporting from Ref~\\\\cite{CLs} the\\nfollowing statements:\\n\\\\begin{displayquote}\\n{\\\\it A specific modification of a purely classical statistical analysis is used to avoid excluding or discovering signals which the search is in fact not sensitive to.}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it The use of\\\\, $\\\\mathrm{CL}_s$ is a conscious decision not to insist on the frequentist concept of full coverage (to guarantee that the confidence interval doesn't include the true value of the parameter in a fixed fraction of experiments).}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it Confidence intervals obtained in this manner do not have the same interpretation as traditional frequentist confidence intervals nor as Bayesian credible intervals.}\\n\\\\end{displayquote}\\n\", \"\\\\section{Inference}\\n\\\\subsection{Two-dimensional uncertainty contours}\\nIn more dimensions, i.e.: for the simultaneous determination of more unknown parameters from a fit,\\nit's still possible to determine multi-dimensional contours corresponding to $1\\\\sigma$ or $2\\\\sigma$\\nprobability level. It should be noted that the scan of $-2\\\\ln L$ in the multidimensional\\nspace, looking for an excursion of $+1$ with respect to the value at the minimum, may give\\nprobability levels smaller than the corresponding values in one dimension.\\nFor a Gaussian case in one dimension, the probability associated to an interval $[-n\\\\sigma,+n\\\\sigma]$ is\\ngiven, integrating Eq.~(\\\\ref{eq:GaussianPDF}), by:\\n\\\\begin{equation}\\nP_{1\\\\mathrm{D}}(n\\\\sigma)= \\\\sqrt{\\\\frac{2}{\\\\pi}}\\\\int_0^ne^{-\\\\frac{x^2}{2}}\\\\,\\\\mathrm{d}x = \\\\mathrm{erf}\\\\left(\\\\frac{n}{\\\\sqrt{2}}\\\\right)\\\\,.\\n\\\\end{equation}\\nFor a two-dimensional Gaussian distribution, i.e.: the product of two independent Gaussian PDF,\\nthe probability associated to the contour with elliptic shape for which $-2\\\\ln L$ increases by $+(n\\\\sigma)^2$ with respect to its\\nminimum is:\\n\\\\begin{equation}\\nP_{2\\\\mathrm{D}}(n\\\\sigma)= \\\\int_0^ne^{-\\\\frac{r^2}{2}}r\\\\,\\\\mathrm{d}r = 1 - e^{-\\\\frac{n^2}{2}}\\\\,.\\n\\\\end{equation}\\nTable.~\\\\ref{tab:GaussianInt1D2D} reports numerical values for Eq.~(\\\\ref{eq:GaussInt1D}) and\\nEq.~(\\\\ref{eq:GaussInt2D}) for various $n\\\\sigma$ levels.\\n\\\\begin{table}[htbp]\\n\\\\caption{Probabilities for 1D interval and 2D contours with different $n\\\\sigma$ levels..}\\n\\\\centering\\n\\\\begin{tabular}{ccc}\\\\hline\\\\hline\\n$n\\\\sigma$ & $P_{1\\\\mathrm{D}}$ & $P_{2\\\\mathrm{D}}$ \\\\\\\\\\\\hline\\n$1\\\\sigma$ & 0.6827 & 0.3934 \\\\\\\\\\n$2\\\\sigma$ & 0.9545 & 0.8647 \\\\\\\\\\n$3\\\\sigma$ & 0.9973 & 0.9889 \\\\\\\\\\n$1.515\\\\sigma$ & & 0.6827 \\\\\\\\\\n$2.486\\\\sigma$ & & 0.9545 \\\\\\\\\\n$3.439\\\\sigma$ & 0.9973 \\\\\\\\\\\\hline\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\nIn two dimensions, for instance, in order to recover a $1\\\\sigma$ probability level in one\\ndimension (68.3\\\\should be considered, and for a $2\\\\sigma$\\nprobability level in one dimension (95.5\\\\Usualy two-dimensional intervals corresponding to one or two sigma are reported,\\nwhose one-dimensional projection correspond to 68\\\\respectively.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Confidence belts}\\nWe have shown that a simple Gaussian measurement is basically a statement about \\nconfidence regions. \\n$x=100\\\\pm 10 $ implies that [90,110] is the 68\\\\ \\nWe want to extend this to less simple scenarios. As a first step, we consider a proportional Gaussian.\\nSuppose we measure $x=100$ from Gaussian measurement with $\\\\sigma= 0.1 x$ (a 10\\\\If the true value is 90 the error is $\\\\sigma=9$ so $x=100$ is more than one standard deviation, whereas if the true value is 110 then $\\\\sigma=11$ and it is less than one standard deviation. 90 and 110 are not equidistant from 100.\\nThis is done with a technique called a confidence belt. The key point is that they are\\nare constructed horizontally and read vertically, using the following procedure (as shown in Fig.~\\\\ref{fig:cbelt}). Suppose that $a$ is the parameter of interest and $x$ is the measurement.\\n\\\\begin{enumerate}\\n\\\\item For each $a$, construct desired\\nconfidence interval \\n(here 68\\\\\\\\item The result $(x,a)$ lies inside the \\nbelt (the red lines), with 68\\\\\\\\item Measure $x$.\\n\\\\item The result $(x,a)$ lies inside the \\nbelt, with 68\\\\ \\\\item Read off the belt limits $a_+$ and $a_-$ at that $x$: in this case they are 111.1, 90.9. \\nSo we can report that $a$ lies in [90.9,111.1] with 68\\\\ \\\\item Other choices for the confidence level value and for the strategy are available.\\n\\\\end{enumerate}\\nThis can be extended to the case of a Poisson distribution, Fig.~\\\\ref{fig:confpois}. \\nThe only difference is that the horizontal axis is discrete as the number observed, $x$, is integer.\\nIn constructing the belt (horizontally) there will not in general be $x$ values available to give $\\\\sum_{x_-}^{x_+}=CL$ and we call, again, on the `at least' in the definition and allow it to be $\\\\sum_{x_-}^{x_+}\\\\ge CL$.\\nThus for a central 90\\\\ for which \\n$\\\\sum_{x=0}^{x_{lo}-1} e^{-a}{a^x \\\\over x!} \\\\leq 0.05$\\nand \\n$\\\\sum_{x=x_{hi}+1}^{\\\\infty} e^{-a}{a^x \\\\over x!} \\\\leq 0.05$.\\nFor the second sum it is easier to calculate\\n$\\\\sum_{x=0}^{x_{hi}} e^{-a}{a^x \\\\over x!} \\\\geq 0.95$\\\\ .\\nWhatever the value of $a$, the probability of the result falling in the belt is 90\\\\ \\n\", '\\\\section{Probability theory}\\n\\\\subsection{Commonly used distributions}\\nBelow a few examples of probability distributions are reported \\nthat are frequently used in physics and more in general in statistical\\napplications.\\n\\\\subsubsection{Gaussian distribution}\\nA {\\\\it Gaussian} or {\\\\it normal} distribution is given by:\\n\\\\begin{equation}\\ng(x;\\\\mu,\\\\sigma) = \\\\frac{1}{\\\\sigma\\\\sqrt{2\\\\pi}}e^{-{(x-\\\\mu)^2}/{2\\\\sigma^2}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu$ and $\\\\sigma$ are parameters equal to the average value and standard deviation of $x$, respectively.\\nIf $\\\\mu=0$ and $\\\\sigma=1$, a Gaussian distribution is also called {\\\\it standard normal distribution}.\\nAn example of\\nGaussian PDF is shown in Fig.~\\\\ref{fig:gaussianPdf}.\\nProbability values corresponding to intervals $[\\\\mu-n\\\\sigma, \\\\mu+n\\\\sigma]$ for a Gaussian\\ndistribution are frequently used as reference, and are reported in Tab.~\\\\ref{tab:GaussianInt}.\\n\\\\begin{table}\\n\\\\caption{Probabilities for a Gaussian PDF corresponding to an interval $[\\\\mu-n\\\\sigma, \\\\mu+n\\\\sigma]$.}\\n\\\\centering\\n\\\\begin{tabular}{cc}\\\\hline\\\\hline\\n$n$ & Prob.\\\\\\\\\\\\hline\\n1 & 0.683 \\\\\\\\\\n2 & 0.954 \\\\\\\\\\n3 & 0.997 \\\\\\\\\\n4 & $1-6.5\\\\times10^{-5}$ \\\\\\\\\\n5 & $1-5.7\\\\times10^{-7}$ \\\\\\\\\\\\hline\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\nMany random variables in real experiments follow, at least approximately, a Gaussian distribution.\\nThis is mainly due to the {\\\\it central limit theorem} that allows to\\napproximate the sum of multiple random variables, regardless of their individual distributions,\\nwith a Gaussian distribution.\\nGaussian PDFs are frequently used to model detector resolution.\\n\\\\subsubsection{Poissonian distribution}\\nA {\\\\it Poissonian} distribution for an integer non-negative random variable $n$ is:\\n\\\\begin{equation}\\nP(n;\\\\nu) = \\\\frac{\\\\nu^n}{n!}e^{-\\\\nu}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\nu$ is a parameter equal to the average value of $n$.\\nThe variance of $n$ is also equal to $\\\\nu$.\\nPoissonian distributions model the number of occurrences of random event uniformly distributed\\nin a measurement range whose rate is known. Examples are the number of rain drops falling in a given area\\nand in a given time interval or the number of cosmic rays crossing a detector in a given time \\ninterval. \\nPoissonian distributions may be approximated with a Gaussian distribution\\nhaving $\\\\mu=\\\\nu$ and $\\\\sigma=\\\\sqrt{\\\\nu}$ for sufficiently large values of $\\\\nu$.\\nExamples of Poissonian distributions are shown in Fig.~\\\\ref{fig:poissonianPdf} with\\nsuperimposed Gaussian distributions as comparison.\\n\\\\subsubsection{Binomial distribution}\\nA {\\\\it binomial} distribution gives the probability to achieve $n$ successful outcomes\\non a total of $N$ independent trials whose individual probability of success is $p$.\\nThe binomial probability is given by:\\n\\\\begin{equation}\\nP(n;N,p) = \\\\frac{N!}{n!(N-n)!}p^n(1-p)^{N-p}\\\\,.\\n\\\\end{equation}\\nThe average value of $n$ for a binomial variable is:\\n\\\\begin{equation}\\n\\\\left<n\\\\right>= N\\\\,p\\n\\\\end{equation}\\nand the variance is:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[n] = N\\\\,p\\\\,(1-p)\\\\,.\\n\\\\end{equation}\\nA typical example of binomial process in physics is the case of a detector\\nwith efficiency $p$, where $n$ is the number of {\\\\it detected} particles\\nover a total number of particles $N$ that {\\\\it crossed} the detector.\\n', '\\\\section{Introduction to Lecture 2}\\nThis lecture deals with two different methods for determining parameters, least squares and likelihood, \\nwhen a functional form is fitted\\nto our data. A simple example would be straight line fitting, where the parameters are the intercept and gradient\\nof the line. However the methods are much more general than this. Also there are other\\nmethods of extracting parameters; these include the more fundamental Bayesian and Frequentist methods,\\nwhich are dealt with in Lecture 3 . \\nThe least squares method also provides a measure of Goodness of Fit for the agreement between the theory with the \\nbest values of the parameters, and the data; this is dealt with in section \\\\ref{GofF}. The likelihood technique plays \\nan important role in the Bayes approach, and likelihood ratios are relevant for choosing between two hypotheses;\\nthis is covered in Lecture 4. \\n', \"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Variations on test statistics}\\nA number of test statistics is proposed in Ref.~\\\\cite{asymptotic} that better\\nserve various purposes. Below the main ones are reported:\\n\\\\begin{itemize}\\n\\\\item {\\\\bf Test statistic for discovery:}\\n\\\\begin{equation}\\nq_0 = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(0), &\\\\hat{\\\\mu}\\\\ge 0\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} < 0\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIn case of a negative estimate of $\\\\mu$ ($\\\\hat{\\\\mu}<0$), the test statistic is set to zero in order to\\nconsider only positive $\\\\hat{\\\\mu}$ as evidence against the background-only hypothesis.\\nWithin an asymptotic approximation, the significance is given by: $Z\\\\simeq\\\\sqrt{q_0}$.\\n\\\\item {\\\\bf Test statistic for upper limit:}\\n\\\\begin{equation}\\nq_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(\\\\mu), &\\\\hat{\\\\mu}\\\\le \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} > \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIf the $\\\\hat{\\\\mu}$ estimate is larger than the assumed value for $\\\\mu$, an upward fluctuation occurred.\\nIn those cases, $\\\\mu$ is not excluded by setting the test statistic to zero.\\n\\\\item {\\\\bf Test statistic for Higgs boson search:}\\n\\\\begin{equation}\\n\\\\tilde q_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0))}, & \\\\hat{\\\\mu} < 0\\\\,,\\\\\\\\\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\vec{\\\\theta}}(\\\\mu))}, & 0\\\\le \\\\hat{\\\\mu} < \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} \\\\ge \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThis test statistics both protects against unphysical cases with $\\\\mu <0$\\nand, as the test statistic for upper limits, protects upper limits\\nin cases of an upward $\\\\hat{\\\\mu}$ fluctuation.\\n\\\\end{itemize}\\nA number of measurements performed at LEP and Tevatron used a\\ntest statistic based on the ratio of the likelihood function evaluated under\\nthe signal plus background hypothesis and under the background only hypothesis,\\ninspired by the Neyman--Pearson lemma:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|s+b)}{L(\\\\vec{x}|b)}\\\\,.\\n\\\\end{equation}\\nIn many LEP and Tevatron analyses, nuisance parameters were treated using the hybrid Cousins--Hyghland approach.\\nAlternatively, one could use a formalism similar to the profile likelihood, \\nsetting $\\\\mu=0$ in the denominator and $\\\\mu=1$ in the numerator, and minimizing\\nthe likelihood functions with respect to the nuisance parameters:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu=1, \\\\hat{\\\\hat{\\\\theta}}(1))}{L(\\\\vec{x}|\\\\mu=0,\\\\hat{\\\\hat{\\\\theta}}(0))}\\\\,.\\n\\\\end{equation}\\nFor all the mentioned test statistics, asymptotic approximations exist and\\nare reported in Ref.~\\\\cite{asymptotic}. Those are based either on Wilks' theorem\\nor on Wald's approximations~\\\\cite{Wald}. If a value $\\\\mu$ is tested, and \\nthe data are supposed to be distributed according to another value of the signal strength $\\\\mu^\\\\prime$,\\nthe following approximation holds, asymptotically:\\n\\\\begin{equation}\\n-2\\\\ln\\\\lambda(\\\\mu) = \\\\frac{(\\\\mu-\\\\hat{\\\\mu})^2}{\\\\sigma^2} + {\\\\cal O}\\\\left(\\\\frac{1}{\\\\sqrt{N}}\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\mu}$ is distributed according to a Gaussian with average $\\\\mu^\\\\prime$ and\\nstandard deviation $\\\\sigma$. The covariance matrix for the nuisance parameters is\\ngiven, in the asymptotic approximation, by:\\n\\\\begin{equation}\\nV_{ij}^{-1} = \\\\left.\\\\left<\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right>\\\\right|_{\\\\mu=\\\\mu^\\\\prime}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu^\\\\prime$ is assumed as value for the signal strength.\\nIn some cases, asymptotic approximations (Eq.~(\\\\ref{eq:waldTestStat})) can be written in terms of an\\n{\\\\it Asimov dataset}~\\\\cite{Asimov}:\\n\\\\begin{displayquote}\\n{\\\\it We define the Asimov data set such that when one uses it to evaluate the estimators\\nfor all parameters, one obtains the true parameter values}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\nIn practice, an Asimov dataset is a single ``representative'' dataset obtained by replacing all\\nobservable (random) varibles with their expecteted value. In particular,\\nall yields in the data sample (e.g.: in a binned case) are replaced with their expected values, that may be non integer values.\\nThe median significance for different cases of test statistics can be computed in this\\nway without need of producing extensive sets of toy Monte Carlo. The implementation\\nof those asymptotic formulate is available in the {\\\\sc RooStats} library, released\\nas part an optional component {\\\\sc Root}~\\\\cite{Root}.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Limits in the presence of background}\\nThis is where it gets tricky.\\nTypically an experiment may observe $N_D$ events, with an expected background $N_B$ and efficiency $\\\\eta$, and wants to present results for $N_S={N_D-N_B \\\\over \\\\eta}$.\\nUncertainties in $\\\\eta$ and $N_B$ are handled by profiling or marginalising.\\nThe problem is that the \\n{\\\\it actual number} of background events is not $N_B$ but Poisson in $N_B$.\\nSo in a straightforward case, if you observe twelve events, with expected background 3.4 and $\\\\eta=1$\\nit is obviously sensible to say $N_S=8.6$\\n(though the error is $\\\\sqrt{12}$ not $\\\\sqrt{8.6}$)\\nBut suppose, with the same background, you see four events, three events or zero events.\\nCan you say $N_S=0.6$? or $-0.4$? Or $-3.4$???\\nWe will look at four methods of handling this, considering as an example the observation of three events with expected background 3.40 and wanting to present the 95\\\\ \\n\\\\subsubsection{Method 1: Pure frequentist}\\n$N_D-N_B$ is an unbiased estimator of $N_S$ and its properties are known.\\nQuote the result. Even if it is non-physical.\\nThe argument for doing so\\nis that\\nthis is needed for balance: if there is really no signal, approximately half of the experiments will give positive values and half negative. \\nIf the negative results are not published, but the positive ones are, the world average will be spuriously high.\\nFor a 95\\\\clearly one of them. So what?\\nA counter-argument is that if\\n$N_D<N_B$, we {\\\\it know} that the background has fluctuated downwards. But this cannot be incorporated \\ninto the formalism.\\nAnyway, the upper limit from 3 is 7.75, as $\\\\sum_0^3 e^{-7.75}7.75^r/r! = 0.05$, and the \\n95\\\\\\n\\\\subsubsection{Method 2: Go Bayesian}\\nAssign a uniform prior to $N_S$, for $N_S>0$, zero for $N_S<0$.\\nThe posterior is then just the likelihood, $P(N_S | N_D,N_B)=e^{-(N_S+N_B)}{(N_S+N_B)^{N_D} \\\\over N_D!}$.\\nThe required limit is obtained from integrating $\\\\int_0^{N_{hi}} P(N_S)\\\\, dN_S = 0.95$\\nwhere\\n$P(N_S)\\\\propto e^{-(N_s+3.40)}{(N_s+3.4)^3 \\\\over 3!}$; this is illustrated in Fig.~\\\\ref{fig:Bayeslimit}\\nand the value of the limit is \\n5.21.\\n\\\\subsubsection{Method 3: Feldman-Cousins}\\nThis---called `the unified approach' by Feldman and Cousins~\\\\cite{FC}---takes a step backwards\\nand considers the ambiguity in the use of confidence belts. \\nIn principle, if you decide to work at, say, 90\\\\This is shown in Fig.~\\\\ref{fig:FC1}.\\nIn practice, if you happen to get a low result you would quote an upper limit, but if you get a high result you would quote a central limit.\\nThis, which they call `flip-flopping', is illustrated in the plot by a break shown here for $r=10$. \\nNow the confidence belt is the green one for $r< 10$ and the red one for $r\\\\geq 10$. The\\nprobability of lying in the band is no longer 90\\\\Flip-flopping invalidates the Frequentist construction, leading to undercoverage. \\nThey show how to avoid this. You draw the plot slightly differently:\\n$r \\\\equiv N_D$ is still the horizontal variable, but as the vertical variable you use $N_S$. \\n(This means a different plot for any different $N_B$, whereas the previous Poisson plot is universal, but this is not a problem.)\\nThis is to be filled using $P(r;N_s)=e^{-(N_s+N_B)}{(N_S+N_B)^r \\\\over r!}$\\\\ .\\nFor each $N_S$ you define a region $R$ such that $\\\\sum_{r\\\\epsilon R}P(r;N_s) \\\\geq 90\\\\You have a choice of strategy that goes beyond `central' or `upper limit': one \\nplausible suggestion would be to\\nrank $r$ by probability and take them in order until the desired total probability content is achieved (which would, incidentally, give the shortest interval).\\nHowever this has the drawback that outcomes with $r < N_B$ will have small probabilities and be excluded for all $N_S$, so that, if such a result does occur, one cannot say anything constructive, just `This was unlikely'. \\nAn improved form of this suggestion is that for each $N_S$, considering each $r$ you compare $P(r;N_S)$ with the largest possible value obtained by varying $N_S$. This is easier than it sounds because this highest value is either at $N_S=r-N_B$ (if $r\\\\geq N_B$) or $N_S=0$ (if $r\\\\leq N_B$ ).\\nRank on the ratio $P(r;N_S)/P(r;N^{best}_S)$ and again take them in order till their sum gives the desired probability.\\nThis gives a band as shown in Fig.~\\\\ref{fig:FC2}, which has $N_B=3.4$. You can see that \\n`flip-flopping' occurs naturally: for small values of $r$ one just has an upper limit, whereas for larger values, above $r=7$, one obtains a lower limit as well. Yet there is a single band, and the coverage is\\ncorrect (i.e. it does not undercover).\\nIn the case we are considering, $r=3$, just an upper limit is given, at $4.86$. \\nLike other good ideas, this has not found universal favour. Two arguments are raised against the method.\\nFirst, that it deprives the physicist of the choice of whether to publish an upper limit or a range. \\nIt could be embarrassing if you\\nlook for something weird and are `forced' to publish a non-zero result. \\nBut this is actually the point, and in such cases one can always explain\\nthat the limits should not be taken as implying that the quantity actually is nonzero.\\nSecondly, if two experiments with different $N_B$ get the same small $N_D$, the one with the higher $N_B$ will quote a smaller limit on $N_S$. The worse experiment gets the better result, which is clearly unfair!\\nBut this is not comparing like with like: for a `bad' experiment with large background to get a small number of events is much less likely than it is for a `good' low background experiment.\\n\\\\subsubsection {Method 4: $CL_s$}\\nThis is a modification of the standard frequentist approach to include the \\nfact, as mentioned above, that a small observed signal implies a downward \\nfluctuation in background~\\\\cite{Read}. Although presented here using just numbers of events, the method is usually extended to use the full likelihood of the result, as will be discussed in Section~\\\\ref{subsection:Extension}.\\nDenote the (strict frequentist) \\nprobability of getting a result this small (or less) from $s+b$ events as \\n$CL_{s+b}$, and the equivalent probability from pure background as \\n$CL_b$ (so \\n$CL_b=CL_{s+b}$ for $s=0$).\\nThen introduce\\n\\\\begin{equation}\\nCL_s={CL_{s+b} \\\\over CL_b}\\n\\\\quad.\\n\\\\end{equation}\\nLooking at Fig.~\\\\ref{fig:CLS}, the $CL_{s+b}$ curve shows that if $s+b$ is small then the probability of getting three events or less is high, near 100\\\\the probability of only getting three events or less is only 5\\\\\\nThe point $s+b=3.4$ corresponds to $s=0$, at which the probability $CL_b$ is 56\\\\incorporate this by renormalizing the (blue) $CL_{s+b}$ curve to have a maximum of 100\\\\physically sensible region, dividing it by 0.56 to get the (green) $CL_s$ curve.\\nThis is treated in the same way as the $CL_{s+b}$ curve, reading off the point at $s+b=8.61$ where it falls to 5\\\\This is larger than the strict frequentist limit: the method over-covers (which, as we have seen, is allowed if not encouraged)\\nand is, in this respect `conservative'\\\\footnote{`Conservative' is a misleading word. It is used by people \\ndescribing their analyses to \\nimply safety and caution, whereas it usually entails cowardice and sloppy thinking.}. This is the same value as the Bayesian Method 2, as it makes the same assumptions. \\n$CL_s$ is not frequentist, just `frequentist inspired'. In terms of statistics there is perhaps little in its favour. But it has an intuitive appeal, and is widely used.\\n\\\\subsubsection{Summary so far}\\nGiven three observed events, and an expected background of 3.4 events, what is\\nthe 95\\\\Possible answers are shown in table~\\\\ref{tab:summary}.\\n\\\\begin{table}[h]\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nStrict Frequentist & 4.35 \\\\\\\\\\nBayesian (uniform prior) & 5.21 \\\\\\\\\\nFeldman-Cousins & 4.86 \\\\\\\\\\n$CL_s$ & 5.21 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{ Upper limits from different methods}\\n\\\\end{table}\\nWhich is `right'? Take your pick!\\nAll are correct. (Well, not wrong.). The golden rule is to say what you are doing, and if possible give the raw numbers. \\n\\\\subsubsection{Extension: not just counting numbers}\\nThese examples have used \\nsimple counting experiments. But a simple number does not (usually) exploit the full information.\\nConsider the illustration in Fig.~\\\\ref{fig:beyondsimple}. One is searching for (or putting an upper limit on) some broad resonance around 7~GeV. One could count the number of events inside some window\\n(perhaps 6 to 8~GeV?) and subtract the estimated background. This might work with high statistics, as in the left, but would be pretty useless with small numbers, as in the right. It is clearly not optimal \\njust to count an event as `in', whether it is at 7.0 or 7.9, and to treat an event as `out', if it is at 8.1 or \\n10.1.\\nIt is better to calculate the \\nLikelihood $\\\\ln L_{s+b}=\\\\sum_i \\\\ln{N_s S(x_i)+N_b B(x_i)} \\\\quad;\\\\quad \\\\ln{ L_b}=\\\\sum_i \\\\ln{N_b B(x_i)}$.\\nThen, for example using $CL_s$, you can work with $L_{s+b}/L_b$, or $-2 \\\\ln{(L_{s+b}/L_b)}$.\\nThe confidence/probability quantities can be found from simulations, or sometimes from data.\\n\\\\subsubsection{Extension: From numbers to masses}\\nLimits on numbers of events can readily be translated into limits on branching ratios,\\n$BR={N_s \\\\over N_{total}}$,\\nor limits on cross sections,\\n$\\\\sigma={N_s \\\\over \\\\int {\\\\cal L} dt}$\\\\ .\\nThese may translate to limits on other, theory, parameters.\\nIn the Higgs search (to take an example) the cross section depends on the mass, $M_H$---and so does the detection efficiency---which may require changing strategy (hence different backgrounds). This leads to the need\\nto basically repeat the analysis for all (of many) $M_H$ values. This can be presented in two ways. \\nThe first is shown in Fig.~\\\\ref{fig:significanceplot}, taken from Ref.~\\\\cite{ATLAS1}. For each $M_H$ (or whatever is being studied) you search for a signal and plot the $CL_s$ (or whatever limit method you prefer) significance \\nin a {\\\\it Significance Plot}. \\nSmall values indicate that it is unlikely to get a signal this large just from background.\\nOne often also plots the expected (from MC) significance, assuming the signal hypothesis is true. This is a measure of a `good experiment'. In this case there is a discovery level \\ndrop at $M_H \\\\approx 125$~GeV, which exceeds the expected significance, though not by much: ATLAS were lucky but not incredibly lucky.\\nThe second method is---for some reason---known as the green-and-yellow plot.\\nThis is basically the same data, but fixing $CL$ at a chosen value: in Fig.~\\\\ref{fig:greenandyellow} it is 95\\\\ You find the limit on signal strength, at this confidence level, and interpret it as \\na limit on the cross section $\\\\sigma / \\\\sigma_{SM}$.\\nAgain, as well as plotting the actual data one also plots the expected (from MC) limit, with variations.\\nIf there is no signal, 68\\\\ \\nSo this figure shows the experimental result as a black line. Around 125~GeV the 95\\\\is more than the Standard Model prediction indicating a discovery. There are peaks between 200 and 300~GeV, but they do not approach the SM value, indicating that they are just fluctuations. The value rises at 600~GeV, but the green (and yellow) bands rise also, showing that the experiment is not\\nsensitive for such high masses: basically it sees nothing but would expect to see nothing.\", \"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Data-Driven Narrative}\\nThe strength of the simulation narrative lies in its direct logical link from the underlying theory to the modeling of the experimental observations. The weakness of the simulation narrative derives from the weaknesses in the simulation itself. Data-driven approaches are more motivated when they address specific deficiencies in the simulation. Before moving to a more abstract or general discussion of the data-driven narrative, let us first consider a few examples.\\nThe first example we have already considered in Sec.~\\\\ref{S:AuxMeas} in the context of the ``on/off'' problem. There we introduced an auxiliary measurement that counted $n_{CR}$ events in a control region to estimate the background $\\\\nu_B$ in the signal region. In order to do this we needed to understand the ratio of the number of events from the background process in the control and signal regions, $\\\\tau$. This ratio $\\\\tau$ either comes from some reasonable assumption or simulation. For example, if one wanted to estimate the background due to jets faking muons $j\\\\to\\\\mu$ for a search selecting $\\\\mu^+\\\\mu^-$ , then one might use a sample of $\\\\mu^\\\\pm\\\\mu^\\\\pm$ events as a control region. Here the motivation for using a data-driven approach is that modeling the processes that lead to $j\\\\to\\\\mu$ rely heavily on the tails of fragmentation functions and detector response, which one might reasonably have some skepticism. If one assumes that control region is expected to have negligible signal in it, that backgrounds that produce $\\\\mu^+\\\\mu^-$ other than the jets faking muons, and that the rate for $j\\\\to\\\\mu^-$ is the same\\\\footnote{Given that the LHC collides $pp$ and not $p\\\\bar{p}$, there is clearly a reason to worry if this assumption is valid.} as the rate for $j\\\\to\\\\mu^+$, then one can assume $\\\\tau=1$. Thus, this background estimate is as trustworthy as the assumptions that went into it. In practice, several of these assumptions may be violated. Another approach is to use simulation of these background processes to estimate the ratio $\\\\tau$; a hybrid of the data-driven and simulation narratives.\\nLet us now consider the search for $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad}. The right plot of Fig~\\\\ref{fig:H2photons} shows the composition of the backgrounds in this search, including the continuum production of $pp\\\\to\\\\gamma\\\\gamma$, the $\\\\gamma$+jets process with a jet faking a photon $j\\\\to\\\\gamma$, and the multi jet process with two jets faking photons. The continuum production of $\\\\gamma\\\\gamma$ has a theoretical uncertainty that is much larger than the statistical fluctuations one would expect in the data. Similarly, the rate of jets faking photons is sensitive to fragmentation and the detector simulation. These uncertainties are large compared to the statistical fluctuations in the data itself. Thus we can use the distribution in Fig~\\\\ref{fig:H2photons} to measure the total background rate. Of course, the signal would also be in this distribution, so one either needs to apply a mass window around the signal and consider the region outside of the window as a sideband control sample or model the signal and background contributions to the distribution. In the case of the $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad} the modeling of the distribution signal and background distributions is not based on histograms from simulation, but instead a continuous function is used as an effective model. I will discuss this effective modeling narrative below, but point out that here this is another example of a hybrid narrative.\\nThe final example to consider is an extension of the `on/off' model, often referred to as the `ABCD' method. Let us start with the `on/off' model:\\n\\\\mbox{ $\\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)\\\\cdot \\\\Pois(n_{\\\\rm CR}|\\\\tau\\\\nu_B)$}. As mentioned above, this requires that one estimate $\\\\tau$ either from simulation or through some assumptions. The ABCD method aims to estimate introduce two new control regions that can be used to measure $\\\\tau$. To see this, let us imagine that the signal and control regions correspond to requiring some continuous variable $x$ being greater than or less than some threshold value $x_c$. If we could introduce a second discriminating variable $y$ such that the distribution for background factorizes $f_B(x,y)=f_B(x)f_B(y)$, then we have a handle to measure the factor $\\\\tau$. Typically, one introduces a threshold $y_c$ so that the signal contribution is small below this threshold\\\\footnote{The relative sign of the cut is not important, but has been chosen for consistency with Fig~\\\\ref{fig:ABCD}.}. Figure~\\\\ref{fig:ABCD} shows an example where $x_c=y_c=5$. With this we these two thresholds we have four regions that we can schematically refer to as A, B, C, and D. In the case of simply counting events in these regions we can write the total expectation as\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + 1\\\\cdot\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\tau_B\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\tau_C\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + \\\\tau_B\\\\tau_C\\\\nu_A \\n\\\\end{eqnarray}\\nwhere $\\\\mu$ is the signal rate in region A, $\\\\epsilon_i$ is the ratio of the signal in the regions B, C, D with respect to the signal in region A, $\\\\nu_i^{MC}$ is the rate of background in each of the regions being estimated from simulation, $\\\\nu_i$ is the rate of the background being estimated with the data driven technique in the signal region, and $\\\\tau_i$ are the ratios of the background rates in the regions B, C, and D with respect to the background in region A. The key is that we have used the factorization $f_B(x,y)=f_B(x)f_B(y)$ to write $\\\\tau_D=\\\\tau_B\\\\tau_C$. The right panel of Fig.~\\\\ref{fig:ABCD} shows a more complicated extension of the ABCD method from a recent ATLAS SUSY analysis~\\\\cite{ATLAS:2011ad}.\\nd\\nAn alternative parametrization, which can be more numerically stable is\\\\\\\\\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + \\\\eta_C\\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\eta_C\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + 1\\\\cdot\\\\nu_D \\n\\\\end{eqnarray}\\n\", \"\\\\section{Conceptual building blocks for modeling}\\n\\\\subsection{Probability densities and the likelihood function}\\nThis section specifies my notations and conventions, which I have chosen with some care.\\\\footnote{As in the case of relativity, notational conventions can make some properties of expressions manifest and help identify mistakes. For example, $g_{\\\\mu\\\\nu}x^\\\\mu y^\\\\nu$ is manifestly Lorentz invariant and $x^\\\\mu + y_\\\\nu$ is manifestly wrong.} Our statistical claims will be based on the outcome of an experiment. When discussing frequentist probabilities, one must consider ensembles of experiments, which may either be real, based on computer simulations, or mathematical abstraction.\\nFigure~\\\\ref{fig:hierarchy} establishes a hierarchy that is fairly general for the context of high-energy physics. Imagine the search for the Higgs boson, in which the search is composed of several ``channels'' indexed by $c$. Here a channel is defined by its associated event selection criteria, not an underlying physical process. In addition to the number of selected events, $n_c$, each channel may make use of some other measured quantity, $x_c$, such as the invariant mass of the candidate Higgs boson. The quantities will be called ``observables'' and will be written in roman letters e.g. $x_c$. The notation is chosen to make manifest that the observable $x$ is frequentist in nature. Replication of the experiment many times will result in different values of $x$ and this ensemble gives rise to a \\\\emph{probability density function} (pdf) of $x$, written $f(x)$, which has the important property that it is normalized to unity\\n\\\\[\\n\\\\int f(x) \\\\;dx\\\\;= 1\\\\;.\\n\\\\]\\nIn the case of discrete quantities, such as the number of events satisfying some event selection, the integral is replaced by a sum. Often one considers a parametric family of pdfs\\n\\\\[\\nf(x | \\\\alpha) \\\\;,\\n\\\\]\\nread ``$f$ of $x$ given $\\\\alpha$'' and, henceforth, referred to as a \\\\emph{probability model} or just \\\\emph{model}. The parameters of the model typically represent parameters of a physical theory or an unknown property of the detector's response. The parameters are not frequentist in nature, thus any probability statement associated with $\\\\alpha$ is Bayesian.\\\\footnote{Note, one can define a conditional distribution $f(x|y)$ when the joint distribution $f(x,y)$ is defined in a frequentist sense.} In order to make their lack of frequentist interpretation manifest, model parameters will be written in greek letters, e.g.: $\\\\mu, \\\\theta, \\\\alpha, \\\\nu$. \\\\footnote{While it is common to write $s$ and $b$ for the number of expected signal and background, these are parameters \\\\emph{not} observables, so I will write $\\\\nu_S$ and $\\\\nu_B$. This is one of few notational differences to Ref.~\\\\cite{asimov}.}\\nFrom the full set of parameters, one is typically only interested in a few: the \\\\emph{parameters of interest}. The remaining parameters are referred to as \\\\emph{nuisance parameters}, as we must account for them even though we are not interested in them directly.\\nWhile $f(x)$ describes the probability density for the observable $x$ for a single event, we also need to describe the probability density for a dataset with many events, $\\\\data = \\\\{x_1,\\\\dots,x_{n}\\\\}$. If we consider the events as independently drawn from the same underlying distribution, then clearly the probability density is just a product of densities for each event. However, if we have a prediction that the total number of events expected, call it $\\\\nu$, then we should also include the overall Poisson probability for observing $n$ events given $\\\\nu$ expected. Thus, we arrive at what statisticians call a marked Poisson model,\\n\\\\begin{equation}\\n\\\\F(\\\\data|\\\\nu,\\\\alpha) = \\\\Pois(n|\\\\nu) \\\\prod_{e=1}^n f(x_e|\\\\alpha) \\\\; ,\\n\\\\end{equation}\\nwhere I use a bold $\\\\F$ to distinguish it from the individual event probability density $f(x)$. In practice, the expectation is often parametrized as well and some parameters simultaneously modify the expected rate and shape, thus we can write $\\\\nu\\\\rightarrow\\\\nu(\\\\alpha)$. In \\\\texttt{RooFit} both $f$ and $\\\\F$ are implemented with a \\\\texttt{RooAbsPdf}; where \\\\texttt{RooAbsPdf::getVal(x)} always provides the value of $f(x)$ and depending on \\\\texttt{RooAbsPdf::extendMode()} the value of $\\\\nu$ is accessed via \\\\texttt{RooAbsPdf::expectedEvents()}.\\nThe \\\\emph{likelihood function} $L(\\\\alpha)$ is numerically equivalent to $f(x|\\\\alpha)$ with $x$ fixed -- or $\\\\F(\\\\data|\\\\alpha)$ with \\\\data\\\\ fixed. The likelihood function should not be interpreted as a probability density for $\\\\alpha$. In particular, the likelihood function does not have the property that it normalizes to unity\\n\\\\[\\n\\\\cancelto{\\\\mathrm{Not ~True!}}{\\\\int L(\\\\alpha) \\\\;d\\\\alpha = 1}\\\\; .\\n\\\\]\\nIt is common to work with the log-likelihood (or negative log-likelihood) function. In the case of a marked Poisson, we have what is commonly referred to as an extended likelihood~\\\\cite{Barlow1990496}\\n\\\\begin{eqnarray}\\\\nonumber\\n-\\\\ln L( \\\\alpha) &=& \\\\underbrace{\\\\nu(\\\\alpha) - n \\\\ln \\\\nu(\\\\alpha)}_{\\\\rm extended~term} - \\\\sum_{e=1}^n \\\\ln f(x_e) + \\\\underbrace{~\\\\ln n! ~}_{\\\\mathrm{constant}}\\\\; .\\n\\\\end{eqnarray}\\nTo reiterate the terminology, \\\\emph{probability density function} refers to the value of $f$ as a function of $x$ given a fixed value of $\\\\alpha$; \\\\emph{likelihood function} refers to the value of $f$ as a function of $\\\\alpha$ given a fixed value of $x$; and \\\\emph{model} refers to the full structure of $f(x|\\\\alpha)$.\\nProbability models can be constructed to simultaneously describe several channels, that is several disjoint regions of the data defined by the associated selection criteria. I will use $e$ as the index over events and $c$ as the index over channels. Thus, the number of events in the $c^{\\\\rm th}$ channel is $n_c$ and the value of the $e^{\\\\rm th}$ event in the $c^{\\\\rm th}$ channel is $x_{ce}$. In this context, the data is a collection of smaller datasets: \\\\mbox{$\\\\datasim=\\\\{\\\\data_1, \\\\dots, \\\\data_{c_{\\\\rm max}}\\\\}=\\\\{\\\\{x_{c=1,e=1}\\\\dots x_{c=1,e=n_c}\\\\}, \\\\dots \\\\{x_{c=c_{\\\\rm max},e=1}\\\\dots x_{c=c_{\\\\rm max},e=n_{c_{\\\\rm max}}} \\\\}\\\\}$}. In \\\\texttt{RooFit} the index $c$ is referred to as a \\\\texttt{RooCategory} and it is used to inside the dataset to differentiate events associated to different channels or categories. The class \\\\texttt{RooSimultaneous} associates the dataset $\\\\data_c$ with the corresponding marked Poisson model. The key point here is that there are now multiple Poisson terms. Thus we can write the combined (or simultaneous) model \\n\\\\begin{equation}\\n\\\\F_{\\\\textrm{sim}}(\\\\datasim|\\\\alpha) = \\\\prod_{c\\\\in\\\\rm channels} \\\\left[ \\\\Pois(n_c|\\\\nu(\\\\alpha)) \\\\prod_{e=1}^{n_c} f(x_{ce}|\\\\alpha) \\\\right] \\\\; ,\\n\\\\end{equation}\\nremembering that the symbol product over channels has implications for the structure of the dataset.\\n\\\\subsection{Auxiliary measurements} \\nAuxiliary measurements or control regions can be used to estimate or reduce the effect of systematic uncertainties. The signal region and control region are not fundamentally different. In the language that we are using here, they are just two different channels. \\nA common example is a simple counting experiment with an uncertain background. In the frequentist way of thinking, the true, unknown background in the signal region is a nuisance parameter, which I will denote $\\\\nu_B$.\\\\footnote{Note, you can think of a counting experiment in the context of Eq.~\\\\ref{Eq:markedPoisson} with $f(x)=1$, thus it reduces to just the Poisson term.} If we call the true, unknown signal rate $\\\\nu_S$ and the number of events in the signal region $n_{\\\\rm SR}$ then we can write the model $\\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)$. As long as $\\\\nu_B$ is a free parameter, there is no ability to make any useful inference about $\\\\nu_S$. Often we have some estimate for the background, which may have come from some control sample with $n_{\\\\rm CR}$ events. If the control sample has no signal contamination and is populated by the same background processes as the signal region, then we can write $\\\\Pois(n_{\\\\rm CR}|\\\\tau \\\\nu_B)$, where $n_{\\\\rm CR}$ is the number of events in the control region and $\\\\tau$ is a factor used to extrapolate the background from the signal region to the control region. Thus the total probability model can be written $\\\\F_{\\\\rm sim}(n_{\\\\rm SR},n_{\\\\rm CR} | \\\\nu_S, \\\\nu_B) = \\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)\\\\cdot \\\\Pois(n_{\\\\rm CR}|\\\\tau\\\\nu_B)$. This is a special case of Eq.~\\\\ref{Eq:simultaneous} and is often referred to as the ``on/off' problem~\\\\cite{Cousins:2008zz}.\\nBased on the control region alone, one would estimate (or `measure') $\\\\nu_B = n_{\\\\rm CR}/\\\\tau$. Intuitively the estimate comes with an `uncertainty' of $\\\\sqrt{n_{\\\\rm CR}}/\\\\tau$. We will make these points more precise in Sec.~\\\\ref{S:estimation}, but the important lesson here is that we can use auxiliary measurements (ie. $n_{\\\\rm CR}$) to describe our uncertainty on the nuisance parameter $\\\\nu_B$ statistically. Furthermore, we have formed a statistical model that can be treated in a frequentist formalism -- meaning that if we repeat the experiment many times $n_{\\\\rm CR}$ will vary and so will the estimate of $\\\\nu_B$. It is common to say that auxiliary measurements `constrain' the nuisance parameters. In principle the auxiliary measurements can be every bit as complex as the main signal region, and there is no formal distinction between the various channels.\\nThe use of auxiliary measurements is not restricted to estimating rates as in the case of the on/off problem above. One can also use auxiliary measurements to constrain other parameters of the model. To do so, one must relate the effect of some common parameter $\\\\alpha_p$ in multiple channels (ie. the signal region and a control regions). This is implicit in Eq.~\\\\ref{Eq:simultaneous}.\\n\\\\subsection{Frequentist and Bayesian reasoning}\\nThe intuitive interpretation of measurement of $\\\\nu_B$ to be $n_{\\\\rm CR}/\\\\tau \\\\pm \\\\sqrt{n_{\\\\rm CR}}/\\\\tau$ is that the parameter $\\\\nu_B$ has a distribution centered around $n_{\\\\rm CR}/\\\\tau$ with a width of $\\\\sqrt{n_{\\\\rm CR}}/\\\\tau$. With some practice you will be able to immediately identify this type of reasoning as Bayesian. It is manifestly Bayesian because we are referring to the probability distribution of a parameter. The frequentist notion of probability of an event is defined as the limit of its relative frequency in a large number of trials. The large number of trials is referred to as an ensemble. In particle physics the ensemble is formed conceptually by repeating the experiment many times. The true values of the parameters, on the other hand, are states of nature, not the outcome of an experiment. The true mass of the $Z$ boson has no frequentist probability distribution. The existence or non-existence of the Higgs boson has no frequentist probability associated with it. There is a sense in which one can talk about the probability of parameters, which follows from Bayes's theorem:\\n\\\\begin{equation}\\nP(A|B) = \\\\frac{P(B|A) P(A)}{P(B)} \\\\; .\\n\\\\end{equation}\\nBayes's theorem is a theorem, so there's no debating it. It is not the case that Frequentists dispute whether Bayes's theorem is true. The debate is whether the necessary probabilities exist in the first place. If one can define the joint probability $P(A,B)$ in a frequentist way, then a Frequentist is perfectly happy using Bayes theorem. Thus, the debate starts at the very definition of probability.\\nThe Bayesian definition of probability clearly can't be based on relative frequency. Instead, it is based on a degree of belief. Formally, the probability needs to satisfy Kolmogorov's axioms for probability, which both the frequentist and Bayesian definitions of probability do. One can quantify degree of belief through betting odds, thus Bayesian probabilities can be assigned to hypotheses on states of nature. In practice human's bets are not generally not `coherent' (see `dutch book'), thus this way of quantifying probabilities may not satisfy the Kolmogorov axioms.\\nMoving past the philosophy and accepting the Bayesian procedure at face value, the practical consequence is that one must supply prior probabilities for various parameter values and/or hypotheses. In particular, to interpret our example measurement of $n_{\\\\rm CR}$ as implying a probability distribution for $\\\\nu_B$ we would write\\n\\\\begin{equation}\\n\\\\pi(\\\\nu_B | n_{\\\\rm CR}) \\\\propto f(n_{\\\\rm CR} | \\\\nu_B) \\\\eta(\\\\nu_B) \\\\; ,\\n\\\\end{equation}\\nwhere $\\\\pi(\\\\nu_B | n_{\\\\rm CR})$ is called the \\\\textit{posterior} probability density, $f(n_{\\\\rm CR} | \\\\nu_B)$ is the likelihood function, and $\\\\eta(\\\\nu_B)$ is the \\\\textit{prior} probability. Here I have suppressed the somewhat curious term $P(n_{\\\\rm CR})$, which can be thought of as a normalization constant and is also referred to as the \\\\textit{evidence}. The main point here is that one can only invert `the probability of $n_{\\\\rm CR}$ given $\\\\nu_B$' to be `the probability of $\\\\nu_B$ given $n_{\\\\rm CR}$' if one supplies a prior. Humans are very susceptible to performing this logical inversion accidentally, typically with a uniform prior on $\\\\nu_B$. Furthermore, the prior degree of belief cannot be derived in an objective way. There are several formal rules for providing a prior based on formal rules (see Jefferey's prior and Reference priors), though these are not accurately described as representing a degree of belief. Thus, that style of Bayesian analysis is often referred to as objective Bayesian analysis.\\n{\\\\flushleft{Some useful and amusing quotes on Bayesian and Frequentist reasoning:}}\\n\\\\begin{quote}\\n{\\\\em ``Using Bayes's theorem doesn't make you a Bayesian, \\\\textbf{always} using Bayes's theorem makes you a Bayesian.''} --unknown\\n\\\\end{quote}\\n\\\\begin{quote}\\n{\\\\em\\n``Bayesians address the questions everyone is interested in by using assumptions that no one believes.\\nFrequentist use impeccable logic to deal with an issue that is of no interest to anyone.''}- Louis Lyons\\n\\\\end{quote}\\n\\\\subsection{Consistent Bayesian and Frequentist modeling of constraint terms} \\nOften a detailed probability model for an auxiliary measurement are not included directly into the model. If the model for the auxiliary measurement were available, it could and should be included as an additional channel as described in Sec.~\\\\ref{S:AuxMeas}. The more common situation for background and systematic uncertainties only has an estimate, ``central value'', or best guess for a parameter $\\\\alpha_p$ and some notion of uncertainty on this estimate. In this case one typically resorts to including idealized terms into the likelihood function, here referred to as ``constraint terms'', as surrogates for a more detailed model of the auxiliary measurement. I will denote this estimate for the parameters as $a_p$, to make it manifestly frequentist in nature. In this case there is a single measurement of $a_p$ per experiment, thus it is referred to as a ``global observable'' in \\\\roostats. The treatment of constraint terms is somewhat \\\\emph{ad hoc} and discussed in more detail in Section~\\\\ref{S:ConstraintExamples}. I make it a point to write constraint terms in a manifestly frequentist form $f(a_p | \\\\alpha_p)$. \\nProbabilities on parameters are legitimate constructs in a Bayesian setting, though they will always rely on a prior. In order to distinguish Bayesian pdfs from frequentist ones, greek letters will be used for their distributions. For instance, a generic Bayesian pdf might be written $\\\\pi(\\\\alpha)$. In the context of a main measurement, one might have a prior for $\\\\alpha_p$ based on some estimate $a_p$. In this case, the prior $\\\\pi(\\\\alpha_p )$ is really a posterior from some previous measurement. It is desirable to write with the help of Bayes theorem\\n\\\\begin{equation}\\n\\\\pi(\\\\alpha_p | a_p) \\\\propto L( \\\\alpha_p ) \\\\eta(\\\\alpha_p) = f(a_p|\\\\alpha_p) \\\\eta(\\\\alpha_p)\\\\; ,\\n\\\\end{equation}\\nwhere $\\\\eta(\\\\alpha_p)$ is some more fundamental prior.\\\\footnote{Glen Cowan has referred to this more fundamental prior as an 'urprior', which is based on the German use of 'ur' for forming words with the sense of `proto-, primitive, original'.} By taking the time to undo the Bayesian reasoning into an objective pdf or likelihood and a prior we are able to write a model that can be used in a frequentist context. Within \\\\roostats, the care is taken to separately track the frequentist component and the prior; this is achieved with the \\\\texttt{ModelConfig} class.\\nIf one can identify what auxiliary measurements were performed to provide the estimate of $\\\\alpha_p$ and its uncertainty, then it is not a logical fallacy to approximate it with a constraint term, it is simply a convenience. However, not all uncertainties that we deal result from auxiliary measurements. In particular, some theoretical uncertainties are not statistical in nature. For example, uncertainty associated with the choice of renormalization and factorization scales and missing higher-order corrections in a theoretical calculation are not statistical. Uncertainties from parton density functions are a bit of a hybrid as they are derived from data but require theoretical inputs and make various modeling assumptions. In a Bayesian setting there is no problem with including a prior on the parameters associated to theoretical uncertainties. In contrast, in a formal frequentist setting, one should not include constraint terms on theoretical uncertainties that lack a frequentist interpretation. That leads to a very cumbersome presentation of results, since formally the results should be shown as a function of the uncertain parameter. In practice, the groups often read Eq.~\\\\ref{eq:urprior} to arrive at an effective frequentist constraint term.\\nI will denote the set of parameters with constraint terms as $\\\\mathbb{S}$ and the global observables $\\\\mathcal{G}=\\\\{a_p\\\\}$ with $p\\\\in\\\\mathbb{S}$. By including the constraint terms explicitly (instead of implicitly as an additional channel) we arrive at the total probability model, which we will not need to generalize any further:\\n\\\\begin{equation}\\n\\\\F_{\\\\textrm{tot}}(\\\\datasim, \\\\mathcal{G}|\\\\alpha) = \\\\prod_{c\\\\in\\\\rm channels} \\\\left[ \\\\Pois(n_c|\\\\nu_c(\\\\alpha)) \\\\prod_{e=1}^{n_c} f_c(x_{ce}|\\\\alpha) \\\\right] \\\\cdot \\\\prod_{p \\\\in \\\\mathbb{S}} f_p(a_p | \\\\alpha_p)\\\\; .\\n\\\\end{equation}\\n\", \"\\\\section{Upper Limits for multi-channel experiments}\\nIn the multi-channel case, the likelihood associated with the signal strength \\\\( \\\\mu \\\\) for the complete observation is determined by the joint likelihood of all channels~\\\\cite{wang2023recent}:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\prod_{i}^{Channels} \\\\mathcal{L}_{i}(\\\\mu).\\n\\\\end{equation}\\nWhere it is assumed that the information in each channel is independently and identically distributed. The definitions of the estimators \\\\( \\\\mathcal{Q}(\\\\mu) \\\\) and \\\\( q_{\\\\mu} \\\\) for a single channel naturally extend to the multi-channel case using the properties of logarithms. Thus, the statistical estimators are fully defined as follows, respectively:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = \\\\sum_{i}^{Channels} \\\\mathcal{Q}_{i}(\\\\mu). \\n\\\\end{equation}\\n\\\\begin{equation}\\nq_{\\\\mu} = \\\\sum_{i}^{Channels} q_{\\\\mu,i}. \\n\\\\end{equation}\\nAs an example, synthetic data associated with a resonance near the measured mass of the Higgs boson \\\\( m_{H} = 125 \\\\ \\\\text{GeV} \\\\) were simulated, with an exponential background component characteristic of the invariant mass of a diphoton system. A total of 30 channels were simulated to numerically illustrate the discovery reported by the CMS collaboration in 2012~\\\\cite{cms2012observation}. Figure~[\\\\ref{fig:15}] shows the resonance data with a mass similar to the measured Higgs boson mass under an exponential background component (blue shaded area). Additionally, the alternative signal + background hypothesis is shown, which could be consistent with the observation.\\nAs in the 2012 search, several signal models with masses ranging from 100 to 160 GeV in steps of 6 GeV will be assumed. For each signal point, the expected upper limit (\\\\( n = b \\\\)), representing a measurement compatible with the background-only hypothesis, and the observed upper limit using the synthetic data will be calculated. Given the data observation, the upper limits allow for excluding the Higgs model at a specific mass or rejecting the background-only hypothesis in favor of the model with this new particle. Typically, this discrepancy is indicated by the difference between the expected and observed upper limits; when this difference is large, it must be quantified in terms of the \\\\( 5\\\\sigma \\\\) criterion to report a discovery~\\\\cite{lista2016practical}.\\nFigure~[\\\\ref{fig:16}] shows the upper limit values as a function of the hypothetical particle mass using the estimator \\\\( \\\\mathcal{Q} \\\\). The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are calculated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{\\\\mathcal{Q}(\\\\mu_{up})} \\\\)~\\\\cite{conway2005calculation}. Note that low- and high-mass models are excluded at a \\\\( 95\\\\ \\nOn the other hand, Figure~[\\\\ref{fig:17}] shows the upper limit values as a function of the hypothetical particle mass using the \\\\( q_{\\\\mu} \\\\) estimator. In the generation of each random experiment, \\\\( \\\\hat{\\\\mu} \\\\) is found using the \\\\texttt{Scipy.optimize} package. The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are estimated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{q_{\\\\mu_{up}}} \\\\). Note the consistency of the results using both estimators. The primary difference lies in the approach to incorporating systematic uncertainties in the estimation of upper limits, significance, and \\\\( 5\\\\sigma \\\\) tension, which is why the profile likelihood is currently used by the CMS and ATLAS collaborations for these estimations~\\\\cite{cms2012observation, atlas2012observation}. Finally, to estimate the discrepancy between the observation and the expected number of events, the p-value of the observation is calculated under the assumption that the background-only hypothesis is correct.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0,\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n \\\\) is the number of observed events.\\n\\\\begin{equation}\\np_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:18}] shows the local p-value as a function of the particle mass. The dashed lines indicate the \\\\( 3\\\\sigma \\\\) evidence region and the \\\\( 5\\\\sigma \\\\) discovery region. This graph illustrates the statistical behavior of the p-value in the search for the Higgs boson in 2012~\\\\cite{cms2012observation}.\\n\", \"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Non-profile frequentist estimator}\\nThe modified frequentist method is generalized through the Neyman-Pearson Lemma, a fundamental statistical result stating that the most powerful test for hypothesis comparison is based on minimizing the Type II error. This error occurs when the null hypothesis (\\\\( H_0 \\\\)), which assumes only background, is not rejected despite being false. The Neyman-Pearson Lemma indicates that, given a significance level, the most efficient statistical test to discriminate between two hypotheses is based on the likelihood ratio~\\\\cite{cranmer2015practical,lista2016practical,cowan2011asymptotic}.\\n\\\\begin{equation}\\nR(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)}.\\n\\\\end{equation}\\nThe formula has an asymptotic approximation to a \\\\(\\\\chi^{2}\\\\) distribution when expressed in terms of logarithms~\\\\cite{lista2016practical}. Generally, this formula is expressed as follows:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = - 2 Ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)} \\\\bigg). \\n\\\\end{equation}\\nThis expression is known as the log-likelihood ratio, and it allows for generalization to experiments with multiple channels. Consider, for example, a single-channel experiment measuring the mass of a hypothetical particle \\\\( m(\\\\rho) \\\\) within the range of 100 to 200 GeV. Suppose the observation is \\\\( n = 105 \\\\), the expected number of background events is \\\\( b = 100 \\\\), and the model for this new particle predicts \\\\( s = 10 \\\\). The statistical estimator based on this distribution model is expressed as follows:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & -2Ln \\\\bigg( \\\\frac{ e^{-(\\\\mu s + b)}(\\\\mu s + b)^{n} }{ e^{-b}b^{n} } \\\\bigg) {} \\\\nonumber \\\\\\\\\\n& = & 2 \\\\bigg( \\\\mu s - n Ln \\\\bigg( 1 + \\\\frac{\\\\mu s} {b} \\\\bigg) \\\\bigg). {}\\n\\\\end{eqnarray}\\nFigure~[\\\\ref{fig:9}] shows the single-channel experiment, where the error bars represent the Poisson uncertainty, \\\\( \\\\epsilon = \\\\sqrt{n} \\\\), associated with the observed number of events. Additionally, it describes the behavior of the estimator as a function of the signal strength, \\\\( \\\\mu \\\\). This reveals that the likelihood ratio defines a convex optimization problem with a unique global minimum, corresponding to the best fit of the model under the null hypothesis to describe the observation. Notably, both hypotheses can fit the observed data, making it essential to determine the set of theories that can be excluded based on the measurement of \\\\( n \\\\) or to assess whether there is sufficient evidence to claim the discovery of the particle in question~\\\\cite{cms2012observation,atlas2022detailed}.\\nThe best-fit value is obtained by differentiating the log-likelihood function with respect to the parameter of interest and evaluating the result at zero. In this case, the minimum can be calculated exactly using elementary methods.\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{n-b}{s} = 0.5\\n\\\\end{equation}\\nEstimating the best model is fundamental in current statistical estimators. In general, obtaining the best fit in experiments with multiple channels and systematic uncertainties requires advanced optimization processes and sampling techniques, which will be described later. Moreover, calculating upper limits involves sampling the distributions of the estimator under both the null and alternative hypotheses. The next section will address the sampling of the estimator and the definition of the confidence level for the signal, known as \\\\( CL_s \\\\).\\n\\\\subsubsection{Sampling of the log-likelihood estimator}\\nTo obtain the upper limit using the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), the estimator can be sampled under both the null and alternative hypotheses; these distributions are labeled \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), respectively. To calculate \\\\( f(\\\\mathcal{Q}|0) \\\\), a random number is generated following a Poisson distribution with \\\\( \\\\mu = 0 \\\\), representing the number of observed events under the null hypothesis. Similarly, the distribution \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\) is obtained using a specific value of \\\\( \\\\mu \\\\)~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:10}] shows a schematic of the shapes of the distributions of the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\). The value \\\\( Q_{obs} \\\\) corresponds to the estimator for the observed number of events \\\\( n \\\\). Typically, the background-only distribution is found to the right of \\\\( Q_{obs} \\\\), while the signal + background distribution is found to the left of \\\\( Q_{obs} \\\\). The degree of agreement between the observation and the models is evaluated through the confidence level, represented by the shaded areas in the plot~\\\\cite{cowan2014statistics}.\\nThe green shaded area represents the p-value of the observation under the background-only hypothesis (\\\\( H_{0} \\\\)) and is expressed as:\\n\\\\begin{equation}\\np_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q},\\n\\\\end{equation}\\nThe p-value of the null hypothesis is related to the well-known power of the test, which is the confidence level, denoted as \\\\( \\\\beta \\\\):\\n\\\\begin{equation}\\n\\\\beta = CL_{b} = 1 - p_{0} = 1 - \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nOn the other hand, the p-value for the signal + background hypothesis (\\\\( H_{1} \\\\)) is represented by the yellow shaded area, which directly corresponds to the confidence level of the \\\\( H_{1} \\\\) hypothesis:\\n\\\\begin{equation}\\np_{\\\\mu} = CL_{s+b} = \\\\int_{\\\\mathcal{Q}_{observed}}^{\\\\infty} f(\\\\mathcal{Q}/\\\\mu) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThus, the statistical significance \\\\( \\\\alpha \\\\) is a particular case of the p-value of the observation under the null hypothesis (\\\\( p_{0} \\\\)) and constitutes evidence in favor of \\\\( H_{1} \\\\) against \\\\( H_{0} \\\\). Consequently, maximizing the significance is a tool for optimizing the search window. In various statistical studies, it is suggested that stronger evidence in favor of \\\\( H_{1} \\\\) over \\\\( H_{0} \\\\) is reflected in~\\\\cite{lista2016practical,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{p_{\\\\mu}}{\\\\beta} = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}. \\n\\\\end{equation}\\nThis completely defines the confidence level of the signal. Using these definitions, the upper limit of the signal strength is obtained through the following strategy: 1) vary the signal strength \\\\( \\\\mu \\\\), 2) sample the distributions \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), 3) calculate the p-values corresponding to the observed estimator, and 4) determine the confidence level \\\\( CL_s(\\\\mu) \\\\). In this way, the upper limit \\\\( \\\\mu^{up} \\\\) for exclusion is given by:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = 0.05\\n\\\\end{equation}\\nThis process is typically computationally expensive due to the sampling of distributions for each value of \\\\( \\\\mu \\\\). In the case of multiple channels and nuisance parameter estimation, parallelization is required to obtain results efficiently. This is crucial, as optimizing the search window at the phenomenological level necessitates maximizing statistical significance or other statistical metrics~\\\\cite{florez2016probing,allahverdi2016distinguishing}.\\nIn the experiment related to the invariant mass channel \\\\( m(\\\\rho) \\\\), we have two key estimates. The first is the expected upper limit, which is assumed under the hypothesis that the observation consists of background nuisance only. In this case, the expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.19 \\\\). This implies that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Exp}} \\\\cdot s = 2.19 \\\\times 10 = 21.9 \\\\) events would be excluded, provided that \\\\( n = b \\\\) events are measured. In the second case, the observed upper limit corresponds to the actual data observation. This observed upper limit is estimated as \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.57 \\\\), meaning that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Obs}} \\\\cdot s = 2.57 \\\\times 10 = 25.7 \\\\) events would be excluded based on the observation. Figure~[\\\\ref{fig:11}] shows the search for the confidence level for both the expected and observed limits~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LEP/UpperLimitLnQ.ipynb}{Source code}}. Additionally, the distribution of the statistical estimator for \\\\( \\\\mu = \\\\mu_{up}^{\\\\text{Obs}} \\\\) is presented as an illustrative example. Finally, results obtained using the professional \\\\texttt{RooFit} package, used by the CMS and ATLAS collaborations for upper limit estimation, are included~\\\\cite{verkerke2006roofit,schott2012roostats}.\\nAn important feature in this type of analysis is that when the upper limits are statistically consistent, as in this case, there is insufficient evidence to claim a discovery. In this scenario, we would accept that the null hypothesis \\\\( H_{0} \\\\) (background-only hypothesis) adequately describes the observation. The first sign of tension between the observed data and the background-only hypothesis arises when the expected and observed upper limits differ significantly. From a statistical perspective, this discrepancy must be evaluated, and to consider the observation of a new phenomenon, the difference must exceed the \\\\( 5\\\\sigma \\\\) threshold~\\\\cite{lista2016practical,cranmer2015practical,cms2012observation}.', '\\\\section{Upper limits}\\n\\\\subsection{Frequentist confidence}\\nGoing back to the discussion of the basics, for frequentists the probability that it will rain tomorrow is meaningless:\\nthere is only one tomorrow, it will either rain or it will not, there is no\\nensemble.\\nThe probability $N_\\\\mathrm{rain}/N_\\\\mathrm{tomorrows}$ is either 0 or 1.\\nTo talk about \\n$P_\\\\mathrm{rain}$ is \"unscientific\"~\\\\cite{vonMises}.\\nThis is unhelpful. But there is a workaround.\\nSuppose some forecast says it will rain and\\nstudies show this forecast is correct 90\\\\We now have an ensemble of statements, and can say:\\n`The statement `It will rain tomorrow\\' has a 90\\\\We shorten this to `It will rain tomorrow, with 90\\\\We state X with confidence $P$ if X is a member of an ensemble of statements of which at least $P$ are true.\\nNote the `at least\\' which has crept into the definition. There are two reasons for it:\\n\\\\begin{enumerate}\\n\\\\item Higher confidences embrace lower ones. If X at 95\\\\\\\\item We can cater for composite hypotheses which are not completely defined.\\n\\\\end{enumerate}\\nThe familiar quoted error is in fact a confidence statement. Consider as an illustration\\nthe Higgs mass measurement (current at the time of writing)\\n$M_H=125.09 \\\\pm 0.24$~GeV.\\nThis does not mean that the probability of the Higgs mass being in the range \\n$124.85 < M_H < 125.33$~GeV is 68\\\\ the Higgs mass is a single, unique, number which either lies in this interval or it does not. \\nWhat we are saying is that \\n$M_H$ has been measured to be $125.09$~GeV with a technique that will give a value within 0.24~GeV of the true value 68\\\\We say: $124.85 < M_H < 125.33~GeV$ with 68\\\\The statement is either true or false (time will tell), but it belongs to a collection of statements of which (at least) 68\\\\\\nSo we construct \\n{\\\\it confidence regions} {also known as confidence intervals}\\n$[x_-,x_+]$ such that $\\\\int_{x_-}^{x_+} P(x) \\\\, dx=CL$.\\nWe have not only a\\nchoice of the probability content (68\\\\\\n\\\\begin{enumerate}\\n\\\\item Symmetric: $\\\\hat x-x_- = x_+ - \\\\hat x$\\\\ ,\\n\\\\item Shortest: Interval that minimises $x_+-x_-$\\\\ ,\\n\\\\item Central: $\\\\int_{-\\\\infty}^{x_-}P(x)\\\\, dx=\\\\int_{x_+}^\\\\infty P(x)\\\\, dx = {1 \\\\over 2} (1-CL)$\\\\ ,\\n\\\\item Upper Limit: $x_-=-\\\\infty$, $\\\\int_{x_+}^\\\\infty P(x)\\\\ , dx=1-CL$\\\\ , and\\n\\\\item Lower Limit: $x_+=\\\\infty$, $\\\\int_{-\\\\infty}^{x_-} P(x)\\\\ , dx=1-CL$\\\\ .\\n\\\\end{enumerate}\\nFor the Gaussian (or any symmetric PDF) 1-3 are the same. \\nWe are particularly concerned with the upper limit: \\nwe observe some small value $x$. We find a value $x_+$ such that for values of $x_+$ or more the probability of getting a result as small as $x$, or even less, is $1-CL$, or even less. \\n', '\\\\section{Discoveries and upper limits}\\n\\\\subsection{Feldman--Cousins intervals}\\nA solution to the flip-flopping problem was developed by Feldman and Cousins~\\\\cite{feldman_cousins}.\\nThey proposed to select confidence interval based on a likelihood-ratio criterion.\\nGiven a value $\\\\theta=\\\\theta_0$ of the parameter of interest, the\\nFeldman--Cousins confidence interval is defined as:\\n\\\\begin{equation}\\nR_\\\\mu = \\\\left\\\\{x : \\\\frac{L(x;\\\\theta_0)}{L(x;\\\\hat{\\\\theta})} >k_\\\\alpha\\\\right\\\\}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\theta}$ is the best-fit value for $\\\\theta$ and the constant $k_\\\\alpha$\\nshould be set in order to ensure the desired confidence level $1-\\\\alpha$.\\nAn example of the confidence belt computed with the Feldman--Cousins approach\\nis shown in Fig.~\\\\ref{fig:fcBelt}\\nfor the Gaussian case illustrated in Fig.~\\\\ref{fig:flipFlop}.\\nWith the Feldman--Cousins approach, the confidence interval smoothly changes\\nfrom a fully asymmetric one, which leads to an upper limit, for low values of $x$, to\\nan asymmetric interval for higher values of $x$ interval, then finally a symmetric interval\\n(to a very good approximation) is obtained for large values of $x$, recovering\\nthe usual result as in Fig.~\\\\ref{fig:flipFlop}.\\nEven for the simplest Gaussian case, the computation of Feldman--Cousins intervals\\nrequires numerical treatment and for complex cases their computation may be very CPU intensive.\\n', '\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Likelihood}\\nLet us assume that $p(x|\\\\theta)$ is a \\\\textbf{probability density function} (pdf) such that\\n$P(A| \\\\theta) = \\\\int_A p(x|\\\\theta) \\\\, dx$ is the probability of the statement $A = x \\\\in R_x$,\\nwhere $x$ denotes possible data, $\\\\theta$\\nthe parameters that\\ncharacterize the probability model, and $R_x$ is a finite set. If $x$ is discrete, then both\\n$p(x|\\\\theta)$ and $P(A|\\\\theta)$ are probabilities. The \\\\textbf{likelihood function} is\\nsimply the probability model $p(x|\\\\theta)$ evaluated at the data $x_O$ actually obtained, i.e., the function $p(x_O|\\\\theta)$.\\nThe following are examples of likelihoods.\\n\\\\begin{quote}\\n\\\\paragraph*{Example 1}\\nIn 1995, CDF and D\\\\O\\\\ discovered the top quark~\\\\cite{Abe:1995hr, Abachi:1995iq} at Fermilab. The D\\\\O\\\\\\nCollaboration found $x = D$ events ($D = 17$). For a counting experiment, the datum can be modeled using\\n\\\\begin{align*}\\np(x | d) & = \\\\textrm{Poisson}(x, d) \\\\quad \\\\textrm{probability to get $x$ events}\\n\\\\\\\\ p(D | d) & = \\\\textrm{Poisson}(D, d) \\\\quad \\\\textrm{likelihood of observation $D$ events}\\n\\\\\\\\ & = d^{D} \\\\exp(-d) / D!\\n\\\\end{align*}\\nWe shall analyze this example in detail in Lectures 2 and 3.\\n\\\\paragraph*{Example 2}\\nFigure~\\\\ref{fig:CI} shows the transverse momentum spectrum of jets in\\n$p p \\\\rightarrow \\\\textrm{jet} + X$ events measured by the CMS Collaboration~\\\\cite{Chatrchyan:2013muj}. The spectrum has $K = 20$\\nbins with total count $N$ that was modeled using the likelihood\\n\\\\begin{align*}\\np(D | p) & = \\\\textrm{Multinomial}(D, N, p), \\\\quad D = D_1,\\\\cdots,D_K, \\\\quad p = p_1,\\\\cdots,p_K\\n\\\\\\\\ \\\\sum_{i=1}^K D_i & = N. \\n\\\\end{align*}\\nThis is an example of a \\\\emph{binned} likelihood.\\n\\\\framebox{\\\\parbox{0.7\\\\textwidth}{\\\\textbf{Exercise 6a:} Show that a multi-Poisson likelihood can be\\\\ written as the\\\\\\\\ product of a multinomial and a Poisson with count $N$}}\\n\\\\paragraph*{Example 3}\\nFigure~\\\\ref{fig:type1a} shows a plot of the distance modulus versus redshift for\\n$N = 580$ Type 1a supernovae~\\\\cite{Suzuki:2011hu}. These heteroscedastic data\\\\footnote{Data in which each item, $x_i$, or group of items has a different uncertainty.} $\\\\{z_i, x_i \\\\pm \\\\sigma_i \\\\}$ are modeled\\nusing the likelihood\\n\\\\begin{align*}\\np(D | \\\\Omega_M, \\\\Omega_\\\\Lambda, Q) & = \\\\prod_{i=1}^N \\\\textrm{Gaussian}(x_i, \\\\mu_i, \\\\sigma_i),\\n\\\\end{align*}\\nwhich is an example of an \\\\emph{un-binned} likelihood. The cosmological model is\\nencoded in the distance modulus function $\\\\mu_i$, which depends on the redshift $z_i$\\nand the matter density and cosmological constant parameters $\\\\Omega_M$ and $\\\\Omega_\\\\Lambda$, respectively. (See Ref.~\\\\cite{Dungan:2009fp} for an accessible introduction to the analysis of these data.)\\n\\\\paragraph*{Example 4}\\nThe discovery of a neutral Higgs boson in 2012 by ATLAS~\\\\cite{Aad:2012tfa} and CMS~\\\\cite{Chatrchyan:2012ufa} in the di-photon final state ($p p \\\\rightarrow H \\\\rightarrow \\\\gamma\\\\gamma$)\\nmade use of an un-binned likelihood of the form,\\n\\\\begin{align*}\\np(x| s, m, w, b) & = \\\\exp[-(s + b)] \\\\prod_{i=1}^N [ s f_s(x_i | m, w) + b f_b(x_i) ] \\n\\\\\\\\\\n\\\\textrm{where } x & = \\\\textrm{di-photon masses} \\\\\\\\\\nm & = \\\\textrm{mass of boson} \\\\\\\\\\nw & = \\\\textrm{width of resonance} \\\\\\\\\\ns & = \\\\textrm{expected (i.e., mean) signal count} \\\\\\\\\\nb & = \\\\textrm{expected background count} \\\\\\\\\\nf_s & = \\\\textrm{signal probability density} \\\\\\\\\\nf_b & = \\\\textrm{background probability density} \\\\\\\\ \\\\\\\\\\n& \\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 6b:} Show that a binned multi-Poisson\\\\\\\\ likelihood yields an un-binned likelihood of this\\\\\\\\ form as the bin widths go to zero}}\\n\\\\end{align*}\\n\\\\end{quote}\\n\\\\bigskip\\nThe likelihood function is arguably the most important quantity in a statistical analysis\\nBecause it can be used to answer questions such as the following.\\n\\\\begin{enumerate}\\n\\\\item How do I estimate a parameter?\\n\\\\item How do I quantify its accuracy?\\n\\\\item How do I test an hypothesis?\\n\\\\item How do I quantify the significance of a result?\\n\\\\end{enumerate}\\nWriting down the likelihood function requires:\\n\\\\begin{enumerate}\\n\\\\item identifying all that is \\\\emph{known}, e.g., the observations,\\n\\\\item identifying all that is \\\\emph{unknown}, e.g., the parameters,\\n\\\\item constructing a probability model for \\\\emph{both}.\\n\\\\end{enumerate}\\nMany analyses in particle physics do not use likelihood functions explicitly. However, it is worth spending time to think about them because doing so encourages a deeper reflection on what is being done, a more systematic approach to the statistical analysis,\\nand ultimately leads to better answers. \\nBeing explicit about what is and is not known in an analysis problem may seem a pointless exercise;\\nsurely these things are obvious. Consider the D\\\\O\\\\ top quark discovery data~\\\\cite{Abachi:1995iq},\\n$D = 17$ events observed with a background estimate of $B = 3.8 \\\\pm 0.6$ events. The\\nuncertainty in 17 is invariably said to be $\\\\sqrt{17} = 4.1$. Not so! The count 17 is perfectly\\nknown: it is 17. What we are uncertain about is the mean count $d$, that is, the parameter\\nof the probability model, which we take to be a Poisson distribution.\\nThe $\\\\pm 4.1$ must somehow be a statement not about 17 but rather about the\\nunknown parameter $d$. We shall explain what the $\\\\pm 4.1$ means in Lecture 2.', '\\\\section{Inference}\\n\\\\subsection{Estimate of Gaussian parameters}\\nIf we have $n$ independent measurements $\\\\vec{x}=(x_1,\\\\cdots,x_n)$ all modeled (exactly or approximatively)\\nwith the same Gaussian PDF,\\nwe can write the negative of twice the logarithm of the likelihood function as follows:\\n\\\\begin{equation}\\n-2\\\\ln L(\\\\vec{x}; \\\\mu) = \\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2} + n(\\\\ln 2\\\\pi + 2\\\\ln\\\\sigma)\\\\,.\\n\\\\end{equation}\\nThe first term, $\\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2}$, is an example of $\\\\chi^2$ variable (see Sec.~\\\\ref{sec:binnedSamples}).\\nAn analytical minimization of $-2\\\\ln L$ with respect to $\\\\mu$,\\nassuming $\\\\sigma^2$ is known, gives the {\\\\it arithmetic mean} as maximum likelihood estimate of $\\\\mu$:\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i\\\\,.\\n\\\\end{equation}\\nIf $\\\\sigma^2$ is also unknown, the maximum likelihood estimate of $\\\\sigma^2$ is:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2} = \\\\frac{1}{n} \\\\sum_{i=1}^m(x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\nThe estimate in Eq.~(\\\\ref{eq:sigma2MLestimate}) can be demonstrated to have an unpleasant feature, called {\\\\it bias}, that\\nwill be discussed in Sec.~\\\\ref{sec:bias}.\\n', \"\\\\section{Inference}\\n\\\\subsection{Binomial intervals}\\nThe Neyman's belt construction may only guarantee approximate coverage in case of a discrete\\nvariable $n$. This because the interval for a discrete variable is a set of integer values,\\n$\\\\{ n_{\\\\mathrm{min}}, \\\\cdots, n_{\\\\mathrm{max}}\\\\}$, and cannot be ``tuned'' like in\\na continuous case. The choice of the discrete interval should be such to provide\\n{\\\\it at least} the desired coverage (i.e.: it may {\\\\it overcover}).\\nFor a binomial distribution, the problem consists of finding the interval such that:\\n\\\\begin{equation}\\n\\\\sum_{n=n_{\\\\mathrm{min}}}^{n_{\\\\mathrm{max}}}\\n\\\\frac{N!}{n!(N-n)!} p^n (1-p)^{N-n} \\\\ge 1-\\\\alpha\\\\,.\\n\\\\end{equation}\\nClopper and Pearson~\\\\cite{clopper_pearson} solved the belt inversion problem for\\ncentral intervals.\\nFor an observed $n = k$, one has to find the lowest $p^{\\\\mathrm{lo}}$ and highest\\n$p^{\\\\mathrm{up}}$ such that:\\n\\\\begin{eqnarray}\\nP(n \\\\ge k | N, p^{\\\\mathrm{lo}}) & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\nP(n \\\\le k | N, p^{\\\\mathrm{up}}) & = & \\\\frac{\\\\alpha}{2}\\\\,.\\n\\\\end{eqnarray}\\nAn example of Neyman belt constructed using the Clopper--Pearson\\nmethod is shown in Fig.~\\\\ref{fig:CloPear}.\\nFor instance for $n = N$, Eq.~(\\\\ref{eq:CP1}) becomes:\\n\\\\begin{equation}\\nP(n\\\\ge N|N,p^{\\\\mathrm{lo}}) = P(n=N|N,p^{\\\\mathrm{lo}}) = (p^{\\\\mathrm{lo}})^N = \\\\frac{\\\\alpha}{2}\\\\,,\\n\\\\end{equation}\\nhence, for the specific case $N=10$:\\n\\\\begin{equation}\\np^{\\\\mathrm{lo}} = \\\\sqrt[10]{\\\\frac{\\\\alpha}{2}} = 0.83\\\\,\\\\text(1-\\\\alpha = 0.683), \\\\,0.74\\\\,(1-\\\\alpha = 0.90)\\\\,.\\n\\\\end{equation}\\nIn fact, in Fig.~\\\\ref{fig:CloPear}, the bottom line of the belt reaches\\nthe value $p=0.83$ for $n=10$.\\nA frequently used approximation, inspired by Eq.~(\\\\ref{eq:binomVar}) is:\\n\\\\begin{equation}\\n\\\\hat{p} = \\\\frac{n}{N},\\\\,\\\\,\\\\,\\\\sigma_{\\\\hat{p}} \\\\simeq \\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{N}}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:varEff}) gives $\\\\sigma_{\\\\hat{p}}=0$ for $n=0$ or $N=n$, which is\\nclearly an underestimate of the uncertainty on $\\\\hat{p}$. For this reason,\\nClopper--Pearson intervals should be preferred to the approximate\\nformula in Eq.~(\\\\ref{eq:varEff}).\\nClopper--Pearson intervals are often defined as ``exact'' in literature,\\nthough exact coverage is often impossible to achieve for discrete variables.\\nFigure~\\\\ref{fig:CloPearCov} shows the coverage of Clopper--Pearson intervals as a\\nfunction of $p$ for $N=10$ and $N=100$ for $1-\\\\alpha = 0.683$. A ``ripple'' structure\\nis present which, for large $N$, tends to gets closer to the nominal 68.3\\\\\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Treatment of nuisance parameters}\\nNuisance parameters have been introduced in Sec.~\\\\ref{sec:extLikFun}.\\nUsually, signal extraction procedures (either parameter fits or upper limits determinations) determine,\\ntogether with parameters of interest, also nuisance parameters that model effects\\nnot strictly related to our final measurement, like\\nbackground yield and shape, detector resolution, etc.\\nNuisance parameters are also used to model sources of systematic\\nuncertainties.\\nOften, the true value of a nuisance parameter is not known, but we may have some\\nestimate from sources that are external to our problem.\\nIn those cases, we can refer to {\\\\it nominal values} of the nuisance parameter and\\ntheir uncertainty. Nominal values of nuisance parameters are random variables\\ndistributed according to some PDF that depend on their true value.\\nA Gaussian distribution is the simplest assumption for nominal values of nuisance parameters.\\nAnyway, this may give negative values corresponding to the\\nleftmost tail, which are not suitable for\\nnon-negative quantities like cross sections.\\nFor instance, we may have an estimate of some background yield $b$ given by:\\n\\\\begin{equation}\\nb = \\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $L_{\\\\mathrm{int}}$ is the estimate of the integrated luminosity (assumed to be known\\nwith negligible uncertainty), $\\\\sigma_b$ is the background cross section evaluated\\nfrom theory, and $\\\\beta$ is a nuisance parameter, whose nominal value is equal to one, representing the\\nuncertainty on the cross-section evaluation. If the uncertainty on $\\\\beta$ is large, one may\\nhave a negative value of $\\\\beta$ with non-negligible probability, hence an unphysical negative value of\\nthe background yield $b$.\\nA safer assumption in such cases is to take a log normal distribution for the uncertain non-negative\\nquantities:\\n\\\\begin{equation}\\nb = e^\\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\beta$ is again distributed according to a normal distribution with nominal value equal to zero,\\nin this case.\\nUnder the Bayesian approach, nuisance parameters don't require a special treatment.\\nIf we have a parameter of interest $\\\\mu$ and a nuisance parameter $\\\\theta$,\\na Bayesian posterior will be obtained as (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(\\\\mu,\\\\theta|\\\\vec{x}) = \\\\frac{\\nL(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\pi(\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta^\\\\prime\\n}\\\\,.\\n\\\\end{equation}\\n$P(\\\\mu|\\\\vec{x})$ can be obtained as marginal PDF of $\\\\mu$ by integrating $P(\\\\mu,\\\\theta|\\\\vec{x})$ over $\\\\theta$:\\n\\\\begin{equation}\\nP(\\\\mu|\\\\vec{x}) = \\\\int P(\\\\mu,\\\\theta|\\\\vec{x}),\\\\mathrm{d}\\\\theta =\\n\\\\frac{\\n\\\\int L(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\\\,\\\\mathrm{d}\\\\theta\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta)\\\\pi(\\\\mu^\\\\prime,\\\\theta)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta\\n}\\\\,.\\n\\\\end{equation}\\nIn the frequentist approach, one possibility is to introduce in the likelihood\\nfunction a model for a data sample that can constrain the nuisance parameter.\\nIdeally, we may have a control sample $\\\\vec{y}$, complementary to the main\\ndata sample $\\\\vec{x}$, that only depends on the nuisance parameter, and\\nwe can write a global likelihood function as:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_y(\\\\vec{y};\\\\theta)\\\\,.\\n\\\\end{equation}\\nUsing control regions to constrain nuisance parameters is usually a good method\\nto reduce systematic uncertainties. Anyway, it may not always be\\nfeasible and in many cases we may just have information abut the nominal\\nvalue $\\\\theta^{\\\\mathrm{nom}}$ of $\\\\theta$ and its distribution obtained from a complementary measurement:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_\\\\theta(\\\\theta^{\\\\mathrm{nom}};\\\\theta)\\\\,.\\n\\\\end{equation}\\n$L_\\\\theta$ may be a Gaussian or log normal distribution in the easiest cases.\\nIn order to achieve a likelihood function that does not depend on\\nnuisance parameters, for many measurements at LEP or Tevatron a method\\nproposed by Cousins and Highland was adopted~\\\\cite{Cousins_Highlands}\\nwhich consists of integrating the likelihood function over the nuisance\\nparameters, similarly to what is done in the Bayesian approach (Eq.~(\\\\ref{eq:BayesianNuisance})).\\nFor this reason, this method was also called hybrid.\\nAnyway the Cousins--Highland does not guarantee to provide exact coverage,\\nand was often used as pragmatic solution in the frequentist context.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Bayesian `credible intervals'}\\nA Bayesian has no problems saying `It will probably rain tomorrow'\\nor `The probability that $124.85 < M_H < 125.33$~GeV is 68\\\\downside, of course, is that another Bayesian can say `It will probably not rain tomorrow' and \\n`The probability that $124.85 < M_H < 125.33~GeV$ is 86\\\\cannot resolve their subjective difference in any objective way.\\nA Bayesian has a prior belief PDF $P(a)$ and defines a region $R$ such that $\\\\int_R P(a)\\\\, da = CL$.\\nThere is the same ambiguity regarding choice of content (68\\\\even if their meaning is different.\\nThere are two happy coincidences.\\nThe first is that \\nBayesian credible intervals on Gaussians, with a flat prior, are the same as Frequentist confidence intervals. If \\nF quotes 68\\\\their results will agree.\\nThe second is that although the Frequentist Poisson upper limit is given by $\\\\sum_{r=0}^{r=r_{data}} e^{-a_{hi}} a_{hi}^r / r!$ whereas the \\nBayesian Poisson flat prior upper limit is given by $\\\\int_0^{a_{hi}} e^{-a} a^{r_{data}} / r_{data}!\\\\, da$,\\nintegration by parts of the Bayesian formula gives a series which is same as the Frequentist limit.\\nA Bayesian will also say : `I see zero events---the probability is 95\\\\ This is (I think) a coincidence---it does not apply for lower limits. But it does avoid heated discussions as to which value to publish.\\n\", '\\\\section{Introduction}\\nParticle physics is the study of the subatomic constituents of matter: How many are there? What are their properties? How do they interact? There are two basic approaches to answering these questions: a theoretical one and an experimental one. On the theoretical side, we can ask: what possible subatomic particles could there be? Remarkably, there are constraints due to theoretical consistency of the underlying theory that strongly limit the types of particles possible. For example, there is a direct logical path from the requirement that things do not start appearing out of nowhere (``unitarity\\'\\') to the Pauli exclusion principle, which keeps matter from imploding. To the dismay of many theorists, however, there seem to be many more self-consistent theories than the one describing nature, and so experiments are essential. \\nThe state-of-the art particle experiment is the Large Hadron Collider (LHC) at CERN on the border between France and Switzerland. \\nIts major success so far is finding the Higgs boson in 2012. The LHC collides protons together at close to the speed of light. This energy is then converted into mass via $E=mc^2$ thereby forming new particles.\\nUsually these particles last for only fractions of a second (the lifetime of the Higgs boson is $10^{-22}$ s); hence, the art of modern experimental particle physics involves finding indications that a particle was made even though we never actually see it. The experimental challenge is complicated by the fact that particles of interest are usually quite rare and look nearly identical to much more common backgrounds. For example, only one in every billion proton collisions at LHC produces a Higgs boson, and only one in ten thousand of these is easy to see. Finding new particles in modern experiments is like finding a particular piece of hay in a haystack.\\nLuckily, hay-in-a-haystack problems are exactly what modern machine learning excels at solving. \\nThere are two aspects of particle physics that make it unique, or at least highly atypical, as compared to other fields where machine learning is applied. First, particle physics is governed by quantum mechanics. Of course, everything is governed by quantum mechanics, but in particle physics the inherent uncertainty of the quantum mechanical world affects the nature of the truth we might hope to learn. Just like how \\nSchr\\\\\"odinger\\'s cat can be alive and dead at the same time, a collision at the LHC can both produce a Higgs boson and not produce a Higgs boson at the same time. In fact, there is quantum mechanical interference between the signal process (protons collide and a Higgs boson is produced) and a background process (protons collide without producing a Higgs). The question ``Was there a Higgs boson in this event?\\'\\' is unanswerable. To be a little more precise, for a given number of particles $n$ produced, the probability distribution for signal and background, differential in the momenta of the particles produced (the phase space) has the form\\n\\\\begin{equation}\\nd P_{\\\\text{data}}^n = |{\\\\mathcal M}_S +{\\\\mathcal M}_B|^2 dp_1 \\\\cdots dp_n \\n\\\\end{equation}\\nHere, ${\\\\mathcal M}_S$ and ${\\\\mathcal M}_B$ are the quantum-mechanical amplitudes ($S$-matrix elements, which are complex numbers) for producing signal and background, and the cross term ${\\\\mathcal M}_S {\\\\mathcal M}_B^\\\\star + {\\\\mathcal M}_B {\\\\mathcal M}_S^\\\\star$ represents the interference. This interference term can be positive (constructive interference) or negative (destructive interference).\\nAlthough an individual event cannot be assigned a truth label, the probability of finding a certain set of particles showing up in the detector depends on whether the Higgs boson exists:\\nfinding the Higgs boson amounts to excluding the background-only hypothesis (Eq.~\\\\eqref{Pdata1} with ${\\\\mathcal M}_S=0$). \\nIn practice, the probability distribution of signal is often strongly peaked, due to a resonance for example, in some small region of phase space. In such regions, background can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_S$. In complementary regions, signal can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_B$. Thus it is commonplace to approximate the full probability distribution with a mixture model. If we sum over possible numbers $n$ of particles and integrate over the momenta in the observable region, we can then write\\n\\\\begin{equation}\\nP_{\\\\text{data}} = \\\\alpha_S \\\\, P_{\\\\text{S}} + \\\\alpha_B \\\\, P_{\\\\text{B}}\\n\\\\end{equation}\\nwith $\\\\alpha_S + \\\\alpha_B=1$. \\nThat is, we treat the probability distribution of the data as a linear combination of the probability distributions for signal and background. The goal is then to determine the coefficients $\\\\alpha_S$ and $\\\\alpha_B$, or often more succinctly, whether $\\\\alpha_S$ is non-zero. \\nEach measured event gives us a number of particles $n$ and point in $n$-particle phase space $\\\\{p_i\\\\}$, with some uncertainty or binning $dp_1\\\\cdots d p_n$,\\ndrawn from the true probability distribution $d P^n_{\\\\text{data}}$.\\nOnly after many draws can we hope to constrain $\\\\alpha_S$. Even within the mixture model approximation, there is still no truth label for individual events. \\nThis is different from, say, distinguishing cats from dogs (or alive cats from dead cats) in an image database. For cats and dogs, even if the distributions overlap, there is a correct answer ($\\\\alpha_S=1$ or $0$ for each event). For particle physics, where the distributions overlap, a particle is both signal {\\\\it and} background.\\nThe second way in which particle physics differs from typical machine learning applications is that particle physics has remarkably accurate simulation tools for producing synthetic data for training. These tools have been developed by experts over more than 40 years. Together, they describe the evolution of a particle collision over 20 orders of magnitude in length. The smallest scale the LHC probes is around $10^{-18}$ m, one thousandth the size of a proton. Here the physics is described by perturbative quantum field theory; particles interact rather weakly and first-principles calculations are accurate. The Higgs boson has a size (Compton wavelength) of $10^{-17}$ m, so it is only at these small distances that we have any hope of examining it. Between $10^{-18}$ m and $10^{-15}$ m, a semi-classical Markov model is used to turn a handful of primordial particles into hundreds of quarks and gluons. Between $10^{-15}$ m and $10^{-6}$ m, the quarks and gluons turn into a zoo of metastable subatomic particles that subsequently decay into hundreds of ``stable\\'\\' particles: pions, protons, neutrons, electrons and photons. These then start interacting with detector components and propagating through the material, as described by other excellent parameterized models. The detector model is accurate from $10^{-6}$ m to the $100$ m size of the LHC detectors (the ATLAS detector at the LHC is 46 meters long). The result is a progression from an order-10 dimensional phase space at the shortest distances, to an order-$10^3$ dimensional phase space at intermediate scales, to an order-$10^8$ dimensional phase space of electronic detector readouts channels. Combined, these simulation tools give a phenomenally robust (but embarrassingly sparse) sampling from this hundred million dimensional space. Around one trillion events have been recorded at the LHC, and a comparable number of events have been simulated, providing hundreds of petabytes of actual and synthetic data to analyze. The first stage of the simulations, up to the stable particle level (the $10^3$ dimensional space) is relatively fast: one million events can be generated in an hour or so on a laptop.\\nThe second stage of the simulation, through the detector, is much slower, taking seconds or even minutes per event. Conveniently, for many applications, the first stage of the simulation is sufficient. \\nNo human being, and as yet no machine, can visualize a hundred million dimensional distribution. So the typical analysis pipeline is to take all of the low-level outputs and aggregate them into a single composite feature, such as the total energy of the particles in some region. Ideally, a histogram of this feature would exhibit a resonance peak or some other salient indication of signal. We additionally want this feature to have a simple physical interpretation, so that we can cross-check the distribution against our intuition. For the Higgs boson, the ``golden discovery channel\\'\\' was two muons (or electrons) and two antimuons (or positrons). A Feynman diagram describing this process looks like\\n\\\\begin{equation}\\n{\\n\\\\parbox{10mm} {\\n\\\\includegraphics[width=0.6\\\\columnwidth,trim = {100 0 00 0}]{hmmmm.pdf}\\n}\\n}\\n\\\\end{equation}\\nThe invariant mass $m=\\\\sqrt{(E_1+E_2+E_3+E_4)^2 - (\\\\vec{p}_1+\\\\vec{p}_2+\\\\vec{p}_3+\\\\vec{p}_4)^2}$, with $E_i$ the energies and $\\\\vec{p}_i$ the momenta,\\nof the four observed particles is a powerful way to discover the signal.\\nFor the Higgs boson signal, the probability density of this feature has a peak at the Higgs boson mass of 125 GeV where the background is very small. Unfortunately, only one in every $10^{13}$ proton collisions will give such a signal. If we do not demand that our signal be background-free, and we also do not demand having any physical interpretation such as we have for a feature like mass, \\nthen we can ask: what feature is the {\\\\it optimal} way of statistically discriminating a signal from its background? Such questions, when supplemented with the enormous amounts of easily produced synthetic data, are ideally suited to modern machine learning methodology.\\n', '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Frequentist upper limit}\\nIn practical terms, the simplest approach for making estimates is to consider single-channel experiments, where we have an observed value ($n$), an expected number of background events ($b$), and an expected number of signal events ($s$). Since the measurement process involves counting events in the channel, we will model it using a Poisson distribution.\\nFor $H_{0}$, the mean of the distribution will be $\\\\lambda = b$, meaning that the observation is explained solely by background events. For $H_{1}$, the mean is given by $\\\\lambda(\\\\mu) = \\\\mu s + b$, where $\\\\mu$ is known as the signal strength and measures the agreement between $H_{1}$ and the observation. In this way, to find the confidence level of both hypotheses with respect to the observation, we use the cumulative Poisson distribution, which is given by:\\n\\\\begin{equation}\\nF_{P}(\\\\lambda) = \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda(\\\\mu))^{i}e^{-\\\\lambda(\\\\mu)}}{(i)!}.\\n\\\\end{equation}\\nWhere $\\\\mu = 0$ for $H_{0}$ and $\\\\mu = 1$ for $H_{1}$. This cumulative distribution has a direct relation with the cumulative $\\\\chi^2(x; k)$ distribution, with $k = 2(n + 1)$ degrees of freedom and $x = 2\\\\lambda$ (see Appendix \\\\ref{sec:AppendixA}). Therefore, the confidence level $CL = 0.95$ is given by~\\\\cite{lista2016practical}:\\n\\\\begin{eqnarray}\\n1-\\\\alpha & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) {} \\\\nonumber \\\\\\\\ \\n0.95 & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)). {}\\n\\\\end{eqnarray}\\nThe signal strength $\\\\mu$ allows us to statistically assess the degree of agreement between the alternative hypothesis and the observation. In general, we can find the upper limit of the signal strength, i.e., the point at which the alternative hypothesis can no longer explain the observation. This means excluding all models with $\\\\mu > \\\\mu_{up}$ at a $3\\\\sigma$ confidence level. By inverting the relation~(\\\\ref{eq:4}), it is possible to calculate the upper limit for all new theories, given the observation $n$, an expected number of background events $b$, and new physics events $s$. Thus, we obtain:\\n\\\\begin{equation}\\n\\\\mu_{up}= \\\\frac{1}{s} (\\\\frac{1}{2}F^{-1}_{\\\\chi^2}(1-\\\\alpha;k=2(n+1)) - b).\\n\\\\end{equation}\\nWhere $\\\\mu_{up}$ is the upper limit at $95\\\\\\nIn the general case, when the number of observations is different from 0, the upper bounds of the model are determined by varying both the observations and the expected background~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:4}] illustrates the behavior of the upper bound $\\\\mu_{up}$ for a given $n$, as a function of the expected nuisance level $b$~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Frequentist/FrequentistUpperLimitScan.ipynb}{Source code}}.\\nIt is important to note that as the number of observations decreases, the upper bound value as a function of the background component tends to negative values. A value of $\\\\mu_{up} < 0$ implies the exclusion of the null hypothesis in the absence of signal events $s$. This scenario presents a contradiction, as it represents a non-physical condition known as the coverage problem of the statistical estimator. To address this issue, variants of the frequentist method have been proposed that correct the coverage problem while adhering to solid probabilistic principles. These approaches include Bayesian methods and a modified version of the frequentist method~\\\\cite{lista2016practical,cranmer2015practical,conway2005calculation}.\\n', \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Excluded and allowed regions as confidence intervals}\\nOften we consider a new physics model that is parametrized by theoretical parameters. For instance, the mass or coupling of a new particle. In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data. Figure~\\\\ref{fig:confidenceIntervals} shows two examples. Figure~\\\\ref{fig:confidenceIntervals}(a) shows an example with $\\\\vec\\\\alpha_{\\\\rm poi} = (\\\\sigma/\\\\sigma_{SM}, M_H)$, where $\\\\sigma/\\\\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model. All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\\\\$\\\\vec\\\\alpha_{\\\\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses. The blue ellipse `is the 68\\\\\\nIn a frequentist setting, these allowed regions are called \\\\textit{confidence intervals} or \\\\textit{confidence regions}, and the parameter points outside them are considered excluded. Associated with a confidence interval is a confidence level, i.e. the 95\\\\\\nHow can one possibly construct a confidence interval has the desired property, that it \\\\textit{covers} the true value with a specified probability, given that we don't know the true value? The procedure for building confidence intervals is called the Neyman Construction~\\\\cite{Neyman}, and it is based on `inverting' a series of hypothesis tests (as described in Sec.~\\\\ref{S:hypothesis test}). In particular, for each value of $\\\\vec\\\\alpha$ in the parameter space one performs a hypothesis test based on some test statistic where the null hypothesis is $\\\\vec\\\\alpha$. Note, that in this context, the null hypothesis is changing for each test and generally is not the background-only. If one wants a 95\\\\\\\\begin{equation}\\nI(\\\\data) = \\\\left\\\\{ \\\\vec\\\\alpha \\\\middle |\\\\, P(T(\\\\data)>k_\\\\alpha \\\\,|\\\\, \\\\vec\\\\alpha) < \\\\alpha \\\\right\\\\} \\\\;,\\n\\\\end{equation}\\nwhere the final $\\\\alpha$ and the subscript $k_\\\\alpha$ refer to the size of the test.\\nSince a hypothesis test with a size of 5\\\\\\\\begin{equation}\\n\\\\textrm{coverage}(\\\\vec\\\\alpha) = P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha) \\\\; .\\n\\\\end{equation}\\nThe equation above can easily be mis-interpreted as the probability the parameter is in a fixed interval $I$; but one must remember that in evaluating the probability above the data $\\\\data$, and, thus, the corresponding intervals produced by the procedure $I(\\\\data)$, are the random quantities. \\nNote, that coverage is a property that can be quantified for any procedure that produces the confidence intervals $I$. Intervals produced using the Neyman Construction procedure are said to ``cover by construction''; however, one can consider alternative procedures that may either under-cover or over-cover. Undercoverage means that \\\\mbox{$P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$} is smaller than desired and over-coverage means that $P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$ is larger than desired. Note that in general coverage depends on the assumed true value $\\\\vec\\\\alpha$.\\nSince one typically is only interested in forming confidence intervals on the parameters of interest, then one could use the supremum $p$-value of Eq.~\\\\ref{Eq:psup}. This procedure ensures that the coverage is at least the desired level, though for some values of $\\\\vec\\\\alpha$ it may over-cover (perhaps significantly). This procedure, which I call the `full construction', is also computationally very intensive when $\\\\vec\\\\alpha$ has many parameters as it require performing many hypothesis tests. In the naive approach where each $\\\\alpha_p$ is scanned in a regular grid, the number of parameter points tested grows exponentially in the number of parameters. There is an alternative approach, which I call the `profile construction'~\\\\cite{Feldman,Cranmer:2005hi}\\nand which statisticians call an `hybrid resampling technique'~ \\\\cite{Hybrid,Bodhi} that is approximate to the full construction, but typically has good coverage properties. We return to the procedures and properties for the different types of Neyman Constructions later.\\nFigure~\\\\ref{fig:NC_schematic} provides an overview of the classic Neyman construction corresponding to the left panel of Fig.~\\\\ref{fig:neyman}. The left panel of Fig.~\\\\ref{fig:neyman} is taken from the Feldman and Cousins's paper~\\\\cite{Feldman:1997qc} where the parameter of the model is denoted $\\\\mu$ instead of $\\\\theta$. For each value of the parameter $\\\\mu$, the acceptance region in $x$ is illustrated as a horizontal bar. Those regions are the ones that satisfy $T(\\\\data)<k_\\\\alpha$, and in the case of Feldman-Cousins the test statistic is the one of Eq.~\\\\ref{eq:tmu}. This presentation of the confidence belt works well for a simple model in which the data consists of a single measurement $\\\\data=\\\\{x\\\\}$. Once one has the confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$ simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the confidence belt.\\nUnfortunately, this convenient visualization doesn't generalize to complicated models with many channels or even a single channel marked Poisson model where $\\\\data=\\\\{x_1,\\\\dots,x_n\\\\}$. In those more complicated cases, the confidence belt can still be visualized where the observable $x$ is replaced with $T$, the test statistic itself. Thus, the boundary of the belt is given by $k_\\\\alpha$ vs. $\\\\mu$ as in the right panel of Figure~\\\\ref{fig:neyman}. The analog to the vertical line in the left panel is now a curve showing how the observed value of the test statistic depends on $\\\\mu$. The confidence interval still corresponds to the intersection of the observed test statistic curve and the confidence belt, which clearly satisfies $T(\\\\data)<k_\\\\alpha$. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\\nNote, a 95\\\\\\\\begin{equation}\\nP(\\\\alpha\\\\in I | \\\\data) = \\\\frac{ \\\\int_{I} \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha }{\\\\int \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha} \\\\;.\\n\\\\end{equation}\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Upper limits for event counting experiments}\\nThe simplest search for a new signal consists of counting the number of events\\npassing a specified selection.\\nThe number of selected events $n$ is distributed according to a Poissonian distribution\\nwhere the expected value, in case of presence of signal plus background ($H_1$)\\nis $s + b$, and for background only ($H_0$) is $b$.\\nAssume we count $n$ events, we then want to compare the two hypotheses $H_1$ and $H_0$.\\nAs simplest case, we can assume that $b$ is known with negligible uncertainty.\\nIf not, uncertainty on its estimate must be taken into account.\\nThe likelihood function for this case can be written as:\\n\\\\begin{equation}\\nL(n;s) = \\\\frac{(s+b)^n}{n!}e^{-(s+b)}\\\\,.\\n\\\\end{equation}\\n$H_0$ corresponds to the case $s=0$.\\nUsing the Bayesian approach, an upper limit\\n$s^{\\\\mathrm{up}}$ on $s$ can be determined by requiring that the posterior probability\\ncorresponding to the interval $[0,s^{\\\\mathrm{up}}]$ is equal to the confidence\\nlevel $1-\\\\alpha$:\\n\\\\begin{equation}\\n1-\\\\alpha = \\\\int_0^{s^{\\\\mathrm{up}}} P(s|n)\\\\,\\\\mathrm{d}s =\\n\\\\frac{\\n\\\\displaystyle\\\\int_0^{s\\\\mathrm{up}} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}{\\n\\\\displaystyle\\\\int_0^{+\\\\infty} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}\\\\,.\\n\\\\end{equation}\\nThe choice of a uniform prior, $\\\\pi(s)=1$, simplifies the computation and\\nEq.~(\\\\ref{eq:poisBayesUL}) reduces to~\\\\cite{Helene}:\\n\\\\begin{equation}\\n\\\\alpha = e^{-s^{\\\\mathrm{up}}}\\\\frac{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{(s^{\\\\mathrm{up}}+b)^m}{m!}\\n}{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{b^m}{m!}\\n}\\\\,.\\n\\\\end{equation}\\nUpper limits obtained with Eq.~(\\\\ref{eq:Helene}) are shown in Fig.~\\\\ref{fig:Helene}.\\nIn the case $b=0$, the results obtained in Eq.~(\\\\ref{eq:BayesianPoissonUL90}) and~(\\\\ref{eq:BayesianPoissonUL95}) are\\nagain recovered.\\nFrequentist upper limits for a counting experiment can be easily computed\\nin case of negligible background ($b=0$). If zero events are observed ($n=0$),\\nthe likelihood function simplifies to:\\n\\\\begin{equation}\\nL(n=0;s) = \\\\mathrm{Poiss}(0;s) = e^{-s}\\\\,.\\n\\\\end{equation}\\nThe inversion of the fully asymmetric Neyman belt reduces to:\\n\\\\begin{equation}\\nP(n\\\\le0;s^{\\\\mathrm{up}}) = P(n=0;s^{\\\\mathrm{up}}) = \\\\alpha \\\\implies s^{\\\\mathrm{up}} = -\\\\ln\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhich lead to results that are numerically identical to the Bayesian computation:\\n\\\\begin{eqnarray}\\ns & < & s^{\\\\mathrm{up}} = 2.303\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.1\\\\,\\\\text{(90\\\\ s & < & s^{\\\\mathrm{up}} = 2.996\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.05\\\\,\\\\text{(95\\\\\\\\end{eqnarray}\\nIn spite of the numerical coincidence, the interpretation of frequentist and Bayesian upper limits remain very different.\\nUpper limits from Eq.~(\\\\ref{eq:FreqPoissonUL90}) and~(\\\\ref{eq:FreqPoissonUL95})\\nanyway suffer from the flip-flopping problem and the coverage is spoiled when deciding to switch\\nfrom an upper limit to a central value depending on the observed significance level.\\nFeldman--Cousins intervals cure the flip-flopping issue and ensure the correct coverage\\n(or may overcover for discrete variables).\\nUpper limits at 90\\\\are reported in Fig.~\\\\ref{fig:fcUL}.\\nThe ``ripple'' structure is due to the discrete nature of Poissonian counting.\\nIt's evident from the figure that, even for $n = 0$, the upper limit decrease as $b$ increases (apart from ripple effects).\\nThis means that if two experiment are designed for an expected background of --say-- 0.1 and 0.01, the\\n``worse'' experiment (i.e.: the one which expects 0.1 events) achieves the best upper limit in case\\nno event is observed ($n=0$), which is the most likely outcome if no signal is present.\\nThis feature was noted in the 2001 edition of the PDG~\\\\cite{PDG2001}\\n\\\\begin{displayquote}\\n{\\\\it The intervals constructed according to the unified procedure [Feldman--Cousins] for a Poisson variable $n$ consisting\\nof signal and background have the property that for $n = 0$ observed events, the upper limit decreases for increasing\\nexpected background. This is counter-intuitive, since it is known that if $n = 0$ for the experiment in question,\\nthen no background was observed, and therefore one may argue that the expected background should not be relevant.\\nThe extent to which one should regard this feature as a drawback is a subject of some controversy.}\\n\\\\end{displayquote}\\nThis counter-intuitive feature of frequentist upper limits is one of the reasons that led to the use in High-Energy Physics of\\na modified approach, whose main feature is that is also prevents rejecting cases where the experiment has little sensitivity\\ndue to statistical fluctuation, as will be described in next Section.\\n\", \"\\\\section{Outline}\\nThis series of five lectures deals with practical aspects of statistical issues that arise in typical \\nHigh Energy Physics analyses. The topics are:\\n\\\\begin{itemize}\\n\\\\item{Introduction. This is largely a reminder of topics which you should have encountered as \\nundergraduates. Some of them are looked at in novel ways, and will hopefully provide new insights.}\\n\\\\item{Least Squares and Likelihoods. We deal with two different methods for parameter determination.\\nLeast Squares is also useful for Goodness of Fit testing, while likelihood ratios play a crucial role in\\nchoosing between two hypotheses.}\\n\\\\item{Bayes and Frequentism. These are two fundamental and very different approaches to statistical\\nsearches. They disagree even in their views on `What is probability?'}\\n\\\\item{Searches for New Physics. Many statistical issues arise in searches for New Physics. These may \\nresult in discovery claims, or alternatively in exclusion of theoretical models in some region of their\\nparameter space (e.g. mass ranges).} \\n\\\\item{Learning to love the covariance matrix. This is relevant for dealing with the possible correlations\\nbetween uncertainties on two or more quantities. The covariance matrix takes care of all these\\ncorrelations, so that you do not have to worry about each situation separately.\\nThis was an unscheduled lecture which was included at the request of several students.}\\n\\\\end{itemize}\\nLectures 3 to 5 are not included in these proceedings but can be found elsewhere~\\\\cite{Lecture3,Lecture4,Lecture5}. \\nThe material in these lectures follows loosely that in my book \\\\cite{LL_book}, together with some \\nsignificant updates (see ref. \\\\cite{LL_book_update}). \\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\nThe most important principle in this approach is that enunciated by \\nthe Polish statistician Jerzy Neyman in the 1930s, namely, \\n\\\\begin{quote}\\n\\\\textbf{The Frequentist Principle}\\nThe goal of a frequentist analysis is to construct statements so that a fraction $f \\\\geq p$ of\\nthem are guaranteed to be true over an infinite ensemble of statements.\\n\\\\end{quote}\\nThe fraction $f$ is called the \\\\textbf{coverage probability}, or coverage for short, and $p$ is called the \\\\textbf{confidence level} (C.L.). A procedure which satisfies the frequentist principle is said to \\\\emph{cover}.\\nThe confidence level as well as the coverage is a property of the\\nensemble of statements. Consequently, the confidence level may change if the ensemble changes. Here is an example of the frequentist principle in action.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nOver the course of a long career, a doctor sees thousands of patients. For each \\npatient he issues one of two conclusions: ``you are sick\" or ``you are well\" depending on the\\nresults of diagnostic measurements. Because he is a frequentist, he has devised\\nan approach to medicine in which although he does not know which of his conclusions\\nwere correct, he can at least retire happy in the knowledge that he was correct at least 75\\\\\\\\end{quote}\\n\\\\bigskip \\nIn a seminal paper published in 1937, Neyman~\\\\cite{Neyman37} \\ninvented the concept of the confidence interval, a way to quantify\\nuncertainty, that respects the frequentist principle. The confidence interval is such an important idea, and its\\nmeaning so different from the superficially similar Bayesian concept of a credible\\ninterval, that it is worth working through the concept in detail.\\n', '\\\\section{Upper Limits with RooFit}\\nDue to the computational complexity of calculating upper limits and experimental sensitivity, high-level statistical packages have been developed for the accurate definition of distribution functions, integration methods, and Monte Carlo generation. Among these, \\\\texttt{RooFit}, developed by Stanford University, stands out for its application in the CMS and ATLAS collaborations~\\\\cite{verkerke2006roofit,schott2012roostats}. Extensive documentation of these implementations can be found in the \\\\href{https://roostatsworkbook.readthedocs.io/en/latest/docs-cls_toys.html}{RooFit Workbook}. This document provides access to the creation of the \\\\texttt{RooFit} workspace, as well as the code to obtain upper limits using the profile binned likelihood method and the asymptotic approximation~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/tree/main/RooStats}{Source code}}. The asymptotic approximation is widely discussed in other sources~\\\\cite{cowan2014statistics,cowan2011asymptotic}, so the conceptual part was omitted in this report. Figure~[\\\\ref{fig:25}] shows the upper limits for the mass point $m(H) = 124 \\\\ \\\\text{GeV}$ using the profile likelihood method and the asymptotic approximation. Note the agreement in the values of both methods, demonstrating a significant discrepancy between observation and the background-only hypothesis~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/tree/main/RooStats}{Source code}}.\\n', \"\\\\section{Likelihood}\\nThe likelihood function is very widely used in many statistics applications. In this \\nSection, we consider it just for Parameter Determination. An important feature of the \\nlikelihood approach is that it can be used with {\\\\bf unbinned} data, and\\nhence can be applied in situations where there are not enough individual observations\\nto construct a histogram for the $\\\\chi^2$ approach. \\nWe start by assuming that we wish to fit our data $x$, using a model $f(x;\\\\mu)$ \\nwhich has one or more free parameters $\\\\mu$, whose value(s) we need to determine. \\nThe function $f$ is known as the `probability distribution' ($pdf$) and \\nspecifies the probability (or probability density, for the data having continuous as\\nopposed to discrete values) for obtaining different values of the data, when the parameter(s)\\nare specified. Without this \\nit is impossible to apply the likelihood (or many other) approaches. \\nFor example $x$ could be observations of a variable of interest within some \\nrange, and $f$ could be\\nany function such as a straight line, with gradient and intercept as parameters.\\nBut we will start with an angular distribution\\n\\\\begin{equation}\\ny(\\\\cos\\\\theta;\\\\beta) = \\\\frac{d\\\\ p}{d\\\\cos\\\\theta} = N(1+\\\\beta \\\\cos^2\\\\theta)\\n\\\\end{equation}\\nHere $\\\\theta$ is the angle at which a particle is observed, $dp/d\\\\cos\\\\theta$ is the $pdf$\\nspecifying the probability density for observing a decay at any $\\\\cos\\\\theta$, $\\\\beta$ is\\nthe parameter we want to determine, and $N$ is the crucial nomalisation factor \\nwhich ensures that the probability of observing a given decay at any $\\\\cos\\\\theta$\\nin the whole range from $-1$ to $+1$ is unity. In this case $N = 1/(2(1+\\\\beta/3))$. \\nThe data consists of $N$ decays, with their individual observations $\\\\cos\\\\theta_i$.\\nAssuming temporarily that the value of the parameter $\\\\beta$ is specified,\\nthe probability density $y_1$ of observing the first decay at $\\\\cos\\\\theta_1$ is\\n\\\\begin{equation}\\ny_1 = N (1+\\\\beta \\\\cos^2\\\\theta_1) = 0.5 (1+\\\\beta \\\\cos^2\\\\theta_1)/(1 + \\\\beta/3),\\n\\\\end{equation}\\nand similarly for the rest of the $N$ observations. Since the individual observations\\nare independent, the overall probability $P(\\\\beta)$ of observing the complete data set\\nof $N$ events is given by the product of the individual probabilities\\n\\\\begin{equation}\\nP(\\\\beta) = \\\\Pi y_i = \\\\Pi \\\\ 0.5 (1+\\\\beta \\\\cos^2\\\\theta_i)/(1 + \\\\beta/3) \\n\\\\end{equation}\\nWe imagine that this is computed for all values of the parameter $\\\\beta$; \\nthen this is known as the likelihood function ${\\\\it L}(\\\\beta)$.\\nThe likelihood method then takes as the estimate of $\\\\beta$ that value which \\nmaximises the likelihood. That is, it is the value which maximises (with respect to \\n$\\\\beta$) the probability density of observing the given data set. Conversely \\nwe rule out values of $\\\\beta$ for which ${\\\\it L}(\\\\beta)$ is very small. The\\nuncertainty on $\\\\beta$ is related to the width of the ${\\\\it L}(\\\\beta)$ \\ndistribution (see later). \\nIt is often convenient to consider the logarithm of the likelihood\\n\\\\begin{equation} \\n{\\\\it l} = \\\\ln{\\\\it L} = \\\\Sigma \\\\ln y_i\\n\\\\end{equation}\\nOne reason for this is that, for a large number of observations \\nsome fraction could have small $y_i$. Then the likelihood, involving the product of the\\n$y_i$, could be very small and may underflow the computer's range for real numbers.\\nIn contrast, {\\\\it l} involves a sum rather than a product, and $\\\\ln y_i$ rather than \\n$y_i$, and so produces a gentler number.\\n\\\\subsection{Likelihood and $pdf$}\\nThe procedure for constructing the likelihood is first to write down the $pdf$, and then to insert into that \\nexpression the observed data values in order to evaluate their product, which is the likelihood. Thus both \\nthe $pdf$ and the likelihood involve the data $x$ and the parameter(s) $\\\\mu$. The difference is that the $pdf$ is a function of $x$ for fixed values of $\\\\mu$, while the likelihood is a function of $\\\\mu$ given the fixed observed \\ndata $x_{obs}$. \\nThus for a Poisson distribution, the probability of observing $n$ events when the rate $\\\\mu$ is specified is \\n\\\\begin{equation}\\nP(n;\\\\mu) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $n$, while the likelihood is\\n\\\\begin{equation}\\nL(\\\\mu;n) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $\\\\mu$ for the fixed observed number $n$.\\n\\\\subsection{Intuitive example: Location and width of peak}\\nWe consider a \\nsituation where we are studying a resonant state which would result in a bump in the mass distribution of its decay particles.\\nWe assume that the bump can be parametrised as a simple Breit-Wigner\\n\\\\begin{equation}\\ny(m;M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nwhere $y$ is the probability density of obtaining a mass $m$ if the location and width the state are $M_0$ and $\\\\Gamma$,\\nthe parameters we want to determine. It is essential that $y$ is normalised, i.e. its integral over all physical values of \\n$m$ is unity; hence the normalisation factor of $\\\\Gamma/(2\\\\pi)$. The data consists of $n$ observations of $m$, as shown in fig. \\\\ref{fig:L_for_Resonance}.\\nAssume for the moment that we know $M_0$ and $\\\\Gamma$. Then the probability density for observing the $i^{th}$\\nevent with mass $m_i$ is\\n\\\\begin{equation}\\ny_i(M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nSince the events are independent, the probability density for observing the whole data sample is\\n\\\\begin{equation}\\ny_{all}(M_0,\\\\Gamma) =\\\\Pi \\\\ \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nand this is known as the likelihood $L(M_0,\\\\Gamma)$. Then the best values for the parameters are taken as\\nthe combination that maximises the probability density for the whole data sample i.e. $L(M_0,\\\\Gamma)$. \\nParameter values for which $L$ is very small compared to its maximum value are rejected, and the uncertainties \\non the parameters are related to the width of the distribution of $L$; we will be more specific later.\\nThe curve in\\nfig. \\\\ref{fig:L_for_Resonance}(left) shows the expected probability distribution for fixed parameter values. The way $L$ is calculated involves\\nmultiplying the heights of the curve at all the observed $m_i$ values. If we now consider varying $M_0$, this moves the curve bodily to the left or right without changing its shape or normalisation. So to determine the best value of $M_0$, we need to find where to locate the curve so that the product of the heights is a maximum; it is plausibe that the peak will be located where the majority of events are to be found.\\nNow we will consider how the optimum value of $\\\\Gamma$ is obtained. A small $\\\\Gamma$ results in a narrow curve, so the masses in the tail will make an even smaller contribution to the product in eqn. \\\\ref{product}, and hence reduce the likelihood. But a large $\\\\Gamma$ is not good, because not only is the width larger, but because of the normalisation condition, the peak height is reduced, and so the observations in the peak region make a smaller contribution to the likelihood. The optimal \\n$\\\\Gamma$ involves a trade-off between these two effects.\\nOf course, in finding the optimal of values of the two parameters, in general it is necessary to find the maximum of the \\nlikelihood as a function of the two parameters, rather than maximising with respect to just one, and then with respect to the other and then stopping (see section \\\\ref{More_variables}).\\n\\\\subsection{Uncertainty on parameter}\\nWith a large amount of data, the likelihood as a function of a parameter $\\\\mu$ is \\noften approximately Gaussian. In that case, ${\\\\it l}$ is an upturned parabola. Then\\nthe following definitions of $\\\\sigma_\\\\mu$, the uncertainty on $\\\\mu_{best}$, \\nyield identical answers:\\n\\\\begin{itemize}\\n\\\\item{The RMS of the likelihood distribution.}\\n\\\\item{[$-\\\\frac{d^2 {\\\\it l}}{d \\\\mu^2}]^{-1/2}$. If you remember that \\nthe second derivative of the log likelihood function is involved because it \\ncontrols the width of the ${\\\\it l}$ distribution, a mneumonic helps \\nyou remember the formula for $\\\\sigma_\\\\mu$: Since $\\\\sigma_\\\\mu$ must have the \\nsame units as $\\\\mu$, the second derivative must appear to the power $-1/2$. But because the\\nlog of the likelihood has a maximum, the second derivative is negative, so the minus \\nsign is necessary before we take the square root.}\\n\\\\item{It is the distance in $\\\\mu$ from the maximum in order to decrease ${\\\\it l}$ by half a unit\\nfrom its maximum value. i.e.\\n\\\\begin{equation}\\n{\\\\it l} (\\\\mu_{best} + \\\\sigma_{\\\\mu}) = {\\\\it l}_{max} - 0.5 \\n\\\\end{equation}\\n}\\n\\\\end{itemize} \\nIn situations where the likelihood is not Gaussian in shape, these three definitions no longer agree.\\nThe third one is most commonly used in that case. Now the upper and lower ends of the intervals can \\nbe asymmetric with respect to the central value. It is a mistake to believe that this method \\nprovides intervals which have a $68\\\\parameter\\\\footnote{Unfortunately, this incorrect statement occurs in my book\\\\cite{LL_book}. It is \\ncorrected in a separate update\\\\cite{LL_book_update}.}.\\nSymmetric uncertainties are easier to work with than asymmetric ones. It is thus sometimes better to quote the \\nuncertainty on a function of the first variable you think of. For example, for a charged particle in a magnetic field,\\nthe reciprocal of the momentum has a nearly symmetric uncertainty. Especially for high\\nmomentum tracks, the upper uncertainty on the momentum can be much larger than the lower one \\ne.g. $1.0\\\\ ^{+1.5}_{-0.4}$ TeV.\\n\\\\subsection{Coverage}\\nAn important feature of any statistical method for estimating a range for some parameter $\\\\mu$ at a \\nspecified confidence level $\\\\alpha$ is its coverage $C$. If the procedure is applied many times, \\nthese ranges will vary because of statistical fluctuations in the observed data. Then $C$ is defined as\\nthe fraction of ranges which contain the true value $\\\\mu_{true}$; it can vary with $\\\\mu_{true}$. \\nIt is very\\nimportant to realise that coverage is a property of the {\\\\bf statistical procedure} and does not apply\\nto your particular measurement. An ideal plot of coverage as a function of $\\\\mu$ would have $C$ constant \\nat its nominal value $\\\\alpha$. For a Poisson counting experiment, figure \\\\ref{fig:PoissonCoverage} shows $C$ as a \\nfunction of the Poisson parameter $\\\\mu$, when the observed number of counts $n$ is used to determine a range \\nfor $\\\\mu$ via the change in log-likelihood being 0.5. The coverage is far from constant at small $\\\\mu$.\\nIf $C$ is smaller than $\\\\alpha$, this is known as undercoverage. Certainly frequentists would regard this \\nas unfortunate; it means that people reading an article containing parameters determined this way are \\nlikely to place more than justified reliance on the quoted range. Methods using the Neyman construction \\nto determine parameter ranges by construction do not have undercoverage. \\nCoverage involves a statement about $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$. This is to be interpreted as a\\nprobability statement about how often the ranges $\\\\mu_l$ to $\\\\mu_u$ contain the (unknown but constant) true\\nvalue $\\\\mu_{true}$. This is a frequentist statement; Bayesians do not want to consider the ensemble of possible\\nresults if the measurement procedure were to be repeated. Thus Bayesians would regard the statement\\nabout $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$ as describing what fraction of their estimated \\nposterior probability density for $\\\\mu_{true}$ would be \\nbetween the fixed values $\\\\mu_l$ and $\\\\mu_u$, derived from their actual measurement.\\n\\\\subsection{More than one parameter}\\nFor the case of just one parameter $\\\\mu$, the likelihood best estimate $\\\\hat{\\\\mu}$ is given \\nby the value of $\\\\mu$ which maximises $L$. Its uncertainty $\\\\sigma_\\\\mu$ is determined either from \\n\\\\begin{equation}\\n1/\\\\sigma_\\\\mu^2 = -d^2\\\\ln L/d\\\\mu^2 ;\\n\\\\end{equation} \\nof by finding how far $\\\\hat{\\\\mu}$ would have to be changed in order to reduce $\\\\ln L$ by 0.5.\\nWhen we have two or more parameters $\\\\beta_i$ the rule for finding the best estimates $\\\\hat{\\\\beta_i}$\\nis still to maximise $L$.\\nFor the uncertainties and their correlations, the generalisation of equation \\\\ref{error} is to construct\\nthe inverse covariance matrix ${\\\\bf M}$, whose elements are given by\\n\\\\begin{equation}\\nM_{ij} = -\\\\frac{\\\\partial^2 \\\\ln L} {\\\\partial \\\\beta_i\\\\ \\\\partial \\\\beta_j} \\n\\\\end{equation} \\nThen the inverse of $\\\\bf{M}$ is the covariance matrix, whose diagonal elements are the variances of $\\\\beta_i$,\\nand whose off-diagonal ones are the covariances.\\nAlternatively (and more common in practice), the uncertainty on a specific $\\\\beta_j$ can be obtained \\nby using the profile likelihood $L_{prof}(\\\\beta_j)$.\\nThis is the likelihood as a function of the specific $\\\\beta_j$, where for each value of $\\\\beta_j,\\\\ L$ has been remaximised\\nwith respect to all the other $\\\\beta$. Then $L_{prof}(\\\\beta_j)$ is used with the `reduce $\\\\ln L_{prof}$ = 0.5' rule\\nto obtain the uncertainty on $\\\\beta_j$. This is equivalent to determining the contour in $\\\\beta$-space where \\n$\\\\ln L = \\\\ln L_{max} - 0.5$, and finding the values $\\\\beta_{j,1}$ and $\\\\beta_{j,2}$ on the contour which are \\nfurthest from $\\\\hat{\\\\beta_j.}$ Then the (probably asymmetric) upper and lower uncertainties on $\\\\beta_j$ are \\ngiven by $\\\\beta_{j,2}- \\\\hat{\\\\beta_j}$ and $\\\\hat{\\\\beta_j} - \\\\beta_{j,1}$ respectively.\\nBecause these are likelihood methods of obtaining the intervals, these estimates of uncertainities provide only\\n{\\\\bf nominal} regions of 68\\\\the region within \\nthe contour described in the previous paragraph for the multidimensional $\\\\beta$ space will have less than 68\\\\overage. To achieve that, the $`0.5'$ in the rule for how much $\\\\ln L$ has to be reduced from its maximum \\nmust be replaced by a larger number, whose value depends on the dimensionality of $\\\\beta$.\\n\", \"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{The Matrix Element Method}\\nIdeally, one would not use a single discriminating variable to distinguish the process of interest from the other background processes, but instead would use as much discriminating power as possible. This implies forming a probability model over a multi-dimensional discriminating variable (ie. a multivariate analysis technique). In principle, both the histogram-based and kernel-based approach generalize to distributions of multi-dimensional discriminating variables; however, in practice, they are limited to only a few dimensions. In the case of histograms this is particularly severe unless one employs clever binning choices, while in the kernel-based approach one can model up to about 5-dimensional distributions with reasonable Monte Carlo sample sizes. In practice, one often uses multivariate algorithms like Neural Networks or boosted decision trees\\\\footnote{A useful toolkit for high-energy physics is TMVA, which is packaged with ROOT~\\\\cite{tmva}.} to map the multiple variables into a single discriminating variable. Often these multivariate techniques are seen as somewhat of a black-box. If we restrict ourselves to discriminating variables associated with the kinematics of final state particles (as opposed to the more detailed signature of particles in the detector), then we can often approximate he detailed simulation of the detector with a parametrized detector response. If we denote the kinematic configuration of all the final state particles in the Lorentz invariant phase space as $\\\\Phi$, the initial state as $i$, the matrix element (potentially averaged over unmeasured spin configurations) as $\\\\mathcal{M}(i,\\\\Phi)$, and the probability due to parton density functions for the initial state $i$ going into the hard scattering as $f(i)$, then we can write that the distribution of the, possibly multi-dimensional, discriminating variable $x$ as\\n\\\\begin{equation}\\nf(x) \\\\propto \\\\int d\\\\Phi \\\\, f(i) |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) \\\\;,\\n\\\\end{equation}\\nwhere $W(x|\\\\Phi)$ is referred to as the transfer function of $x$ given the final state configuration $\\\\Phi$. It is natural to think of $W(x|\\\\Phi)$ as a conditional distribution, but here I let $W$ encode the efficiency and acceptance so that we have\\n\\\\begin{equation}\\n\\\\frac{\\\\sigma_{\\\\rm eff.}}{\\\\sigma} = \\\\frac{\\\\int dx \\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) }{\\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 }\\\\;.\\n\\\\end{equation}\\nOtherwise, the equation above looks like another application one Bayes's theorem where $W(x|\\\\Phi)$ plays the role of the pdf/likelihood function and $\\\\mathcal{M}(i,\\\\Phi)$ plays the role of the prior over the $\\\\Phi$. It is worth pointing out that this is a frequentist use of Bayes's theorem since $d\\\\Phi$ is the Lorentz invariant phase space which explicitly has a measure associated with it.\\n\", \"\\\\section{Upper Limits using the profile binned likelihood}\\nThe profile binned likelihood method is chosen by particle physics collaborations to present phenomenology studies and experimental analyses. As shown above, this method is based on estimating the signal confidence level through a fully frequentist approach focused on parameter estimation~\\\\cite{lista2016practical,barlow2002systematic}. To include systematic effects, the likelihood function is extended with appropriate distribution functions to describe efficiency effects with a given width \\\\( \\\\sigma \\\\). This methodology requires maximizing the likelihood function and obtaining the background profile as a function of the signal strength \\\\( \\\\mu \\\\). The improvement lies in replacing the normalization of the posterior distribution with a multivariable optimization problem, which is computationally less costly and leads to limits that allow for better model exclusion. The statistical estimator is given by~\\\\cite{conway2005calculation,cms2012observation,atlas2012observation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = - 2 ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu, \\\\hat{\\\\hat{b}}(\\\\mu) )}{ \\\\mathcal{L}(\\\\hat{\\\\mu},\\\\hat{b}) } \\\\bigg).\\n\\\\end{equation}\\nwhere \\\\( \\\\hat{\\\\mu} \\\\) and \\\\( \\\\hat{b} \\\\) are the unconditional maximum likelihood estimators, and \\\\( \\\\hat{\\\\hat{b}}(\\\\mu) \\\\) is the conditional maximum likelihood estimator. Frequentist limits are generally less restrictive and allow for exploring regions of significance while adequately accounting for systematic effects. Similar to the Bayesian approach, maximizing the likelihood as a function of \\\\( \\\\mu \\\\) enables finding the profile likelihood that propagates the effect of the background parameters \\\\( \\\\epsilon \\\\).\\nSystematic effects lead to higher upper limits, which restrict the model exclusion power and experimental sensitivity. For example, in a single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), the \\\\texttt{optimize} package is used to find the best-fit parameters, and Monte Carlo methods are applied to sample the estimator \\\\( q_{\\\\mu} \\\\). The extended likelihood function is given by:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu, \\\\epsilon) = \\\\frac{ e^{ -(\\\\mu s + \\\\epsilon b) } (\\\\mu s + \\\\epsilon b)^{n} }{n!} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^{2}} } e^{ -\\\\frac{(1-\\\\epsilon)^{2}}{2\\\\sigma^{2}} },\\n\\\\end{equation}\\nwhere maximizing the likelihood function for the efficiency \\\\( \\\\epsilon \\\\) yields the conditional nuisance estimator \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\):\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) = \\\\frac{1}{2b} \\\\bigg[ ( b -\\\\mu s - \\\\sigma^{2}b^{2}) + \\\\sqrt{ (b + \\\\mu s - \\\\sigma^{2}b^{2})^{2} + 4 b^{2} \\\\sigma^{2}n} \\\\bigg]. \\n\\\\end{equation}\\nFor the single-channel experiment, Figure~[\\\\ref{fig:22}] shows the profile of the nuisance parameter for various values of \\\\( \\\\mu \\\\). The maximum value of the likelihood function shifts depending on the signal strength \\\\( \\\\mu \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/ProfileLikelihood/ProfileLikelihoodNuissance.ipynb}{Source code}}. The right plot shows the maximum likelihood estimate \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\) using both Equation~(\\\\ref{eq:profile}) and the optimization package. This result indicates the maximal behavior of the uncertainty for each value of the signal strength \\\\( \\\\mu \\\\), ensuring model exclusion as restrictive as allowed by the uncertainty \\\\( \\\\sigma \\\\). More generally, the multi-channel case requires a fully numerical procedure to find the profiles of the nuisance parameters and \\\\( q_{\\\\mu} \\\\)~\\\\cite{lista2016practical,cranmer2015practical}.\\nTo illustrate the behavior of the estimator \\\\( q_{\\\\mu} \\\\) as a function of the systematic uncertainty \\\\( \\\\sigma \\\\), a sweep over the signal strength \\\\( \\\\mu \\\\) is performed while optimizing the estimator for the current values of the observation, background component, and signal events. Figure~[\\\\ref{fig:23}] shows the profile binned likelihood for the single-channel experiment as a function of the width of the systematic uncertainty. A larger uncertainty leads to a higher upper limit, which can be quantified using Wald's approximation \\\\( Z(3\\\\sigma) = \\\\sqrt{q_{\\\\mu}} \\\\)~\\\\cite{cowan2011asymptotic}. \\nThe green dashed line represents the observed upper limit for each profile likelihood. For example, for \\\\( \\\\sigma = 0.20 \\\\), the observed upper limit is \\\\( \\\\mu_{up} \\\\approx 4.5 \\\\), which contrasts with the value obtained from the Bayesian approach (Table~[\\\\ref{tb:3}], \\\\( \\\\mu_{up} = 4.91 \\\\)), a smaller value. The right plot represents the search for the p-value while fixing the value of the systematic uncertainty. Obtaining the pseudo-data requires generating an observation consistent with the background-only hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\), and for the signal + background hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\mu s + \\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\) in each observable channel. For each generated value of \\\\( n \\\\) across all channels, the optimization of \\\\( (\\\\hat{\\\\mu}, \\\\hat{\\\\epsilon}, \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu)) \\\\) is performed.\\nTable~[\\\\ref{tb:4}] shows the upper limit values using the grouped profile likelihood method for several values of \\\\( \\\\sigma \\\\). In comparison with the upper limits obtained by the Bayesian method, consistency is observed within the statistical confidence levels inherent to the sampling.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\n$\\\\sigma$ & Profile likelihood Ratio & MCMC algorithm \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{2}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 \\\\\\\\\\n0.10 & 3.52 & 3.31 \\\\\\\\\\n0.20 & 4.57 & 4.66 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nFor the multi-channel case, results are very close between the upper limits without uncertainty and those found using the profile likelihood method. This behavior is attributed to the combined uncertainty of the background across the 30 channels, which does not significantly affect the confidence in the signal strength, especially in the resonance region~\\\\cite{cms2022portrait,atlas2012observation,cowan2011asymptotic}. Table~[\\\\ref{tb:5}] shows the upper limit values for several mass points \\\\( m(H) \\\\) and uncertainty \\\\( \\\\sigma \\\\).\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\nMass($H$)[GeV] & $\\\\sigma$ & Method: Profile likelihood ratio \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & $\\\\mu_{up}(95\\\\ 110 & 0.1 & 0.43 \\\\\\\\\\n110 & 0.2 & 0.45 \\\\\\\\\\n\\\\hline\\n124 & 0.1 & 1.45 \\\\\\\\\\n124 & 0.2 & 1.46 \\\\\\\\\\n\\\\hline\\n142 & 0.1 & 0.29 \\\\\\\\\\n142 & 0.2 & 0.31 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength for different mass points using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\n\\\\subsection{Experimental sensitivity with systematic effects}\\nIn the case of determining the experimental sensitivity for a specific model $s$, the Asimov data $n = s + b$ are used, and a modification is applied to the statistical estimator $q_{\\\\mu}$~\\\\cite{lista2016practical}.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nIn the Wald approximation, the significance is approximately given by:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\sqrt{q_{0}}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:24}] shows the profile likelihood for various values of the systematic uncertainty $\\\\sigma$. It demonstrates how statistical significance is impacted by uncertainty. In general, greater uncertainty in the estimation of background events leads to a loss of sensitivity in a potential experimental analysis aiming to validate a new hypothetical model.\\nThere are alternative methods to establish statistical significance~\\\\cite{cowan2011asymptotic}, such as:\\n\\\\begin{equation}\\nZ_{0}(\\\\sigma) = \\\\frac{s}{\\\\sqrt{s+(1+\\\\sigma)b}}. \\n\\\\end{equation}\\nHowever, in general, experimental sensitivity is overestimated because the maximal information of $\\\\hat{\\\\hat{b}}$ is not fully captured in the profile likelihood. Table~[\\\\ref{tb:6}] shows the statistical significance for various values of systematic uncertainty $\\\\sigma$. This effect reduces experimental sensitivity and should be calculated using the profile binned likelihood method. Note that the approximation $Z_{0} = s / \\\\sqrt{s + b}$ is no longer valid due to the convolution effect of counting and efficiency distributions.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & $Z$ & $Z_{0}=s/\\\\sqrt{s+b}$ & $Z_{0}(\\\\sigma)$ \\\\\\\\\\n\\\\hline\\n0.05 & 0.878 & 0.932 & 0.931 \\\\\\\\\\n0.1 & 0.695 & 0.932 & 0.912 \\\\\\\\\\n0.2 & 0.443 & 0.932 & 0.877 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Statistical significance as a function of systematic uncertainty for the case of a single-channel experiment. Note how the approximation $Z_{0}$ becomes invalid even for $s \\\\ll b$.}\\n\\\\end{center}\\n\\\\end{table}\\nAs shown in Table~[\\\\ref{tb:6}], when there is a 20\\\\\\n\", \"\\\\section{Errors}\\n\\\\subsection{Asymmetric errors}\\nSo what happens if you plot the likelihood function and it is not symmetric like \\nFig.~\\\\ref{fig:ML} but looks more like Fig.~\\\\ref{fig:asym}?\\nThis \\narises in many cases when numbers are small. For instance, in a simple Poisson count suppose you observe one event. $P(1;\\\\lambda)=\\\\lambda e^{-\\\\lambda}$ is not symmetric: $\\\\lambda=1.5$ is more likely to fluctuate down to 1 than $\\\\lambda=0.5$ is to\\nfluctuate up to 1.\\nYou can read off $\\\\sigma_+$ and $\\\\sigma_-$ from the two $\\\\Delta \\\\ln L=-{1 \\\\over 2}$ crossings, but they are different.\\nThe result can then be given as $a^{+\\\\sigma_+}_{-\\\\sigma_-}$. What happens after that?\\nThe first advice is to avoid this if possible. \\nIf you get $\\\\hat a=4.56$ with $\\\\sigma_+=1.61, \\\\sigma_-=1.59$ \\nthen quote this as $4.6 \\\\pm 1.6$ rather than $4.56^{+1.61}_{-1.59}$.\\nThose extra significant digits have no real meaning. If you can convince yourself that the difference between\\n$\\\\sigma_+$ and $\\\\sigma_-$ is small enough to be ignored then you should do so, as the alternative brings in a whole \\nlot of trouble and it's not worth it.\\nBut there will be some cases where the difference is too great to be swept away, so\\nlet's consider that case.\\nThere are two problems that arise: combination of measurements and combination of errors.\\n\\\\subsubsection{Combination of measurements with asymmetric errors}\\nSuppose you have two measurements of the same parameter $a$: $\\\\hat {a_1}^{+\\\\sigma^+_1}_{-\\\\sigma^-_1}$\\nand $\\\\hat {a_2}^{+\\\\sigma^+_2}_{-\\\\sigma^-_2}$ and you want to combine them to give the best estimate and, of course, its error. For symmetric errors the answer is well established to be\\n$\\\\hat a = {\\\\hat a_1/\\\\sigma_1^2 + \\\\hat a_2/\\\\sigma_2^2 \\\\over 1/\\\\sigma_1^2 + 1/\\\\sigma_2^2}$.\\nIf you know the likelihood functions, you can do it. The joint likelihood is just the sum.\\nThis is shown in Fig.~\\\\ref{fig:combine1} where \\nthe\\nred and green curves are measurements of $a$. \\nThe log likelihood functions just add (blue), from which the peak is found and the $\\\\Delta \\\\ln L=-\\\\half$ errors read off.\\nBut you don't know the full likelihood function: just 3 points (and that it had a maximum at the second).\\nThere are, of course, an infinite number of curves that could be drawn, and several models have been tried (cubics, constrained quartic...) on likely instances---see Ref.~\\\\cite{asym} for details. Some do better than others.\\nThe two most plausible are\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2}\\\\left( {a-\\\\hat a \\\\over \\\\sigma+\\\\sigma'(a-\\\\hat a)}\\\\right)^2\\n\\\\quad \\\\text{and}\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2} {\\\\left( a-\\\\hat a \\\\right)^2 \\\\over V+V'(a-\\\\hat a)}\\n\\\\quad.\\n\\\\end{equation}\\nThese are similar to the Gaussian parabola, but the denominator is not constant. It varies with the value of $a$, being linear either in the standard deviation or in the variance.\\nBoth are pretty good. The first does better with errors on $\\\\log a$ (which are asymmetric if $a$ is symmetric: such asymmetric error bars are often seen on plots where the $y$ axis is logarithmic), the\\nsecond does better with Poisson measurements.\\nFrom the 3 numbers given one readily obtains\\n\\\\begin{equation}\\n\\\\sigma={2 \\\\sigma^+\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-} \\\\qquad \\n\\\\sigma'={\\\\sigma^+-\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-}\\n\\\\end{equation}\\nor, if preferred\\n\\\\begin{equation}\\nV= \\\\sigma^+\\\\sigma^- \\\\qquad\\nV'=\\\\sigma^+-\\\\sigma^- \\n\\\\quad.\\n\\\\end{equation}\\nFrom the total likelihood you then find the maximum of sum, numerically, and the $\\\\Delta \\\\ln L=-{1\\\\over 2}$ points.\\nCode for doing this is available on GitHub\\\\footnote{\\\\url{ https://github.com/RogerJBarlow/Asymmetric-Errors}} in\\nboth R and Root.\\nAn example is shown in Fig.~\\\\ref{fig:asymex}. Combining $1.9^{+0.7}_{-0.5}$, $2.4^{+0.6}_{-0.8}$ and $3.1^{+0.5}_{-0.4}$ gives $2.76 ^{+0.29}_{-0.27} \\\\ .$\\n\\\\subsubsection{Combination of errors for asymmetric errors}\\nFor symmetric errors, given $x \\\\pm \\\\sigma_x, y \\\\pm \\\\sigma_y$, (and $\\\\rho_{xy}=0$) the error on \\n$f(x,y)$\\nis the sum in quadrature: $\\\\sigma_f^2 =\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2 + \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2\\n\\\\sigma_y^2$.\\nWhat is the equivalent for the error on $f(x,y)$ when the errors are asymmetric,\\n$x ^{+\\\\sigma^+_x}_{-\\\\sigma^-_x}, y^{+\\\\sigma^+_y}_{-\\\\sigma^-_y}$? Such a problem arises frequently at the end of an analysis when the systematic errors from various sources are all combined.\\nThe standard procedure---which you will see done, though it has not, to my knowledge, been written down anywhere---is to add the positive and negative errors\\nin quadrature separately: ${\\\\sigma^+_f}^2={\\\\sigma^+_x}^2+{\\\\sigma^+_y}^2$,\\\\quad \\n${\\\\sigma^-_f}^2={\\\\sigma^-_x}^2+{\\\\sigma^-_y}^2$.\\nThis looks plausible, but it is \\n{\\\\em manifestly wrong} as it breaks the central limit theorem.\\nTo see this, suppose you have to average \\n$N$ i.i.d. variables each with the same errors which are asymmetric: $\\\\sigma^+ = 2 \\\\sigma^-$ . \\nThe \\nstandard procedure reduces both $\\\\sigma^+$ and $\\\\sigma^-$ by a factor $1/\\\\sqrt N$, but the skewness remains.\\nThe positive error is twice the negative error. This is therefore not Gaussian, and never will be, even as $N \\\\to \\\\infty$.\\nYou can see what's happening by considering the combination of two of these measurements. They both may fluctuate upwards, or they both may fluctuate downwards, and yes, the upward fluctuation will be, on average, twice as big. But there is a 50\\\\ chance of one upward and one downward fluctuation, which is not considered in the standard procedure.\\nFor simplicity we write $z_i={\\\\partial f \\\\over \\\\partial x_i} (x_i-x^0_i)$, the deviation of the parameter \\nfrom its nominal value, scaled by the differential. The individual likelihoods are again parametrized\\nas Gaussian with a linear dependence of the standard deviation or of the variance, giving\\n\\\\begin{equation}\\n\\\\ln L(\\\\vec z)=- \\\\half \\\\sum_i \\\\left( { z_i \\\\over \\\\sigma_i + \\\\sigma'_i z_i}\\\\right)^2 \\\\qquad {\\\\rm or}\\\\qquad\\n- \\\\half \\\\sum_i { z_i ^2\\\\over V_i + V'_i z_i}\\n\\\\quad,\\n\\\\end{equation}\\nwhere $\\\\sigma, \\\\sigma', V,V'$ are obtained from Eqs.~\\\\ref{eq:asyms} or \\\\ref{eq:asymV}.\\nThe $z_i$ are nuisance parameters (as described later) and can be removed by profiling. \\nLet $u=\\\\sum z_i$ be the total deviation in the quoted $f$ arising from the individual deviations.\\nWe form $\\\\hat L(u)$ as the maximum of $L(\\\\vec z)$ subject to the constraint $\\\\sum_i z_i=u$.\\nThe method of undetermined multipliers readily gives the solution\\n\\\\begin{equation}\\nz_i=u {w_i \\\\over \\\\sum_j w_j}\\n\\\\quad,\\n\\\\end{equation}\\nwhere\\n\\\\begin{equation}\\nw_i = {(\\\\sigma_i + \\\\sigma'_i z_i)^3 \\\\over 2 \\\\sigma_i}\\n\\\\qquad {\\\\rm or } \\\\qquad\\n{(V_i+V'_i z_i)^2 \\\\over 2V_i + V'_i z_i}\\n\\\\quad.\\n\\\\end{equation}\\nThe equations are nonlinear, but can be solved iteratively. At $u=0$ all the $z_i$ are zero. Increasing (or decreasing) $u$ in small steps, Eqs.~\\\\ref{eq:comsol1} and \\\\ref{eq:comsol2} are applied successively to give the $z_i$ and the $w_i$: convergence is rapid. The value of $u$ which maximises the likelihood should in principle be applied as a correction to the quoted result.\\nPrograms to do this are also available on the GitHub site.\\nAs an example, consider a counting experiment with a number of backgrounds, each determined by an ancillary Poisson experiment, and that for simplicity each background was determined by running the apparatus for the same time as the actual experiment. (In practice this is unlikely, but scale factors\\ncan easily be added.)\\nSuppose two backgrounds are measured, one giving four events and the other five. These would be reported, using $\\\\Delta ln L=-\\\\half$ errors, as $4^{+2.346}_{-1.682}$ and $5^{+2.581}_{-1.916}$.\\nThe method, using linear $V$, gives the combined error on the background count as ${}^{+3.333}_{-2.668}$.\\nIn this simple case we can check the result against the total background count of nine events, which has errors ${}^{+3.342}_{-2.676}$. The agreement is impressive. Further examples of the same total, partitioned differently, are shown in table~\\\\ref{tab:asymcom}.\\n\\\\begin{table}[h]\\n\\\\begin{centering}\\n\\\\begin{tabular}{ l c c c c }\\nInputs & \\\\multicolumn {2} {c} {Linear $\\\\sigma$}& \\\\multicolumn {2} {c} {Linear $V$}\\\\\\\\\\n& $\\\\sigma^-$ & $\\\\sigma^+$ & $\\\\sigma^-$ & $\\\\sigma^+$ \\\\\\\\\\n\\\\hline\\n4+5 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+6 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+3+3 & 2.630 & 3.278 & 2.659 & 3.323 \\\\\\\\\\n1+1+1+1+1+1+1+1+1 & 2.500 & 3.098 & 2.610 & 3.270 \\\\\\\\\\n\\\\end{tabular}\\n\\\\caption{ Various combinations of Poisson errors. The target value is $\\\\sigma^-=2.676$, $\\\\sigma^+=3.342$}\\n\\\\end{centering}\\n\\\\end{table}\\n\", '\\\\section{Lecture 3: The Bayesian Approach}\\n\\\\subsection{The Top Quark Discovery: A Bayesian Analysis}\\nIn this section we shall perform the following calculations as a way to illustrate a typical\\nBayesian analysis,\\n\\\\begin{enumerate}\\n\\\\item compute the posterior density $p(s | D)$,\\n\\\\item compute a 68\\\\ \\\\item compute the global Bayes factor $B_{10} = p(D | H_1) / p(D | H_0)$.\\n\\\\end{enumerate}\\n\\\\subsubsection*{Probability model}\\nThe first step in any serious statistical analysis is to think deeply about what has been done in\\nthe physics analysis; for example, to trace in detail the steps that led to the background estimates, determine the independent systematic effects and identify explicitly what is known about them. Although, by tradition, we tend to think of potential data $x$ separately from the parameters $s$ and $b$, it should be recognized that this is done for convenience. The full probability model is the joint\\nprobability\\n\\\\begin{align*}\\np(x, s, b | I),\\n\\\\end{align*}\\nwhich, as is true of \\\\emph{all} probability models, is conditional on the information and assumptions, $I$, that define the abstract space $\\\\Omega$ (see Sec.~\\\\ref{sec:prob}). \\nIn these lectures, we have\\nomitted the conditioning data $I$, and will continue to do so here, but it should not\\nbe forgotten that it is always present and may differ from one probability model to another.\\nThe full probability model $p(x, s, b)$ can be factorized is several ways, all of which are\\nmathematically valid. However, we find it convenient to factorize the model in the following way\\n\\\\begin{align}\\np(x, s, b) = p(x | s, b) \\\\, \\\\pi(s, b),\\n\\\\end{align}\\nwhere we have introduced the symbol $\\\\pi$ in order to highlight the distinction we choose\\nto make between this\\npart of the model and the remainder. We are entirely free to decide how much of the model\\nwe place in $p(x | s, b)$ and how much in $\\\\pi(s, b)$; what matters is the form of the\\nfull model $p(x, s, b)$. In\\nthe frequentist analysis of the top quark discovery data, we took $N$ and $B$ to be the data $D$. We did so because in the frequentist approach, the function $\\\\pi(s, b)$ does not exist and consequently we have no choice but to include everything in the function $p(x| s, b)$. \\nOne virtue of a Bayesian perspective is that we are not bound by this stricture. To make the\\npoint explicitily, we take the probability distribution, $p(x | s, b)$, to be\\n\\\\begin{align}\\np(x|s, b) = \\\\textrm{Poisson}(x, s + b).\\n\\\\end{align}\\nThe interpretation\\nof $p(x | s, b)$ is clear: it is the probability to observe $x$ events \\\\emph{given} that the mean event count is $s + b$. \\nWhat does $\\\\pi(s, b)$ represent? This function is the \\\\textbf{prior} that encodes what we \\\\emph{know}, or \\\\emph{assume}, about the mean background and signal independently\\nof the potential observations $x$. The prior $\\\\pi(s, b)$ can be factored in two ways,\\n\\\\begin{align}\\n\\\\pi(s , b) & = \\\\pi(s | b ) \\\\, \\\\pi(b), \\\\nonumber \\\\\\\\ \\n& = \\\\pi(b | s ) \\\\, \\\\pi(s),\\n\\\\end{align}\\nboth of which accord with the probability rules. The factorizations remind us that the parameters\\n$s$ and $b$ may not be probabilistically independent. However, we shall assume that they are, at least at this stage of the analysis, in which case it is permissible to write,\\n\\\\begin{align}\\n\\\\pi(s , b) & = \\\\pi(s) \\\\, \\\\pi(b).\\n\\\\end{align}\\nWe first consider the background prior $\\\\pi(b)$ and ask: what do we know about the background? \\nWe know the count $Q$ in the control region and \\nwe have an estimate of the\\ncontrol region to\\nsignal region scale factor $k$. The likelihood for $Q$ is taken to be \\n\\\\begin{align}\\np(Q | k, b) = \\\\textrm{Poisson}(Q, k b),\\n\\\\end{align}\\nfrom which, together with a prior $\\\\pi(k, b)$, we can compute the posterior density\\n\\\\begin{align}\\np(b | Q, k) = p(Q | k, b) \\\\, \\\\pi(k, b) / p(Q).\\n\\\\end{align}\\nAs usual, we factorize the prior, $\\\\pi(k, b) = \\\\pi(k|b) \\\\pi_0(b) $,\\nwhere we have introduced the subscript $0$ to distinguish $\\\\pi_0(b)$ from the background prior\\nassociated with Eq.~(\\\\ref{eq:pxsb}). Then, we consider the separate factors $\\\\pi_0(b)$\\nand $\\\\pi(k | b)$.\\nWhat do we know about $b$ at this stage?\\nClearly, $b \\\\geq 0$. But, that is all we know apart from the background likelihood, Eq.~(\\\\ref{eq:pQkb}). \\nToday, after a century of\\nargument and discussion, the consensus amongst statisticians is that there is no\\nunique way to represent such vague information. However, \\nwell founded ways to construct such priors are available, see for example Ref.~\\\\cite{Demortier:2010sn}\\nand references therein; but for simplicity we take the prior $\\\\pi_0(b) = 1$, that is, the \\\\textbf{flat prior}. If the uncertainty in $k$ can be neglected, the (proper!) prior for $k$ is $\\\\pi(k|b) = \\\\delta(k - Q/B)$, which amounts to replacing $k$ in Eq.~(\\\\ref{eq:pbQk}) by $Q/B$. When the dust\\nsettles, we find\\n\\\\begin{align}\\np(b | Q, k) = \\\\textrm{Gamma}(k b, 1, Q+1) = \\\\frac{e^{-k b} (k b)^Q} {\\\\Gamma(Q+1)},\\n\\\\end{align}\\nfor the posterior density of $b$, \\nwhich can serve as the prior $\\\\pi(b)$ associated with Eq.~(\\\\ref{eq:pxsb}).\\nBy construction, $p(x, s, b)$ is identical in form to the likelihood in Eq.~(\\\\ref{eq:toplh}); we have \\nsimply availed ourselves of the freedom to factorize $p(x, s, b)$ as we wish and therefore to\\nreinterpret the factors. This freedom is useful because it makes it possible to keep the\\nlikelihood simple while relegating the complexity to the prior. This may not seem, at first, to be terribly helpful; after all, we arrived at the same mathematical form as Eq.~(\\\\ref{eq:toplh}). However, the complexity can be substantially mitigated through the numerical treatment of the prior, as discussed at the end of the next section. The likelihood, as we have conceptualized the problem, is given by \\n\\\\begin{align}\\np(D| s, b) = \\\\frac{e^{-(s+b)} (s + b)^D}{D!},\\n\\\\end{align}\\nwhere $D = 17$ events. \\nThe final ingredient is the prior $\\\\pi(s)$. At this stage, all we know is that $s \\\\geq 0$. Again,\\nthere is no unique way to specify $\\\\pi(s)$, though as noted there are well founded methods to\\nconstruct it. We shall variously assume either the improper prior $\\\\pi(s) = 1$ or the proper prior $\\\\pi(s) = \\\\delta(s - 14)$. \\n\\\\subsubsection*{Marginal likelihood}\\nAfter this somewhat discursive discussion of the probability model, we have done the hard part: building the full probability model. Hereafter, the rest of the Bayesian\\nanalysis is mere computation.\\nIt is convenient to eliminate the nuisance parameter $b$,\\n\\\\begin{align}\\np(D | s, H_1) & = \\\\int_0^\\\\infty p(D | s, b ) \\\\, \\\\pi(b ) d(k b),\\\\nonumber\\\\\\\\\\n& = \\\\frac{1}{Q} (1- x)^2 \\\\sum_{r=0}^N \\\\textrm{Beta}(x, r+1, Q) \\\\, \\n\\\\textrm{Poisson}(N - r| s ),\\\\\\\\\\n\\\\textrm{where } x & = 1/(1+k), \\\\nonumber\\\\\\\\ \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 10:} Show this} \\\\nonumber\\n\\\\end{align}\\nand thereby arrive at the marginal likelihood $p(D | s, H_1)$. This example, the \\\\textbf{Poisson-gamma} model is particularly simple and lends itself to exact calculation. However, the complexity rapidly increases as the prior\\nbecomes more and more complicated. In the probability model that is used in the Higgs boson analyses at the LHC, the part we would consider the prior, $\\\\pi(\\\\mu, m_H, \\\\omega)$, is of enormous complexity. However, the part that we would call the likelihood, $p(D|\\\\mu, m_H, \\\\omega)$, is relatively simple. The parameter $\\\\mu$ denotes one or more signal strengths --- the ratio of the cross section times branching fraction to that predicted by the Standard Model (SM), and\\n$m_H$ is the Higgs boson mass. The parameter $\\\\omega$ represent the expected (and therefore unknown) SM signal predictions and the expected backgrounds. When faced with such complexity, it proves useful to use a \\\\textbf{hierarchical Bayesian model}. Briefly, the prior \\n$\\\\pi(\\\\mu, m_H, \\\\omega)$ is written as\\n\\\\begin{align*}\\n\\\\pi(\\\\mu, m_H, \\\\omega) & = \\\\pi(\\\\omega| \\\\mu, m_H) \\\\, \\\\pi(\\\\mu, m_H), \\\\\\\\\\n\\\\textrm{where } \\\\pi(\\\\omega| \\\\mu, m_H) \\n& = \\\\int \\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H) \\\\, \\\\pi(\\\\phi | \\\\mu, m_H) \\\\, d\\\\phi.\\n\\\\end{align*}\\nThe prior $\\\\pi(\\\\phi | \\\\mu, m_H)$ models the lowest level systematic parameters that define\\nquantities such as the jet energy scale, lepton efficiencies, trigger efficiencies, and the parton distribution functions. It is usually straightforward to sample from this prior. Moreover,\\nthe function $\\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H)$ is nothing more than prior for the expected signal and\\nbackground parameters $\\\\omega$, which through estimates $\\\\hat{\\\\omega}$ depend implicitly\\non the parameters $\\\\phi$. The prior $\\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H)$ is generally quite simple;\\nfor binned data it is just a product of gamma (or gamma mixture) densities; more generally,\\nit is a product of gamma, Gaussian, or log-normal densities. Consequently, \\nthe marginalizations over $\\\\omega$ can be done in two steps: first generate a point $\\\\phi_i$\\nfrom $\\\\pi(\\\\phi| \\\\mu, m_H)$, then generate a point $\\\\omega_i$ from $\\\\pi(\\\\omega|\\\\phi_i, \\\\mu, m_H)$. In that way, the enormous complexity of explicitly modeling the dependence of $\\\\omega$ on\\n$\\\\phi$ is avoided, with the added benefit that all, possibly very complicated, correlations (in principle, to all orders) are accounted for automatically. The marginal likelihood can be\\napproximated by\\n\\\\begin{align}\\np(D | \\\\mu, m_H) \\\\approx \\\\frac{1}{M} \\\\sum_{m=1}^M p(D | \\\\mu, m_H, \\\\omega_m).\\n\\\\end{align}\\nWhat we have just described is merely integration via a Monte Carlo approximation. The point is that the sampling required to compute $pi(D | \\\\mu, m_H)$ can be run in $M$ parallel analysis jobs, each of which is given a different random number seed in order to sample a\\nsingle pair of points $\\\\phi_m$ and $\\\\omega_m$. The results of such a Bayesian analysis would be the likelihood $p(D| \\\\mu, m_H, \\\\omega$ and an ensemble of points $\\\\{ \\\\omega_m \\\\}$.\\n\\\\subsubsection*{Posterior density}\\nGiven the marginal likelihood $p(D | s, H_1)$ and a prior $\\\\pi(s)$ we can compute the posterior density,\\n\\\\begin{align}\\np(s | D, H_1) & = p(D | s, H_1) \\\\, \\\\pi(s) / p(D | H_1), \\\\\\\\\\n\\\\textrm{where,} \\\\nonumber \\\\\\\\\\np(D | H_1) & = \\\\int_0^\\\\infty p(D | s, H_1) \\\\, \\\\pi(s) \\\\, ds. \\\\nonumber\\n\\\\end{align}\\nAgain, for simplicity, we assume a flat prior for the signal, $\\\\pi(s) = 1$ and\\nfind\\n\\\\begin{align}\\np(s | D, H_1) & = \\\\frac{\\\\sum_{r=0}^N \\\\textrm{Beta}(x, r + 1, Q) \\\\, \\\\textrm{Poisson}( N - r| s)}\\n{\\\\sum_{r=0}^N \\\\textrm{Beta}(x, r + 1, Q)}, \\\\\\\\ \\\\medskip\\n& \\\\framebox{\\n\\\\parbox{0.6\\\\textwidth}{\\\\textbf{Exercise 11:} \\\\textrm{Derive an expression for} \\n$p(s | D, H_1)$ \\nassuming $\\\\pi(s) = $ Gamma$(q s, 1, M + 1)$ where $q$ and $M$ are constants}\\n}\\n\\\\nonumber\\n\\\\end{align}\\nfrom which we can compute the central \\\\textbf{credible interval} $[9.9 , 18.4]$ for $s$ at\\n68\\\\\\n\\\\subsubsection{Bayes factor}\\nAs noted, the number $p(D | H_1)$ can be used to perform a hypothesis test. But, as argued\\nabove, we need to use a proper prior for the signal, that is, a prior that integrates to one.\\nThe simplest such prior is a $\\\\delta$-function, e.g., $\\\\pi(s) = \\\\delta(s - 14)$. Using this prior,\\nwe find\\n\\\\begin{align*}\\np(D | H_1) = p(D | 14, H_1) = 9.28 \\\\times 10^{-2}.\\n\\\\end{align*}\\nSince the background-only hypothesis $H_0$ is nested in $H_1$, and defined by $s = 0$, the number $p(D | H_0)$ is given by $p(D|0, H_1)$, which yields\\n\\\\begin{align*}\\np(D | H_0) = p(D | 0, H_1) = 3.86 \\\\times 10^{-6}.\\n\\\\end{align*}\\nWe conclude that the hypothesis $s = 14$ is favored over $s = 0$ by a Bayes factor of 24,000. In order to avoid large numbers, the Bayes factor can be mapped into a (signed) measure akin\\nto the frequentist ``$n$-sigma\"~\\\\cite{Sezen},\\n\\\\begin{align}\\nZ = \\\\textrm{sign}(\\\\ln B_{10}) \\\\sqrt{2 |\\\\ln B_{10}|}, \\n\\\\end{align}\\nwhich gives $Z = 4.5$. Negative values of $Z$ correspond to hypotheses that are excluded.', '\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Expected sensitivity and bands}\\nThe expected sensitivity for limits and discovery are useful quantities, though subject to some degree of ambiguity. Intuitively, the expected upper limit is the upper limit one would expect to obtain if the background-only hypothesis is true. Similarly, the expected significance is the significance of the observation assuming the standard model signal rate (at some $\\\\mh$). To find the expected limit one needs a distribution $f(\\\\mu_up | \\\\mu=0,\\\\vec\\\\theta)$. To find the expected significance one needs the distribution $f(Z | \\\\mu=1,\\\\vec\\\\theta)$ or, equivalently, $f(p_0 | \\\\mu=1,\\\\vec\\\\theta)$. We use the median instead of the mean, as it is invariant to the choice of $Z$ or $p_0$. More importantly, is that the expected limit and significance depend on the value of the nuisance parameters $\\\\vec\\\\theta$, for which we do not know the true values. Thus, the expected limit and significance will depend on some convention for choosing $\\\\vec\\\\theta$. While many nuisance parameters have a nominal estimate (i.e. the global observables in the constraint terms), others do not (eg. the exponent in the $H\\\\to\\\\gamma\\\\gamma$ background model). Thus, we choose a convention that treats all of the nuisance parameters consistently, which is the profiled value based on the observed data. Thus for the expected limit we use $ f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))$ and for the expected significance we use $f(p_0 | \\\\mu=1,\\\\hat{\\\\hat{\\\\vec\\\\theta}}(\\\\mu=1, \\\\rm obs))$. An unintuitive and possibly undesirable feature of this choice is that the expected limit and significance depend on the observed data through the conventional choice for $\\\\vec\\\\theta$.\\nWith these distributions we can also define bands around the median upper limit. Our standard limit plot shows a dark green band corresponding to $\\\\mu_{\\\\pm 1}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 1}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 1) \\n\\\\end{equation}\\nand a light yellow band corresponding to $\\\\mu_{\\\\pm 2}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 2}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 2) \\n\\\\end{equation}\\n', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\\\subsection{Model Selection}\\nConceptually, hypothesis testing in the Bayesian approach (also called model selection)\\nproceeds exactly the same way as any other Bayesian calculation: we compute the \\nposterior density,\\n\\\\begin{align}\\np(\\\\theta, \\\\omega, H | D) & = \\\\frac{p(D | \\\\theta, \\\\omega, H) \\\\, \\\\pi(\\\\theta, \\\\omega, H)} {p(D)},\\n\\\\end{align}\\nand marginalize it with respect to all parameters except the ones that label\\nthe hypotheses or models, $H$, \\n\\\\begin{align}\\np(H | D ) & = \\\\int p(\\\\theta, \\\\omega, H | D) \\\\, d\\\\theta \\\\, d\\\\omega.\\n\\\\end{align}\\nEquation~(\\\\ref{eq:pHD}) is\\nthe probability of hypothesis $H$ given the observed data $D$.\\nIn principle, the parameters $\\\\omega$ could also depend on $H$. For example, suppose\\nthat $H$ labels different parton distribution function (PDF) models, say CT10, MSTW, and\\nNNPDF, then $\\\\omega$ would indeed depend on the PDF model and should be written\\nas $\\\\omega_H$.\\nIt is usually more convenient to arrive at the probability $p(H|D)$ in stages.\\n\\\\begin{enumerate}\\n\\\\item Factorize the prior in the most convenient form,\\n\\\\begin{align}\\n\\\\pi(\\\\theta, \\\\omega_H, H) & = \\\\pi(\\\\theta, \\\\omega_H | H) \\\\, \\\\pi(H), \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\theta |\\\\omega_H, H) \\\\, \\\\pi(\\\\omega_H | H) \\\\, \\\\pi(H),\\\\\\\\\\n\\\\textrm{or} \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\omega_H |\\\\theta, H) \\\\, \\\\pi(\\\\theta | H) \\\\, \\\\pi(H).\\n\\\\end{align}\\nOften, we can assume that the parameters of interest $\\\\theta$ are independent,\\n\\\\emph{a priori}, of both the nuisance\\nparameters $\\\\omega_H$ and the model label $H$, in which case we can write,\\n$\\\\pi(\\\\theta, \\\\omega_H, H) = \\\\pi(\\\\theta) \\\\, \\\\pi(\\\\omega_H|H) \\\\, \\\\pi(H)$.\\n\\\\item Then, for each hypothesis, $H$, compute the function\\n\\\\begin{align}\\np(D | H ) = \\\\int p(D | \\\\theta, \\\\omega_H, H) \\\\, \\\\pi(\\\\theta, \\\\omega | H) \\\\, d\\\\theta \\\\,\\nd\\\\omega.\\n\\\\end{align}\\n\\\\item Then, compute the probability of each hypothesis,\\n\\\\begin{align}\\np(H | D ) =\\\\frac{p(D | H) \\\\, \\\\pi(H)} {\\\\sum_H p(D | H) \\\\, \\\\pi(H)}.\\n\\\\end{align} \\n\\\\end{enumerate}\\nClearly, in order to compute $p(H | D)$ it is necessary to specify the priors $\\\\pi(\\\\theta, \\\\omega | H)$ and $\\\\pi(H)$. With some effort, it is possible to arrive at an acceptable form for\\n$\\\\pi(\\\\theta, \\\\omega | H)$, however, it is highly unlikely that consensus could ever be reached on the discrete prior\\n$\\\\pi(H)$. At best, one may be able to adopt a convention. For example, if by convention two hypotheses $H_0$ and $H_1$ are to be regarded as equally likely, \\\\emph{a priori},\\nthen it would make sense to assign $\\\\pi(H_0) = \\\\pi(H_1) = 0.5$.\\nOne way to circumvent the specification of the prior $\\\\pi(H)$ is to compare the probabilities,\\n\\\\begin{align}\\n\\\\frac{p(H_1 | D )}{p(H_0 | D)} =\\\\left[ \\\\frac{p(D | H_1)}{p(D | H_0} \\\\right] \\\\,\\n\\\\frac{ \\\\pi(H_1)} {\\\\pi(H_0)}.\\n\\\\end{align}\\nand use only the term in brackets, called the global \\\\textbf{Bayes factor}, $B_{10}$, as a way to\\ncompare hypotheses. The Bayes factor specifies by how much the relative probabilities\\nof two hypotheses changes as a result of incorporating new data, $D$. The word global \\nindicates that we have marginalized over all the parameters of the two models. The \\\\emph{local}\\nBayes factor, $B_{10}(\\\\theta)$ is defined by\\n\\\\begin{align}\\nB_{10}(\\\\theta) & = \\\\frac{p(D| \\\\theta, H_1)}{p(D| H_0)}, \\\\\\\\\\n\\\\textrm{where}, \\\\nonumber\\\\\\\\\\np(D| \\\\theta, H_1) & \\\\equiv \\\\int p(D | \\\\theta, \\\\omega_{H_1}, H_1) \\\\, \\\\pi(\\\\omega_{H_1} | H_1) \\\\, d\\\\omega_{H_1},\\n\\\\end{align}\\nare the \\\\textbf{marginal} or integrated likelihoods in which we have assumed the \\\\emph{a priori}\\nindependence of $\\\\theta$ and $\\\\omega_{H_1}$. We have further assumed\\nthat the marginal likelihood $H_0$ is independent of $\\\\theta$, which is a very\\ncommon situation. For example, $\\\\theta$ could be the expected signal count $s$,\\nwhile $\\\\omega_{H_1} = \\\\omega$ could be the expected background $b$. In this case, the\\nhypothesis $H_0$ is a special case of $H_1$, namely, it is the same as $H_1$ with $s = 0$. An hypothesis that is a special case of another \\nis said to be \\\\textbf{nested} in the more general hypothesis. The Bayesian example, discussed below, will\\nmake this clearer. \\nThere is a subtlety that may be missed: because of the way we have\\ndefined $p(D|\\\\theta, H)$, we\\nneed to multiply $p(D| \\\\theta, H)$ by the prior $\\\\pi(\\\\theta)$ and then integrate with respect\\nto $\\\\theta$ in order to calculate $p(D | H)$.\\n\\\\subsubsection{A Word About Priors}\\nConstructing a prior for nuisance parameters is generally neither controversial (for most parameters) nor problematic. Such difficulties as do arise occur when the priors must, of necessity,\\ndepend on expert judgement. For example, one theorist may\\ninsist that a uniform prior within a finite interval is a reasonable prior for the factorization scale in a QCD calculation, while in the expert judgement of another the interval should be twice as large.\\nClearly, in this case, there is no getting around the fact that the prior for this parameter is \\nunavoidably subjective. However, once a choice is made, a prior $\\\\pi(\\\\omega_H|H)$ that integrates to one can be constructed.\\nThe Achilles heal of the Bayesian approach is the need to specify the prior $\\\\pi(\\\\theta)$,\\nfor the parameters of interest,\\nat the start of the inference chain when we know almost nothing\\nabout these parameters. Careless specification of this prior can yield\\nresults that are unreliable or even nonsensical. The mandatory requirement is that \\nthe posterior density be proper, that is integrate to unity. Ideally, the same should hold\\nfor priors. A very extensive literature exists on the topic of prior specification\\nwhen the available information is extremely limited. However, a discussion of this\\ntopic is beyond the scope of these lectures; but, we shall make a few remarks.\\nFor model selection, we need to proceed with caution because\\nBayes factor are sensitive to the choice of priors and therefore less robust than posterior densities. Suppose that the prior $\\\\pi(\\\\theta) = C f(\\\\theta)$, where $C$ is a normalization\\nconstant. The global Bayes factor for the two hypotheses $H_1$ and $H_0$ can be written as\\n\\\\begin{align}\\nB_{10} = C \\\\frac{\\\\int p(D | \\\\theta, H_1) \\\\, f(\\\\theta) \\\\, d\\\\theta}{p(D | H_0)}.\\n\\\\end{align}\\nTherefore, if the constant $C$ is ill defined, typically because $\\\\int f(\\\\theta) \\\\, d\\\\theta = \\\\infty$,\\nthe Bayes factor will likewise be ill defined. For this reason, it is generally recommended\\nthat an improper prior not be used for parameters $\\\\theta$ that occur only in one hypothesis, here $H_1$. However, for parameters that are common to all hypotheses, it is permissible to\\nuse improper priors because the ill defined constant cancels in the Bayes factor.\\nThe discussion so far has been somewhat abstract. The next section therefore works through a detailed example of a possible Bayesian analysis of the D\\\\O\\\\ top discovery data.\\n', \"\\\\section{Appendix B: Statistical significance for small values of signal}\\nThere are two approaches to assess the significance of a potential new physics signal: using the estimator $\\\\mathcal{Q}$ or through the test statistic $q_{\\\\mu}$. In the first approach, a Gaussian approximation is obtained by evaluating the statistical estimator at its central values.\\n\\\\begin{equation}\\n-2ln \\\\mathcal{Q}_{i} = 2 s_{i} - 2 n_{i} ln \\\\bigg( 1 + \\\\frac{s_{i}}{b_{i}} \\\\bigg).\\n\\\\end{equation}\\nBy defining $w_{i} = \\\\ln \\\\left( 1 + \\\\frac{s_{i}}{b_{i}} \\\\right)$, we can calculate the expected value for the distributions of kackground only ($b$) and signal + background ($s+b$), respectively.\\n\\\\begin{eqnarray}\\n< -2ln \\\\mathcal{Q}_{i} >_{b} & = & 2s_{i} - 2(b_{i}) w_{i} {} \\\\nonumber \\\\\\\\\\n< -2ln \\\\mathcal{Q}_{i} >_{s+b} & = & 2s_{i} - 2(s_{i} + b_{i}) w_{i} {} \\n\\\\end{eqnarray}\\nIn this way, we can quantify the number of standard deviations between the expected number of background events and signal + background events across all channels.\\n\\\\begin{eqnarray}\\nZ_{0} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 b_{i}w_{i}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} b_{i} w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{B}w} = \\\\frac{S}{\\\\sqrt{B}}. {}\\n\\\\end{eqnarray}\\nWhere the standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){b}$ is calculated as: \\n\\\\begin{eqnarray}\\n\\\\sigma_{b}^{2} & = & E( (x- E(x))^{2} ) {} \\\\nonumber \\\\\\\\ \\n& = & E( (2 s_{i} - 2 n_{i} w_{i} - 2s_{i} + 2b_{i} w_{i})^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}E( n_{i}^{2} - 2n_{i}b_{i} + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( E(n_{i}^{2}) - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( \\\\sigma_{n}^{2} + E(n_{i})^{2} - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4b_{i}w_{i}^{2}. {}\\n\\\\end{eqnarray}\\nWith $\\\\sigma_{b}^{2} = \\\\sigma_{n}^{2} = E(n_{i}) = b_{i}$, which follows a Poisson distribution. Similarly, the number of standard deviations for the signal + background distribution is:\\n\\\\begin{eqnarray}\\nZ_{1} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{s+b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 (s_{i}+b_{i})w_{i}^{2}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} (s_{i}+b_{i}) w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{S+B}w} = \\\\frac{S}{\\\\sqrt{S+B}}. {}\\n\\\\end{eqnarray}\\nThe standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){s+b}$ is calculated in a similar way:\\n\\\\begin{equation}\\n\\\\sigma_{s+b}^{2} = E( (x- E(x))^{2} ) = 4(s_{i}+b_{i})w_{i}^{2}.\\n\\\\end{equation}\\nOn the other hand, in the second scenario, the test statistic $q_{\\\\mu}$, in the context of single-channel counting experiments, is defined from the Poisson likelihood function:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\frac{(\\\\mu s +b)^{n}}{n!}e^{-(\\\\mu s + b)}.\\n\\\\end{equation}\\nThe profile likelihood for the null hypothesis $\\\\mu = 0$ is given by:\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2ln\\\\frac{\\\\mathcal{L}(0)}{\\\\mathcal{L}(\\\\hat{\\\\mu})} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }. \\n\\\\end{cases}\\n\\\\end{equation}\\nIn this case, the maximum likelihood parameter is $\\\\hat{\\\\mu} = \\\\frac{n - b}{s}$. Using the Asimov data, where $n = s + b$, and Wilks' approximation~\\\\cite{conway2005calculation}, we have:\\n\\\\begin{equation}\\nZ_{0} = \\\\sqrt{q_{0}} = \\n\\\\begin{cases} \\n\\\\sqrt{2((s+b)ln(1+\\\\frac{s}{b}) - s)} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nFinally, by expanding the logarithmic function under the condition $s \\\\ll b$, we obtain the Gaussian approximation of the significance:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\frac{s}{\\\\sqrt{b}}.\\n\\\\end{equation}\\n\\\\end{document}\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Modified frequentist method}\\nIn general, the construction of frequentist estimators is based on determining the confidence level associated with a statistical estimator. For a statistic \\\\( Q \\\\), the confidence level of the hypothesis considering only the background component is given by the probability that \\\\( Q \\\\) takes a value less than or equal to the observed value \\\\( Q_{obs} \\\\)~\\\\cite{lista2016practical, barlow2019practical, read2002presentation}.\\n\\\\begin{equation}\\nCL_{b} = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{b}}{dQ} dQ,\\n\\\\end{equation}\\nwhere \\\\( Q_{obs} \\\\) depends on the observed values: \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Similarly, the confidence level for the signal + background hypothesis is defined by the probability that \\\\( Q \\\\) is less than or equal to \\\\( Q_{obs} \\\\), thus:\\n\\\\begin{equation}\\nCL_{s+b}(\\\\mu) = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{\\\\mu s+b}}{dQ} dQ,\\n\\\\end{equation}\\nThe modification of the frequentist method involves the renormalization of the confidence level for the alternative hypothesis:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = CL_{s+b}(\\\\mu)/C_{b}.\\n\\\\end{equation}\\nThe purpose of this definition is to maintain the coverage of the estimator to protect the null hypothesis \\\\( H_{0} \\\\). In other words, the exclusion values of \\\\( \\\\mu \\\\) are positive. In particular, \\\\( CL_{s} \\\\) for event counting is given by:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\sum_{i=0}^{n} \\\\frac{e^{-(\\\\mu s + b)} (\\\\mu s + b)^{i}}{i!} \\\\bigg/ \\n\\\\sum_{i=0}^{n} \\\\frac{e^{-(b)} (b)^{i}}{i!}.\\n\\\\end{equation}\\nUsing the expression for the cumulative Poisson distribution (Appendix~\\\\ref{sec:AppendixA}), it is possible to find the value of \\\\( CL_{s} \\\\) for different values of the signal strength \\\\( \\\\mu \\\\). Figure~[\\\\ref{fig:8}] shows the exploration of the p-value as a function of the signal strength for \\\\( b \\\\approx n = 0 \\\\) and \\\\( s = 1 \\\\). The upper limit \\\\( \\\\mu_{up} = 2.99 \\\\) is consistent with values obtained using previous methods.\\nIn the previous sections, estimations were made for a specific point defined by \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Table~[\\\\ref{tb:2}] summarizes the calculation of upper limits for the three methods discussed above, evaluated for different values of \\\\( n \\\\) and \\\\( b \\\\) while keeping \\\\( s = 1 \\\\) constant~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/ModifiedFrequentist/ModifiedUpperLimit.ipynb}{Source code}}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{lcccc}\\n\\\\hline\\nObservation ($n$) & Expected background ($b$) & Frequentist & Bayesian & Modified frequentist \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0 & 0 & 2.99 & 3.00 & 2.99 \\\\\\\\\\n& 1 & 1.99 & 3.00 & 2.99 \\\\\\\\\\n& 2 & 0.99 & 3.00 & 2.99 \\\\\\\\\\n& 3 & 0.00 & 3.00 & 2.99 \\\\\\\\\\n\\\\hline\\n1 & 0 & 4.74 & 4.76 & 4.75 \\\\\\\\\\n& 1 & 3.74 & 4.11 & 4.12 \\\\\\\\\\n& 2 & 2.74 & 3.82 & 3.82 \\\\\\\\\\n& 3 & 1.74 & 3.65 & 3.65 \\\\\\\\\\n\\\\hline\\n2 & 0 & 6.29 & 6.30 & 6.29 \\\\\\\\\\n& 1 & 5.29 & 5.41 & 5.42 \\\\\\\\\\n& 2 & 4.29 & 4.83 & 4.83 \\\\\\\\\\n& 3 & 3.29 & 4.45 & 4.45 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nThe development of the concept of confidence level and the application of the Neyman-Pearson Lemma for the signal hypothesis have facilitated the creation of frequentist statistical estimators that are unbiased by prior distributions. At the LEP collider, a parameter-independent estimator \\\\( Q(\\\\mu) \\\\) was developed~\\\\cite{cms2022portrait, cranmer2015practical}. More recently, at the LHC, the estimator \\\\( q_{\\\\mu} \\\\) has been employed, which is based on the profile likelihood~\\\\cite{lista2016practical, jme2010cms}. This estimator maximizes the parameters for statistical and systematic uncertainty within the likelihood function to incorporate these effects in the calculation of upper limits, experimental sensitivity, or the potential observation of new physics.\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Profile likelihood}\\nMost of the recent searches at LHC use the so-called {\\\\it profile likelihood}\\napproach for the treatment of nuisance parameters~\\\\cite{asymptotic}.\\nThe approach is based on the test statistic built as the following likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{L(\\\\vec{x};\\\\mu,\\\\hat{\\\\hat{\\\\theta}}(\\\\mu))}{L(\\\\vec{x};\\\\hat{\\\\mu},\\\\hat{\\\\theta})}\\\\,,\\n\\\\end{equation}\\nwhere in the denominator both $\\\\mu$ and $\\\\theta$ are fit simultaneously\\nas $\\\\hat{\\\\mu}$ and $\\\\hat{\\\\theta}$, respectively, and\\nin the numerator $\\\\mu$ is fixed, and $\\\\hat{\\\\hat{\\\\theta}}(\\\\mu)$ is the best fit of $\\\\theta$\\nfor the fixed value of $\\\\mu$. The motivation for the choice of Eq.~(\\\\ref{eq:profLike})\\nas the test statistic comes from Wilks' theorem that allows to approximate asymptotically\\n$-2\\\\ln\\\\lambda(\\\\mu)$ as a $\\\\chi^2$~\\\\cite{Wilks}.\\nIn general, Wilks' theorem applies if we have two hypotheses $H_0$ and $H_1$ that\\nare {\\\\it nested}, i.e.: they can be expressed as sets of nuisance parameters\\n$\\\\vec{\\\\theta}\\\\in\\\\Theta_0$ and $\\\\vec{\\\\theta}\\\\in\\\\Theta_1$, respectively, such that\\n$\\\\Theta_0\\\\subseteq\\\\Theta_1$. Given the likelihood function:\\n\\\\begin{equation}\\nL = \\\\prod_{i=1}^N L(\\\\vec{x}_i, \\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nif $H_0$ and $H_1$ are nested, then the following quantity,\\nfor $N\\\\rightarrow\\\\infty$, is distributed as a $\\\\chi^2$ with a number of degrees of freedom\\nequal to the difference of the $\\\\Theta_1$ and $\\\\Theta_0$ dimensionalities: \\n\\\\begin{equation}\\n\\\\chi_r^2 = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_0}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_1}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nIn case of a search for a new signal where the parameter of interest is $\\\\mu$,\\n$H_0$ corresponds to $\\\\mu = 0$ and $H_1$ to any $\\\\mu\\\\ge0$, Eq.~(\\\\ref{eq:wilks})\\ngives:\\n\\\\begin{equation}\\n\\\\chi_r^2(\\\\mu) = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu,\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\mu^\\\\prime,\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu^\\\\prime,\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nConsidering that the supremum is equivalent to the\\nbest fit value, the profile likelihood defined in Eq.~(\\\\ref{eq:profLike}) is obtained.\\nAs a concrete example of application of the profile likelihood,\\nconsider a signal with a Gaussian distribution over a background\\ndistributed according to an exponential distribution. A pseudoexperiment that was randomly-extracted\\naccordint to such a model is shown in Fig.~\\\\ref{fig:toyGplusB}, where a signal yield\\n$s=40$ was assumed on top of a background yield $b=100$, exponentially\\ndistributed in the range of the random variable $m$ from 100 to 150~GeV.\\nThe signal was assumed centered at 125~GeV with a standard deviation of 6~GeV,\\nreminding the Higgs boson invariant mass spectrum.\\nThe signal yields $s$ is fit from data.\\nAll parameters in the model are fixed, except the background yield,\\nwhich is assumed to be known with some level of uncertainty modeled\\nwith a log normal distribution whose corresponding nuisance parameter is called $\\\\beta$.\\nThe likelihood function for the model, which only depends on two parameters,\\n$s$ and $\\\\beta$, is, in case of a single measurement $m$:\\n\\\\begin{equation}\\nL(m;s,\\\\beta) = L_0(m;s,b_0 = be^\\\\beta) L_\\\\beta(\\\\beta;\\\\sigma_\\\\beta)\\\\,,\\n\\\\end{equation}\\nwhere:\\n\\\\begin{eqnarray}\\nL_0(m;s,b_0) & = & \\\\frac{e^{-(s+b_0)}}{n!}\\\\left(\\ns \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} e^{-{(m-\\\\mu)^2}/{2\\\\sigma^2}}+b_0\\\\lambda e^{-\\\\lambda m}\\n\\\\right)\\\\,, \\\\\\\\\\nL_\\\\beta(\\\\beta;\\\\sigma_\\\\beta) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma_\\\\beta}e^{-{\\\\beta^2}/{2\\\\sigma^2}}\\\\,.\\n\\\\end{eqnarray}\\nIf we measure a set values $\\\\vec{m}=(m_1,\\\\,\\\\cdots\\\\,m_N)$, the likelihood function is:\\n\\\\begin{equation}\\nL(\\\\vec{m};s,\\\\beta) = \\\\prod_{i=1}^N L(m_i;s,\\\\beta)\\\\,.\\n\\\\end{equation}\\nThe scan of $-\\\\ln\\\\lambda(s)$ is shown in Fig.~\\\\ref{fig:plScan}, where the profile likelihood\\nwas evaluated assuming $\\\\sigma_\\\\beta=0$ (no uncertainty on $b$, blue curve) or $\\\\sigma_\\\\beta=0.3$\\n(red curve). The minimum value of $-\\\\ln\\\\lambda(s)$ is equal to zero, since\\nat the minimum numerator and denominator in Eq.~(\\\\ref{eq:profLike}) are identical.\\nIntroducing the uncertainty on $\\\\beta$ (red curve) makes the curve broader.\\nThis causes an increase of the uncertainty on the estimate of $s$, whose uncertainty\\ninterval is obtained by intersecting the curve of the negative logarithm of the profile likelihood\\nwith an horizontal line at $-\\\\ln\\\\lambda(s) = 0.5$ (green line in Fig.~\\\\ref{fig:plScan}\\\\footnote{\\nThe plot in Fig.~\\\\ref{fig:plScan} was generated with the library {\\\\sc RooStats}\\nin {\\\\sc Root}~\\\\cite{Root}, which by default, uses $-\\\\ln\\\\lambda$ instead of $-2\\\\ln\\\\lambda$.\\n}).\\nIn order to evaluate the significance of the observed signal, Wilks' theorem can be\\nused. If we assume $\\\\mu=0$ (null hypothesis), the quantity $q_0 = -2\\\\ln\\\\lambda(0)$\\ncan be approximated with a $\\\\chi^2$ having one degree of freedom. Hence, the significance\\ncan be approximately evaluated as:\\n\\\\begin{equation}\\nZ\\\\simeq \\\\sqrt{q_0}\\\\,.\\n\\\\end{equation}\\n$q_0$ is twice the intercept of the curve in Fig.~\\\\ref{fig:plScan} with the vertical axis,\\nand gives an approximate significance of $Z\\\\simeq\\\\sqrt{2\\\\times6.66} = 3.66$,\\nin case of no uncertainty on $b$, and $Z\\\\simeq\\\\sqrt{2\\\\times3.93} = 2.81$, in case\\nthe uncertainty on $b$ is considered. \\nIn this example, the effect of background yield uncertainty reduces the\\nsignificance bringing it below the evidence level ($3\\\\sigma$).\\nThose numerical values can be verified\\nby running many pseudo experiments (toy Monte Carlo) assuming $\\\\mu=0$ and\\ncomputing the corresponding $p$-value. In complex cases, the computation\\nof $p$-values using toy Monte Carlo may become unpractical, and Wilks'\\napproximation provides a very convenient, and often rather precise,\\nalternative calculation.\\n\", '\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Probability}\\nWhen the weather forecast specifies that there is a 80\\\\have an intuitive sense of what this means. Likewise, most people have an intuitive \\nunderstanding of what it means to say that there is a 50-50 chance for a tossed coin to land\\nheads up. Probabilistic ideas are thousands of years old, but, starting in the sixteenth century these ideas were formalized into increasingly rigorous mathematical theories of probability. \\nIn the theory formulated by Kolmogorov in 1933, $\\\\Omega$ is\\nsome fixed mathematical space, $E_1, E_2, \\\\cdots \\\\subset \\\\Omega$ are subsets (called events) defined in some\\nreasonable way\\\\footnote{If $E_1, E_2, \\\\cdots$ are meaningful subsets of $\\\\Omega$, so to is the complement $\\\\overline{E}_1, \\\\overline{E}_2, \\\\cdots$ of each, as are\\ncountable unions and intersections of these subsets.}, and $P(E_j)$ is a number \\nassociated with subset $E_j$. These numbers satisfy the\\n\\\\begin{align*}\\n\\\\textbf{Kolmogorov Axioms} \\\\\\\\\\n& \\\\quad 1. \\\\quad P(E_j) \\\\geq 0 \\\\\\\\\\n& \\\\quad 2. \\\\quad P(E_1 + E_2 + \\\\cdots) = P(E_1) + P(E_2) + \\\\cdots\\n\\\\quad\\\\textrm{for disjoint subsets} \\\\\\\\\\n& \\\\quad 3. \\\\quad P(\\\\Omega) = 1.\\n\\\\end{align*}\\nConsider two subsets $A = E_1$ and $B = E_2$. The quantity $AB$ means $A$ \\\\emph{and} $B$, while $A + B$ means $A$ \\\\emph{or}\\n$B$, with associated probabilities $P(AB)$ and $P(A+B)$, respectively. Kolmogorov assumed, not unreasonably given the intuitive origins of probability, that probabilities sum to unity; hence the axiom $P(\\\\Omega) = 1$. However, this assumption can be dropped so that probabilities remain meaningful even if $P(\\\\Omega) = \\\\infty$~\\\\cite{Taraldsen}. \\nFigure~\\\\ref{fig:venn} suggests another probability, namely, the number $P(A|B) = P(AB) / P(B)$, called the \\\\textbf{conditional probability} of\\n$A$ given $B$. This permits statements such as: ``the probability that this track was\\ncreated by an electron given the measured track parameters\" or ``the probability to observe\\n17 events given that the mean background is 3.8 events\".\\nConditional probability is a very powerful idea, but the term itself is misleading. It implies that there are two kinds of\\nprobability: conditional and unconditional. In fact, \\\\emph{all} probabilities are conditional in\\nthat they always depend on a specific set of conditions, namely, those that\\ndefine the space $\\\\Omega$. It is entirely possible to embed a family of subsets of $\\\\Omega$ \\ninto another space $\\\\Omega^\\\\prime$ which assigns to each family member a different\\nprobability $P^\\\\prime$. A probability is defined only relative to some space of possibilities $\\\\Omega$.\\n$A$ and $B$ are said to be mutually exclusive if $P(AB) = 0$, that is, if the truth\\nof one denies the truth of the other. They are said to be exhaustive if $P(A) + P(B) = 1$. \\nFigure~\\\\ref{fig:venn} suggests the theorem\\n\\\\begin{align}\\nP(A + B) = P(A) + P(B) - P(AB), \\\\\\\\\\n\\\\framebox{\\\\textbf{Exercise 3:} Prove theorem}\\\\nonumber\\n\\\\end{align}\\nwhich can be deduced from the rules given above. Another useful theorem is \\nan immediate consequence of the commutativity of ``anding\" \\n$P(AB) = P(BA)$ and the definition of $P(A|B)$,\\nnamely,\\n\\\\begin{align}\\n\\\\textbf{Bayes Theorem} \\\\nonumber\\\\\\\\\\n&P(B|A ) = \\\\frac{P(A|B) P(B)}{P(A)}, \\n\\\\end{align} \\nwhich provides a way to convert the probability $P(A|B)$ to the probability $P(B|A)$. \\nUsing Bayes theorem, we can, for example,\\ndeduce the probability $P(e|x)$ that a particle is an electron, $e$, given a set of measurements, $x$, from the\\nprobability $P(x|e)$ of a set of measurements given that the particle is an electron.\\n\\\\subsubsection{Probability Distributions}\\nIn this section, we illustrate the use of these rules to derive more complicated \\nprobabilities. First we start with a definition:\\n\\\\begin{quote}\\nA \\\\textbf{Bernoulli trial}, named after the Swiss mathematician Jacob Bernoulli (1654 -- 1705), is an experiment with only two possible outcomes: $S = \\\\textrm{success}$ or $F = \\\\textrm{failure}$. \\n\\\\end{quote}\\n\\\\begin{quote}\\n\\\\paragraph*{Example} Each collision between protons at the Large Hadron Collider (LHC) is a Bernoulli trial in which something interesting happens ($S$) or does not ($F$). Let $p$ be\\nthe probability of a success, which is assumed to be the \\\\emph{same for each trial}. Since $S$\\nand $F$ are exhaustive, the probability of a failure is $1 - p$. For a given order $O$ of $n$\\nproton-proton collisions and exactly $k$ successes, and therefore exactly $n - k$ failures, the probability $P(k, O , n, p)$ is given by\\n\\\\begin{align}\\nP(k, O, n, p) = p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nIf the order $O$ of successes and failures is judged to be irrelevant, we can eliminate the order\\nfrom the problem by summing over all possible orders,\\n\\\\begin{align}\\nP(k, n, p) = \\\\sum_O P(k, O, n, p) = \\\\sum_O p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nThis procedure is called \\\\textbf{marginalization}. It is one of the most important operations in probability calculations. Every term in the sum in Eq.~(\\\\ref{eq:Pkn}) is identical and there\\nare $\\\\binom{n}{k}$ of them. This yields the \\\\textbf{binomial distribution},\\n\\\\begin{align}\\n\\\\textrm{Binomial(k, n, p)} \\\\equiv \\\\binom{n}{k} p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nBy definition, the mean number of successes $a$ is given by\\n\\\\begin{align}\\na & = \\\\sum_{k=0}^n k \\\\, \\\\textrm{Binomial(k, n, p)}, \\\\nonumber\\\\\\\\\\n& = p n. \\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 4:} Show this} \\\\nonumber\\n\\\\end{align}\\nAt the LHC $n$ is a number in the trillions, while for successes of interest such as the creation of a Higgs boson\\nthe probability $p << 1$. In this case, it proves convenient to consider the \\nlimit $p \\\\rightarrow 0, n \\\\rightarrow \\\\infty$ in such a way that $a$ remains constant. In\\nthis limit\\n\\\\begin{align}\\n\\\\textrm{Binomial(k, n, p)} & \\\\rightarrow e^{-a} a^k / k! , \\\\nonumber\\\\\\\\\\n& \\\\equiv \\\\textrm{Poisson}(k, a).\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 5:} Show this} \\\\nonumber\\n\\\\end{align}\\n\\\\end{quote}\\n\\\\bigskip\\n\\\\noindent\\nBelow we list the most common probability distributions.\\n\\\\begin{align}\\n&\\\\textbf{Discrete distributions}\\\\nonumber\\\\\\\\\\n& \\\\textrm{Binomial}(k, n, p) & \\\\binom{n}{k} p^k (1 - p^{n-k} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Poisson}(k, a) & a^k \\\\exp(-a) / k! \\\\nonumber\\\\\\\\\\n& \\\\textrm{Multinomial}(k, n, p) & \\\\frac{n!}{k_1!\\\\cdots k_K!} \\\\prod_{i=1}^K p_i^{k_i}, \\\\quad \\\\sum_{i=1}^K p_i = 1, \\\\sum_{i=1}^K k_i = n \\\\nonumber\\\\\\\\\\n&\\\\textbf{Continuous densities}\\\\nonumber\\\\\\\\\\n& \\\\textrm{Uniform}(x, a) & 1 / a \\\\nonumber\\\\\\\\\\n& \\\\textrm{Gaussian}(x, \\\\mu, \\\\sigma) & \\\\exp[-(x - \\\\mu)^2 / (2 \\\\sigma^2)] / (\\\\sigma \\\\sqrt{2\\\\pi}) \\\\nonumber\\\\\\\\\\n&\\\\textrm{(also known as the Normal density)}\\\\nonumber\\\\\\\\\\n&\\\\textrm{LogNormal}(x, \\\\mu, \\\\sigma) & \\\\exp[-(\\\\ln x - \\\\mu)^2 / (2 \\\\sigma^2)] / (x \\\\sigma \\\\sqrt{2\\\\pi})\\n\\\\nonumber\\\\\\\\\\n& \\\\textrm{Chisq}(x, n) & x^{n/2 -1} \\\\exp(-x /2) / [2^{n/2} \\\\Gamma(n/2)] \\\\nonumber\\\\\\\\\\n& \\\\textrm{Gamma}(x, a, b) & x^{a -1} a^b \\\\exp(- a x) / \\\\Gamma(b) \\\\nonumber\\\\\\\\\\n&\\\\textrm{Exp}(x, a) & a \\\\exp(- a x) \\\\nonumber\\\\\\\\\\n&\\\\textrm{Beta}(x, n, m) & \\\\frac{\\\\Gamma(n+m)}{\\\\Gamma(m) \\\\, \\\\Gamma(n)}\\nx^{n-1} \\\\, (1 - x)^{m-1} \\n\\\\end{align}\\nParticle physicists tend to use the term probability distribution for both discrete and \\ncontinuous functions, such as the Poisson and Gaussian distributions, respectively. But, strictly speaking, the continuous functions are probability \\\\emph{densities}, not probability\\ndistributions. In order to compute a probability from a density we need to integrate the density \\nover a finite set in $x$.\\n\\\\paragraph*{Discussion}\\nProbability is the foundation for models of non-deterministic data generating mechanisms,\\nsuch as particle collisions at the LHC. A \\\\textbf{probability model} is the probability\\ndistribution together with all the assumptions on which the distribution is based. For example,\\nsuppose we wish to count, during a given period of time, the number of entries $N$ in a given\\ntransverse momentum ($p_\\\\text{T}$) bin due to particles created in proton-proton collisions\\nat the LHC; that is, suppose we wish to perform a counting experiment. If we assume that \\nthe probability to obtain a count in this bin is very small and that the number of proton-proton collisions is very large, then it is common practice to use a Poisson distribution to model the\\ndata generating mechanism, which yields the bin count $N$. If we have multiple independent bins, we \\nmay choose to model the data generating mechanism as a product of Poisson distributions. Or,\\nperhaps, we may prefer to model the possible counts conditional on a fixed total count in which case\\na multinomial distribution would be appropriate.\\nSo far, we have assumed the meaning of the word probability to be self-evident. However, \\nthe meaning of probability~\\\\cite{Daston} has been the subject of debate for more than two centuries and\\nthere is no sign that the debate will end anytime soon. Probability, in spite of its intuitive\\nbeginnings, is an\\nabstraction. Therefore, for it to be of practical use it must be \\\\emph{interpreted}. The two most\\nwidely used interpretations of probability are:\\n\\\\begin{enumerate}\\n\\\\item \\\\textbf{degree of belief} in, or plausibility of, a proposition, for example, \\n``It will snow at CERN on December 18th\", and the\\n\\\\item \\\\textbf{relative frequency} of outcomes in an \\\\emph{infinite} ensemble of\\ntrials, for example, the relative frequency of Higgs boson \\ncreation in an infinite number of proton-proton collisions. \\n\\\\end{enumerate} \\nThe first interpretation is the older, while the second was championed by influential\\nmathematicians and logicians starting in the mid-nineteenth century and became \\nthe dominant interpretation. Of the two interpretations, however,\\nthe older is the more general in that it encompasses the latter and can be\\nused in contexts in which the latter makes no sense. The relative frequency, or \\\\textbf{frequentist}, interpretation is useful for situations in which one can contemplate counting the number of times $k$ a given outcome is realized in $n$ trials, as in the example of\\na counting experiment. The relative frequency $r = k / n$ is expected to converge, in a \\nsubtle but well-defined\\nsense, to some number $p$ that satisfies the rules of probability. It should noted, however, that \\nthe numbers $k/n$ and $p$ are conceptually distinct. The former is something we can actually calculate, while there is no \\\\emph{finite} operational way to calculate the latter from data. \\nThe probability $p$, even when interpreted as a relative frequency, remains an abstraction. \\nOn the other hand, the\\ndegrees of belief, which is the basis of the \\\\emph{Bayesian} approach to statistics (see Lecture 2), are just that: the\\ndegree to which a rational being \\\\emph{ought} to believe in the veracity of a given statement. The word ``ought\" in the last sentence is important: probability theory, with probabilities interpreted\\nas degrees of belief, is \\\\emph{not} a model of how human beings actually reason in situations\\nof uncertainty; rather probability theory when interpreted this way is a normative theory in\\nthat it specifies how an idealized\\nreasoning being, or system, ought to reason when faced with uncertainty. \\nThere is a school of thought that argues that degrees of belief should be an individual\\'s own assessment of\\nher or his degree of belief in a statement, which are then to be updated using the probability rules. The problem with this position is that it presupposes probability theory to be a model of\\nhuman reasoning, which we argue it is not --- a position confirmed by numerous psychological experiments.\\nIt is perhaps better to think of degrees of belief as numbers that inform one\\'s reasoning rather than as numbers\\nthat describe it, and relative frequencies as numbers that characterize stochastic data generation\\nmechanisms. Both are probabilities and both are useful. \\n', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\nIn this lecture, we introduce the Bayesian approach to inference starting with a\\ndescription of its salient features and ending with a detailed example, again using the top quark\\ndiscovery data from D\\\\O. \\nThe main point to be understood about the Bayesian approach is that it is merely applied\\nprobability theory (see Sec.~\\\\ref{sec:prob}).\\nA method is Bayesian if\\n\\\\begin{itemize}\\n\\\\item it is based on the degree of belief interpretation of probability and\\n\\\\item it uses Bayes theorem\\n\\\\begin{align}\\np(\\\\theta, \\\\omega | D) & = \\\\frac{p(D|\\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega)}{p(D)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nD & = \\\\textrm{ observed data}, \\\\nonumber \\\\\\\\\\n\\\\theta & = \\\\textrm{ parameters of interest}, \\\\nonumber\\\\\\\\\\n\\\\omega & = \\\\textrm{ nuisance parameters}, \\\\nonumber\\\\\\\\\\np(\\\\theta, \\\\omega| D) & = \\\\textrm{posterior density}, \\\\nonumber\\\\\\\\\\n\\\\pi(\\\\theta, \\\\omega) & = \\\\emph{prior density (or prior for short)}. \\\\nonumber\\n\\\\end{align}\\n\\\\end{itemize}\\nfor \\\\emph{all} inferences. The result of a Bayesian inference is the posterior density\\n$p(\\\\theta, \\\\omega | D$ from which, if desired, various summaries can be extracted. The parameters can be discrete or continuous and nuisance parameters are eliminated by \\nmarginalization,\\n\\\\begin{align}\\np(\\\\theta | D) & = \\\\int p(\\\\theta, \\\\omega | D ) \\\\, d\\\\omega, \\\\\\\\\\n& \\\\propto \\\\int p(D | \\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega) \\\\, d\\\\omega. \\\\nonumber\\n\\\\end{align}\\nThe function $\\\\pi(\\\\theta, \\\\omega)$, called the prior, encodes whatever information we have\\nabout the parameters $\\\\theta$ and $\\\\omega$ independently of the data $D$. A key\\nfeature of the Bayesian approach is recursion; the use of\\nthe posterior density $p(\\\\theta, \\\\omega|D)$ or one, or more, of its marginals as the prior in a subsequent analysis. \\nThese simple rules yield an extremely powerful and general inference model. Why\\nthen is the Bayesian approach not more widely used in particle physics? The \\nanswer is partly historical: the frequentist approach was dominant at the dawn of particle physics. It is also partly the widespread perception that the Bayesian\\napproach is too subjective to be useful for scientific work. However, there is published\\nevidence\\nthat this view is mistaken, witness the success of Bayesian methods in \\nhigh-profile analyses in particle physics such as the discovery of single top quark\\nproduction at the Tevatron~\\\\cite{Abazov:2009ii, Aaltonen:2009jj}.\\n', '\\\\section{Profile frequentist estimator}\\nThe estimator \\\\( \\\\mathcal{Q} \\\\) has been used in calculating upper limits for the mass of the Higgs boson. However, incorporating systematic effects is complex. Including these effects requires randomly varying the central values to obtain \\\\( f(\\\\mathcal{Q},\\\\mu) \\\\), which increases the variance and thus expands the upper limits. This phenomenon will be analyzed in detail in the section dedicated to systematic effects~\\\\cite{barlow2002systematic}. Currently, phenomenology and experimental analyses employ a statistical estimator, \\\\( q_{\\\\mu} \\\\), which combines the unbiased frequentist approach with the incorporation of systematic effects through the profile likelihood. This estimator requires maximizing the likelihood function with respect to signal strength, and for single-channel experiments, it is given by~\\\\cite{lista2016practical,cranmer2015practical}:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(\\\\hat{\\\\mu})}.\\n\\\\end{equation}\\nWhere \\\\( \\\\hat{\\\\mu} \\\\) is the maximum likelihood estimator of the observation \\\\( n \\\\). By appropriately applying the logarithmic function, the estimator can be reformulated as a minimization problem. The hypothesis test without considering systematic effects is expressed as follows:\\n\\\\begin{equation}\\nq_{\\\\mu} = -2ln(\\\\lambda(\\\\mu)).\\n\\\\end{equation}\\nFinally, to quantify the degree of disagreement between the observation and the hypothesis, the p-value of the observation is calculated:\\n\\\\begin{equation}\\np_{\\\\mu} = \\\\int_{q_{\\\\mu,obs}}^{\\\\infty} f(q_{\\\\mu}/\\\\mu) dq_{\\\\mu},\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu, \\\\text{obs}} \\\\) is the observed value of the estimator in the data, and \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) represents the distribution of the estimator for a specific value of \\\\( \\\\mu \\\\). Generally, this is an optimization problem to obtain the best fit of the model, followed by a sampling problem to determine \\\\( f(q_{\\\\mu}/\\\\mu) \\\\). Specifically, in the case of upper limit searches, the statistical test simplifies to~\\\\cite{lista2016practical,read2002presentation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = \\n\\\\begin{cases} \\n-2ln(\\\\lambda(\\\\mu)) & \\\\hat{\\\\mu} \\\\le \\\\mu \\\\\\\\\\n0 & \\\\hat{\\\\mu} > \\\\mu.\\n\\\\end{cases}\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu} = 0 \\\\) is adjusted to avoid excluding values smaller than the maximum likelihood estimator (i.e., in the non-physical case). With these definitions, the confidence level (\\\\( CL_{b} \\\\)) associated with the observation under the background-only hypothesis is expressed as:\\n\\\\begin{equation}\\nCL_{b} = 1-p_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{\\\\mu}/0) dq_{\\\\mu},\\n\\\\end{equation}\\nTherefore, the confidence level of the signal, \\\\( CL_{s}(\\\\mu) \\\\), is defined as:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}.\\n\\\\end{equation}\\nAs in the case of the unprofiled estimator, the upper limit is defined by \\\\( CL_{s}(\\\\mu_{up}) = 0.05 \\\\), which corresponds to the model exclusion criterion. For the single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), a scan is performed by sampling the distribution \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) for both hypotheses. Figure~[\\\\ref{fig:13}] shows the scan of the confidence level for the signal strength as a function of \\\\( \\\\mu \\\\), as well as the distributions of \\\\( H_{0} \\\\) and \\\\( H_{1} \\\\) for \\\\( \\\\mu=1 \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LHC/UpperLimit_qm.ipynb}{Source code}}.\\nAt each point, the maximum likelihood estimator is obtained using the specialized \\\\texttt{optimize} package~\\\\cite{virtanen2018scipy}. The expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.13 \\\\), and the observed upper limit is \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.48 \\\\). These values are fully consistent with the upper limits obtained using the estimator \\\\( \\\\mathcal{Q} \\\\).\\n', '\\\\section{Inference}\\n\\\\subsection{Approximate error evaluation for maximum likelihood estimates}\\nA parabolic approximation of $-2\\\\ln L$ around the minimum is equivalent to a Gaussian approximation,\\nwhich may be sufficiently accurate in many but not all cases. For a Gaussian model, $-2\\\\ln L$\\nis given by:\\n\\\\begin{equation}\\n-2\\\\ln L(\\\\vec{x};\\\\mu, \\\\sigma) = \\\\sum_{i=1}^n\\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2}+\\\\text{const.}\\\\,.\\n\\\\end{equation}\\nAn approximate estimate of the covariance matrix is obtained from the\\n$2^{\\\\mathrm{nd}}$ order partial derivatives with respect to the fit parameters at the minimum:\\n\\\\begin{equation}\\nV_{ij}^{-1} = -\\\\left.\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right|_{\\\\theta_k=\\\\hat{\\\\theta}_k,\\\\,\\\\forall k}\\\\,.\\n\\\\end{equation}\\nAnother approximation alternative to the parabolic one from Eq.~(\\\\ref{eq:errLikVar}) is the evaluation of the excursion\\nrange of $-2\\\\ln L$ around the minimum, as visualized in Fig.~\\\\ref{fig:likeScan}.\\nThe uncertainty interval can be determined as the range around the minimum of $-2\\\\ln L$ for which $-2\\\\ln L$ increases by $+1$ (or $+n^2$ for a $n\\\\sigma$ interval).\\nErrors can be asymmetric with this approach if the curve is asymmetric.\\nFor a Gaussian case the result is identical to the $2^{\\\\mathrm{nd}}$ order derivative matrix (Eq.~(\\\\ref{eq:errLikVar})).\\n', '\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Overdensity estimation}\\nIn order to train the most powerful ML-based classifier to discriminate signal from background, one would ideally train a network in a supervised manner with labeled data. This relies on a signal hypothesis that is chosen a-priori. An early attempt at discriminating background from \"everything else\" in order to obtain some degree of model-independence, was demonstrated in Ref.~\\\\citen{antiqcd}. Targeting searches for new physics in hadronic final states, a classifier was trained to discriminate QCD jets from various potential signal jets using Monte Carlo simulation. The disadvantage of such an approach is the dependence on signal simulation and which signals are to be included in the training.\\nAlthough simulated particle physics data is highly accurate over several orders of magnitude in length scale, simulation is known to not fully accurately reproduce collider data and this disagreement affects the tagging performance. Using weakly- or self-supervised (see Section~\\\\ref{sec:selfsupervised}) methods, algorithms can be trained directly on the data itself which has the added benefit of not having to derive transfer factors when training on synthetic data and testing on real data.\\n\\\\subsubsection{Weakly supervised methods}\\nIn weakly supervised learning, impure or noisy data sources can be used to label signal and background data in such a way that models can be trained in a supervised manner. Such methods can be utilized for anomaly detection when the signal is unknown, but there exist datasets where both signal and background are expected to be present in some relative fraction. This can be achieved by placing weak assumptions on the signal and background processes using domain knowledge.\\nThe goal of the weakly supervised methods we will discuss here, is to learn an approximation of the likelihood ratio $R(x)$ between the underlying probability densities of background $p_\\\\text{bg}(x)$ and data (possibly including signal) $p_\\\\text{data}(x)$, as a function of some input variables $x$:\\n\\\\begin{equation} \\nR(x)=\\\\frac{p_\\\\text{data}(x)}{p_\\\\text{bg}(x)}.\\n\\\\end{equation}\\nThis likelihood ratio, if it could be learned exactly, would be the most powerful model-agnostic anomaly detector, as given \\n\\\\begin{equation}\\np_\\\\text{data}(x)=(1-\\\\epsilon)p_\\\\text{bg}(x)+\\\\epsilon p_\\\\text{sig}(x),\\n\\\\end{equation}\\nwhere $p_\\\\text{sig}(x)$ is the probability density of signal, it would be monotonically related to the signal-to-background LR for any signal present in the data. A strategy for learning a good approximation of the likelihood ratio $R(x)$, is to train a classifier between data from a signal enriched region and samples drawn from a (fully data-driven) background model. If the background model is accurate and the classifier is well-trained, this approaches the likelihood ratio $R(x)$ by the Neyman-Pearson Lemma~\\\\cite{neyman1933ix}.\\nHence, the aim is to test whether the signal region data contains a combination of signal and background data. In the event that there are signal events present in the signal region, the classifier can differentiate between the signal region data and the background template. The true signal events are expected to have higher classification scores than the true background data. A cut on this classifier score can then be used to enhance the significance of signal events, making it a useful anomaly detection metric.\\nIn Ref.~\\\\citen{Dery2017WeaklySC} a method referred to as Learning from Label Proportions~\\\\cite{LLP} was utilized to discriminate between quarks and gluons using impure data samples. Despite not having access to the per-instance labels, the class proportions could be derived using domain knowledge. A supervised task was then defined using the class proportions themselves as the target, although operating the algorithm at a per-instance level. This concept has been extended in in the Classification WithOut Labels (CWola)~\\\\cite{cwola} framework. In this setup, the class proportions themselves do not need to be known, and it is enough to have two datasets at hand with an unequal fraction of signal instances in each set. A standard classifier can then be trained to discriminate between the two mixed datasets, and this can be shown to be the optimal classifier to discriminate between signal and background instances. The larger the difference in signal fraction between is dataset, the better the classifier becomes. The challenge is being able to design such mixed datasets, especially for a model-independent setup.\\nThe CWola strategy has been demonstrated and deployed for various model-independent search setups. In Ref.~\\\\citen{cwolabumphunt}, the authors introduce the \\\\textit{CWola bumphunt}. In this setup, one attempts to look for new, heavy generic particles that resonate around the particle mass in the dijet invariant mass spectrum. Starting from the weak assumption that this is a localized, narrow resonance, two mixed samples are created in the following way: The region in the dijet invariant mass close to the particle mass is defined as the signal-enriched mixed sample, and the regions next to it are defined as background-enriched regions. This is illustrated in Fig.~\\\\ref{fig:fig1}. In this way, the dijet invariant mass sideband regions serve as the background samples; these can serve as a good model for the background if the input features are statistically independent from the dijet invariant mass. If there is a signal present in the signal-like region around the particle mass, the classifier learns to identify it, while in the absence of a signal the classifier will likely learn random noise as there would be no difference between the two groups of events. It is crucial that the features being used for classification are not correlated with the dijet invariant mass. Otherwise, the classifier will be able to differentiate background events in the signal region from those in the adjacent dijet invariant mass regions used as the background-enriched mixed sample. Background events within the signal region will then be classified as signal-like, which can introduce artificial sculpting of the dijet invariant mass distribution. Note that the above strategy only works for narrow resonances, if there is a significant amount of signal in both datasets, as would be the case for a broad resonance, the classification performance is reduced. This method was used to analyze data collected by the ATLAS experiment in the search for generic new heavy resonances decaying into jets in Ref.~\\\\citen{ATLAS:2020iwa}, a first of its kind using weak supervision for model-agnostic searches. The power of this analysis can be seen in Figure~\\\\ref{fig:atlascwola}. This plot shows 95\\\\\\nThis methodology can also be applied in other setups than for a dijet bumphunt. In Ref.~\\\\citen{cwola_monojet}, model-agnostic learning using the CWola method is harnessed in order to improve the sensitivity of searches for new physics models with anomalous jet dynamics and a mono-jet signature. Focusing on cases where a heavy new particle decays into two jets which hadronize partially in the dark sector (making them \\\\textit{semi-visible jets}), and where one of the jets become completely invisible and the other partially visible, anomaly detection is utilized to detect the semi-visible anomalous jet. The degree of visibility of this jet can vary, making it difficult to train a supervised algorithm for each visibility fraction. Rather, CWola is deployed to train a generic anomalous jet identification classifier. The dominant background for a mono jet search is the electroweak production of vector bosons and jets, where the vector boson further decays to neutrinos, $Z(\\\\nu\\\\nu)$+jets. The experimental signature is missing transverse energy and a jet, mimicking the signal signature. Taking advantage of the fact that the vector boson also can decay visibly into two leptons and in these cases the jet remains the same, a background enriched control CWola sample can be defined using $Z(\\\\ell\\\\ell)$+jets events. None of the signal should be present in events with a di-lepton and jet signature. A model-independent anomalous jet tagger is then trained supervised to discriminate between jets coming from a $(\\\\ell\\\\ell)+jet$ and a $(\\\\nu\\\\nu)$+jet sample. If the monojet signature is present, CWola guarantees that the best algorithm trained to distinguish between these two regions, is also the best algorithm to discriminate between a normal SM jet from the V+jet background, and a semi-visible anomalous jet.\\nThis illustrates how generally CWola can be used. The only requirement is that one is able to define regions of the data depleted and enriched in signal, and that the signal and background events are statistically equal in the two regions. In terms of model independence, some degree of signal assumption is needed in order to define appropriate mixed samples.\\nMethods can also be used to bootstrap CWola and further improve the classification performance. In \\\\citen{Amram:2020ykb}, a powerful and model-independent anomalous jet tagger is defined starting from the CWola hunting methodology, but defining the mixed samples for training differently. Targeting signals where both jets in the event are anomalous, the key idea is that for a resonance decaying to a pair of anomalous jets, one can use an initial self- or supervised classifier (like an autoencoder, see Section~\\\\ref{sec:selfsupervised}) \\nto tag an event as signal-like or background-like using one jet and then use that information to construct samples for training a classifier using the other jet with weak supervision. By using an autoencoder as an initial classifier, one can group events into a signal-like and background-like sample based on the anomaly score on one of the jets (assuming that if the one jet is anomalous, the other must be too). A classifier can then be trained for the other jet that has not been tagged, where the mixed samples are defined based on the anomaly score of the tagged jet.\\nAnother weakly-supervised method for over-density detection is ANODE~\\\\cite{nachman2020anode}. In ANODE, conditional neural density estimation is used in order to interpolate probability densities from a data sideband into the data signal region. This interpolation is used and compared to the probability density of the actual data observed in the signal region, and used to construct a likelihood ratio as in Eq.~\\\\ref{eq:weak-supervision-likelihood-ratio}. This implies having to learn both the interpolated likelihood of the background in the signal region, as well as the likelihood for data in the signal region (see Fig.~\\\\ref{fig:fig1}). An improvement on this method is CATHODE (Classifying Anomalies Through Outer Density Estimation)~\\\\cite{Hallin:2021wme}. In CATHODE, rather than directly constructing the likelihood ratio, one rather samples events from the trained background estimator after it is interpolated into the signal region. This avoids having to learn the likelihood of data in the signal region. Then, a classifier is trained to discriminate data in the signal region from the data samples from the interpolated density estimator. This algorithm was first demonstrated for searches for heavy particles decaying into two jets. CATHODE proceeds by first training a conditional normalizing flow~\\\\cite{rezende2016variational} on the dijet invariant mass sidebands and then interpolating this into the signal region; samples from this flow are used as the background model and should correctly take into account any correlations between the input features and the dijet invariant mass.\\nNormalizing flows are a type of generative model that learn to transform a simple probability distribution (usually a standard Gaussian distribution) into a more complex distribution that resembles the target distribution of the data. This is achieved by defining a sequence of invertible transformations that map samples from the simple distribution to samples from the target distribution. The resulting model can be used to generate new data samples, perform density estimation, and compute likelihoods. Invertibility is important, as it ensures that the transformation has a well-defined inverse, which is needed for density estimation and likelihood computation. The key challenge in designing normalizing flows is to ensure that the resulting distribution is both complex enough to capture the target distribution and easy to work with, in the sense that likelihood computation and sampling are efficient. Recent work has focused on designing more expressive and flexible transformations, such as coupling layers, which allow for the transformation to depend on only a subset of the input variables~\\\\cite{dinh2017realnvp}. CATHODE utilizes such a normalizing flow to estimate the background density, conditioned on the dijet invariant mass. The density can then be interpolated into the dijet invariant mass signal region, while accounting for all correlations between the input features. Finally, a classifier is trained to distinguish between the artificially generated background samples from the normalizing flow (trained in data sidebands) and actual samples from the data signal region, yielding an estimate of the likelihood ratio as an anomaly metric (following the CWola paradigm).\\nA similar method is CURTAINS~\\\\cite{Raine:2022hht}. This method also takes advantage of a conditioned invertible neural network to learn the distribution of background events in a sideband and then use that to transform datapoints to those of the target distribution in the signal region. CURTAINs use an optimal transport loss to train the network to minimize the distance between the model output and the target data. The goal is to approximate the optimal transport function between two points in feature space when moving along the resonant spectrum. As a result, instead of generating new samples to create a background template, CURTAINs transforms the data in the side-bands to equivalent data points with a mass in the signal region. This approach eliminates the need to match data encodings to an intermediate prior distribution, which is the case of CATHODE, as it can lead to mismodelling of underlying correlations between the observables in the data if the trained posterior is not in perfect agreement with the prior distribution. Additionally, CURTAINs can also be employed to transform side-band data into validation regions, rather than simply constructing the background template in the signal region, making the algorithm easier to validate and test. Once the CURTAINs density estimation algorithm has been trained, a similar approach as in CATHODE is taken. Specifically, the transformed data (from sideband to signal region) is assumed to represent a sample of pure background events, while the signal region data represents a mixture of signal and background. A CWola classifier is trained to discriminate between the two datasets based on this assumption. \\nIn Ref.~\\\\citen{klein2022flows,curtains2}, an improvement of the CURTAINs technique is introduced, where a maximum likelihood estimation is used instead of an optimal transport loss. This improves the fidelity of the transformed data and is significantly faster and easier to train.\\nMore recently, diffusion models~\\\\cite{10.5555/3045118.3045358}, emerging as potent tools for high-dimensional density estimation, have been explored both for overdensity estimation~\\\\cite{sengupta2023improving} and for outlier detection~\\\\cite{mikuni2023highdimensional}.\\nThere are also weakly supervised methods that take advantage of simulation in the training of density estimators. In Simulation Assisted Likelihood-free Anomaly Detection (SALAD), a reweighting function for reweighting simulation to match data in the data sidebands is trained. This (parametrized) reweighting function is then interpolated into the signal region. Finally, a classifier to discriminate between the two is trained to get the likelihood ratio. Another simulation-assisted technique is Flow-enhanced transportation for anomaly detection (FETA)~\\\\cite{feta}, a mixture of SALAD and CURTAINS. A normalizing flow is trained in the sideband to map MC simulation to data. This learned flow is then applied to simulation in the signal region to obtain an approximation of the background.\\nThere are caveats when deploying weakly supervised methods.\\nAsymptotically, a weakly supervised classifier will converge to the performance of a fully supervised one. But in practice, performance typically degrades with smaller samples sizes available for training and lower fractions of signal events in the data sample. However, one can still obtain signal versus background classifiers with reasonable performance even with signal fractions well below 1\\\\\\n', \"\\\\section{Inference}\\n\\\\subsection{Likelihood function for binned samples}\\nSometimes data are available in form of a binned histogram. This may be convenient\\nwhen a large number of entries is available, and computing an unbinned likelihood function (Eq.~(\\\\ref{eq:unbinnedLikeFun}))\\nwould be too much computationally expansive.\\nIn most of the cases, each bin content is independent on any other bin and all obey Poissonian distributions,\\nassuming that bins contain event-counting information.\\nThe likelihood function can be written as product of Poissonisn PDFs corresponding to each bin\\nwhose number of entries is given by $n_i$ .\\nThe expected number of entries in each bin depends on some unknown parameters: $\\\\mu_i = \\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nThe function to be minimized, in order to fit $\\\\theta_1, \\\\cdots,\\\\theta_n$, is the following:\\n\\\\begin{eqnarray}\\n-2\\\\ln L(\\\\vec{n};\\\\vec{\\\\theta}) & = &\\n-2\\\\ln \\\\prod_{i=1}^{n_{\\\\mathrm{bins}}}\\\\mathrm{Poiss}(n_i;\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)) \\\\\\\\\\n& = & -2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\ln \\\\frac{\\ne^{-\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)^{n_i}\\n}{n_i!}\\\\\\\\\\n& = & 2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\left(\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)\\n-n_i\\\\ln\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m) +\\\\ln{n_i!}\\\\right)\\\\,.\\n\\\\end{eqnarray}\\nThe expected number of entries in each bin, $\\\\mu_i$, is often approximated by a continuous function $\\\\mu(x)$\\nevaluated at the center of the bin $x=x_i$.\\nAlternatively, $\\\\mu_i$ can be given by the superposition of other histograms ({\\\\it templates}),\\ne.g.: the sum of histograms obtained from different simulated processes.\\nThe overall yields of the considered processes may be left as free parameters in the fit in order to constrain\\nthe normalization of simulated processes from data, rather than relying on simulation prediction,\\nwhich may affected by systematic uncertainties.\\nThe distribution of the number of entries in each bin can be approximated,\\nfor sufficiently large number of entries,\\nby a Gaussian with standard deviation equal to $\\\\sqrt{n_i}$. \\nMaximizing $L$ is equivalent to minimize:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\frac{\\\\left(n_i-\\\\mu(x_i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\right)^2\\n}{n_i }\\n\\\\end{equation}\\nEquation~(\\\\ref{eq:NeymanChi2}) defines the so-called Neyman's $\\\\chi^2$ variable.\\nSometimes, the denominator $n_i$ is replaced by $\\\\mu_i = \\\\mu (x_i; \\\\theta_1, \\\\cdots, \\\\theta_m)$\\n(Pearson's $\\\\chi^2$) in order to avoid cases with $n_i$ equal to zero or very small.\\nAnalytic solutions exist in a limited number of simple cases, e.g.: if $\\\\mu$ is a linear function.\\nIn most of the realistic cases, the $\\\\chi^2$ minimization is performed numerically, as for\\nmost of the unbinned maximum likelihood fits.\\nBinned fits are, in many cases, more convenient with respect to unbinned fits because the number of input\\nvariables decreases from the total number of entries to the number of bins.\\nThis leads usually to simpler and faster numerical implementations,\\nin particular when unbinned fits become unpractical in cases of very large number of entries.\\nAnyway, for limited number of entries, a fraction of the information is lost when\\nmoving from an unbinned to a binned sample and a possible loss of precision may occur.\\nThe maximum value of the likelihood function obtained from an umbinned maximum likelihood fit doesn't in general\\nprovide information about the quality ({\\\\it goodness}) of the fit.\\nInstead, the minimum value of the $\\\\chi^2$ in a fit with a Gaussian underlying model\\nis distributed according to a known PDF given by:\\n\\\\begin{equation}\\nP(\\\\chi^2;n) =\\\\frac{2^{-{n}/{2}}}{\\\\Gamma\\\\left({n}/{2}\\\\right)}\\n\\\\chi^{n-2}e^{-\\\\frac{\\\\chi^2}{2}}\\\\,,\\n\\\\end{equation}\\nwhere $n$ is the {\\\\it number of degrees of freedom}, equal to the number of\\nbins minus the number of fit parameters.\\nThe cumulative distribution (Eq.~(\\\\ref{eq:cumulative})) of $P(\\\\chi^2; n)$ follows a uniform distribution between from 0 to 1,\\nand it is an example of {\\\\it p-value} (See Sec.~\\\\ref{sec:HypTest}).\\nIf the true PDF model deviates from the assumed distribution, the distribution of the $p$-value will be more peaked around zero\\ninstead of being uniformly distributed.\\nIt's important to note that $p$-values are not the ``probability of the fit hypothesis'',\\nbecause that would be a Bayesian probability, with a completely different meaning, and should be evaluated\\nin a different way.\\nIn case of a Poissonian distribution of the number of bin entries that may deviate from the Gaussian approximation,\\nbecause of small number of entries,\\na better alternative to the Gaussian-inspired Neyman's or Pearson's $\\\\chi^2$ has been proposed\\nby Baker and Cousins~\\\\cite{baker_cousins} using the following likelihood ratio\\nas alternative to Eq.~(\\\\ref{eq:PoisBinLik}):\\n\\\\begin{eqnarray}\\n\\\\chi^2_{\\\\lambda} & = & -2\\\\ln\\\\prod_i\\\\frac{L(n_i;\\\\mu_i)}{L(n_i;n_i)} = -2\\\\ln\\\\prod_i\\\\frac{e^{-\\\\mu_i}\\\\mu_i^{n_i}}{n_i!}\\n\\\\frac{n_i!}{e^{-{n_i}}n_i^{n_i}} \\\\\\\\\\n& = & 2\\\\sum_i\\\\left[\\n\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)- n_i + n_i\\\\ln\\\\left(\\n\\\\frac{n_i}{\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\n\\\\right) \\\\right]\\\\,.\\n\\\\end{eqnarray}\\nEquation~(\\\\ref{eq:BakCous}) gives the same minimum value as the Poisson likelihood function,\\nsince a constant term has been added to the log-likelihood function in Eq.~(\\\\ref{eq:PoisBinLik}),\\nbut in addition it provides goodness-of-fit information, since it asymptotically obeys a $\\\\chi^2$\\ndistribution with $n - m$ degrees of freedom. This is due to Wilks' theorem, discussed\\nin Sec.~\\\\ref{sec:profLik}.\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The test statistics and estimators of $\\\\mu$ and $\\\\vec\\\\theta$}\\nThis definitions in this section are all relative to a given dataset $\\\\datasim$ and value of the global observables $\\\\globs$, thus we will suppress their appearance. The nomenclature follows from Ref.~\\\\cite{asimov}.\\nThe maximum likelihood estimates (MLEs) $\\\\hat\\\\mu$ and $\\\\hat{\\\\vec\\\\theta}$ and the values of the parameters that maximize the likelihood function $L(\\\\mu,\\\\vec\\\\theta)$ or, equivalently, minimize $-\\\\ln L(\\\\mu,\\\\vec\\\\theta)$. The dependence of the likelihood function on the data propagates to the values of the MLEs, so when needed the MLEs will be given subscripts to indicate the data set used. For instance, $\\\\hat{\\\\vec\\\\theta}_{\\\\rm obs}$ is the MLE of $\\\\vec\\\\theta$ derived from the observed data and global observables. \\nThe conditional maximum likelihood estimate (CMLEs) $\\\\hathatthetamu$ is the value of $\\\\vec\\\\theta$ that maximizes the likelihood function with $\\\\mu$ fixed; it can be seen as a multidimensional function of the single variable $\\\\mu$. Again, the dependence on $\\\\datasim$ and $\\\\globs$ is implicit. This procedure for choosing specific values of the nuisance parameters for a given value of $\\\\mu$, $\\\\datasim$, and $\\\\globs$ is often referred to as ``profiling''. Similarly, $\\\\hathatthetamu$ is often called ``the profiled value of $\\\\vec\\\\theta$''.\\nGiven these definitions, we can construct the profile likelihood ratio\\n\\\\begin{equation}\\n{\\\\lambda}({\\\\mu}) = \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } \\\\;,\\n\\\\end{equation}\\nwhich depends explicitly on the parameter of interest $\\\\mu$, implicitly on the data $\\\\datasim$ and global observables $\\\\globs$, and is independent of the nuisance parameters $\\\\vec\\\\theta$ (which have been eliminated via ``profiling'').\\nIn any physical theory the rate of signal events is non-negative, thus $\\\\mu\\\\ge 0$. However, it is often convenient to allow $\\\\mu<0$ (as long as the pdf $f_c(x_c | \\\\mu,\\\\vec\\\\theta)\\\\ge 0$ everywhere). In particular, $\\\\hat\\\\mu<0$ indicates a deficit of events signal-like with respect to the background only and the boundary at $\\\\mu=0$ complicates the asymptotic distributions. Ref.~\\\\cite{asimov} uses a trick that is equivalent to requiring $\\\\mu\\\\ge 0$ while avoiding the formal complications of a boundary, which is to allow $\\\\mu< 0$ and impose the constraint in the test statistic itself. In particular, one defines $\\\\tilde \\\\lambda(\\\\mu)$\\n\\\\begin{equation}\\n\\\\tilde{\\\\lambda}({\\\\mu}) = \\\\left\\\\{ \\n\\\\begin{array}{ll} \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } & \\\\hat{\\\\mu} \\\\ge 0 , \\\\\\\\*[0.3 cm]\\n\\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) } {L(0,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0)) } & \\\\hat{\\\\mu} < 0 \\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nThis is not necessary when ensembles of pseudo-experiments are generated with ``Toy'' Monte Carlo techniques, but since they are equivalent we will write $\\\\tilde\\\\lambda$ to emphasize the boundary at $\\\\mu=0$.\\nFor discovery the test statistic $\\\\tilde{q}_0$ is used to differentiate the background-only hypothesis $\\\\mu=0$ from the alternative hypothesis $\\\\mu>0$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{0} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} > 0\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} \\\\le 0 \\n\\\\end{array} \\\\right. \\n\\\\end{equation}\\nNote that $\\\\tilde{q}_0$ is test statistic for a one-sided alternative. Note also that if we consider the parameter of interest $\\\\mu\\\\ge 0$, then it is equivalent to the two-sided test (because there are no values of $\\\\mu$ less than $\\\\mu=0$. \\nFor limit setting the test statistic $\\\\tilde{q}_{\\\\mu}$ is used to differentiate the hypothesis of signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser rate $\\\\mu'<\\\\mu$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{\\\\mu} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} \\\\le \\\\mu\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\n\\\\end{array} \\\\right. \\\\quad = \\\\quad \\\\: \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{lll} - 2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(0, \\\\hat{\\\\hat{\\\\theta}}(0))} &\\n\\\\hat{\\\\mu} < 0 \\\\;, \\\\\\\\*[0.2 cm] -2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}})} &\\n0 \\\\le \\\\hat{\\\\mu} \\\\le \\\\mu \\\\;, \\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\\\;.\\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nNote that $\\\\tilde{q}_{\\\\mu}$ is a test statistic for a one-sided alternative; it is a test statistic for a one-sided upper limit. \\nThe test statistic $\\\\tilde{t}_\\\\mu$ is used to differentiate signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser or greater rate $\\\\mu' \\\\ne\\\\mu$.\\n\\\\begin{equation}\\n\\\\tilde{t}_{\\\\mu} = - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) \\\\; . \\n\\\\end{equation}\\nNote that $\\\\tilde{t}_\\\\mu$ is a test statistic for a two-sided alternative (as in the case of the Feldman-Cousins technique, though this is more general as it incorporates nuisance parameters). Note that if we consider the parameter of interest $\\\\mu\\\\ge 0$ and we the test at $\\\\mu=0$ then there is no ``other side'' and we have $\\\\tilde{t}_{\\\\mu=0} = \\\\tilde{q}_0$. Finally, if one relaxes the constraint $\\\\mu\\\\ge0$ then the two-sided test statistic is written $t_\\\\mu$ or, simply, $-2\\\\ln\\\\lambda(\\\\mu)$.\\n\", \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Gaussian distribution}\\nThe Gaussian or normal distribution (shown in Fig.~\\\\ref{fig:Gaussians}) is of widespread usage in data analysis. Under suitable conditions, in\\na repeated series of measurements $x$ with accuracy $\\\\sigma$ when the true value of the quantity\\nis $\\\\mu$, the distribution of $x$ is given by a Gaussian\\\\footnote{However, it is often the case \\nthat such a distribution has heavier tails than the Gaussian.}. A mathematical motivation is given by the \\nCentral Limit Theorem, which states that the sum of\\na large number of variables with (almost) any distributions is approximately Gaussian. \\nFor the Gaussian, the probability density $y(x)$ of an observation $x$ is given by\\n\\\\begin{equation}\\ny(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi} \\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\n\\\\end{equation}\\nwhere the parameters $\\\\mu$ and $\\\\sigma$ are respectively the centre and width of the distribution.\\nThe factor $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$ is required to normalise the area under the curve, so that $y(x)$\\ncan be directly interpreted as a probability density.\\nThere are several properties of $\\\\sigma$:\\n\\\\begin{itemize}\\n\\\\item{The mean value of $x$ is $\\\\mu$, and the standard deviation of its distribution is $\\\\sigma$. Since the usual symbol for \\nstandard deviation is $\\\\sigma$, this leads to the formula $\\\\sigma = \\\\sigma$ (which is not so trivial as it seems, since the \\ntwo $\\\\sigma$s have different meanings). This explains the curious factor of 2 in the denominator of the exponential,\\nsince without it, the two types of $\\\\sigma$ would not be equal.}\\n\\\\item{The value of $y$ at the $\\\\mu \\\\pm \\\\sigma$ is equal to the peak height multiplied by $e^{-0.5}$ = 0.61.\\nIf we are prepared to overlook the difference between 0.61 and 0.5, $\\\\sigma$ is the half-width of the distribution at\\n`half' the peak height.}\\n\\\\item{The fractional area in the range $x = \\\\mu - \\\\sigma$ to $\\\\mu + \\\\sigma$ is 0.68. Thus for a series of unbiassed, independent Gaussian \\ndistributed measurements}, about 2/3 are expected to lie within $\\\\sigma$ of the true value.\\n\\\\item{The peak height of $y$ at $x=\\\\mu$ is $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$. It is reasonable that this is proportional to \\n$1/\\\\sigma$ as the width is proportional to $\\\\sigma$, so $\\\\sigma$ cancels out in the product\\nof the height and width, as is \\nrequired for a distribution normalised to unity.}\\n\\\\end{itemize}\\nFor deciding whether an experimental measurement is consistent with a theory, more useful than the Gaussian distribution \\nitself is its tail area beyond $r$, a number of standard deviations from the central value (see Fig.~\\\\ref{fig:Gauss_tail}). \\nThis gives the probability of obtaining a result as extreme as ours or more so as a consequence of statistical fluctuations, \\nassuming that the theory is correct (and that our measurement is unbiassed, it is Gaussian distributed, etc.). If this \\nprobability is small, the measurement and the theory may be inconsistent. \\nFigure~\\\\ref{fig:Gauss_tail} has two different vertical scales, the left one for the probability of a fluctuation in a specific \\ndirection, and the right side for a fluctuation in either direction. Which to use depends on the particular\\nsituation. For example if we were performing a neutrino oscillation disappearance experiment, we would be looking for \\na reduction in the number of events as compared with the no-oscillation scenario, and hence would be interested in \\njust the single-sided tail. In contrast searching for any deviation from the Standard Model expectation, maybe the two-sided tails would be more relevant. \\n\", '\\\\section{Worked example: Lifetime determination}\\nHere we consider an experiment which has resulted in $N$ observed decay times $t_i$ of a particle\\nwhose lifetime $\\\\tau$ we want to determine. The probability density for observing a decay at time $t$ \\nis \\n\\\\begin{equation}\\np(t;\\\\tau) = (1/\\\\tau) \\\\ e^{-t/\\\\tau}\\n\\\\end{equation} \\nNote the essential normalisation factor $1/\\\\tau$; without this the likelihood method does not work.\\nIt should be realised that realistic situations are more complicated than this. For example, we ignore\\nthe possibility of backgrounds, time resolution which smears the expected values of $t$, acceptance or \\nefficiency effects which vary with $t$, etc., but this enables us to estimate $\\\\tau$ and its uncertainty\\n$\\\\sigma_{\\\\tau}$ analytically. In real practical cases, it is almost always necessary to calculate the \\nlikelihood as a function of $\\\\tau$ numerically. \\nFrom equation \\\\ref{exp} we calculate the log-likelihood as\\n\\\\begin{equation}\\n\\\\ln L(\\\\tau) = \\\\ln[\\\\Pi\\\\ (1/\\\\tau) e^{-t_i/\\\\tau}] \\\\ \\\\ = \\\\ \\\\ \\\\Sigma (-\\\\ln \\\\tau - t_i/\\\\tau)\\n\\\\end{equation}\\nDifferentiating $\\\\ln L(\\\\tau)$ with respect to $\\\\tau$ and setting the derivative to zero then yields\\n\\\\begin{equation}\\n\\\\tau = \\\\Sigma t_i/N\\n\\\\end{equation}\\nThis equation has an appealing feature, as it can be read as ``The mean lifetime is equal to the mean lifetime\",\\nwhich sounds as if it must be true. However, what it really says is not quite so trivial: ``Our\\nbest estimate of the lifetime parameter $\\\\tau$ is equal to the mean of the $N$ observed decay times in our \\nexperiment.\"\\nWe next calculate $\\\\sigma_{\\\\tau}$ from the second derivative of $\\\\ln L$, and obtain \\n\\\\begin{equation}\\n\\\\sigma_{\\\\tau} = \\\\tau/\\\\sqrt N\\n\\\\end{equation}\\nThis exhibits a common feature that the uncertainty of our parameter estimate decreases as $1/\\\\sqrt N$ as we \\ncollect more and more data. However, a potential problem arises from the fact that our estimated uncertainty\\nis proportional to our estimate of the parameter. This is relevant if we are trying to combine different experimental\\nresults on the lifetime of a particle. For combining procedures which weight each result by $1/\\\\sigma^2$, a \\nmeasurement where the fluctuations in the observed times result in a low estimate of $\\\\tau$\\nwill tend to be over-weighted (compare the section on `Combining Experiments\\' in Lecture 1), \\nand so the weighted average would be biassed \\ndownwards. This shows that it is better to combine different experiments at the data level, rather than \\nsimply trying to use their results.\\nOne final point to note about our simplified example is that the likelihood $L(\\\\tau)$ depends on the \\nobservations only via the {\\\\bf sum} of the times $\\\\Sigma t_i$ i.e. their {\\\\bf distribution} is \\nirrelevant. Thus the likelihood distributions for two experiments having the same number of events and the \\nsame sum of observed decay times, but with one having the decay times consistent with an exponential \\ndistribution and the other having something completely different (e.g. all decays occur at the same time), \\nwould have identical likelihood\\nfunctions. This provides an example of the fact that the unbinned likelihood function does not in general provide\\nuseful information on Goodness of Fit. \\n', \"\\\\section{Probability distributions and their properties} \\nWe have to make a simple distinction between two sorts of data: \\\\emph{integer} data and \\\\emph{real-number} data\\\\footnote{Other branches of science have to include a third, \\\\emph{categorical} data, but we will ignore that.}.\\nThe first covers results which are of their nature whole numbers: the numbers of kaons produced in \\na collision, or the number of entries falling into some bin of a histogram.\\nGenerically let's call such numbers $r$. They have probabilities $P(r)$ which are dimensionless.\\nThe second covers results whose values are real (or floating-point) numbers. There are lots of these:\\nenergies, angles, invariant masses $\\\\dots$\\nGenerically let's call such numbers $x$, and they have probability density functions $P(x)$\\nwhich have \\ndimensions of $[x]^{-1}$, so $\\\\int_{x_1}^{x_2} P(x) dx$ or $P(x)\\\\, dx$ are probabilities.\\nYou will also sometimes meet the cumulative distribution $C(x)=\\\\int_{-\\\\infty}^x P(x') \\\\, dx'$.\\n\\\\subsection{Expectation values}\\nFrom $P(r)$ or $P(x)$ one can form the expectation value\\n\\\\begin{equation}\\n\\\\langle f \\\\rangle =\\\\sum_r f(r) P(r) \\\\qquad {\\\\rm or} \\\\qquad \\\\langle f \\\\rangle = \\\\int f(x) P(x) \\\\, dx \\n\\\\quad,\\n\\\\end{equation}\\nwhere the sum or integral is taken as appropriate.\\nSome authors write this as\\n$E(f)$, but I personally prefer the angle-bracket notation. You may think it looks too much like quantum mechanics,\\nbut in fact it's quantum mechanics which looks like statistics: an expression like $\\\\langle \\\\psi | \\\\hat Q | \\\\psi \\\\rangle$\\nis the average value of an operator $\\\\hat Q$ in some state $\\\\psi$, where `average value' has exactly the same \\nmeaning and significance. \\n\\\\subsubsection{Mean and standard deviation} \\nIn particular the {\\\\em mean}, often written $\\\\mu$, is given by \\n$ \\\\langle r \\\\rangle = \\\\sum_r r P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle x \\\\rangle =\\\\int x P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent Similarly one can write \\nhigher {\\\\em moments} \\n$\\\\mu_k = \\\\langle r^k \\\\rangle = \\\\sum_r r^k P(r)\\\\qquad {\\\\rm or } \\\\qquad \\\\langle x^k \\\\rangle =\\\\int x^k P(x) \\\\, dx \\\\quad,$ \\n\\\\noindent and {\\\\em central moments} \\n$\\\\mu'_k = \\\\langle (r-\\\\mu)^k \\\\rangle = \\\\sum_r (r-\\\\mu)^k P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle (x-\\\\mu)^k \\\\rangle =\\\\int (x-\\\\mu)^k P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent The second central moment is known as the \\n{\\\\em variance} \\n$\\\\mu'_2=V= \\\\sum_r (r-\\\\mu)^2 P(r) = \\\\langle r^2 \\\\rangle - \\\\langle r \\\\rangle ^2$\\n\\\\qquad \\nor \\\\qquad $ \\\\int (x-\\\\mu)^2 P(x) \\\\, dx = \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2$\\n\\\\noindent It is easy to show that $\\\\langle (x-\\\\mu)^2 \\\\rangle =\\\\langle x^2 \\\\rangle -\\\\mu^2$. The {\\\\em standard deviation} is just the square root of the variance $\\\\sigma=\\\\sqrt{V}$.\\nStatisticians usually use variance, perhaps because formulae come out simpler. Physicists usually use standard deviation,\\nperhaps because it has the same dimensions as the variable being studied, and can be drawn as an error bar on a plot. \\nYou may also meet {\\\\em skew}, which is $\\\\gamma=\\\\langle (x-\\\\mu)^3 \\\\rangle /\\\\sigma^3$ and {\\\\em kurtosis}, $h=\\\\langle (x-\\\\mu)^4 \\\\rangle /\\\\sigma^4 -3$.\\nDefinitions vary, so be careful. Skew is a dimensionless measure of the asymmetry of a distribution. Kurtosis is\\n(thanks to that rather arbitrary looking 3 in the definition) \\nzero for a Gaussian distribution (see Section~\\\\ref{sec:measurement}): positive kurtosis indicates a\\nnarrow core with a wide tail, negative kurtosis indicates the tails are reduced. \\n\\\\subsubsection{Covariance and correlation}\\nIf your data are \\n2-dimensional pairs $(x,y)$,\\nthen besides forming $\\\\langle x \\\\rangle, \\\\langle y \\\\rangle, \\\\sigma_x$ etc., you can also form the \\n{\\\\em Covariance}\\n${\\\\rm Cov}(x,y)=\\\\langle (x-\\\\mu_x)(y-\\\\mu_y) \\\\rangle = \\\\langle xy \\\\rangle - \\\\langle x \\\\rangle \\\\langle y \\\\rangle \\\\quad.$\\nExamples are shown in Fig.~\\\\ref{fig:covariance}. If there is a tendency for positive fluctuations in $x$ to be associated with positive fluctuations in $y$ (and therefore negative with negative) then\\nthe product $(x_i-\\\\overline x)(y_i-\\\\overline y)$ tends to be positive and the covariance is greater than 0. A negative covariance, as in the 3rd plot, happens if a positive fluctuation in one variable is associated with a negative fluctuation in the other.\\nIf the variables are independent then a positive variation in $x$ is equally likely to be associated with a positive or a negative variation in $y$ and the covariance is zero, as in the first plot. However the converse is not always the case, there can be two-dimensional distributions where the covariance is zero, but the two variables are not independent, as is shown in the fourth plot.\\nCovariance is useful, but it has dimensions. Often one uses the \\n{\\\\em correlation}, which is just\\n\\\\begin{equation}\\n\\\\rho={{\\\\rm Cov}(x,y)\\\\over \\\\sigma_x \\\\sigma_y}\\n\\\\quad.\\n\\\\end{equation}\\nIt is easy to show that\\n$\\\\rho$ lies between 1 (complete correlation) and -1 (complete anticorrelation). \\n$\\\\rho=0$ if $x$ and $y$ are independent.\\nIf there are more than two variables---the alphabet runs out so let's call them \\n$(x_1,x_2,x_3\\\\dots x_n)$---\\nthen these generalise to the \\n{\\\\em covariance matrix}\\n${\\\\bf V}_{ij}=\\\\langle x_i x_j \\\\rangle - \\\\langle x_i \\\\rangle \\\\langle x_j \\\\rangle$ \\n\\\\noindent and the {\\\\em \\ncorrelation matrix}\\n${\\\\bf \\\\rho}_{ij}={{\\\\bf V}_{ij} \\\\over \\\\sigma_i \\\\sigma_j} \\\\quad.$\\n\\\\noindent The diagonal of $\\\\bf V$ is $\\\\sigma_i^2$. \\nThe\\ndiagonal of $\\\\bf \\\\rho$ is 1.\\n\\\\subsection{Binomial, Poisson and Gaussian}\\nWe now move from considering the general properties of distributions to considering three specific ones.\\nThese are the ones you will most commonly meet for the distribution of the original data\\n(as opposed to quantities constructed from it). Actually the first, the binomial, is not nearly as common as the second, the Poisson; and the third, the Gaussian, is overwhelmingly more common. However it is \\nuseful to consider all three as concepts are built up from the simplest to the more sophisticated.\\n\\\\subsubsection {The binomial distribution }\\nThe binomial distribution is easy to understand as it basically describes the familiar\\\\footnote{Except, as it happens, in Vietnam, where coins have been completely replaced by banknotes.} tossing of coins. \\nIt describes the number $r$ of successes in $N$ trials, each with probability $p$ of success.\\n$r$ is discrete so the process is described by a probability distribution\\n\\\\begin{equation}\\nP(r;p,N)={N! \\\\over r! (N-r)!} p^r q^{N-r} \\n\\\\quad,\\n\\\\end{equation}\\nwhere $ q \\\\equiv 1-p$.\\nSome examples are shown in Fig.~\\\\ref{fig:binom}.\\nThe distribution has \\nmean $\\\\mu=Np$, variance $V=Npq$, and standard deviation $\\\\sigma=\\\\sqrt{Npq}$.\\n\\\\subsubsection {The Poisson distribution}\\nThe Poisson distribution also describes the probability of some discrete number $r$,\\nbut rather than a fixed number of `trials' it considers a random rate $\\\\lambda$: \\n\\\\begin{equation}\\nP(r;\\\\lambda)=e^{-\\\\lambda}{\\\\lambda^r \\\\ \\\\over r!}\\n\\\\quad.\\n\\\\end{equation}\\nIt is linked to the binomial---the Poisson is the \\nlimit of the binomial---as $N\\\\to \\\\infty$, $p \\\\to 0$ with $np=\\\\lambda=constant$. Figure~\\\\ref{fig:poisson} shows various examples. It has mean $\\\\mu=\\\\lambda$, variance $V=\\\\lambda$, and standard deviation $\\\\sigma=\\\\sqrt{\\\\lambda}=\\\\sqrt{\\\\mu}$.\\nThe clicks of a Geiger counter are the standard illustration of a Poisson process.\\nYou will meet it a lot as it applies to event counts---on their own or in histogram bins.\\nTo help you think about the Poisson, here is a simple question (which\\ndescribes a situation \\nI have seen in practice, more than once, from people who ought to know better).\\n\\\\\\n\\\\hrule\\n\\\\\\nYou need to know the efficiency of your PID system for positrons.\\nYou find 1000 data events where 2 tracks have a combined mass of 3.1~GeV ($J/\\\\psi)$ and the negative track is \\nidentified as an $e^-$ (`Tag-and-probe' technique).\\nIn 900 events the $e^+$ is also identified. In 100 events it is not. The efficiency is 90\\\\\\nWhat about the error?\\nColleague A says $\\\\sqrt{900}=30$ so efficiency is $90.0 \\\\pm 3.0 $\\\\\\ncolleague B says $\\\\sqrt{100}=10$ so efficiency is $90.0 \\\\pm 1.0 $\\\\\\nWhich is right?\\n\\\\\\n\\\\hrule\\n\\\\\\nPlease think about this before turning the page...\\n\\\\vfill\\\\eject\\n{Neither---both are wrong}. This is binomial not Poisson: $p=0.9, N=1000$.\\n\\\\noindent The error is $\\\\sqrt{Npq}=\\\\sqrt{1000 \\\\times 0.9 \\\\times 0.1}$ (or $\\\\sqrt{1000 \\\\times 0.1 \\\\times 0.9}$) =$\\\\sqrt{90} = 9.49$ so the efficiency is $90.0 \\\\pm 0.9$ \\\\\\n\\\\subsubsection{The Gaussian distribution}\\nThis is by far the most important statistical distribution.\\nThe probability density function (PDF) for a variable $x$ is given by the formula\\n\\\\begin{equation} \\nP(x;\\\\mu,\\\\sigma)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-{(x-\\\\mu)^2 \\\\over 2 \\\\sigma^2}}\\n\\\\quad.\\n\\\\end{equation}\\nPictorially this is shown in Fig.~\\\\ref{fig:gauss}.\\nThis is sometimes called the `bell curve', though in fact a real bell does not have flared edges like that.\\nThere is (in contrast to the Poisson and binomial) \\nonly one \\nGaussian curve, as $\\\\mu$ and $\\\\sigma$ are just location and scale parameters.\\nThe mean is $\\\\mu$ and the standard deviation is $\\\\sigma$. The \\nSkew is zero, as it is symmetric, and the kurtosis is zero by construction.\\nIn statistics, and most disciplines, this is known as the {\\\\em normal distribution}. Only in physics is it known as `The Gaussian'---perhaps because the word `normal' already has so many meanings. \\nThe reason for the importance of the Gaussian is the {\\\\em central limit theorem} (CLT) that states:\\nif the variable $X$ is the sum of $N$ variables $x_1,x_2\\\\dots x_N$ then:\\n\\\\begin{enumerate}\\n\\\\item Means add: $ \\\\langle X \\\\rangle = \\\\langle x_1 \\\\rangle + \\\\langle x_2 \\\\rangle + \\\\dots \\\\langle x_N \\\\rangle$, \\n\\\\item Variances add: $V_X=V_1+V_2 +\\\\dots V_N$,\\n\\\\item If the variables $x_i$ are independent and identically distributed (i.i.d.) then $P(X)$ tends to a Gaussian for large $N$.\\n\\\\end{enumerate}\\n(1) is obvious, (2) is pretty obvious, and means that standard deviations add in quadrature, and that the standard deviation of an average falls like $1\\\\over \\\\sqrt N$, (3) applies whatever the form of the original $P(x)$.\\nBefore proving this, it is helpful to see a demonstration to convince yourself that the implausible assertion in (3)\\nactually does happen.\\nTake a uniform distribution from 0 to 1, as shown in the top left subplot of Fig.~\\\\ref{fig:CLT}. It is flat. Add two such numbers and the distribution is triangular, between 0 and 2, as shown in the top right.\\nWith 3 numbers, at the bottom left, it gets curved. With 10 numbers, at the bottom right, it looks pretty Gaussian. The proof follows. \\n\\\\begin{proof}\\nFirst, introduce the characteristic function $\\\\langle e^{i k x} \\\\rangle = \\\\int e^{i k x } P(x) \\\\, dx = \\\\tilde P(k)$.\\nThis can usefully be thought of as an expectation value and as a Fourier transform, FT.\\nExpand the exponential as a series\\n$\\\\langle e^{i k x} \\\\rangle = \\\\langle 1+ikx+{(ikx)^2 \\\\over 2!}+{(ikx)^3 \\\\over 3!}\\\\dots \\\\rangle = 1 + ik \\\\langle x \\\\rangle +(ik)^2{\\\\langle x^2 \\\\rangle \\\\over 2!} + (ik^3) {\\\\langle x^3 \\\\rangle \\\\over 3!} \\\\dots$. \\nTake the logarithm and use the expansion $\\\\ln(1+z)=z-{z^2 \\\\over 2 } + {z^3 \\\\over 3} \\\\dots$\\nThis gives a power series in $(ik)$, where the coefficient ${\\\\kappa_r \\\\over r!}$ of $(ik)^r$ is made up of expectation values of $x$ of total power $r$\\n$\\\\kappa_1= \\\\langle x \\\\rangle, \\\\kappa_2= \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2 =, \\\\kappa_3= \\\\langle x^3 \\\\rangle -3 \\\\langle x^2 \\\\rangle \\\\langle x \\\\rangle +2 \\\\langle x \\\\rangle^3 \\\\dots$ \\nThese are called the semi-invariant cumulants of Thi\\\\`ele . Under a change of scale $\\\\alpha$, $\\\\kappa_r \\\\to \\\\alpha^r \\\\kappa_r$. Under a change in location only $\\\\kappa_1$ changes.\\nIf $X$ is the sum of i.i.d. random variables, $x_1+x_2+x_3...$, then $P(X)$ is the convolution of $P(x)$ with itself $N$ times.\\nThe FT of a convolution is the product of the individual FTs,\\nthe logarithm of a product is the sum of the logarithms,\\nso $P(X)$ has cumulants $K_r=N \\\\kappa_r$.\\nTo make graphs commensurate, you need to scale the $X$ axis by the\\nstandard deviation, which grows like $\\\\sqrt{N}$. The cumulants of the scaled graph are $K'_r = N^{1-r/2} \\\\kappa_r$. \\nAs $N \\\\to \\\\infty$, these vanish for $r>2$, leaving a quadratic.\\nIf the log is a quadratic, the exponential is a Gaussian. So $\\\\tilde P(X)$ is Gaussian.\\nAnd finally, the inverse FT of a Gaussian is also a Gaussian.\\n\\\\end{proof} \\nEven if the distributions are not identical, the CLT tends to apply, unless one (or two) dominates.\\nMost `errors' fit this, being compounded of many different sources.\\n\", '\\\\section{Inference}\\n\\\\subsection{Maximum likelihood estimates}\\nThe maximum likelihood method takes as best-fit values of the unknown parameter\\nthe values that maximize the likelihood function (defined Sec.~\\\\ref{sec:likeFun}).\\nThe maximization of the likelihood function can be performed analytically only in the simplest cases,\\nwhile a numerical treatment is needed in most of the realistic cases.\\n{\\\\sc Minuit}~\\\\cite{minuit} is historically the most widely used minimization software engine in High Energy Physics.\\n\\\\subsubsection{Extended likelihood function}\\nGiven a sample of $N$ measurements of the variables $\\\\vec{x}=(x_1, \\\\cdots, x_n)$, the likelihood function expresses the probability\\ndensity evaluated for our sample as a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\n\\\\prod_{i=1}^Nf(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nThe size $N$ of the sample is in many cases also a random variable. In those cases,\\nthe {\\\\it extended likelihood function} can be defined as:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) \\\\prod_{i=1}^N f(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,,\\n\\\\end{equation}\\nwhere $P(N;\\\\theta_1,\\\\cdots,\\\\theta_m)$ is the distribution of $N$, and in practice is always a Poissonian \\nwhose expected rate parameter is a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) = \\\\frac{\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)^N e^{-\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)}}{N!}\\\\,.\\n\\\\end{equation}\\nIn many cases, either with a standard or an extended likelihood function,\\nit may be convenient to use $-\\\\ln L$ or $-2\\\\ln L$ in the numerical treatment\\nrather than $L$, because\\nthe product of the various terms is transformed into the sum of the logarithms of\\nthose terms, which may have advantages in the computation.\\nFor a Poissonian process that is given by the sum of a signal plus a background process,\\nthe extended likelihood function may be written as:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\n\\\\frac{(s+b)^N e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nf_sP_s(x_i;\\\\vec{\\\\theta}) + f_b P_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $s$ and $b$ are the signal and background expected yields, respectively,\\n$f_s$ and $f_b$ are the fraction of signal and background events, namely:\\n\\\\begin{eqnarray}\\nf_s & = & \\\\frac{s}{s+b} \\\\,,\\\\\\\\\\nf_b & = & \\\\frac{b}{s+b} \\\\,,\\n\\\\end{eqnarray}\\nand $P_s$ and $P_b$ are the PDF of the variable $x$ for signal and background,\\nrespectively.\\nReplacing $f_s$ and $f_b$ into Eq.~(\\\\ref{eq:extLikSB}) gives:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) = \\\\frac{e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIt may be more convenient to use the negative logarithm of Eq.~(\\\\ref{eq:extLikInt}),\\nthat should be minimize in order to determine the best-fit values of $s$, $b$ and $\\\\vec{\\\\theta}$:\\n\\\\begin{equation}\\n-\\\\ln L(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\ns + b -\\\\sum_{i=1}^N\\\\ln\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right) +\\\\ln N!\\\\,.\\n\\\\end{equation}\\nThe last term $\\\\ln N!$ is a constant with respect to the fit parameters,\\nand can be omitted in the minimization.\\nIn many cases, instead of using $s$ as parameter of interest,\\nthe {\\\\it signal strength} $\\\\mu$ is introduced, defined by the following equation:\\n\\\\begin{equation}\\ns = \\\\mu s_0\\\\,,\\n\\\\end{equation}\\nwhere $s_0$ is the theory prediction for the signal yield $s$.\\n$\\\\mu=1$ corresponds to the nominal value of the theory prediction for the signal yield.\\nAn example of unbinned maximum likelihood fit is given in Fig.~\\\\ref{fig:sbFit},\\nwhere the data are fit with a model inspired to Eq.~(\\\\ref{eq:extLikInt}), with\\n$P_s$ and $P_b$ taken as a Gaussian and an exponential distribution, respectively.\\nThe observed variable has been called $m$ in that case because the spectrum resembles an invariant mass peak,\\nand the position of the peak at 3.1~GeV reminds a $\\\\mathrm{J}/\\\\psi$ particle.\\nThe two PDFs can be written as:\\n\\\\begin{eqnarray}\\nP_s(m) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}e^{-\\\\frac{(m-\\\\mu)^2}{2\\\\sigma^2}}\\\\,,\\\\\\\\\\nP_b(m) & = & \\\\lambda e^{-\\\\lambda m}\\\\,.\\n\\\\end{eqnarray}\\nThe parameters $\\\\mu$, $\\\\sigma$ and $\\\\lambda$ are fit together with the\\nsignal and background yields $s$ and $b$. While $s$ is our {\\\\it parameter of interest},\\nbecause we will eventually determine a production cross section or branching fraction from\\nits measurement, the other additional parameters, that are not directly\\nrelated to our final measurement, are said {\\\\it nuisance parameters}.\\nIn general, nuisance parameters are needed to model background yield,\\ndetector resolution and efficiency, various parameters modeling the\\nsignal and background shapes, etc. Nuisance parameters are also important\\nto model {\\\\it systematic uncertainties}, as will be discussed more in\\ndetails in the following sections.\\n', \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\", \"\\\\section{Errors}\\n\\\\subsection{Errors in 2 or more dimensions}\\nFor 2 (or more) dimensions, one plots the log likelihood and defines regions using contours in $\\\\Delta \\\\ln L$ (or $\\\\Delta \\\\chi^2\\\\equiv - 2 \\\\Delta \\\\ln L$). An example is given in Fig.~\\\\ref{fig:CMS}.\\nThe link between the $\\\\Delta \\\\ln L$ values and the significance changes. \\nIn 1D, there is a 68\\\\In 2D, a 1$\\\\sigma$ square would give a probability $0.68^2 = 47\\\\If one rounds off the corners and draws a 1$\\\\sigma$ contour \\nat $\\\\Delta \\\\ln L=-\\\\half$ this falls to 39\\\\To retrieve the full 68\\\\ $\\\\Delta \\\\chi^2=2.27$.\\nFor 95\\\\\\nThe necessary value is obtained from the $\\\\chi^2$ distribution---described later. It can be found by the R function {\\\\tt qchisq(p,n)} or the Root function {\\\\tt TMath::ChiSquareQuantile(p,n)}, where the desired \\nprobability {\\\\tt p} and number of degrees of freedom {\\\\tt n} are the arguments given.\\n\\\\subsubsection{Nuisance parameters}\\nIn the example of Fig.~\\\\ref{fig:CMS}, both $C_V$ and $C_F$ are interesting.\\nBut in many cases one is interested only in one (or some) of the quantities and the others\\nare `nuisance parameters' that one would like to remove, reducing the dimensionality of the \\nquoted result. There are two methods of doing this, one (basically) frequentist and one Bayesian.\\nThe frequentist uses the {\\\\it profile likelihood} technique. Suppose that there are two parameters,\\n$a_1$ and $a_2$, where $a_2$ is a nuisance parameter, and so one wants to reduce the \\njoint likelihood function $L(x;a_1,a_2)$ to some function $\\\\hat L(a_1)$.\\nTo do this one scans across the values of $a_1$ and inserts $\\\\hat{\\\\hat a}_2(a_1)$, the value of $a_2$ which maximises the likelihood for that particular $a_1$\\n\\\\begin{equation}\\n\\\\hat L (x,a_1)=L(a_1,\\\\hat{\\\\hat a}_2(a_1))\\n\\\\end{equation}\\n\\\\noindent and the location of the maximum and the $\\\\Delta \\\\ln L=\\\\-\\\\half$ errors are\\nread off as usual. \\nTo see why this works---though this is not a very rigorous motivation---suppose one had a likelihood function\\nas shown in Fig.~\\\\ref{fig:profile}.\\nThe horizontal axis is for the parameter of interest, $a_1$, and the vertical for the nuisance parameter $a_2$.\\nDifferent values of $a_2$ give different results (central and errors) for $a_1$.\\nIf it is possible to transform to $a_2'(a_1,a_2)$ so that $L$ factorises, then we can write\\n$L(a_1,a_2')=L_1(a_1)L_2(a_2')$: this is shown in the plot on the right.\\nWe suppose that this is indeed possible. In the case here, and other not-too-complicated cases, it clearly is, although\\nit will not be so in more complicated topologies with multiple peaks. \\nThen using the transformed graph, \\nwhatever the value of $a_2'$, one would get the same result for $a_1$.\\nThen one can present this result for $a_1$, independent of anything about $a_2'$.\\nThere is no need to factorise explicitly: the\\npath of central $a_2'$ value as a function of $a_1$ (the central of the 3 lines on the right hand plot)\\nis the path of the peak, and that path can be located in the first plot (the transformation only stretches the $a_2$ axis, it does not change the heights).\\nThe Bayesian method uses the technique called {\\\\it marginalisation}, which \\njust integrates over $a_2$.\\nFrequentists can not do this as they are not allowed to integrate likelihoods over the parameter:\\n$\\\\int P(x;a)\\\\, dx$ is fine, but $\\\\int P(x;a)\\\\, da$ is off limits.\\nNevertheless this can be a very helpful alternative to profiling, specially for many nuisance parameters.\\nBut if you use it you must be aware that this is strictly Bayesian.\\nReparametrizing $a_2$ (or choosing a different prior) will give different results for $a_1$.\\nIn many cases, where the effect of the nuisance parameter is small, this does not\\nhave a big effect on the result.\\n\", \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{The Profile Likelihood}\\nAs noted in Sec.~\\\\ref{sec:likelihood}, likelihood functions can be used to estimate the parameters\\non which they depend. The method of choice to do so, in a frequentist analysis, is called \\\\textbf{maximum\\nlikelihood}, a method first used by Karl Frederick Gauss, \\\\emph{The Prince of Mathematics}, but developed into a formidable statistical tool in the 1930s by \\nSir Ronald A. Fisher~\\\\cite{Fisher}, perhaps the most influential statistician of the twentieth century.\\nFisher showed that a good way to estimate the parameters of a likelihood function is to pick\\nthe value that maximizes it. Such estimates are called \\\\textrm{maximum likelihood estimates} (MLE). In general, a function into which data can be inserted to yield an MLE of\\na parameter is called a maximum likelihood estimator. For simplicity, we shall use the \\nsame abbreviation MLE\\nto mean both the estimate and the estimator and we shall not be too\\npicky about distinguishing the two. The D\\\\O\\\\ top quark\\ndiscovery example illustrates the\\nmethod. \\n\\\\begin{quote}\\n\\\\paragraph*{Example: Top Quark Discovery Revisited}\\nWe start by listing\\n\\\\begin{align*}\\n& \\\\textbf{the knowns} \\\\\\\\\\n& D = N, B \\\\text{ where} \\\\\\\\\\n& N = 17 \\\\textrm{ observed events} \\\\\\\\\\n& B = 3.8 \\\\textrm{ estimated background events with uncertainty } \\\\delta B = 0.6 \\\\\\\\\\n&\\\\textbf{and the unknowns} \\\\\\\\\\n& b \\\\quad\\\\textrm{mean background count}\\\\\\\\\\n& s \\\\quad\\\\textrm{mean signal count}.\\n\\\\end{align*} \\nNext, we construct a probability model for the data $D = N, B$ assuming that \\n$N$ and $B$ are statistically independent. Since this is a counting\\nexperiment, we shall assume that $p(x| s, b)$ is a Poisson distribution with mean\\ncount $s + b$. In the absence of details about how the background $B$ was arrived\\nat, the standard assumption is that data of the form $y \\\\pm \\\\delta y$ can be modeled\\nwith a Gaussian (or normal) density. However, we can do a bit better. Background estimates are usually based on auxiliary experiments, either real or simulated, that define control regions. \\nSuppose that the observed count in the control region is $Q$ and the mean count is $b k$, where $k$ (ideally) is the known scale factor between the control and signal regions. We can model these data with a\\nPoisson distribution with count $Q$ and mean $b k$. But, we are given $B$ and $\\\\delta B$ rather than $Q$ and $k$, so we need a model to relate the two pairs of numbers. \\nThe simplest model is $B = Q / k$ and $\\\\delta B = \\\\sqrt{Q} / k$ from which we can infer an effective count $Q$ using $Q = (B / \\\\delta B)^2$. What of the scale factor $k$? Well, since it\\nis not given, it must be estimated. The obvious estimate is $Q / B = B / \\\\delta B^2$.\\nWith these assumptions, our likelihood function is\\n\\\\begin{eqnarray}\\np(D | s, b) & = & \\\\textrm{Poisson}(N, s + b) \\\\, \\\\textrm{Poisson}(Q, bk), \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nQ & = & (B / \\\\delta B)^2 = 41.11,\\\\nonumber\\\\\\\\\\nk & = & B / \\\\delta B^2 = 10.56. \\\\nonumber\\n\\\\end{eqnarray}\\nThe first term in Eq.~(\\\\ref{eq:toplh}) is the likelihood for the count $N = 17$, while\\nthe second term is the likelihood for $B = 3.8$, or equivalently the count $Q$. The\\nfact that $Q$ is not an integer causes no difficulty: we merely write\\nthe Poisson distribution as\\n$(bk)^Q \\\\exp(-bk) / \\\\Gamma(Q+1)$, which permits continuation to non-integer counts $Q$.\\nThe maximum likelihood estimators \\nfor $s$ and $b$ are found by maximizing Eq.~(\\\\ref{eq:toplh}), that is, by solving the equations \\n\\\\begin{align}\\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial s} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{s} = N - B, \\\\nonumber\\\\\\\\ \\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial b} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{b} = B, \\\\nonumber\\n\\\\end{align}\\nas expected.\\nA more complete analysis would account for the uncertainty in\\n$k$. One way is to introduce two more control regions with observed counts $V$ and $W$ and mean counts $v$ and $w k$, respectively, and extend \\nEq.~(\\\\ref{eq:toplh}) with two more Poisson distributions.\\n\\\\end{quote}\\n\\\\bigskip\\nThe maximum likelihood method is the most widely used method for\\nestimating parameters because it generally leads to reasonable estimates. But the\\nmethod has features, or encourages practices, which, somewhat uncharitably, we label the\\ngood, the bad, and the ugly!\\n\\\\begin{itemize}\\n\\\\item \\\\emph{The Good}\\n\\\\begin{itemize}\\n\\\\item Maximum likelihood estimators are consistent: the RMS goes to zero as more\\nand more data are included in the likelihood. This is an extremely important property,\\nwhich basically says it makes sense to take more data because we shall get more accurate results. One would not knowingly use an inconsistent estimator!\\n\\\\item If an unbiased estimator for a parameter exists the maximum\\nlikelihood method will find it.\\n\\\\item Given the MLE for $s$, the MLE for any function $y = g(s)$ of $s$ is,\\nvery conveniently, just $\\\\hat{y} = g(\\\\hat{s})$. This is a very nice practical feature which\\nmakes it possible to maximize the likelihood using the most convenient parameterization of it and then transform back to the parameter of interest at the end. \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Bad (according to some!)}\\n\\\\begin{itemize}\\n\\\\item In general, MLEs are biased.\\\\\\\\ \\\\\\\\\\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 7:} Show this\\\\\\\\\\nHint: Taylor expand $y = g(\\\\hat{s} + h)$ about the MLE $\\\\hat{s}$,\\\\\\\\\\nthen consider its ensemble average. }} \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Ugly (according to some!)}\\n\\\\begin{itemize}\\n\\\\item The fact that most MLEs are biased encourages the routine application of bias correction, which can waste data and, sometimes, yield absurdities.\\n\\\\end{itemize}\\n\\\\end{itemize}\\n\\\\noindent\\nHere is an example of the seriously ugly.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nFor a discrete probability distribution $p(k)$, the \\\\textbf{moment generating function} is the ensemble average\\n\\\\begin{align*}\\nG(x) & = < e^{xk} > \\\\\\\\\\n& = \\\\sum_{k} e^{xk} \\\\, p(k).\\n\\\\end{align*}\\nFor the binomial, with parameters $p$ and $n$, this is\\n\\\\begin{align*}\\nG(x) & = (e^x p + 1 - p)^n, \\\\quad \\\\framebox{\\\\textbf{Exercise 8a:} Show this}\\n\\\\end{align*}\\nwhich is useful for calculating \\\\textbf{moments}\\n\\\\begin{align*}\\n\\\\mu_r & = \\\\left. \\\\frac{d^rG}{dx^r}\\\\right |_{x=0} = \\\\sum_k k^r \\\\, p(k),\\n\\\\end{align*}\\ne.g., $\\\\mu_2 = (np)^2 + np - np^2$ for the binomial distribution.\\nGiven that $k$ events out $n$ pass a set of cuts, the MLE of the event selection efficiency is\\nthe obvious estimate $\\\\hat{p} = k / n$. The equally obvious estimate of $p^2$ is $( k / n)^2$.\\nBut,\\n\\\\begin{align*}\\n< ( k / n)^2 > & = p^2 + V / n , \\\\quad \\\\framebox{\\\\textbf{Exercise 8b:} Show this}\\n\\\\end{align*}\\nso $(k / n)^2$ is a biased estimate of $p^2$ with positive bias $V / n$. The unbiased estimate of $p^2$ is\\n\\\\begin{align*}\\nk(k-1) / [ n (n - 1)] , \\\\quad \\\\framebox{\\\\textbf{Exercise 8c:} Show this}\\n\\\\end{align*}\\nwhich, for a single success, i.e., $k = 1$, yields the sensible estimate $\\\\hat{p} = 1 / n$, but\\nthe less than helpful one $\\\\hat{p^2} = 0!$\\n\\\\end{quote}\\n\\\\bigskip\\nIn order to infer a value for the parameter of interest, for example, \\nthe signal $s$ in\\nour 2-parameter likelihood function in Eq.~(\\\\ref{eq:toplh}), the likelihood\\nmust be reduced to one involving the parameter of interest only, here $s$, \\nby somehow getting rid of all the \\\\textbf{nuisance} parameters, here the background\\nparameter $b$. A nuisance parameter is simply a parameter that is not of current interest.\\nIn a strict frequentist calculation, this reduction to the parameter of interest must be done\\nin such a way as to respect the frequentist principle: \\\\emph{coverage probability $\\\\geq$ confidence level}. In general, this is very difficult to do exactly.\\nIn practice, we replace all nuisance parameters by their \\\\textbf{conditional maximum likelihood\\nestimates} (CMLE). The CMLE is the maximum likelihood estimate conditional on\\na \\\\emph{given} value of the current parameter (or parameters) of interest. In the top discovery example, we construct an estimator of $b$ as a function of $s$, $\\\\hat{b}(s)$, and\\nreplace $b$ in the likelihood $p(D | s, b)$ by $\\\\hat{b}(s)$ to yield a function\\n$p_{PL}(D | s)$ called the \\\\textbf{profile likelihood}.\\n\\\\begin{quote}\\n\\\\emph{Since the profile likelihood entails an approximation, namely, replacing unknown parameters by their conditional estimates, it is not the likelihood but rather an approximation to it. Consequently, \\nthe frequentist principle is not guaranteed to be satisfied exactly.}\\n\\\\end{quote}\\nThis does not seem to be much progress. However, things are much better than they may appear because of\\nan important theorem proved by Wilks in 1938. If certain conditions are met, roughly that the\\nMLEs do not occur on the boundary of the parameter space and the likelihood becomes\\never more Gaussian as the data become more numerous --- that is, in the so-called\\n\\\\textbf{asymptotic limit}, then if the true density of $x$ is $p(x| s, b)$ the random number\\n\\\\begin{align}\\nt(x, s) & = -2 \\\\ln \\\\lambda(x, s), \\\\\\\\\\n\\\\textrm{where } \\\\lambda(x, s) & = \\\\frac{p_{PL}(x | s)}{ p_{PL}(x | \\\\hat{s})}, \\n\\\\end{align}\\nhas a probability density that converges to a $\\\\chi^2$ density with one degree of\\nfreedom. More generally, if the numerator of $\\\\lambda$ contains $m$ free parameters the\\nasymptotic density of $t$ is a $\\\\chi^2$ density with $m$ degrees of freedom. Therefore, we may take $t(D, s)$ to be a $\\\\chi^2$ variate, at least\\napproximately, and solve $t(D, s) = n^2$ for $s$ to get \\napproximate $n$-standard deviation confidence intervals. In particular, if we solve $t(D, s) = 1$, we\\nobtain approximate 68\\\\Wilks' theorem provides the main justification for using the profile likelihood. \\nWe again use the top discovery example to illustrate the procedure.\\n\\\\begin{quote} \\n\\\\paragraph*{Example: Top Quark Discovery Revisited Again}\\nThe conditional MLE of $b$ is found to be\\n\\\\begin{align}\\n\\\\hat{b}(s) & = \\\\frac{g + \\\\sqrt{g^2 + 4 (1 + k) Q s}}{2(1+k)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\ng & = N + Q - (1+k) s.\\\\nonumber\\n\\\\end{align}\\nThe likelihood $p(D | s, b)$ is shown in Fig.~\\\\ref{fig:toppl}(a) together with\\nthe graph of $\\\\hat{b}(s)$. The mode (i.e. the peak) occurs at $s = \\\\hat{s} = N - B$.\\nBy solving $$-2 \\\\ln \\\\frac{p_{PL}(17 | s)}{ p_{PL}(17 | 17 - 3.8)} = 1$$ for $s$ we get two solutions\\n$s = 9.4$ and $s = 17.7$. Therefore, we can make the statement\\n$s \\\\in [9.4, 17.7]$ at approximately 68\\\\$-\\\\ln \\\\lambda(17, s)$ created using \\nthe {\\\\tt RooFit}~\\\\cite{RooFit} and {\\\\tt RooStats}~\\\\cite{RooStats} packages.\\n\\\\smallskip\\n\\\\framebox{\\\\textbf{Exercise 9:} Verify this interval using the {\\\\tt RooFit/RooStats} package}\\n\\\\medskip\\nIntervals constructed this way are not guaranteed to\\nsatisfy the frequentist principle. In practice, however, \\ntheir coverage is very good for the typical probability models\\nused in particle physics, even for modest amounts of\\ndata. This is illustrated in Fig.~\\\\ref{fig:wilks}, which shows how rapidly the density of $t(x, s)$ \\nconverges to a $\\\\chi^2$ density for the probability distribution $p(x, y| s, b) = \\\\textrm{Poisson}(x|s+b) \\\\textrm{Poisson}(y | b)$\\\\footnote{It was the difficulty of extracting information\\nfrom this distribution that compelled the \\nauthor (against his will) to repair his parlous knowledge of statistics~\\\\cite{Fidecaro:1985cm}!}.\\nThe figure also shows what happens if we impose the restriction $\\\\hat{s} \\\\geq 0$, that is,\\nwe forbid negative signal estimates.\\n\\\\end{quote}\\n\", \"\\\\section{Probability}\\n\\\\subsection {Bayesian probability}\\nThe Bayesian definition of probability is that $P_A$ represents your belief in $A$.\\n1 represents certainty, 0 represents total disbelief.\\nIntermediate values can be calibrated by asking whether you would prefer to bet on $A$, or on a white ball being drawn from an urn containing a mix of white and black balls. \\nThis avoids the limitations of frequentist probability---coins, dice, kaons, rain tomorrow, existence of supersymmetry (SUSY) can all have probabilities assigned to them.\\nThe drawback is that your value for $P_A$ may be different from mine, or anyone else's. It is\\nalso called subjective probability.\\nBayesian probability makes great use of Bayes' theorem, in the form\\n\\\\begin{equation}P(Theory|Data)= {P(Data|Theory) \\\\over P(Data) } \\\\times P(Theory) \\\\quad.\\n\\\\end{equation}\\n$P(Theory)$ is called the {\\\\em prior}: your initial belief in $Theory$. $P(Data|Theory)$ is the {\\\\em Likelihood}:\\nthe probability of getting $Data$ if $Theory$ is true. $P(Theory|Data)$ is the {\\\\em Posterior}: your belief in $Theory$ \\nin the light of a particular $Data$ being observed.\\nSo this all works very sensibly. If the data observed is predicted by the theory, your belief in that theory is boosted,\\nthough this is moderated by the probabilty that the data could have arisen anyway. Conversely, if data is observed which \\nis disfavoured by the theory, your belief in that theory is weakened.\\nThe process can be chained. \\nThe posterior from a first experiment can be taken as the prior for a second experiment, and so on. \\nWhen you write out the factors you find that the order doesn't matter. \\n\\\\subsubsection{Prior distributions}\\nOften, though, the theory being considered is not totally defined: it may contain a parameter (or several parameters)\\nsuch as a mass, coupling constant, or decay rate. Generically we will call this $a$, with the proviso that it may\\nbe multidimensional. \\nThe prior is now not a single number $P(Theory)$ \\nbut a probability distribution $P_0(a)$. \\n$\\\\int_{a_1}^{a_2} P_0(a)\\\\, da $ is your prior belief that $a$ lies between $a_1$ and $a_2$.\\n$\\\\int_{-\\\\infty}^{\\\\infty} P_0(a)\\\\, da$ is your original $P(Theory)$. This is generally taken as 1, which is valid provided the possibility that the theory that is false is matched by some value of $a$---for example if the coupling constant for a hypothetical particle is zero, that accommodates any belief that it might not exist. Bayes' theorem then runs:\\n\\\\begin{equation} P_1(a;x) \\\\propto L(a;x) P_0(a)\\n\\\\quad.\\n\\\\end{equation}\\nIf the range of $a$ is infinite, $P_0(a)$ may be vanishingly small (this is called an `improper prior'). \\nHowever this is not a problem. Suppose, for example, that all we know about $a$ is that it is non-negative, and we\\nare genuinely equally open to its having any value. We write $P_0(a)$ as $C$, so $\\\\int_{a_1}^{a_2} P_0(a)\\\\, da =C(a_2-a_1)$.\\nThis probability is vanishingly small: if you were offered the choice of a bet on $a$ lying within the range $[a_1,a_2]$\\nor of drawing a white ball from an urn containing 1 white ball and $N$ black balls, you would choose the latter, however large $N$ was. However it is not zero: if the urn contained $N$ black balls, but no white ball, your betting choice would change. After a measurement you have\\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) C da'} C$, and the factors of $C$ can be cancelled (which, and this is the point, you could {\\\\em not} do if $C$ were exactly zero) giving \\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) da'} $ or,\\n$\\nP_1(a;x) \\\\propto L(a;x) $,\\nand you can then just normalize $P_1(a)$ to 1.\\nFigure~\\\\ref{fig:bayes1} shows Eq.~\\\\ref{eq:bayes} at work. Suppose $a$ is known to lie between 0 and 6, and\\nthe prior distribution is taken as flat, as shown in the left hand plot. A measurement of $a$ gives a result \\n$4.4 \\\\pm 1.0$~, as shown in the central plot. The product of the two gives (after normalization) the posterior, as shown in the right hand plot.\\n\\\\subsubsection{Likelihood}\\nThe likelihood---the number $P(Data|Theory)$---is now generalised to the function $L(a,x)$, where $x$ is the observed value of the data. Again, $x$ may be multidimensional, but in what follows it is not misleading to ignore that.\\nThis can be confusing. For example, anticipating Section~\\\\ref{sec:poisson}, the probability of getting $x$ counts from a Poisson process with mean $a$ is\\n\\\\begin{equation}\\nP(x,a)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWe also write\\n\\\\begin{equation}\\nL(a,x)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWhat's the difference? Technically there is none. These are identical joint functions of two variables ($x$ and $a$)\\nto which we have just happened to have given different names. Pragmatically we regard Eq.~\\\\ref{eq:lone}\\nas describing the probability of getting various different $x$ from some fixed $a$, whereas Eq.~\\\\ref{eq:ltwo}\\ndescribes the likelihood for various different $a$ from some given $x$. \\nBut be careful with the term `likelihood'. If $P(x_1,a)>P(x_2,a)$ then $x_1$ is more probable (whatever you mean by that) than\\n$x_2$. If $L(a_1,x)>L(a_2,x)$ it does not mean that $a_1$ is more likely (however you define that) than $a_2$.\\n\\\\subsubsection{Shortcomings of Bayesian probability}\\nThe big problem with Bayesian probability is that it is subjective.\\nYour $P_0(a)$ and my $P_0(a)$ may be different---so how can we compare results?\\nScience does, after all, take pride in being objective: it handles real facts, not opinions.\\nIf you present a Bayesian result from your search for the $X$ particle this embodies\\nthe actual experiment and your irrational prior prejudices. I am interested in your experiment but not\\nin your irrational prior prejudices---I have my own---and it is unhelpful if you combine the two.\\nBayesians sometimes ask about the right prior they should use. \\nThis is the wrong question. The prior is what you believe, and only you know that.\\nThere is an argument made for taking the prior as uniform. This is sometimes\\ncalled the \\n`Principle of ignorance' and justified as being impartial. But this is misleading, even dishonest. \\nIf $P_0(a)$ is taken as constant, favouring no particular value, then it is not constant for $a^2$ or $\\\\sqrt a$ or $\\\\ln a$,\\nwhich are equally valid parameters. \\nIt is true that with lots of data, $P_1(a)$ decouples from $P_0(a)$.\\nThe final result depends only on the measurements.\\nBut this is not the case with little data---and that's the situation we're usually in---when doing statistics properly matters.\\nAs an example, suppose you make a Gaussian measurement (anticipating slightly Section~\\\\ref{sec:measurement}).\\nYou consider a prior flat in $a$ and a prior flat in $\\\\ln a$. This latter is quite sensible---it says you expect a \\nresult between 0.1 and 0.2 as being equally likely as a result between 1 and 2, or 10 and 20.\\nThe posteriors are shown in Fig.~\\\\ref{fig:differentpriors}.\\nFor an `accurate' result of $3\\\\pm 0.5$ the posteriors are very close. For an `intermediate' result\\nof $4.0 \\\\pm 1.0$ there is an appreciable difference in the peak value and the shape. For a `poor'\\nmeasurement of $5.0 \\\\pm 2.0$ the posteriors are {\\\\em very} different.\\nSo you should never just quote results from a single prior. \\nTry several forms of prior and examine the spread of results. If they are pretty much the same\\nyou are vindicated. This is called\\n`robustness under choice of prior' and it is standard practice for statisticians. If they are different\\nthen the data are telling you about the limitations of your results.\\n\\\\subsubsection {Jeffreys' prior}\\nJeffreys~\\\\cite{Jeffreys} suggested a technique now known as the Jeffreys' or {\\\\em objective prior}: that\\nyou should \\nchoose a prior flat in a transformed variable $a'$ for which the Fisher information, ${\\\\cal I} =-\\\\left< {\\\\partial^2 L(x;a)\\n\\\\over \\\\partial a^2}\\\\right> $ is constant. \\nThe Fisher information (which is important in maximum likelihood estimation, as described in Section~\\\\ref{sec:ML})\\nis a measure of how much a measurement tells you about the parameter: a large ${\\\\cal I}$ has a likelihood function with a sharp peak and will tell you (by some measure) a lot about $a$; a small ${\\\\cal I}$ has a featureless likelihood function\\nwhich will not be useful. Jeffrey's principle is that the prior should not favour or disfavour particular values of the parameter.\\nIt is equivalently---and more conveniently---used as taking a prior in the original $a$ which is proportional to\\n$\\\\sqrt{\\\\cal I}$.\\nIt has not been universally adopted for various reasons. Some practitioners like to be able to include their own\\nprior belief into the analysis. It also makes the prior dependent on the experiment (in the form of the likelihood function). \\nThus if ATLAS and CMS searched for the same new $X$ particle they would use different priors for $P_0(M_X)$, \\nwhich is (to some people) absurd.\\nSo it is not universal---but when you are selecting a bunch of priors to test robustness---the Jefferys' prior \\nis a strong contender for inclusion.\\n\", \"\\\\section{Experimental sensitivity using the $\\\\mathcal{Q}$ estimator}\\nThe experimental sensitivity to detect a new physics signal depends on multiple factors, including the accelerator's integrated luminosity, data quality, detector triggers, simulations, and accurate estimation of events associated with known physics (background)~\\\\cite{barlow2002systematic}. Given the vast parameter space and variety of theories, there arises a need to identify a specific search region to efficiently focus experimental efforts towards generating new discoveries~\\\\cite{casadei2011statistical}.\\nPhenomenology in high-energy physics (HEP) defines this search region as the signal region, determined by the expected number of new physics events for certain observables, such as invariant mass, transverse mass, etc. A common strategy to identify this region involves maximizing the statistical significance of observing \\\\( n = b + \\\\mu s \\\\), with \\\\( \\\\mu = 1 \\\\) events consistent with the new theory, assuming the background-only hypothesis (\\\\( H_{0} \\\\)) is true. This expected number of events is referred to as Asimov data~\\\\cite{lista2016practical,cowan2011asymptotic}, and it is used to determine the parameter space window of the theory measurable in the experiment with a specific luminosity. The statistical estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), with expected value \\\\( n = s + b \\\\), is given by:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & 2( \\\\mu s - nLn(1 + \\\\frac{\\\\mu s}{b}) ) {} \\\\nonumber \\\\\\\\\\n\\\\mathcal{Q}(1) & = & 2( s - (s+b)Ln(1 + \\\\frac{s}{b}) ). {}\\n\\\\end{eqnarray}\\nFrom this value, \\\\( \\\\mathcal{Q}_{\\\\text{obs}}(1) \\\\) is calculated, which allows estimating the significance in the context of a distribution corresponding to background-only hypothesis:\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{\\\\text{obs}}(1)} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThe optimal signal region for the experimental search is determined by finding the expected number of new physics events that maximizes the statistical significance. Thus:\\n\\\\begin{equation}\\ns_{best} = \\\\max_{s} \\\\alpha(s).\\n\\\\end{equation}\\nThis value of \\\\( \\\\alpha \\\\) is converted into units of standard deviations from a \\\\( \\\\mathcal{N}(0,1) \\\\) distribution. For now, this estimate assumes that the number of background events is well-determined, with no associated statistical or systematic error. Systematic effects on the background can modify both the signal region and the upper limits of theoretical predictions, and must be taken into account in phenomenological studies. Figure~[\\\\ref{fig:12}] illustrates the distribution \\\\( f(\\\\mathcal{Q},0) \\\\) with \\\\( Q_{obs}(1) \\\\), which is used to estimate the significance of the new physics model with \\\\( s=10 \\\\) and \\\\( b=100 \\\\) background events. The value of \\\\( p_{0} \\\\) is \\\\( \\\\alpha = 0.1705 \\\\), which corresponds to \\\\( Z_{0} = 0.952 \\\\) standard deviations. This result can also be approximated using the estimator~\\\\cite{florez2016probing,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nZ_{0} = \\\\frac{s}{\\\\sqrt{s+b}} \\\\approx 0.953.\\n\\\\end{equation}\\nThis estimator is considered an approximate estimation of the signal significance and is valid when \\\\( s \\\\ll b \\\\) (Appendix~\\\\ref{sec:AppendixB}). It is worth noting that this estimator has been widely used in optimizing search regions beyond the Standard Model in the context of phenomenology and experimental analysis~\\\\cite{florez2016probing,allahverdi2016distinguishing,cms2012observation,atlas2012observation}.\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The distribution of the test statistic and $p$-values}\\nThe test statistic should be interpreted as a single real-valued number that represents the outcome of the experiment. More formally, it is a mapping of the data to a single real-valued number: \\\\mbox{$\\\\tilde{q}_\\\\mu: \\\\datasim,\\\\globs \\\\rightarrow \\\\mathbb{R}$}. For the observed data the test statistic has a given value, eg. $\\\\tilde{q}_{\\\\mu,\\\\rm obs}$. If one were to repeat the experiment many times the test statistic would take on different values, thus, conceptually, the test statistic has a distribution. Similarly, we can use our model to generate pseudo-experiments using Monte Carlo techniques or more abstractly consider the distribution. Since the number of expected events $\\\\nu(\\\\mu,\\\\vec\\\\theta)$ and the distributions of the discriminating variables $f_c(x_c|\\\\mu,\\\\vec\\\\theta)$ explicitly depend on $\\\\vec\\\\theta$ the distribution of the test statistic will also depend on $\\\\vec\\\\theta$. Let us denote this distribution \\n\\\\begin{equation}\\nf(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\;,\\n\\\\end{equation}\\nand we have analogous expressions for each of the test statistics described above.\\nThe $p$-value for a given observation under a particular hypothesis ($\\\\mu,\\\\vec\\\\theta$) is the probability for an equally or more `extreme' outcome than observed assuming that hypothesis \\n\\\\begin{equation}\\np_{\\\\mu,\\\\vec\\\\theta} = \\\\int_{\\\\tilde{q}_{\\\\mu,\\\\rm obs}}^\\\\infty f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\, d\\\\tilde{q}_\\\\mu\\\\;.\\n\\\\end{equation}\\nThe logic is that small $p$-values are evidence against the corresponding hypothesis. In Toy Monte Carlo approaches, the integral above is really carried out in the space of the data $\\\\int d\\\\datasim d\\\\globs$.\\nThe immediate difficulty is that we are interested in $\\\\mu$ but the $p$-values depend on both $\\\\mu$ and $\\\\vec\\\\theta$. In the frequentist approach the hypothesis $\\\\mu=\\\\mu_0$ would not be rejected unless the $p$-value is sufficiently small \\\\textit{for all} values of $\\\\vec\\\\theta$. Equivalently, one can use the supremum $p$-value for over all $\\\\vec\\\\theta$ to base the decision to accept or reject the hypothesis at $\\\\mu=\\\\mu_0$.\\n\\\\begin{equation}\\np^{\\\\rm sup}_{\\\\mu} = \\\\sup_{\\\\vec\\\\theta}\\\\; p_{\\\\mu,\\\\vec\\\\theta} \\n\\\\end{equation}\\nThe key conceptual reason for choosing the test statistics based on the profile likelihood ratio is that asymptotically (ie. when there are many events) the distribution of the profile likelihood ratio \\\\mbox{$\\\\lambda(\\\\mu=\\\\mu_{\\\\rm true})$} is independent of the values of the nuisance parameters. This follows from Wilks's theorem. In that limit $p^{\\\\rm sup}_{\\\\mu} = p_{\\\\mu,\\\\vec\\\\theta}$ for all $\\\\vec\\\\theta$. \\nThe asymptotic distributions \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu, \\\\vec\\\\theta)$} and \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu', \\\\vec\\\\theta)$} are known and described in Sec.~\\\\ref{sec:asymptotic}. For results based on generating ensembles of pseudo-experiements using Toy Monte Carlo techniques does not assume the form of the distribution $f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta)$, but knowing that it is approximately independent of $\\\\vec\\\\theta$ means that one does not need to calculate $p$-values for all $\\\\vec\\\\theta$ (which is not computationally feasible). Since there may still be some residual dependence of the $p$-values on the choice of $\\\\vec\\\\theta$ we would like to know the specific value of $\\\\vec\\\\theta^{\\\\rm sup}$ that produces the supremum $p$-value over $\\\\vec\\\\theta$. Since larger $p$-values indicate better agreement of the data with the model, it is not surprising that choosing $\\\\vec\\\\theta^{\\\\rm sup}=\\\\hathatthetamu$ is a good estimate of $\\\\vec\\\\theta^{\\\\rm sup}$. This has been studied in detail by statisticians, and is called the Hybrid Resampling method and is referred to in physics as the `profile construction'~\\\\cite{Feldman,Cranmer,hybridResampling,Bodhi}.\\nBased on the discussion above, the following $p$-value is used to quantify consistency with the hypothesis of a signal strength of $\\\\mu$:\\n\\\\begin{equation}\\np_{\\\\mu}=\\\\int_{\\\\tilde q_{\\\\mu,\\\\rm obs}}^{\\\\infty} f(\\\\tilde q_\\\\mu|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu,\\\\textrm{obs})) \\\\,d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nA standard 95\\\\\\nTo calculate the $CL_s$ upper limit, we define $p'_\\\\mu$ as a ratio of p-values,\\n\\\\begin{equation}\\np'_\\\\mu=\\\\frac{p_\\\\mu}{1-p_b} \\\\; ,\\n\\\\end{equation}\\nwhere $p_b$ is the $p$-value derived from the same test statistic under the background-only hypothesis\\n\\\\begin{equation}\\np_b=1-\\\\int_{\\\\tilde q_{\\\\mu,obs}}^\\\\infty f(\\\\tilde q_\\\\mu|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nThe $CL_s$ upper-limit on $\\\\mu$ is denoted $\\\\mu_{up}$ and obtained by solving for $p'_{\\\\mu_{up}}=5\\\\It is worth noting that while confidence intervals produced with the ``CLs'' method over cover, a value of $\\\\mu$ is regarded as excluded at the 95\\\\ \\nFor the purposes discovery one is interested in compatibility of the data with the background-only hypothesis. Statistically, a discovery corresponds to rejecting the background-only hypothesis. This compatibility is based on the following $p$-value\\n\\\\begin{equation}\\np_0=\\\\int_{\\\\tilde q_{0,obs}}^\\\\infty f(\\\\tilde q_0|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_0 \\\\;.\\n\\\\end{equation}\\nThis $p$-value is also based on the background-only hypothesis, but the test statistic $\\\\tilde q_0$ is suited for testing the background-only while the test statistic $\\\\tilde{q}_\\\\mu$ in Eq.~\\\\ref{eq:pb} is suited for testing a hypothesis with signal.\\nIt is customary to convert the background-only $p$-value into the quantile (or ``sigma'') of a unit Gaussian. This conversion is purely conventional and makes no assumption that the test statistic $q_0$ is Gaussian distributed. The conversion is defined as:\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1-p_0) ;\\\\,\\n\\\\end{equation}\\nwhere $\\\\Phi^{-1}$ is the inverse of the cumulative distribution for a unit Gaussian. One says the significance of the result is $Z\\\\sigma$ and the standard discovery convention is $5\\\\sigma$, corresponding to $p_0=2.87 \\\\cdot 10^{-7}$.\\n\", '\\\\section{Poisson distribution}\\nThe Poisson distribution (see Fig.~\\\\ref{fig:Poissons}) applies to situations where we are counting a series of observations which are occuring randomly and independently during a fixed time interval $t$, where the underlying rate $r$ is constant. The observed number $n$ will fluctuate when the experiment is repeated, and can in principle take any integer value from zero to infinity. The Poisson probabilty of observing $n$ decays is given by\\n\\\\begin{equation}\\nP_n = e^{-rt} (rt)^n/n!\\n\\\\end{equation}\\nIt applies to the number of decays observed from a large number $N$ of radioactive nuclei, when the observation time $t$ is small compared to the lifetime $\\\\tau$. It will not apply if $t$ is much larger than $\\\\tau$, or if the detection system has a dead time, so that after observing a decay the detector cannot observe another decay for a period $T_{dead}$.\\nAnother example is the number of counts in any specific bin of a histogram when the data is accumulated over a fixed time. \\nThe average number of observations is given by \\n\\\\begin{equation}\\n<n> = \\\\Sigma n P_n = rt\\n\\\\end{equation}\\nIf we write the expected number as $\\\\mu$, the Poisson probability becomes\\n\\\\begin{equation}\\nP_n = e^{-\\\\mu} \\\\mu^n/n!\\n\\\\end{equation}\\nIt is also relatively easy to show that the variance \\n\\\\begin{equation}\\n\\\\sigma^2 = \\\\Sigma (n - \\\\mu)^2 P_n = \\\\mu\\n\\\\end{equation}\\nThis leads to the well-known $n \\\\pm \\\\sqrt n$ approximation for the value of the Poisson parameter when we have $n$ counts. This approximation is, however, particularly bad when there are zero observed events; then $0\\\\pm0$ incorrectly suggests that the Poisson parameter can be only zero.\\nPoisson probabilities can be regarded as the limit of Binomial ones as the number of trials $N$ tends to infinity and the Binomial probability of success $p$ tends to zero, but the product $Np$ remains constant at $\\\\mu$. \\nWhen the Poisson mean becomes large, the distribution of observed counts approximates to a Gaussian (although the Gaussian is a continuous distribution extending down to $-\\\\infty$, while a Poisson observable can only take on non-negative integral values). This approximation is useful for the $\\\\chi^2$ method for parameter estimation and goodness of fit (see Lecture 2).\\n\\\\subsection{Relation of Poisson and Binomial Distributions}\\nAn interesting example of the relationship between the Poisson and Binomial distributions is exhibited by the following example.\\nImagine that the number of people attending a series of lectures is Poisson distributed with a constant \\nmean $\\\\nu$, and that the fraction of them who are male is $p$. Then the overall probability $P$ of \\nhaving N people of whom $M$ are male and $F = N - M$ are female is given by the product of the Poisson \\nprobability $P_{pois}$ for $N$ and the binomial probability $P_{bin}$ for $M$ of the $N$ people being male. i.e.\\n\\\\begin{equation}\\nP = P_{pois} P_{bin} = \\\\frac{e^{-\\\\nu} \\\\nu^N}{N!} \\\\times \\\\frac{N!}{M!F!} p^M (1-p)^F\\n\\\\end{equation} \\nThis can be rearranged as\\n\\\\begin{equation}\\nP = \\\\frac{e^{-\\\\nu p} (\\\\nu p)^M}{M!} \\\\times \\\\frac{e^{-\\\\nu (1-p)} (\\\\nu(1-p))^F}{F!}\\n\\\\end{equation} \\nThis is the product of two Poissons, one with Poisson parameter $\\\\nu p$, the expected number of males, and the other with parameter $\\\\nu(1-p)$, the expected number of females. Thus with a Poisson-varying total number of observations, divided into two categories (here male and female), we can regard this as Poissonian in the total number and Binomial in the separate categories, or as two independent Poissons, one for each category. Other situations to which this applies could be radioactive nuclei, with decays detected in the forward or backward hemispheres; cosmic ray showers, initiated by protons or by heavier nuclei; patients arriving at a hospital emergency centre, who survive or who die; etc.\\n\\\\subsection{For your thought}\\nThe first few Poisson probabilities $P(n;\\\\mu)$ are\\n\\\\begin{equation} \\n\\\\begin{split}\\nP(0) = e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ P(1) = \\\\mu e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ P(2) = (\\\\mu^2/2!)\\\\ e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ etc.\\n\\\\end{split}\\n\\\\end{equation}\\nThus for small $\\\\mu$, $P(1)$ and $P(2)$ are approximately $\\\\mu$ and $\\\\mu^2/2$ respectively. But if the probability\\nof one rare event happening is $\\\\mu$, why is the probability for 2 independent rare events not equal to $\\\\mu^2$?\\n', \"\\\\section{Inference}\\n\\\\subsection{Neyman's confidence intervals}\\nA procedure to determine frequentist {\\\\it confidence intervals} is due to\\nNeyman~\\\\cite{neyman_belt}. It proceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Scan the allowed range of the unknown parameter of interest $\\\\theta$.\\n\\\\item Given a value $\\\\theta_0$ of $\\\\theta$, compute the interval $[x_1(\\\\theta_0), x_2(\\\\theta_0)]$ that contains $x$ with a probability $1 - \\\\alpha$\\n({\\\\it confidence level}, or CL) equal to 68.3\\\\ For this procedure, a choice of interval ({\\\\it ordering rule}) is needed, as discussed in Sec.~\\\\ref{sec:BayesianInference}.\\n\\\\item For the observed value of $x$, invert the confidence belt: find the corresponding interval $[\\\\theta_1(x), \\\\theta_2(x)]$.\\n\\\\end{itemize}\\nBy construction, a fraction of the experiments equal to $1 -\\\\alpha$ will measure $x$ such that the corresponding\\n{\\\\it confidence interval} $[\\\\theta_1(x), \\\\theta_2(x)]$ contains ({\\\\it covers}) the true value of $\\\\theta$.\\nIt should be noted that the random variables are $\\\\theta_1(x)$ and $\\\\theta_2(x)$, not $\\\\theta$.\\nAn example of application of the Neyman's belt construction and inversion is shown in Fig.~\\\\ref{fig:NeymanBelt}.\\nThe simplest application of Neyman's belt construction can be done with a Gaussian\\ndistribution with known parameter $\\\\sigma=1$, as shown in Fig.~\\\\ref{fig:NeymanGaussianBelt}.\\nThe belt inversion is trivial and gives the expected result: a central value $\\\\hat{\\\\mu} = x$\\nand a confidence interval $[\\\\mu_1, \\\\mu_2] = [x - \\\\sigma, x + \\\\sigma]$.\\nThe result can be quoted as $\\\\mu = x\\\\pm\\\\sigma$, similarly to what was determined\\nwith Eq.~(\\\\ref{eq:trivialML}).\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Measurement as parameter estimation}\\nOne of the most common tasks of the working physicist is to estimate some model parameter. We do it so often, that we often don't realize it. For instance, the sample mean $\\\\bar{x} = \\\\sum_{e=1}^n x_e / n$ is an estimate for the mean, $\\\\mu$, of a Gaussian probability density $f(x|\\\\mu,\\\\sigma) =\\\\Gauss(x|\\\\mu,\\\\sigma)$. More generally, an \\\\textit{estimator} $\\\\hat{\\\\alpha}(\\\\data)$ is some function of the data and its value is used to estimate the true value of some parameter $\\\\alpha$. There are various abstract properties such as variance, bias, consistency, efficiency, robustness, etc~\\\\cite{James}. The bias of an estimator is defined as $B(\\\\hat\\\\alpha) = E[ \\\\hat\\\\alpha ]-\\\\alpha$, where $E$ means the expectation value of \\\\mbox{$E[ \\\\hat\\\\alpha ]=\\\\int\\\\hat\\\\alpha(x) f(x)dx$} or the probability-weighted average. Clearly one would like an unbiased estimator. The variance of an estimator is defined as $var[\\\\hat\\\\alpha] = E[ (\\\\alpha - E[\\\\hat{\\\\alpha}] )^2 ]$; and clearly one would like an estimator with the minimum variance. Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cram\\\\'er-Rao bound (that is the inverse of the Fisher information, which we will refer to again later). \\nThe most widely used estimator in physics is the maximum likelihood estimator (MLE). It is defined as the value of $\\\\alpha$ which maximizes the likelihood function $L(\\\\alpha)$. Equivalently this value, $\\\\hat{\\\\alpha}$, maximizes $\\\\log L(\\\\alpha)$ and minimizes $-\\\\log L(\\\\alpha)$. The most common tool for finding the maximum likelihood estimator is \\\\texttt{Minuit}, which conventionally minimizes $-\\\\log L(\\\\alpha)$ (or any other function)~\\\\cite{James:1975dr}. The jargon is that one `fits' the function and the maximum likelihood estimate is the `best fit value'. \\nWhen one has a multi-parameter likelihood function $L(\\\\vec{\\\\alpha})$, then the situation is slightly more complicated. The maximum likelihood estimate for the full parameter list, $\\\\hat{\\\\vec{\\\\alpha}}$, is clearly defined. The various components $\\\\hat{\\\\alpha}_p$ are referred to as the \\\\textit{unconditional maximum likelihood estimates}. In the physics jargon, one says all the parameters are `floating'. One can also ask about maximum likelihood estimate of $\\\\alpha_p$ is with some other parameters $\\\\vec{\\\\alpha}_o$ fixed; this is called the \\\\textit{conditional maximum likelihood estimate} and is denoted $\\\\hat{\\\\hat{\\\\alpha}}_p(\\\\vec{\\\\alpha}_o)$. These are important quantities for defining the profile likelihood ratio, which we will discuss in more detail later. The concept of variance of the estimates is also generalized to the covariance matrix $cov[\\\\alpha_p, \\\\alpha_{p'}] = E[(\\\\hat\\\\alpha_p - \\\\alpha_p)(\\\\hat\\\\alpha_{p'}- \\\\alpha_{p'})]$ and is often denoted $\\\\Sigma_{pp'}$. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie. $cov[\\\\alpha_p, \\\\alpha_{p}] = var[\\\\alpha_p]$.\\nIn the case of a Poisson model $\\\\Pois(n|\\\\nu)$ the maximum likelihood estimate of $\\\\nu$ is simply \\\\mbox{$\\\\hat{\\\\nu}=n$}. Thus, it follows that the variance of the estimator is $var[\\\\hat{\\\\nu}]=var[n]=\\\\nu$. Thus if the true rate is $\\\\nu$ one expects to find estimates $\\\\hat{\\\\nu}$ with a characteristic spread around $\\\\nu$; it is in this sense that the measurement has a estimate has some uncertainty or `error' of $\\\\sqrt{n}$. We will make this statement of uncertainty more precise when we discuss frequentist confidence intervals.\\nWhen the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution.\\\\footnote{There are various conditions that must be met for this to be true, but skip the fine print in these lectures. There are two conditions that are most often violated in particle physics, which will be addressed later.} This does not depend on the pdf $f(x)$ having a Gaussian form. For small samples this isn't the case, but this limiting distribution is often referred to as an \\\\textit{asymptotic distribution}.\\nFurthermore, under most circumstances in particle physics, the maximum likelihood estimate approaches the minimum variance or Cram\\\\'er-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by\\n\\\\begin{equation}\\n\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = E\\\\left[- \\\\frac{\\\\partial^2 \\\\log f(x|\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\middle| \\\\;\\\\vec\\\\alpha \\\\right ] \\\\;,\\n\\\\end{equation}\\nwhere I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the true value $\\\\vec\\\\alpha$. The right side of Eq.~\\\\ref{Eq:expfisher} is called the (expected) Fisher information matrix. Remember that the expectation involves an integral over the observables. Since that integral is difficult to perform in general, one often uses the observed Fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data\\n\\\\begin{equation}\\n\\\\tilde\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = - \\\\frac{\\\\partial^2 \\\\log L(\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\; .\\n\\\\end{equation}\\nThis is what \\\\texttt{Minuit}'s \\\\texttt{Hesse} algorithm\\\\footnote{The matrix is called the Hessian, hence the name.} calculates to estimate the covariance matrix of the parameters.\\n\", '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Incorporating Monte Carlo statistical uncertainty on the histogram templates}\\nThe histogram based approach described above are based Monte Carlo simulations of full detector simulation. These simulations are very computationally intensive and often the histograms are sparsely populated. In this case the histograms are not good descriptions of the underlying distribution, but are estimates of that distribution with some statistical uncertainty. Barlow and Beeston outlined a treatment of this situation in which each bin of each sample is given a nuisance parameter for the true rate, which is then fit using both the data measurement and the Monte Carlo estimate~\\\\cite{Barlow:1993dm}. This approach would lead to several hundred nuisance parameters in the current analysis. Instead, the \\\\texttt{HistFactory} employs a lighter weight version in which there is only one nuisance parameter per bin associated with the total Monte Carlo estimate and the total statistical uncertainty in that bin. If we focus on an individual bin with index $b$ the contribution to the full statistical model is the factor\\n\\\\begin{equation}\\n\\\\Pois(n_b | \\\\nu_b(\\\\vec\\\\alpha) + \\\\gamma_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)) \\\\, \\\\Pois(m_b | \\\\gamma_b \\\\tau_b) \\\\;,\\n\\\\end{equation}\\nwhere $n_b$ is the number of events observed in the bin, $\\\\nu_b(\\\\vec\\\\alpha)$ is the number of events expected in the bin where Monte Carlo statistical uncertainties need not be included (either because the estimate is data driven or because the Monte Carlo sample is sufficiently large), $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)$ is the number of events estimated using Monte Carlo techniques where the statistical uncertainty needs to be taken into account. Both expectations include the dependence on the parameters $\\\\vec\\\\alpha$. The factor $\\\\gamma_b$ is the nuisance parameter reflecting that the true rate may differ from the Monte Carlo estimate $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) $ by some amount. If the total statistical uncertainty is $\\\\delta_b$, then the relative statistical uncertainty is given by $\\\\nu_b^{\\\\rm MC}/\\\\delta_b$. This corresponds to a total Monte Carlo sample in that bin of size $m_b = (\\\\delta_b/\\\\nu_b^{\\\\rm MC})^2$. Treating the Monte Carlo estimate as an auxiliary measurement, we arrive at a Poisson constraint term $ \\\\Pois(m_b | \\\\gamma_b \\\\tau_b)$, where $m_b$ would fluctuate about $\\\\gamma_b \\\\tau_b$ if we generated a new Monte Carlo sample. Since we have scaled $\\\\gamma$ to be a factor about 1, then we also have $\\\\tau_b=(\\\\nu_b^{\\\\rm MC}/\\\\delta_b)^2$; however, $\\\\tau_b$ is treated as a fixed constant and does not fluctuate when generating ensembles of pseudo-experiments.\\nIt is worth noting that the conditional maximum likelihood estimate $\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha)$ can be solved analytically with a simple quadratic expression.\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha) = \\\\frac{-B + \\\\sqrt{B^2 - 4 AC}}{2A} \\\\;,\\n\\\\end{equation}\\nwith\\n\\\\begin{equation}\\nA = \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)^2 + \\\\tau_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nB= \\\\nu_b(\\\\vec\\\\alpha) \\\\tau + \\\\nu_b(\\\\vec\\\\alpha) \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - n_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - m_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nC= m_b \\\\nu_b(\\\\vec\\\\alpha) \\\\;.\\n\\\\end{equation}\\nIn a Bayesian technique with a flat prior on $\\\\gamma_b$, the posterior distribution is a gamma distribution. Similarly, the distribution of $\\\\hat\\\\gamma_b$ will take on a skew distribution with an envelope similar to the gamma distribution, but with features reflecting the discrete values of $m_b$. Because the maximum likelihood estimate of $\\\\gamma_b$ will also depend on $n_b$ and $\\\\hat{\\\\vec\\\\alpha}$, the features from the discrete values of $m_b$ will be smeared. This effect will be more noticeable for large statistical uncertainties where $\\\\tau_b$ is small and the distribution of $\\\\hat\\\\gamma_b$ will have several small peaks. For smaller statistical uncertainties where $\\\\tau_b$ is large the distribution of $\\\\hat\\\\gamma_b$ will be approximately Gaussian.', '\\\\section{Errors}\\n\\\\subsection{Errors from likelihood}\\nFor large $N$, the $\\\\ln L(a,x)$ curve is a parabola, as shown in Fig.~\\\\ref{fig:ML}.\\nAt the maximum, a Taylor expansion gives $\\\\ln L(a)=\\\\ln L(\\\\hat a)+{1 \\\\over 2} (a-\\\\hat a)^2 {d^2 \\\\ln L \\\\over da^2}$ $\\\\dots$\\nThe maximum likelihood estimator saturates the MVB, so \\n\\\\begin{equation}\\nV_{\\\\hat a}=-1/\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>\\n\\\\qquad \\\\sigma_{\\\\hat a}=\\\\sqrt{-{1 \\\\over {d^2 \\\\ln L \\\\over da^2}}}\\n\\\\quad.\\n\\\\end{equation}\\nWe approximate the expectation value $\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>$ by the actual value in this case\\n$ \\\\left. d^2 \\\\ln L \\\\over da^2 \\\\right|_{a=\\\\hat a}$ (for a discussion of the introduced inaccuracy, see Ref.~\\\\cite{DeltaML}).\\nThis can be read off the curve, as also shown in Fig.~\\\\ref{fig:ML}. The maximum gives the estimate.\\nYou then draw a line $1\\\\over 2$ below that (of course nowadays this is done within the code, not with pencil and ruler, but the visual image is still valid). This line $\\\\ln L(a)=\\\\ln L(\\\\hat a)-{1 \\\\over 2}$ intersects the likelihood curve at the points\\n$a=\\\\hat a \\\\pm \\\\sigma_{\\\\hat a}$. \\nIf you are working with $\\\\chi^2$, $L\\\\propto e^{-{1 \\\\over 2}\\\\chi^2}$ so the line is $\\\\Delta \\\\chi^2=1$.\\nThis gives $\\\\sigma$, or 68\\\\\\n', \"\\\\section{Probability theory}\\n\\\\subsection{Bayes theorem}\\nConsidering two events $A$ and $B$, using Eq.~(\\\\ref{eq:condProb}) twice, we can write:\\n\\\\begin{eqnarray}\\nP(A|B) & = & \\\\frac{P(A\\\\cap B)}{P(B)} \\\\,,\\\\\\\\\\nP(B|A) & = & \\\\frac{P(A\\\\cap B)}{P(A)} \\\\,,\\n\\\\end{eqnarray}\\nfrom which the following equation derives:\\n\\\\begin{equation}\\nP(A|B)P(B) = P(B|A)P(A)\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:bayesInterm}) can be written in the following form, that\\ntakes the name of {\\\\it Bayes theorem}:\\n\\\\begin{equation}\\n\\\\boxed{\\nP(A|B) = \\\\frac{P(B|A)P(A)}{P(B)}\\\\,.\\n}\\n\\\\end{equation}\\nIn Eq.~(\\\\ref{eq:BayesTheorem}), $P(A)$ has the role of {\\\\it prior} probability and\\n$P(A|B)$ has the role of {\\\\it posterior} probability.\\nBayes theorem, that has its validity in any probability approach, including the frequentist one,\\ncan also be used to assign a posterior probability to a claim $H$ that is necessarily not a random\\nevent, given a corresponding prior probability $P(H)$ and the observation of an event $E$ whose\\nprobability, if $H$ is true, is given by $P(E|H)$:\\n\\\\begin{equation}\\nP(H|E) = \\\\frac{P(E|H)P(H)}{P(E)}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:BayesRule}) is the basis of Bayesian approach to probability. It\\ndefines in a {\\\\it rational way} a role to modify one's prior\\nbelief in a claim $H$ given the observation of $E$.\\nThe following problem is an example of application of\\nBayes theorem in a frequentist environment.\\nImagine you have a particle identification detector that identifies muons with high efficiency,\\nsay $\\\\varepsilon=95\\\\Given a particle in a data sample that is identified as a muon, what is the probability that\\nit is really a muon? The answer to this question can't be given unless we know more information\\nabout the composition of the sample, i.e.: what is the fraction of muons and pions in the data sample.\\nUsing Bayes theorem, we can write:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{P(+|\\\\mu)P(\\\\mu)}{P(+)}\\\\,,\\n\\\\end{equation}\\nwhere `$+$' denotes a positive muon identification, $P(\\\\mu|+)=\\\\varepsilon$ is the probability to positively\\nidentify a muon, $P(\\\\mu)$ is the fraction of muons in our sample ({\\\\it purity}) and\\n$P(+)$ is the probability to positively identify a particle randomly chosen from our sample.\\nIt's possible to decompose $P(+)$ as:\\n\\\\begin{equation}\\nP(+) = P(+|\\\\mu) P(\\\\mu) + P(+|\\\\pi) P(\\\\pi)\\\\,,\\n\\\\end{equation}\\nwhere $P(+|\\\\pi)=\\\\delta$ is the probability to positively identify a pion and $P(\\\\pi)=1-P(\\\\mu)$ is the\\nfraction of pions in our samples, that we suppose is only made of muons and pions.\\nEq.~(\\\\ref{eq:pPlusInt}) is a particular case of the {\\\\it law of total probability} which\\nallows to decompose the probability of an event $E_0$ as:\\n\\\\begin{equation}\\nP(E_0) = \\\\sum_{i=1}^n P(E_0|A_i) P(A_i)\\\\,,\\n\\\\end{equation}\\nwhere the sets $A_i$ are all pairwise disjoint and constitute a partition of the sample space.\\nUsing the decomposition from Eq.~(\\\\ref{eq:pPlusInt}) in\\nEq.~(\\\\ref{eq:pMuPlusInt}), one gets:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{\\\\varepsilon P(\\\\mu)}{\\\\varepsilon P(\\\\mu) + \\\\delta P(\\\\pi)}\\\\,.\\n\\\\end{equation}\\nIf we assume that our sample contains a fraction $P(\\\\mu)=4\\\\pions, we have:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{0.95 \\\\cdot 0.04}{0.95 \\\\cdot 0.04 + 0.05\\\\cdot 0.96}\\\\simeq 0.44\\\\,.\\n\\\\end{equation}\\nIn this case, even if the selection efficiency is very high, given the low sample purity,\\na particle positively identified as a muon has a probability less than 50\\\\\\n\", \"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\", \"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Consistent Bayesian and Frequentist modeling}\\nThe variational estimates $\\\\eta^\\\\pm$ and $\\\\sigma^\\\\pm$ typically correspond to so called ``$\\\\pm 1\\\\sigma$ variations'' in the source of the uncertainty. Here we are focusing on the source of the uncertainty, not its affect on rates and shapes. For instance, we might say that the jet energy scale has a 10\\\\\\nIt is often advocated that a ``log-normal'' or ``gamma'' distribution for $\\\\alpha_p$ is more appropriate than a gaussian constraint~\\\\cite{CousinsLogNormal}. This is particularly clear in the case of bounded parameters and large uncertainties. Here we must take some care to build a probability model that can maintain a consistent interpretation in Bayesian a frequentist settings. Table~\\\\ref{tab:constraints} summarizes a few consistent treatments of the frequentist pdf, the likelihood function, a prior, and the resulting posterior.\\n\\\\begin{table}[*htb]\\n\\\\center\\n\\\\begin{tabular}{llll}\\nPDF & Likelihood $\\\\propto$ & Prior $\\\\pi_0$ & Posterior $\\\\pi$ \\\\\\\\ \\\\hline\\n$G(a_p | \\\\alpha_p, \\\\sigma_p)$ & $G(\\\\alpha_p | a_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\alpha_p)\\\\propto$ const & $G(\\\\alpha_p | a_p, \\\\sigma_p)$ \\\\\\\\\\n$\\\\Pois(n_p | \\\\tau_p \\\\beta_p)$ & $\\\\PGamma(\\\\beta_p | A=\\\\tau_p; B=1+n_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto$ const & $\\\\PGamma(\\\\beta_p | A=\\\\tau_p; B=1+n_p)$ \\\\\\\\\\n$\\\\LN(n_p | \\\\beta_p, \\\\sigma_p)$ & $ \\\\beta_p \\\\cdot \\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto $ const & $\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ \\\\\\\\\\n$\\\\LN(n_p | \\\\beta_p, \\\\sigma_p)$ & $\\\\beta_p \\\\cdot\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto 1/\\\\beta_p $ & $\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$\\\\\\\\\\n\\\\end{tabular}\\n\\\\caption{Table relating consistent treatments of PDF, likelihood, prior, and posterior for nuisance parameter constraint terms.}\\n\\\\end{table}\\nFinally, it is worth mentioning that the uncertainty on some parameters is not the result of an auxiliary measurement -- so the constraint term idealization, it is not just a convenience, but a real conceptual leap. This is particularly true for theoretical uncertainties from higher-order corrections or renormalizaiton and factorization scale dependence. In these cases a formal frequentist analysis would not include a constraint term for these parameters, and the result would simply depend on their assumed values. As this is not the norm, we can think of reading Table~\\\\ref{tab:constraints} from right-to-left with a subjective Bayesian prior $\\\\pi(\\\\alpha)$ being interpreted as coming from a fictional auxiliary measurement.\\n\\\\subsubsubsection{Gaussian Constraint}\\nThe Gaussian constraint for $\\\\alpha_p$ corresponds to the familiar situation. It is a good approximation of the auxiliary measurement when the likelihood function for $\\\\alpha_p$ from that auxiliary measurement has a Gaussian shape. More formally, it is valid when the maximum likelihood estimate of $\\\\alpha_p$ (eg. the best fit value of $\\\\alpha_p$) has a Gaussian distribution. Here we can identify the maximum likelihood estimate of $\\\\alpha_p$ with the global observable $a_p$, remembering that it is a number that is extracted from the data and thus its distribution has a frequentist interpretation. \\n\\\\begin{equation}\\nG(a_p | \\\\alpha_p, \\\\sigma_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma_p^2}} \\\\exp \\\\left[ -\\\\frac{(a_p - \\\\alpha_p)^2}{2\\\\sigma_p^2} \\\\right]\\n\\\\end{equation}\\nwith $\\\\sigma_p=1$ by default.\\nNote that the PDF of $a_p$ and the likelihood for $\\\\alpha_p$ are positive for all values. \\n\\\\subsubsubsection{Poisson (``Gamma'') constraint}\\nWhen the auxiliary measurement is actually based on counting events in a control region (eg. a Poisson process), a more accurate to describe the auxiliary measurement with a Poisson distribution. It has been shown that the truncated Gaussian constraint can lead to undercoverage (overly optimistic) results, which makes this issue practically relevant~\\\\cite{Cousins:2008zz}. Table~\\\\ref{tab:constraints} shows that a Poisson PDF together with a uniform prior leads to a gamma posterior, thus this type of constraint is often called a ``gamma'' constraint. This is a bit unfortunate since the gamma distribution is manifestly Bayesian and with a different choice of prior, one might not arrive at a gamma posterior. When dealing with the Poisson constraint, it is no longer convenient to work with our conventional scaling for $\\\\alpha_p$ which can be negative. Instead, it is more natural to think of the number of events measured in the auxiliary measurement $n_p$ and the mean of the Poisson parameter. This information is not usually available, instead one usually has some notion of the relative uncertainty in the parameter $\\\\sigma_p^{\\\\rm rel}$ (eg. a the jet energy scale is known to 10\\\\\\\\begin{equation}\\n\\\\Pois(n_p | \\\\tau_p \\\\alpha_p) =\\\\frac{ (\\\\tau_p \\\\alpha_p)^{n_p} \\\\; e^{-\\\\tau_p \\\\alpha_p} } {n_p!}\\n\\\\end{equation}\\nHere we can use the fact that Var$[n_p]=\\\\sqrt{\\\\tau_p\\\\alpha_p}$ and reverse engineer the nominal auxiliary measurement \\n\\\\begin{equation}\\nn_p^0 = \\\\tau_p = (1/\\\\sigma_{p}^{\\\\rm rel})^2\\\\; .\\n\\\\end{equation}\\nwhere the superscript $0$ is to remind us that $n_p$ will fluctuate in repeated experiments but $n_p^0$ is the value of our measured estimate of the parameter.\\nOne important thing to keep in mind is that there is only one constraint term per nuisance parameter, so there must be only one $\\\\sigma_p^{rel}$ per nuisance parameter. This $\\\\sigma_p^{rel}$ is related to the fundamental uncertainty in the source and we cannot infer this from the various response terms $\\\\eta_{ps}^\\\\pm$ or $\\\\sigma_{pub}^\\\\pm$. \\nAnother technical difficulty is that the Poisson distribution is discrete. So if one were to say the relative uncertainty was 30\\\\\\\\begin{equation}\\n\\\\PGamma(\\\\alpha_p | A=\\\\tau_p, B=n_p-1) = A (A \\\\alpha_p)^{B} e^{-A \\\\alpha_p} / \\\\Gamma(B)\\\\;.\\n\\\\end{equation}\\nThis approach works fine for likelihood fits, Bayesian calculations, and frequentist techniques based on asymptotic approximations, but it does not offer a consistent treatment of the pdf for the global observable $n_p$ that is needed for techniques based on Monte Carlo sampling. \\n\\\\subsubsubsection{Log-normal constraint}\\nFrom Eadie et al., ``The log-normal distribution represents a random variable whose logarithm follows a normal distribution. It provides a model for the error of a process involving many small multiplicative errors (from the Central Limit Theorem). It is also appropriate when the value of an observed variable is a random proportion of the previous observation.''~\\\\cite{Eadie:qy,CousinsLogNormal}. This logic of multiplicative errors applies to the the measured value, not the parameter. Thus, it is natural to say that there is some auxiliary measurement (global observable) with a log-normal distribution. As in the gamma/Poisson case above, let us again say that the global observable is $n_p$ with a nominal value\\n\\\\begin{equation}\\nn_p^0 = \\\\tau_p = (1/\\\\sigma_{p}^{\\\\rm rel})^2\\\\; .\\n\\\\end{equation}\\nThen the conventional choice for the corresponding log-normal distribution is\\n\\\\begin{equation}\\n\\\\LN(n_p | \\\\alpha_p, \\\\kappa_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right]\\n\\\\end{equation}\\nwhile the likelihood function is (blue curve in Fig.~\\\\ref{fig:lognormal}(a)).\\n\\\\begin{equation}\\nL( \\\\alpha_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right];.\\n\\\\end{equation}\\nTo get to the posterior for $\\\\alpha_p$ given $n_p$ we need an ur-prior $\\\\eta(\\\\alpha_p$)\\n\\\\begin{equation}\\n\\\\pi( \\\\alpha_p) \\\\propto \\\\eta(\\\\alpha_p) \\\\; \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right]\\n\\\\end{equation}\\nIf $\\\\eta(\\\\alpha_p)$ is uniform, then the posterior looks like the red curve in Fig.~\\\\ref{fig:lognormal}(b). However, when paired with an ``ur-prior'' $\\\\eta(\\\\alpha_p) \\\\propto 1/\\\\alpha_p$ (green curve in Fig.~\\\\ref{fig:lognormal}(b)), this results in a posterior distribution that is also of a log-normal form for $\\\\alpha_p$ (blue curve in Fig.~\\\\ref{fig:lognormal}(b)).\\n\", '\\\\section{Experimental sensitivity using the $q_{\\\\mu}$ estimator}\\nThe statistical significance is calculated using a strategy similar to that illustrated for the estimator \\\\( \\\\mathcal{Q} \\\\). First, the value \\\\( q_{0, \\\\text{obs}} \\\\) is obtained using the statistical test:\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n = b + s \\\\) for signal sensitivity. The significance is obtained by finding the p-value of the possible observation \\\\( n \\\\) using the background-only distribution \\\\( f(q_{0} / 0) \\\\):\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nIn the asymptotic limit, the significance can be approximated as \\\\( Z_{0} \\\\approx \\\\sqrt{q_{0}} \\\\)~\\\\cite{lista2016practical, cowan2014statistics}. As mentioned previously, the signal region is obtained by finding the event window that maximizes the statistical significance, Equation~(\\\\ref{eq:maxsig}). In the single-channel case, a statistical significance of \\\\( \\\\alpha = 0.174 \\\\) is obtained, which translates to \\\\( Z_{0} = 0.94 \\\\) standard deviations, consistent with the significance calculation using the estimator \\\\( \\\\mathcal{Q} \\\\). Figure~[\\\\ref{fig:14}] shows the background-only distribution and the observed \\\\( q_{0, \\\\text{obs}} \\\\) value in the Asimov data. The area under the distribution for positive values of \\\\( q_{0, \\\\text{obs}} \\\\) represents the statistical significance of the expected new physics events~\\\\cite{cranmer2015practical}.\\n', \"\\\\section{Inference}\\n\\\\subsection{Bayesian inference}\\nOne example of inference is the use of Bayes theorem to determine the posterior PDF\\nof an unknown parameter $\\\\theta$ given an observation $x$:\\n\\\\begin{equation}\\nP(\\\\theta|x) = \\\\frac{L(x;\\\\theta)\\\\pi(\\\\theta)}{\\n\\\\int L(x;\\\\theta)\\\\pi(\\\\theta)\\\\,\\\\mathrm{d}\\\\theta}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\pi(\\\\theta)$ is the prior PDF.\\nThe posterior $P(\\\\theta|x)$ contains all the information we can obtain from $x$\\nabout $\\\\theta$.\\nOne example of possible outcome for $P(\\\\theta|x)$ is shown in Fig.~\\\\ref{fig:BayesIntCentralSym}\\nwith two possible choices of uncertainty interval (left and right plots).\\nThe most probable value, $\\\\hat{\\\\theta}$, also called {\\\\it mode}, shown as dashed line\\nin both plots, can be taken as central value\\nfor the parameter $\\\\theta$.\\nIt's worth noting that if $\\\\pi(\\\\theta)$ is assumed to be a constant,\\n$\\\\hat{\\\\theta}$ corresponds to the maximum of \\nthe likelihood function ({\\\\it maximum likelihood estimate}, see Sec.~\\\\ref{sec:maxLik}).\\nDifferent choices of 68.3\\\\can be taken. A central interval $[\\\\theta_1,\\\\theta_2]$,\\nrepresented in the left plot in Fig.~\\\\ref{fig:BayesIntCentralSym}\\nas shaded area, is obtained in order to have equal areas under the two extreme tails:\\n\\\\begin{eqnarray}\\n\\\\int_{-\\\\infty}^{\\\\theta_1} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\n\\\\int^{+\\\\infty}_{\\\\theta_2} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & \\\\frac{\\\\alpha}{2}\\\\,, \\n\\\\end{eqnarray}\\nwhere $\\\\alpha = 1 - 68.3\\\\ \\nAnother example of a possible coice of 68.3\\\\where a symmetric interval is taken, corresponding to:\\n\\\\begin{eqnarray}\\n\\\\int_{\\\\hat{\\\\theta}-\\\\delta}^{\\\\hat{\\\\theta}+\\\\delta} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\,.\\\\\\\\\\n\\\\end{eqnarray}\\nTwo extreme choices of fully asymmetric probability intervals are shown in Fig.~\\\\ref{fig:BayesIntHiLo},\\nleading to an upper (left) or lower (right) limit to the parameter $\\\\theta$.\\nFor upper or lower limits, usually a 90\\\\of the usual 68.3\\\\ \\nThe intervals in Fig.~\\\\ref{fig:BayesIntHiLo} are chosen such that:\\n\\\\begin{eqnarray}\\n\\\\int_{-\\\\infty}^{\\\\theta^{\\\\mathrm{up}}} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\quad \\\\text{(left plot)}\\\\,,\\\\\\\\\\n\\\\int^{+\\\\infty}_{\\\\theta^{\\\\mathrm{lo}}} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\quad \\\\text{(right plot)}\\\\,,\\\\\\\\\\n\\\\end{eqnarray}\\nwhere in this case $\\\\alpha = 0.1$.\\n\\\\subsubsection{Example of Bayiesian inference: Poissonian counting}\\nIn a counting experiment, i.e.: the only information relevant to measure the yield of our signal is the\\nnumber of events $n$ that pass a given selection, a Poissonian can be used\\nto model the distribution of $n$ with an expected number of events $s$:\\n\\\\begin{equation}\\nP(n;s) = \\\\frac{s^n e^{-s}}{n!}\\\\,.\\n\\\\end{equation}\\nIf a particular value of $n$ is measured, the posterior PDF of $s$ is (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(s|n) = \\\\frac{\\\\displaystyle\\n\\\\frac{s^n e^{-s}}{n!} \\\\pi(s)\\n}{\\\\displaystyle\\n\\\\int_0^{\\\\infty} \\\\frac{s^{\\\\prime n} e^{-s^{\\\\prime}}}{n!} \\\\pi(s^\\\\prime)\\\\,\\\\mathrm{d}s^\\\\prime\\n}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\pi(s)$ is the assumed prior for $s$. If we take $\\\\pi(s)$ to be uniform, performing the\\nintegration gives a denominator equal to one, hence:\\n\\\\begin{equation}\\nP(s|n) =\\\\frac{s^n e^{-s}}{n!}\\\\,.\\n\\\\end{equation}\\nNote that though Eqs.~(\\\\ref{eq:PoissonBayesInference}) and~(\\\\ref{eq:PoissonBayesInferencePost}) lead to the\\nsame expression, the former is a probability for the discrete random variable $n$, the latter is a\\nposterior PDF of the unknown parameter $s$.\\nFrom Eq.~(\\\\ref{eq:PoissonBayesInference}), the mode $\\\\hat{s}$ is equal to $n$, but\\n$\\\\left<s\\\\right> = n+1$, due to the asymmetric distribution of $s$, and\\n$\\\\mathbbm{V}\\\\mathrm{ar}[s] = n+1$, while the variance of $n$ for a Poissonian\\ndistribution is $\\\\sqrt{s}$ (Sec.~\\\\ref{sec:Poissonian}).\\nFigure~\\\\ref{fig:PoissonBayesPost} shows two cases of posterior PDF of $s$, for the cases $n=5$ (left)\\nand for $n=0$ (right). In the case $n=5$, a central value $\\\\hat{s}=5$ can be taken as most probable value.\\nIn that plot, a central interval was chosen (Eq.~(\\\\ref{eq:BayesCentralLeft},~\\\\ref{eq:BayesCentralRight})).\\nFor the case $n=0$, the most probable value of $s$ is $\\\\hat{s}=0$. A fully asymmetric interval corresponding\\nto a probability $1-\\\\alpha$ leads to an upper limit:\\n\\\\begin{equation}\\ne^{-s^{\\\\mathrm{up}}} = \\\\alpha\\\\,,\\n\\\\end{equation}\\nwhich then leads to:\\n\\\\begin{eqnarray}\\ns & < & s^{\\\\mathrm{up}} = 2.303\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.1\\\\,\\\\text{(90\\\\ s & < & s^{\\\\mathrm{up}} = 2.996\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.05\\\\,\\\\text{(95\\\\\\\\end{eqnarray}\\n\", \"\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Parametrizing the alternative hypothesis}\\nFor true model-independent searches, no assumption should be made about the alternative model. In Ref.~\\\\citen{Kuusela_2012}, the alternative hypothesis is parametrized as a mixture of the background model and a number of additional Gaussians. Another method is the New Physics Learning Machine (NPLM)~\\\\cite{nplm}. Here, the alternative hypothesis is being parametrized by the network itself: given a dataset and a reference sample (like Monte Carlo simulation or data from a data sideband), a neural network is constructed such that it parametrises the alternative model as small perturbations away from the reference. When this model is trained, it learns the maximum likelihood fit to the data by construction, since its loss incorporates the log likelihood of the data. Its output is the ratio between the best-fit data distribution and the reference distribution, which is used as a test statistic to select data that displays a high level of discrepancy with the reference model. This ratio measures the disagreement between the reference model and the data and can be used for hypothesis testing. An overview of the NPLM design in shown in Figure~\\\\ref{fig:nplm}. A drawback of this method is the difficulty of defining the reference sample $\\\\mathcal{R}$. For example, $\\\\mathcal{R}$ can be a taken from Monte Carlo simulation, with the caveat that this might be a less than optimal approximation to nature. Alternatively, the reference sample can be taken from a data sideband. However, in this case the difficulty is to find a region that is signal free, but still statistically identical to the data signal region. \\nIntegrating NPLM with techniques such as CURTAINs or CATHODE offers a potential method for creating the reference sample. This entails training a conditional density estimator with data from signal-free sidebands, enabling effective extrapolation into the signal region.\\nFor the technique to be effective and avoid generating false positives, it's crucial that the density estimation maintains a high degree of accuracy throughout the entire spectrum of the variable of interest. One challenge arises when integrating NPLM in its full power, which is capable of identifying overdensities across multiple dimensions simultaneously, with conditional density estimation. This integration demands conditioning on multiple variables at the same time, adding a layer of difficulty to the process.\\n\\\\noindent\\nA challenge in utilizing anomaly detection for discovering new physics lies in the inherent difficulty of optimizing these algorithms when the nature of the signal remains unknown a priori. Moreover, the sensitivity of various anomaly detection methods can vary considerably depending on the type of signal, as demonstrated in Figure~\\\\ref{fig:pvals} and elaborated in Ref.~\\\\citen{Harris:2881089}. The best one can hope to do is to monitor the performance on wide variety of different potential signals \", \"\\\\section{Upper limits}\\n\\nMany analyses are `searches for...' and \\nmost of these are unsuccessful.\\nBut you have to say something! Not just `We looked, but we didn't see anything'.\\nThis is done using the construction of frequentist confidence intervals and/or Bayesian credible intervals.\\n\", '\\\\section{Frequentist Statistical Procedures}\\n\\nHere I summarize the procedure used by the LHC Higgs combination group for computing frequentist $p$-values uses for \\nquantifying the agreement with the background-only hypothesis and for determining exclusion limits. \\nThe procedures are based on the profile likelihood ratio test statistic. \\nThe parameter of interest is the overall signal strength factor $\\\\mu$, which acts as a scaling to the total rate of signal events. We often write $\\\\mu=\\\\sigma/\\\\sigma_{SM}$, where $\\\\sigma_{SM}$ is the standard model production cross-section; however, it should be clarified that the same $\\\\mu$ factor is used for all production modes and could also be seen as a scaling on the branching ratios. The signal strength is called so that $\\\\mu=0$ corresponds to the background-only model and $\\\\mu=1$ is the standard model signal. It is convenient to separate the full list of parameters $\\\\vec\\\\alpha$ into the parameter of interest $\\\\mu$ and the nuisance parameters $\\\\vec\\\\theta$: $\\\\vec\\\\alpha=(\\\\mu,\\\\vec\\\\theta)$.\\nFor a given data set $\\\\datasim$ and values for the global observables $\\\\globs$ there is an associated likelihood function over $\\\\mu$ and $\\\\theta$ derived from combined model over all the channels including all the constraint terms in Eq.~\\\\ref{Eq:ftot}\\n\\\\begin{equation}\\nL(\\\\mu,\\\\vec\\\\theta;\\\\datasim,\\\\globs) = \\\\F_{\\\\rm tot}(\\\\datasim,\\\\globs|\\\\mu,\\\\vec\\\\theta) \\\\;.\\n\\\\end{equation}\\nThe notation $L(\\\\mu,\\\\vec\\\\theta)$ leaves the dependence on the data implicit, which can lead to confusion. Thus, we will explicitly write the dependence on the data when the identity of the dataset is important and only suppress $\\\\datasim,\\\\globs$ when the statements about the likelihood are generic.\\nWe begin with the definition of the procedure in the abstract and then describe three implementations of the method based on asymptotic distributions, ensemble tests (Toy Monte Carlo), and importance sampling.\\n', \"\\\\section{Discoveries and upper limits}\\n\\nThe process towards a discovery, from the point of view of data analysis,\\nproceeds starting with a test of our data sample against two hypotheses concerning the theoretical underlying model:\\n\\\\begin{itemize}\\n\\\\item $H_0$: the data are described by a model that contains background only;\\n\\\\item $H_1$: the data are described by a model that contains a new signal plus background.\\n\\\\end{itemize}\\nThe discrimination between the two hypotheses can be based on a test statistic $\\\\lambda$ whose distribution\\nis known under the two considered hypotheses.\\nWe may assume that $\\\\lambda$ tends to have (conventionally) large values if $H_1$ is true and small values if $H_0$ is true.\\nThis convention is consistent with using as test statistic the likelihood ratio $\\\\lambda =L(x|H_1)/L(x|H_0)$,\\nas in the Neyman--Pearson lemma (Eq.~(\\\\ref{eq:neymanPearsonLemma})).\\nUnder the frequentist approach, it's possible to compute a $p$-value equal to the probability that\\n$\\\\lambda$ is greater or equal to than the value $\\\\lambda^{\\\\mathrm{obs}}$ observed in data.\\nSuch $p$-value is usually converted into an equivalent probability computed as the area\\nunder the rightmost tail of a standard normal distribution:\\n\\\\begin{equation}\\np = \\\\int_Z^{+\\\\infty} \\\\frac{1}{\\\\sqrt{2\\\\pi}}e^{-{x^2}/{2}}\\\\,\\\\mathrm{d}x = 1 - \\\\Phi(Z)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\Phi$ is the cumulative (Eq.~(\\\\ref{eq:cumulative})) of a standard normal distribution.\\n$Z$ in Eq.~(\\\\ref{eq:significance}) is called {\\\\it significance level}.\\nIn literature conventionally a signal with a significance of at least 3 ($3\\\\sigma$ {\\\\it level})\\nis claimed as {\\\\it evidence}. It corresponds to a $p$-value of \\n$1.35\\\\times 10^{-3}$ or less. If the significance exceeds 5 ($5\\\\sigma$ {\\\\it level}), i.e.: the\\n$p$-value is below $2.9\\\\times10^{-7}$, one is allowed to claim the {\\\\it observation} of the new signal.\\nIt's worth noting that the probability that background produces a large test statistic is not equal to\\nthe probability of the null hypothesis (background only), which has only a Bayesian sense.\\nFinding a large significance level, anyway, is only part of the discovery process in the\\ncontext of the scientific method. Below a sentence is reported from a recent statement of the\\nAmerican Statistical Association:\\n\\\\begin{displayquote}\\n{\\\\it The p-value was never intended to be a substitute for scientific reasoning. Well-reasoned statistical arguments contain much more than the value of a single number and whether that number exceeds an arbitrary threshold. The ASA statement is intended to steer research into a `post p < 0.05 era'}~\\\\cite{pvalASA}.\\n\\\\end{displayquote}\\nThis was also remarked by the physicists community, for instance by Cowan {\\\\it et al.}:\\n\\\\begin{displayquote}\\n\\\\textit{It should be emphasized that in an actual scientific context, rejecting the background-only hypothesis in a statistical sense is only part of discovering a new phenomenon. One's \\\\textbf{degree of belief} that a new process is present will depend in general on other factors as well, such as the plausibility of the new signal hypothesis and the degree to which it can describe the data}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\n\", \"\\\\section{Probability theory}\\n\\\\subsection{The likelihood function}\\nThe outcome of on experiment can be modeled as a set of random variables $x_1, \\\\cdots, x_n$\\nwhose distribution takes into account both intrinsic physics randomness (theory)\\nand detector effects (like resolution, efficiency, etc.).\\nTheory and detector effects can be described according to some parameters\\n$\\\\theta_1, \\\\cdots, \\\\theta_m$ whose values are, in most of the cases, unknown.\\nThe overall PDF, evaluated for our observations $x_1, \\\\cdots, x_n$, is called {\\\\it likelihood function}:\\n\\\\begin{equation}\\nL=f(x_1,\\\\cdots,x_n;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nIn case our sample consists of $N$ {\\\\it independent measurements}, typically each corresponding to a\\ncollision event, the likelihood function can be written as:\\n\\\\begin{equation}\\nL=\\\\prod_{i=1}^Nf(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nThe likelihood function provides a useful implementation of Bayes rule\\n(Eq.~(\\\\ref{eq:BayesRule})) in the case of a measurement constituted by the observation of \\ncontinuous random variables $x_1, \\\\cdots, x_n$. The posterior PDF of the unknown parameters\\n$\\\\theta_1, \\\\cdots, \\\\theta_m$ can be determined as:\\n\\\\begin{equation}\\nP(\\\\theta_1,\\\\cdots,\\\\theta_m|x_1,\\\\cdots,x_n) =\\n\\\\frac{\\nL(x_1,\\\\cdots,x_n;\\\\theta_,\\\\cdots,\\\\theta_m)\\\\pi(\\\\theta_1,\\\\cdots,\\\\theta_m)\\n}{\\n\\\\int L(x_1,\\\\cdots,x_n;\\\\theta_,\\\\cdots,\\\\theta_m)\\\\pi(\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,\\\\mathrm{d}\\\\theta^m\\\\\\n}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\pi(\\\\theta_1, \\\\cdots, \\\\theta_m)$ is the subjective prior probability and\\nthe denominator is a normalization factor obtained with a decomposition similar to Eq.~(\\\\ref{eq:pPlusInt}).\\nEquation~(\\\\ref{eq:likeBayes}) can be interpreted as follows:\\nthe observation of $x_1, \\\\cdots, x_n$ modifies the prior knowledge of the unknown parameters $\\\\theta_1,\\\\cdots, \\\\theta_m$.\\nIf $\\\\pi(\\\\theta_1, \\\\cdots, \\\\theta_m)$ is sufficiently smooth and $L$ is sharply peaked around the true values of\\nthe parameters $\\\\theta_1,\\\\cdots, \\\\theta_m$, the resulting posterior will not be strongly dependent on the prior's choice.\\nBayes theorem in the form of Eq.~(\\\\ref{eq:likeBayes}) can be \\napplied sequentially for repeated independent observations.\\nIn fact, if we start with a prior $P_0(\\\\vec{\\\\theta})$, we can\\ndetermine a posterior:\\n\\\\begin{equation}\\nP_1(\\\\vec{\\\\theta})\\\\propto P_0(\\\\vec{\\\\theta})\\\\cdot L_1(\\\\vec{x}_1;\\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nwhere $L_1(\\\\vec{x_1};\\\\vec{\\\\theta})$ is the likelihood function corresponding to the observation\\n$\\\\vec{x}_1$. Subsequently, we can use $P_1$ as new prior for a second observation $\\\\vec{x}_2$, and\\nwe can determine a new posterior:\\n\\\\begin{equation}\\nP_2(\\\\vec{\\\\theta})\\\\propto P_1(\\\\vec{\\\\theta})\\\\cdot L_2(\\\\vec{x}_2;\\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nand so on:\\n\\\\begin{equation}\\nP_3(\\\\vec{\\\\theta})\\\\propto P_2(\\\\vec{\\\\theta})\\\\cdot L_3(\\\\vec{x}_3;\\\\vec{\\\\theta})\\\\,.\\\\\\n\\\\end{equation}\\nFor independent observations $\\\\vec{x}_1$, $\\\\vec{x}_2$, $\\\\vec{x}_2$, the combined likelihood\\nfunction can be written as the product of individual likelihood functions (Eq.~(\\\\ref{eq:indVar})):\\n\\\\begin{equation}\\nP_3(\\\\vec{\\\\theta})\\\\propto P_0(\\\\vec{\\\\theta})\\\\cdot\\nL_1(\\\\vec{x}_1;\\\\vec{\\\\theta})\\\\cdot\\nL_2(\\\\vec{x}_2;\\\\vec{\\\\theta})\\\\cdot\\nL_3(\\\\vec{x}_3;\\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nconsistently with Eq.~(\\\\ref{eq:postP3L3Int}).\\nThis allows to use consistently the repeated application of Bayes rule as sequential\\nimprovement of knowledge from subsequent observations.\"}},\n",
       "       {'entity_name': 'maximum likelihood and profile likelihood methods', 'entity_type': 'analysis_technique', 'description': 'A collection of statistical techniques used for estimating parameters of statistical models by maximizing the likelihood function. This includes methods such as maximum likelihood estimation, profile likelihood, and their applications in hypothesis testing and parameter estimation, often accounting for nuisance parameters and providing a basis for constructing confidence intervals and upper limits in particle physics.', 'relevant_passages': {\"\\\\section{Errors}\\n\\\\subsection{Asymmetric errors}\\nSo what happens if you plot the likelihood function and it is not symmetric like \\nFig.~\\\\ref{fig:ML} but looks more like Fig.~\\\\ref{fig:asym}?\\nThis \\narises in many cases when numbers are small. For instance, in a simple Poisson count suppose you observe one event. $P(1;\\\\lambda)=\\\\lambda e^{-\\\\lambda}$ is not symmetric: $\\\\lambda=1.5$ is more likely to fluctuate down to 1 than $\\\\lambda=0.5$ is to\\nfluctuate up to 1.\\nYou can read off $\\\\sigma_+$ and $\\\\sigma_-$ from the two $\\\\Delta \\\\ln L=-{1 \\\\over 2}$ crossings, but they are different.\\nThe result can then be given as $a^{+\\\\sigma_+}_{-\\\\sigma_-}$. What happens after that?\\nThe first advice is to avoid this if possible. \\nIf you get $\\\\hat a=4.56$ with $\\\\sigma_+=1.61, \\\\sigma_-=1.59$ \\nthen quote this as $4.6 \\\\pm 1.6$ rather than $4.56^{+1.61}_{-1.59}$.\\nThose extra significant digits have no real meaning. If you can convince yourself that the difference between\\n$\\\\sigma_+$ and $\\\\sigma_-$ is small enough to be ignored then you should do so, as the alternative brings in a whole \\nlot of trouble and it's not worth it.\\nBut there will be some cases where the difference is too great to be swept away, so\\nlet's consider that case.\\nThere are two problems that arise: combination of measurements and combination of errors.\\n\\\\subsubsection{Combination of measurements with asymmetric errors}\\nSuppose you have two measurements of the same parameter $a$: $\\\\hat {a_1}^{+\\\\sigma^+_1}_{-\\\\sigma^-_1}$\\nand $\\\\hat {a_2}^{+\\\\sigma^+_2}_{-\\\\sigma^-_2}$ and you want to combine them to give the best estimate and, of course, its error. For symmetric errors the answer is well established to be\\n$\\\\hat a = {\\\\hat a_1/\\\\sigma_1^2 + \\\\hat a_2/\\\\sigma_2^2 \\\\over 1/\\\\sigma_1^2 + 1/\\\\sigma_2^2}$.\\nIf you know the likelihood functions, you can do it. The joint likelihood is just the sum.\\nThis is shown in Fig.~\\\\ref{fig:combine1} where \\nthe\\nred and green curves are measurements of $a$. \\nThe log likelihood functions just add (blue), from which the peak is found and the $\\\\Delta \\\\ln L=-\\\\half$ errors read off.\\nBut you don't know the full likelihood function: just 3 points (and that it had a maximum at the second).\\nThere are, of course, an infinite number of curves that could be drawn, and several models have been tried (cubics, constrained quartic...) on likely instances---see Ref.~\\\\cite{asym} for details. Some do better than others.\\nThe two most plausible are\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2}\\\\left( {a-\\\\hat a \\\\over \\\\sigma+\\\\sigma'(a-\\\\hat a)}\\\\right)^2\\n\\\\quad \\\\text{and}\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2} {\\\\left( a-\\\\hat a \\\\right)^2 \\\\over V+V'(a-\\\\hat a)}\\n\\\\quad.\\n\\\\end{equation}\\nThese are similar to the Gaussian parabola, but the denominator is not constant. It varies with the value of $a$, being linear either in the standard deviation or in the variance.\\nBoth are pretty good. The first does better with errors on $\\\\log a$ (which are asymmetric if $a$ is symmetric: such asymmetric error bars are often seen on plots where the $y$ axis is logarithmic), the\\nsecond does better with Poisson measurements.\\nFrom the 3 numbers given one readily obtains\\n\\\\begin{equation}\\n\\\\sigma={2 \\\\sigma^+\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-} \\\\qquad \\n\\\\sigma'={\\\\sigma^+-\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-}\\n\\\\end{equation}\\nor, if preferred\\n\\\\begin{equation}\\nV= \\\\sigma^+\\\\sigma^- \\\\qquad\\nV'=\\\\sigma^+-\\\\sigma^- \\n\\\\quad.\\n\\\\end{equation}\\nFrom the total likelihood you then find the maximum of sum, numerically, and the $\\\\Delta \\\\ln L=-{1\\\\over 2}$ points.\\nCode for doing this is available on GitHub\\\\footnote{\\\\url{ https://github.com/RogerJBarlow/Asymmetric-Errors}} in\\nboth R and Root.\\nAn example is shown in Fig.~\\\\ref{fig:asymex}. Combining $1.9^{+0.7}_{-0.5}$, $2.4^{+0.6}_{-0.8}$ and $3.1^{+0.5}_{-0.4}$ gives $2.76 ^{+0.29}_{-0.27} \\\\ .$\\n\\\\subsubsection{Combination of errors for asymmetric errors}\\nFor symmetric errors, given $x \\\\pm \\\\sigma_x, y \\\\pm \\\\sigma_y$, (and $\\\\rho_{xy}=0$) the error on \\n$f(x,y)$\\nis the sum in quadrature: $\\\\sigma_f^2 =\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2 + \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2\\n\\\\sigma_y^2$.\\nWhat is the equivalent for the error on $f(x,y)$ when the errors are asymmetric,\\n$x ^{+\\\\sigma^+_x}_{-\\\\sigma^-_x}, y^{+\\\\sigma^+_y}_{-\\\\sigma^-_y}$? Such a problem arises frequently at the end of an analysis when the systematic errors from various sources are all combined.\\nThe standard procedure---which you will see done, though it has not, to my knowledge, been written down anywhere---is to add the positive and negative errors\\nin quadrature separately: ${\\\\sigma^+_f}^2={\\\\sigma^+_x}^2+{\\\\sigma^+_y}^2$,\\\\quad \\n${\\\\sigma^-_f}^2={\\\\sigma^-_x}^2+{\\\\sigma^-_y}^2$.\\nThis looks plausible, but it is \\n{\\\\em manifestly wrong} as it breaks the central limit theorem.\\nTo see this, suppose you have to average \\n$N$ i.i.d. variables each with the same errors which are asymmetric: $\\\\sigma^+ = 2 \\\\sigma^-$ . \\nThe \\nstandard procedure reduces both $\\\\sigma^+$ and $\\\\sigma^-$ by a factor $1/\\\\sqrt N$, but the skewness remains.\\nThe positive error is twice the negative error. This is therefore not Gaussian, and never will be, even as $N \\\\to \\\\infty$.\\nYou can see what's happening by considering the combination of two of these measurements. They both may fluctuate upwards, or they both may fluctuate downwards, and yes, the upward fluctuation will be, on average, twice as big. But there is a 50\\\\ chance of one upward and one downward fluctuation, which is not considered in the standard procedure.\\nFor simplicity we write $z_i={\\\\partial f \\\\over \\\\partial x_i} (x_i-x^0_i)$, the deviation of the parameter \\nfrom its nominal value, scaled by the differential. The individual likelihoods are again parametrized\\nas Gaussian with a linear dependence of the standard deviation or of the variance, giving\\n\\\\begin{equation}\\n\\\\ln L(\\\\vec z)=- \\\\half \\\\sum_i \\\\left( { z_i \\\\over \\\\sigma_i + \\\\sigma'_i z_i}\\\\right)^2 \\\\qquad {\\\\rm or}\\\\qquad\\n- \\\\half \\\\sum_i { z_i ^2\\\\over V_i + V'_i z_i}\\n\\\\quad,\\n\\\\end{equation}\\nwhere $\\\\sigma, \\\\sigma', V,V'$ are obtained from Eqs.~\\\\ref{eq:asyms} or \\\\ref{eq:asymV}.\\nThe $z_i$ are nuisance parameters (as described later) and can be removed by profiling. \\nLet $u=\\\\sum z_i$ be the total deviation in the quoted $f$ arising from the individual deviations.\\nWe form $\\\\hat L(u)$ as the maximum of $L(\\\\vec z)$ subject to the constraint $\\\\sum_i z_i=u$.\\nThe method of undetermined multipliers readily gives the solution\\n\\\\begin{equation}\\nz_i=u {w_i \\\\over \\\\sum_j w_j}\\n\\\\quad,\\n\\\\end{equation}\\nwhere\\n\\\\begin{equation}\\nw_i = {(\\\\sigma_i + \\\\sigma'_i z_i)^3 \\\\over 2 \\\\sigma_i}\\n\\\\qquad {\\\\rm or } \\\\qquad\\n{(V_i+V'_i z_i)^2 \\\\over 2V_i + V'_i z_i}\\n\\\\quad.\\n\\\\end{equation}\\nThe equations are nonlinear, but can be solved iteratively. At $u=0$ all the $z_i$ are zero. Increasing (or decreasing) $u$ in small steps, Eqs.~\\\\ref{eq:comsol1} and \\\\ref{eq:comsol2} are applied successively to give the $z_i$ and the $w_i$: convergence is rapid. The value of $u$ which maximises the likelihood should in principle be applied as a correction to the quoted result.\\nPrograms to do this are also available on the GitHub site.\\nAs an example, consider a counting experiment with a number of backgrounds, each determined by an ancillary Poisson experiment, and that for simplicity each background was determined by running the apparatus for the same time as the actual experiment. (In practice this is unlikely, but scale factors\\ncan easily be added.)\\nSuppose two backgrounds are measured, one giving four events and the other five. These would be reported, using $\\\\Delta ln L=-\\\\half$ errors, as $4^{+2.346}_{-1.682}$ and $5^{+2.581}_{-1.916}$.\\nThe method, using linear $V$, gives the combined error on the background count as ${}^{+3.333}_{-2.668}$.\\nIn this simple case we can check the result against the total background count of nine events, which has errors ${}^{+3.342}_{-2.676}$. The agreement is impressive. Further examples of the same total, partitioned differently, are shown in table~\\\\ref{tab:asymcom}.\\n\\\\begin{table}[h]\\n\\\\begin{centering}\\n\\\\begin{tabular}{ l c c c c }\\nInputs & \\\\multicolumn {2} {c} {Linear $\\\\sigma$}& \\\\multicolumn {2} {c} {Linear $V$}\\\\\\\\\\n& $\\\\sigma^-$ & $\\\\sigma^+$ & $\\\\sigma^-$ & $\\\\sigma^+$ \\\\\\\\\\n\\\\hline\\n4+5 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+6 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+3+3 & 2.630 & 3.278 & 2.659 & 3.323 \\\\\\\\\\n1+1+1+1+1+1+1+1+1 & 2.500 & 3.098 & 2.610 & 3.270 \\\\\\\\\\n\\\\end{tabular}\\n\\\\caption{ Various combinations of Poisson errors. The target value is $\\\\sigma^-=2.676$, $\\\\sigma^+=3.342$}\\n\\\\end{centering}\\n\\\\end{table}\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Lecture 3: The Bayesian Approach}\\n\\\\subsection{The Top Quark Discovery: A Bayesian Analysis}\\nIn this section we shall perform the following calculations as a way to illustrate a typical\\nBayesian analysis,\\n\\\\begin{enumerate}\\n\\\\item compute the posterior density $p(s | D)$,\\n\\\\item compute a 68\\\\ \\\\item compute the global Bayes factor $B_{10} = p(D | H_1) / p(D | H_0)$.\\n\\\\end{enumerate}\\n\\\\subsubsection*{Probability model}\\nThe first step in any serious statistical analysis is to think deeply about what has been done in\\nthe physics analysis; for example, to trace in detail the steps that led to the background estimates, determine the independent systematic effects and identify explicitly what is known about them. Although, by tradition, we tend to think of potential data $x$ separately from the parameters $s$ and $b$, it should be recognized that this is done for convenience. The full probability model is the joint\\nprobability\\n\\\\begin{align*}\\np(x, s, b | I),\\n\\\\end{align*}\\nwhich, as is true of \\\\emph{all} probability models, is conditional on the information and assumptions, $I$, that define the abstract space $\\\\Omega$ (see Sec.~\\\\ref{sec:prob}). \\nIn these lectures, we have\\nomitted the conditioning data $I$, and will continue to do so here, but it should not\\nbe forgotten that it is always present and may differ from one probability model to another.\\nThe full probability model $p(x, s, b)$ can be factorized is several ways, all of which are\\nmathematically valid. However, we find it convenient to factorize the model in the following way\\n\\\\begin{align}\\np(x, s, b) = p(x | s, b) \\\\, \\\\pi(s, b),\\n\\\\end{align}\\nwhere we have introduced the symbol $\\\\pi$ in order to highlight the distinction we choose\\nto make between this\\npart of the model and the remainder. We are entirely free to decide how much of the model\\nwe place in $p(x | s, b)$ and how much in $\\\\pi(s, b)$; what matters is the form of the\\nfull model $p(x, s, b)$. In\\nthe frequentist analysis of the top quark discovery data, we took $N$ and $B$ to be the data $D$. We did so because in the frequentist approach, the function $\\\\pi(s, b)$ does not exist and consequently we have no choice but to include everything in the function $p(x| s, b)$. \\nOne virtue of a Bayesian perspective is that we are not bound by this stricture. To make the\\npoint explicitily, we take the probability distribution, $p(x | s, b)$, to be\\n\\\\begin{align}\\np(x|s, b) = \\\\textrm{Poisson}(x, s + b).\\n\\\\end{align}\\nThe interpretation\\nof $p(x | s, b)$ is clear: it is the probability to observe $x$ events \\\\emph{given} that the mean event count is $s + b$. \\nWhat does $\\\\pi(s, b)$ represent? This function is the \\\\textbf{prior} that encodes what we \\\\emph{know}, or \\\\emph{assume}, about the mean background and signal independently\\nof the potential observations $x$. The prior $\\\\pi(s, b)$ can be factored in two ways,\\n\\\\begin{align}\\n\\\\pi(s , b) & = \\\\pi(s | b ) \\\\, \\\\pi(b), \\\\nonumber \\\\\\\\ \\n& = \\\\pi(b | s ) \\\\, \\\\pi(s),\\n\\\\end{align}\\nboth of which accord with the probability rules. The factorizations remind us that the parameters\\n$s$ and $b$ may not be probabilistically independent. However, we shall assume that they are, at least at this stage of the analysis, in which case it is permissible to write,\\n\\\\begin{align}\\n\\\\pi(s , b) & = \\\\pi(s) \\\\, \\\\pi(b).\\n\\\\end{align}\\nWe first consider the background prior $\\\\pi(b)$ and ask: what do we know about the background? \\nWe know the count $Q$ in the control region and \\nwe have an estimate of the\\ncontrol region to\\nsignal region scale factor $k$. The likelihood for $Q$ is taken to be \\n\\\\begin{align}\\np(Q | k, b) = \\\\textrm{Poisson}(Q, k b),\\n\\\\end{align}\\nfrom which, together with a prior $\\\\pi(k, b)$, we can compute the posterior density\\n\\\\begin{align}\\np(b | Q, k) = p(Q | k, b) \\\\, \\\\pi(k, b) / p(Q).\\n\\\\end{align}\\nAs usual, we factorize the prior, $\\\\pi(k, b) = \\\\pi(k|b) \\\\pi_0(b) $,\\nwhere we have introduced the subscript $0$ to distinguish $\\\\pi_0(b)$ from the background prior\\nassociated with Eq.~(\\\\ref{eq:pxsb}). Then, we consider the separate factors $\\\\pi_0(b)$\\nand $\\\\pi(k | b)$.\\nWhat do we know about $b$ at this stage?\\nClearly, $b \\\\geq 0$. But, that is all we know apart from the background likelihood, Eq.~(\\\\ref{eq:pQkb}). \\nToday, after a century of\\nargument and discussion, the consensus amongst statisticians is that there is no\\nunique way to represent such vague information. However, \\nwell founded ways to construct such priors are available, see for example Ref.~\\\\cite{Demortier:2010sn}\\nand references therein; but for simplicity we take the prior $\\\\pi_0(b) = 1$, that is, the \\\\textbf{flat prior}. If the uncertainty in $k$ can be neglected, the (proper!) prior for $k$ is $\\\\pi(k|b) = \\\\delta(k - Q/B)$, which amounts to replacing $k$ in Eq.~(\\\\ref{eq:pbQk}) by $Q/B$. When the dust\\nsettles, we find\\n\\\\begin{align}\\np(b | Q, k) = \\\\textrm{Gamma}(k b, 1, Q+1) = \\\\frac{e^{-k b} (k b)^Q} {\\\\Gamma(Q+1)},\\n\\\\end{align}\\nfor the posterior density of $b$, \\nwhich can serve as the prior $\\\\pi(b)$ associated with Eq.~(\\\\ref{eq:pxsb}).\\nBy construction, $p(x, s, b)$ is identical in form to the likelihood in Eq.~(\\\\ref{eq:toplh}); we have \\nsimply availed ourselves of the freedom to factorize $p(x, s, b)$ as we wish and therefore to\\nreinterpret the factors. This freedom is useful because it makes it possible to keep the\\nlikelihood simple while relegating the complexity to the prior. This may not seem, at first, to be terribly helpful; after all, we arrived at the same mathematical form as Eq.~(\\\\ref{eq:toplh}). However, the complexity can be substantially mitigated through the numerical treatment of the prior, as discussed at the end of the next section. The likelihood, as we have conceptualized the problem, is given by \\n\\\\begin{align}\\np(D| s, b) = \\\\frac{e^{-(s+b)} (s + b)^D}{D!},\\n\\\\end{align}\\nwhere $D = 17$ events. \\nThe final ingredient is the prior $\\\\pi(s)$. At this stage, all we know is that $s \\\\geq 0$. Again,\\nthere is no unique way to specify $\\\\pi(s)$, though as noted there are well founded methods to\\nconstruct it. We shall variously assume either the improper prior $\\\\pi(s) = 1$ or the proper prior $\\\\pi(s) = \\\\delta(s - 14)$. \\n\\\\subsubsection*{Marginal likelihood}\\nAfter this somewhat discursive discussion of the probability model, we have done the hard part: building the full probability model. Hereafter, the rest of the Bayesian\\nanalysis is mere computation.\\nIt is convenient to eliminate the nuisance parameter $b$,\\n\\\\begin{align}\\np(D | s, H_1) & = \\\\int_0^\\\\infty p(D | s, b ) \\\\, \\\\pi(b ) d(k b),\\\\nonumber\\\\\\\\\\n& = \\\\frac{1}{Q} (1- x)^2 \\\\sum_{r=0}^N \\\\textrm{Beta}(x, r+1, Q) \\\\, \\n\\\\textrm{Poisson}(N - r| s ),\\\\\\\\\\n\\\\textrm{where } x & = 1/(1+k), \\\\nonumber\\\\\\\\ \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 10:} Show this} \\\\nonumber\\n\\\\end{align}\\nand thereby arrive at the marginal likelihood $p(D | s, H_1)$. This example, the \\\\textbf{Poisson-gamma} model is particularly simple and lends itself to exact calculation. However, the complexity rapidly increases as the prior\\nbecomes more and more complicated. In the probability model that is used in the Higgs boson analyses at the LHC, the part we would consider the prior, $\\\\pi(\\\\mu, m_H, \\\\omega)$, is of enormous complexity. However, the part that we would call the likelihood, $p(D|\\\\mu, m_H, \\\\omega)$, is relatively simple. The parameter $\\\\mu$ denotes one or more signal strengths --- the ratio of the cross section times branching fraction to that predicted by the Standard Model (SM), and\\n$m_H$ is the Higgs boson mass. The parameter $\\\\omega$ represent the expected (and therefore unknown) SM signal predictions and the expected backgrounds. When faced with such complexity, it proves useful to use a \\\\textbf{hierarchical Bayesian model}. Briefly, the prior \\n$\\\\pi(\\\\mu, m_H, \\\\omega)$ is written as\\n\\\\begin{align*}\\n\\\\pi(\\\\mu, m_H, \\\\omega) & = \\\\pi(\\\\omega| \\\\mu, m_H) \\\\, \\\\pi(\\\\mu, m_H), \\\\\\\\\\n\\\\textrm{where } \\\\pi(\\\\omega| \\\\mu, m_H) \\n& = \\\\int \\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H) \\\\, \\\\pi(\\\\phi | \\\\mu, m_H) \\\\, d\\\\phi.\\n\\\\end{align*}\\nThe prior $\\\\pi(\\\\phi | \\\\mu, m_H)$ models the lowest level systematic parameters that define\\nquantities such as the jet energy scale, lepton efficiencies, trigger efficiencies, and the parton distribution functions. It is usually straightforward to sample from this prior. Moreover,\\nthe function $\\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H)$ is nothing more than prior for the expected signal and\\nbackground parameters $\\\\omega$, which through estimates $\\\\hat{\\\\omega}$ depend implicitly\\non the parameters $\\\\phi$. The prior $\\\\pi(\\\\omega| \\\\phi, \\\\mu, m_H)$ is generally quite simple;\\nfor binned data it is just a product of gamma (or gamma mixture) densities; more generally,\\nit is a product of gamma, Gaussian, or log-normal densities. Consequently, \\nthe marginalizations over $\\\\omega$ can be done in two steps: first generate a point $\\\\phi_i$\\nfrom $\\\\pi(\\\\phi| \\\\mu, m_H)$, then generate a point $\\\\omega_i$ from $\\\\pi(\\\\omega|\\\\phi_i, \\\\mu, m_H)$. In that way, the enormous complexity of explicitly modeling the dependence of $\\\\omega$ on\\n$\\\\phi$ is avoided, with the added benefit that all, possibly very complicated, correlations (in principle, to all orders) are accounted for automatically. The marginal likelihood can be\\napproximated by\\n\\\\begin{align}\\np(D | \\\\mu, m_H) \\\\approx \\\\frac{1}{M} \\\\sum_{m=1}^M p(D | \\\\mu, m_H, \\\\omega_m).\\n\\\\end{align}\\nWhat we have just described is merely integration via a Monte Carlo approximation. The point is that the sampling required to compute $pi(D | \\\\mu, m_H)$ can be run in $M$ parallel analysis jobs, each of which is given a different random number seed in order to sample a\\nsingle pair of points $\\\\phi_m$ and $\\\\omega_m$. The results of such a Bayesian analysis would be the likelihood $p(D| \\\\mu, m_H, \\\\omega$ and an ensemble of points $\\\\{ \\\\omega_m \\\\}$.\\n\\\\subsubsection*{Posterior density}\\nGiven the marginal likelihood $p(D | s, H_1)$ and a prior $\\\\pi(s)$ we can compute the posterior density,\\n\\\\begin{align}\\np(s | D, H_1) & = p(D | s, H_1) \\\\, \\\\pi(s) / p(D | H_1), \\\\\\\\\\n\\\\textrm{where,} \\\\nonumber \\\\\\\\\\np(D | H_1) & = \\\\int_0^\\\\infty p(D | s, H_1) \\\\, \\\\pi(s) \\\\, ds. \\\\nonumber\\n\\\\end{align}\\nAgain, for simplicity, we assume a flat prior for the signal, $\\\\pi(s) = 1$ and\\nfind\\n\\\\begin{align}\\np(s | D, H_1) & = \\\\frac{\\\\sum_{r=0}^N \\\\textrm{Beta}(x, r + 1, Q) \\\\, \\\\textrm{Poisson}( N - r| s)}\\n{\\\\sum_{r=0}^N \\\\textrm{Beta}(x, r + 1, Q)}, \\\\\\\\ \\\\medskip\\n& \\\\framebox{\\n\\\\parbox{0.6\\\\textwidth}{\\\\textbf{Exercise 11:} \\\\textrm{Derive an expression for} \\n$p(s | D, H_1)$ \\nassuming $\\\\pi(s) = $ Gamma$(q s, 1, M + 1)$ where $q$ and $M$ are constants}\\n}\\n\\\\nonumber\\n\\\\end{align}\\nfrom which we can compute the central \\\\textbf{credible interval} $[9.9 , 18.4]$ for $s$ at\\n68\\\\\\n\\\\subsubsection{Bayes factor}\\nAs noted, the number $p(D | H_1)$ can be used to perform a hypothesis test. But, as argued\\nabove, we need to use a proper prior for the signal, that is, a prior that integrates to one.\\nThe simplest such prior is a $\\\\delta$-function, e.g., $\\\\pi(s) = \\\\delta(s - 14)$. Using this prior,\\nwe find\\n\\\\begin{align*}\\np(D | H_1) = p(D | 14, H_1) = 9.28 \\\\times 10^{-2}.\\n\\\\end{align*}\\nSince the background-only hypothesis $H_0$ is nested in $H_1$, and defined by $s = 0$, the number $p(D | H_0)$ is given by $p(D|0, H_1)$, which yields\\n\\\\begin{align*}\\np(D | H_0) = p(D | 0, H_1) = 3.86 \\\\times 10^{-6}.\\n\\\\end{align*}\\nWe conclude that the hypothesis $s = 14$ is favored over $s = 0$ by a Bayes factor of 24,000. In order to avoid large numbers, the Bayes factor can be mapped into a (signed) measure akin\\nto the frequentist ``$n$-sigma\"~\\\\cite{Sezen},\\n\\\\begin{align}\\nZ = \\\\textrm{sign}(\\\\ln B_{10}) \\\\sqrt{2 |\\\\ln B_{10}|}, \\n\\\\end{align}\\nwhich gives $Z = 4.5$. Negative values of $Z$ correspond to hypotheses that are excluded.', '\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Expected sensitivity and bands}\\nThe expected sensitivity for limits and discovery are useful quantities, though subject to some degree of ambiguity. Intuitively, the expected upper limit is the upper limit one would expect to obtain if the background-only hypothesis is true. Similarly, the expected significance is the significance of the observation assuming the standard model signal rate (at some $\\\\mh$). To find the expected limit one needs a distribution $f(\\\\mu_up | \\\\mu=0,\\\\vec\\\\theta)$. To find the expected significance one needs the distribution $f(Z | \\\\mu=1,\\\\vec\\\\theta)$ or, equivalently, $f(p_0 | \\\\mu=1,\\\\vec\\\\theta)$. We use the median instead of the mean, as it is invariant to the choice of $Z$ or $p_0$. More importantly, is that the expected limit and significance depend on the value of the nuisance parameters $\\\\vec\\\\theta$, for which we do not know the true values. Thus, the expected limit and significance will depend on some convention for choosing $\\\\vec\\\\theta$. While many nuisance parameters have a nominal estimate (i.e. the global observables in the constraint terms), others do not (eg. the exponent in the $H\\\\to\\\\gamma\\\\gamma$ background model). Thus, we choose a convention that treats all of the nuisance parameters consistently, which is the profiled value based on the observed data. Thus for the expected limit we use $ f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))$ and for the expected significance we use $f(p_0 | \\\\mu=1,\\\\hat{\\\\hat{\\\\vec\\\\theta}}(\\\\mu=1, \\\\rm obs))$. An unintuitive and possibly undesirable feature of this choice is that the expected limit and significance depend on the observed data through the conventional choice for $\\\\vec\\\\theta$.\\nWith these distributions we can also define bands around the median upper limit. Our standard limit plot shows a dark green band corresponding to $\\\\mu_{\\\\pm 1}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 1}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 1) \\n\\\\end{equation}\\nand a light yellow band corresponding to $\\\\mu_{\\\\pm 2}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 2}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 2) \\n\\\\end{equation}\\n', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\\\subsection{Model Selection}\\nConceptually, hypothesis testing in the Bayesian approach (also called model selection)\\nproceeds exactly the same way as any other Bayesian calculation: we compute the \\nposterior density,\\n\\\\begin{align}\\np(\\\\theta, \\\\omega, H | D) & = \\\\frac{p(D | \\\\theta, \\\\omega, H) \\\\, \\\\pi(\\\\theta, \\\\omega, H)} {p(D)},\\n\\\\end{align}\\nand marginalize it with respect to all parameters except the ones that label\\nthe hypotheses or models, $H$, \\n\\\\begin{align}\\np(H | D ) & = \\\\int p(\\\\theta, \\\\omega, H | D) \\\\, d\\\\theta \\\\, d\\\\omega.\\n\\\\end{align}\\nEquation~(\\\\ref{eq:pHD}) is\\nthe probability of hypothesis $H$ given the observed data $D$.\\nIn principle, the parameters $\\\\omega$ could also depend on $H$. For example, suppose\\nthat $H$ labels different parton distribution function (PDF) models, say CT10, MSTW, and\\nNNPDF, then $\\\\omega$ would indeed depend on the PDF model and should be written\\nas $\\\\omega_H$.\\nIt is usually more convenient to arrive at the probability $p(H|D)$ in stages.\\n\\\\begin{enumerate}\\n\\\\item Factorize the prior in the most convenient form,\\n\\\\begin{align}\\n\\\\pi(\\\\theta, \\\\omega_H, H) & = \\\\pi(\\\\theta, \\\\omega_H | H) \\\\, \\\\pi(H), \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\theta |\\\\omega_H, H) \\\\, \\\\pi(\\\\omega_H | H) \\\\, \\\\pi(H),\\\\\\\\\\n\\\\textrm{or} \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\omega_H |\\\\theta, H) \\\\, \\\\pi(\\\\theta | H) \\\\, \\\\pi(H).\\n\\\\end{align}\\nOften, we can assume that the parameters of interest $\\\\theta$ are independent,\\n\\\\emph{a priori}, of both the nuisance\\nparameters $\\\\omega_H$ and the model label $H$, in which case we can write,\\n$\\\\pi(\\\\theta, \\\\omega_H, H) = \\\\pi(\\\\theta) \\\\, \\\\pi(\\\\omega_H|H) \\\\, \\\\pi(H)$.\\n\\\\item Then, for each hypothesis, $H$, compute the function\\n\\\\begin{align}\\np(D | H ) = \\\\int p(D | \\\\theta, \\\\omega_H, H) \\\\, \\\\pi(\\\\theta, \\\\omega | H) \\\\, d\\\\theta \\\\,\\nd\\\\omega.\\n\\\\end{align}\\n\\\\item Then, compute the probability of each hypothesis,\\n\\\\begin{align}\\np(H | D ) =\\\\frac{p(D | H) \\\\, \\\\pi(H)} {\\\\sum_H p(D | H) \\\\, \\\\pi(H)}.\\n\\\\end{align} \\n\\\\end{enumerate}\\nClearly, in order to compute $p(H | D)$ it is necessary to specify the priors $\\\\pi(\\\\theta, \\\\omega | H)$ and $\\\\pi(H)$. With some effort, it is possible to arrive at an acceptable form for\\n$\\\\pi(\\\\theta, \\\\omega | H)$, however, it is highly unlikely that consensus could ever be reached on the discrete prior\\n$\\\\pi(H)$. At best, one may be able to adopt a convention. For example, if by convention two hypotheses $H_0$ and $H_1$ are to be regarded as equally likely, \\\\emph{a priori},\\nthen it would make sense to assign $\\\\pi(H_0) = \\\\pi(H_1) = 0.5$.\\nOne way to circumvent the specification of the prior $\\\\pi(H)$ is to compare the probabilities,\\n\\\\begin{align}\\n\\\\frac{p(H_1 | D )}{p(H_0 | D)} =\\\\left[ \\\\frac{p(D | H_1)}{p(D | H_0} \\\\right] \\\\,\\n\\\\frac{ \\\\pi(H_1)} {\\\\pi(H_0)}.\\n\\\\end{align}\\nand use only the term in brackets, called the global \\\\textbf{Bayes factor}, $B_{10}$, as a way to\\ncompare hypotheses. The Bayes factor specifies by how much the relative probabilities\\nof two hypotheses changes as a result of incorporating new data, $D$. The word global \\nindicates that we have marginalized over all the parameters of the two models. The \\\\emph{local}\\nBayes factor, $B_{10}(\\\\theta)$ is defined by\\n\\\\begin{align}\\nB_{10}(\\\\theta) & = \\\\frac{p(D| \\\\theta, H_1)}{p(D| H_0)}, \\\\\\\\\\n\\\\textrm{where}, \\\\nonumber\\\\\\\\\\np(D| \\\\theta, H_1) & \\\\equiv \\\\int p(D | \\\\theta, \\\\omega_{H_1}, H_1) \\\\, \\\\pi(\\\\omega_{H_1} | H_1) \\\\, d\\\\omega_{H_1},\\n\\\\end{align}\\nare the \\\\textbf{marginal} or integrated likelihoods in which we have assumed the \\\\emph{a priori}\\nindependence of $\\\\theta$ and $\\\\omega_{H_1}$. We have further assumed\\nthat the marginal likelihood $H_0$ is independent of $\\\\theta$, which is a very\\ncommon situation. For example, $\\\\theta$ could be the expected signal count $s$,\\nwhile $\\\\omega_{H_1} = \\\\omega$ could be the expected background $b$. In this case, the\\nhypothesis $H_0$ is a special case of $H_1$, namely, it is the same as $H_1$ with $s = 0$. An hypothesis that is a special case of another \\nis said to be \\\\textbf{nested} in the more general hypothesis. The Bayesian example, discussed below, will\\nmake this clearer. \\nThere is a subtlety that may be missed: because of the way we have\\ndefined $p(D|\\\\theta, H)$, we\\nneed to multiply $p(D| \\\\theta, H)$ by the prior $\\\\pi(\\\\theta)$ and then integrate with respect\\nto $\\\\theta$ in order to calculate $p(D | H)$.\\n\\\\subsubsection{A Word About Priors}\\nConstructing a prior for nuisance parameters is generally neither controversial (for most parameters) nor problematic. Such difficulties as do arise occur when the priors must, of necessity,\\ndepend on expert judgement. For example, one theorist may\\ninsist that a uniform prior within a finite interval is a reasonable prior for the factorization scale in a QCD calculation, while in the expert judgement of another the interval should be twice as large.\\nClearly, in this case, there is no getting around the fact that the prior for this parameter is \\nunavoidably subjective. However, once a choice is made, a prior $\\\\pi(\\\\omega_H|H)$ that integrates to one can be constructed.\\nThe Achilles heal of the Bayesian approach is the need to specify the prior $\\\\pi(\\\\theta)$,\\nfor the parameters of interest,\\nat the start of the inference chain when we know almost nothing\\nabout these parameters. Careless specification of this prior can yield\\nresults that are unreliable or even nonsensical. The mandatory requirement is that \\nthe posterior density be proper, that is integrate to unity. Ideally, the same should hold\\nfor priors. A very extensive literature exists on the topic of prior specification\\nwhen the available information is extremely limited. However, a discussion of this\\ntopic is beyond the scope of these lectures; but, we shall make a few remarks.\\nFor model selection, we need to proceed with caution because\\nBayes factor are sensitive to the choice of priors and therefore less robust than posterior densities. Suppose that the prior $\\\\pi(\\\\theta) = C f(\\\\theta)$, where $C$ is a normalization\\nconstant. The global Bayes factor for the two hypotheses $H_1$ and $H_0$ can be written as\\n\\\\begin{align}\\nB_{10} = C \\\\frac{\\\\int p(D | \\\\theta, H_1) \\\\, f(\\\\theta) \\\\, d\\\\theta}{p(D | H_0)}.\\n\\\\end{align}\\nTherefore, if the constant $C$ is ill defined, typically because $\\\\int f(\\\\theta) \\\\, d\\\\theta = \\\\infty$,\\nthe Bayes factor will likewise be ill defined. For this reason, it is generally recommended\\nthat an improper prior not be used for parameters $\\\\theta$ that occur only in one hypothesis, here $H_1$. However, for parameters that are common to all hypotheses, it is permissible to\\nuse improper priors because the ill defined constant cancels in the Bayes factor.\\nThe discussion so far has been somewhat abstract. The next section therefore works through a detailed example of a possible Bayesian analysis of the D\\\\O\\\\ top discovery data.\\n', \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that the modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was no real reason to insure that the correlations are all correct. \\nA commonplace, and often implicit, belief is that that although the correlations in the synthetic data may not be exactly the same as the correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using the synthetic data would be to train on real data. Unfortunately, while the data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. And even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. Thus there are two ways to proceed. First, we can try to to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to get truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the faction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, recall that these simulations have different components. The short-distance simulation, which produces of order 1000 particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely exciting, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. An proof-of-principle implementation of this idea is OminFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then the mapping can be inverted to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up my many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background an some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, were some supervised training is used to guide data-driven estimation technique is a very promising area for future development of ML for particle physics. \\n\", '\\\\section{Profile frequentist estimator}\\nThe estimator \\\\( \\\\mathcal{Q} \\\\) has been used in calculating upper limits for the mass of the Higgs boson. However, incorporating systematic effects is complex. Including these effects requires randomly varying the central values to obtain \\\\( f(\\\\mathcal{Q},\\\\mu) \\\\), which increases the variance and thus expands the upper limits. This phenomenon will be analyzed in detail in the section dedicated to systematic effects~\\\\cite{barlow2002systematic}. Currently, phenomenology and experimental analyses employ a statistical estimator, \\\\( q_{\\\\mu} \\\\), which combines the unbiased frequentist approach with the incorporation of systematic effects through the profile likelihood. This estimator requires maximizing the likelihood function with respect to signal strength, and for single-channel experiments, it is given by~\\\\cite{lista2016practical,cranmer2015practical}:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(\\\\hat{\\\\mu})}.\\n\\\\end{equation}\\nWhere \\\\( \\\\hat{\\\\mu} \\\\) is the maximum likelihood estimator of the observation \\\\( n \\\\). By appropriately applying the logarithmic function, the estimator can be reformulated as a minimization problem. The hypothesis test without considering systematic effects is expressed as follows:\\n\\\\begin{equation}\\nq_{\\\\mu} = -2ln(\\\\lambda(\\\\mu)).\\n\\\\end{equation}\\nFinally, to quantify the degree of disagreement between the observation and the hypothesis, the p-value of the observation is calculated:\\n\\\\begin{equation}\\np_{\\\\mu} = \\\\int_{q_{\\\\mu,obs}}^{\\\\infty} f(q_{\\\\mu}/\\\\mu) dq_{\\\\mu},\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu, \\\\text{obs}} \\\\) is the observed value of the estimator in the data, and \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) represents the distribution of the estimator for a specific value of \\\\( \\\\mu \\\\). Generally, this is an optimization problem to obtain the best fit of the model, followed by a sampling problem to determine \\\\( f(q_{\\\\mu}/\\\\mu) \\\\). Specifically, in the case of upper limit searches, the statistical test simplifies to~\\\\cite{lista2016practical,read2002presentation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = \\n\\\\begin{cases} \\n-2ln(\\\\lambda(\\\\mu)) & \\\\hat{\\\\mu} \\\\le \\\\mu \\\\\\\\\\n0 & \\\\hat{\\\\mu} > \\\\mu.\\n\\\\end{cases}\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu} = 0 \\\\) is adjusted to avoid excluding values smaller than the maximum likelihood estimator (i.e., in the non-physical case). With these definitions, the confidence level (\\\\( CL_{b} \\\\)) associated with the observation under the background-only hypothesis is expressed as:\\n\\\\begin{equation}\\nCL_{b} = 1-p_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{\\\\mu}/0) dq_{\\\\mu},\\n\\\\end{equation}\\nTherefore, the confidence level of the signal, \\\\( CL_{s}(\\\\mu) \\\\), is defined as:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}.\\n\\\\end{equation}\\nAs in the case of the unprofiled estimator, the upper limit is defined by \\\\( CL_{s}(\\\\mu_{up}) = 0.05 \\\\), which corresponds to the model exclusion criterion. For the single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), a scan is performed by sampling the distribution \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) for both hypotheses. Figure~[\\\\ref{fig:13}] shows the scan of the confidence level for the signal strength as a function of \\\\( \\\\mu \\\\), as well as the distributions of \\\\( H_{0} \\\\) and \\\\( H_{1} \\\\) for \\\\( \\\\mu=1 \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LHC/UpperLimit_qm.ipynb}{Source code}}.\\nAt each point, the maximum likelihood estimator is obtained using the specialized \\\\texttt{optimize} package~\\\\cite{virtanen2018scipy}. The expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.13 \\\\), and the observed upper limit is \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.48 \\\\). These values are fully consistent with the upper limits obtained using the estimator \\\\( \\\\mathcal{Q} \\\\).\\n', \"\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Bayesian upper limits}\\nSince the upper limits obtained through the frequentist approach can lead to non-physical statistical boundaries, alternative approaches can improve the results. One of the most promising strategies is based on Bayes' theorem:\\n\\\\begin{equation}\\nP(\\\\bm{\\\\theta}/x) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\bm{\\\\theta})\\\\Pi(\\\\bm{\\\\theta})}{P(\\\\bm{x})}.\\n\\\\end{equation}\\nWhere $P(\\\\bm{\\\\theta}|\\\\bm{x})$ represents the probability that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true given the set of observations $\\\\bm{x}$, and is known as the \\\\textit{posterior distribution}. $\\\\mathcal{L}(\\\\bm{x}|\\\\bm{\\\\theta})$, known as the \\\\textit{likelihood function}, describes the probability of observing $\\\\bm{x}$ given that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true. On the other hand, $\\\\Pi(\\\\bm{\\\\theta})$ is the \\\\textit{prior distribution}, which reflects the probability that the hypothesis $\\\\bm{\\\\theta}$ is true before the observations are made, and $P(\\\\bm{x})$ is the \\\\textit{total probability} of observing $\\\\bm{x}$ across all hypotheses. In optimization processes, this latter distribution is considered a normalization factor for the posterior distribution~\\\\cite{wang2023recent}. From a parameter estimation perspective, sampling from the posterior distribution generally requires robust methods, such as the Metropolis-Hastings algorithm~\\\\cite{chib1995understanding}.\\nBy incorporating an appropriate prior distribution, such as one with a minimum at $\\\\mu=0$, the coverage problem present in the frequentist approach is corrected. This implies that, for no value of the parameter of interest $\\\\mu$, is the null hypothesis excluded. An unbiased distribution is given by:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nThe value of $\\\\mu^{max}$ is chosen to obtain p-values consistent with the critical region (i.e., $\\\\alpha = 0.05$). However, the degree of subjectivity in selecting the prior distribution introduces ambiguity in the calculation of credible confidence intervals, as well as in the upper limits. In general, it has been observed that Bayesian parameter estimation leads to less restrictive upper limits, which are dependent on the choice of the prior distribution. This characteristic has limited the use of Bayesian upper limits in physics analyses with real observations~\\\\cite{cms2012observation,atlas2012observation}.\\nReturning to the simplified model with no observation, $b \\\\approx n = 0$, and $s=1$, the Bayesian upper limit for this new model is $\\\\mu_{up} = 2.999$ at $95\\\\\\n\\\\begin{equation}\\nP(\\\\bm{x}) = \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu)\\\\Pi(\\\\mu) d \\\\mu.\\n\\\\end{equation}\\nAlthough both methods seem to lead to the same results for the estimation of upper limits, varying the background component and the number of observed events reveals an adjustment in the upper limits that corrects the coverage issue in the estimation. Figure~[\\\\ref{fig:6}] shows the behavior of the upper limit as a function of the background component and the number of observations. Note how the null hypothesis is not excluded when the prior distribution is correctly chosen. However, manipulating the prior distribution can result in a significant shift in the upper limit value. For this reason, the Bayesian method is not widely used in the analysis of real data.\\nAdditionally, it is possible to estimate the upper limit using the Markov Chain Monte Carlo (MCMC) technique~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Bayesian/MetropolisSampling.ipynb}{Source Code}}. This method allows sampling from the posterior distribution based on Markov processes, as described in various sources~\\\\cite{chib1995understanding, raftery1992practical}. In general, this approach enables the construction of the marginal posterior function for the calculation of upper limits, the estimation of standard errors ($2\\\\sigma$), and the necessary parameter estimation in methods such as the profile of maximum likelihood, discussed later. Figure~[\\\\ref{fig:7}] shows the sampling of the posterior distribution for the toy model, leading to an approximation of the upper limit using the $P_{95}$ percentile.\\nThe MCMC algorithm is widely used for estimation in real multichannel experiments and incorporates systematic effects. The development of these ideas requires non-Bayesian approaches that do not depend on the choice of the prior distribution. An initial non-Bayesian approach that protects the null hypothesis is known as the modified frequentist method~\\\\cite{read2002presentation, cms2022portrait}.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Modified frequentist method}\\nIn general, the construction of frequentist estimators is based on determining the confidence level associated with a statistical estimator. For a statistic \\\\( Q \\\\), the confidence level of the hypothesis considering only the background component is given by the probability that \\\\( Q \\\\) takes a value less than or equal to the observed value \\\\( Q_{obs} \\\\)~\\\\cite{lista2016practical, barlow2019practical, read2002presentation}.\\n\\\\begin{equation}\\nCL_{b} = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{b}}{dQ} dQ,\\n\\\\end{equation}\\nwhere \\\\( Q_{obs} \\\\) depends on the observed values: \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Similarly, the confidence level for the signal + background hypothesis is defined by the probability that \\\\( Q \\\\) is less than or equal to \\\\( Q_{obs} \\\\), thus:\\n\\\\begin{equation}\\nCL_{s+b}(\\\\mu) = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{\\\\mu s+b}}{dQ} dQ,\\n\\\\end{equation}\\nThe modification of the frequentist method involves the renormalization of the confidence level for the alternative hypothesis:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = CL_{s+b}(\\\\mu)/C_{b}.\\n\\\\end{equation}\\nThe purpose of this definition is to maintain the coverage of the estimator to protect the null hypothesis \\\\( H_{0} \\\\). In other words, the exclusion values of \\\\( \\\\mu \\\\) are positive. In particular, \\\\( CL_{s} \\\\) for event counting is given by:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\sum_{i=0}^{n} \\\\frac{e^{-(\\\\mu s + b)} (\\\\mu s + b)^{i}}{i!} \\\\bigg/ \\n\\\\sum_{i=0}^{n} \\\\frac{e^{-(b)} (b)^{i}}{i!}.\\n\\\\end{equation}\\nUsing the expression for the cumulative Poisson distribution (Appendix~\\\\ref{sec:AppendixA}), it is possible to find the value of \\\\( CL_{s} \\\\) for different values of the signal strength \\\\( \\\\mu \\\\). Figure~[\\\\ref{fig:8}] shows the exploration of the p-value as a function of the signal strength for \\\\( b \\\\approx n = 0 \\\\) and \\\\( s = 1 \\\\). The upper limit \\\\( \\\\mu_{up} = 2.99 \\\\) is consistent with values obtained using previous methods.\\nIn the previous sections, estimations were made for a specific point defined by \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Table~[\\\\ref{tb:2}] summarizes the calculation of upper limits for the three methods discussed above, evaluated for different values of \\\\( n \\\\) and \\\\( b \\\\) while keeping \\\\( s = 1 \\\\) constant~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/ModifiedFrequentist/ModifiedUpperLimit.ipynb}{Source code}}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{lcccc}\\n\\\\hline\\nObservation ($n$) & Expected background ($b$) & Frequentist & Bayesian & Modified frequentist \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0 & 0 & 2.99 & 3.00 & 2.99 \\\\\\\\\\n& 1 & 1.99 & 3.00 & 2.99 \\\\\\\\\\n& 2 & 0.99 & 3.00 & 2.99 \\\\\\\\\\n& 3 & 0.00 & 3.00 & 2.99 \\\\\\\\\\n\\\\hline\\n1 & 0 & 4.74 & 4.76 & 4.75 \\\\\\\\\\n& 1 & 3.74 & 4.11 & 4.12 \\\\\\\\\\n& 2 & 2.74 & 3.82 & 3.82 \\\\\\\\\\n& 3 & 1.74 & 3.65 & 3.65 \\\\\\\\\\n\\\\hline\\n2 & 0 & 6.29 & 6.30 & 6.29 \\\\\\\\\\n& 1 & 5.29 & 5.41 & 5.42 \\\\\\\\\\n& 2 & 4.29 & 4.83 & 4.83 \\\\\\\\\\n& 3 & 3.29 & 4.45 & 4.45 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nThe development of the concept of confidence level and the application of the Neyman-Pearson Lemma for the signal hypothesis have facilitated the creation of frequentist statistical estimators that are unbiased by prior distributions. At the LEP collider, a parameter-independent estimator \\\\( Q(\\\\mu) \\\\) was developed~\\\\cite{cms2022portrait, cranmer2015practical}. More recently, at the LHC, the estimator \\\\( q_{\\\\mu} \\\\) has been employed, which is based on the profile likelihood~\\\\cite{lista2016practical, jme2010cms}. This estimator maximizes the parameters for statistical and systematic uncertainty within the likelihood function to incorporate these effects in the calculation of upper limits, experimental sensitivity, or the potential observation of new physics.\\n', \"\\\\section{Appendix B: Statistical significance for small values of signal}\\nThere are two approaches to assess the significance of a potential new physics signal: using the estimator $\\\\mathcal{Q}$ or through the test statistic $q_{\\\\mu}$. In the first approach, a Gaussian approximation is obtained by evaluating the statistical estimator at its central values.\\n\\\\begin{equation}\\n-2ln \\\\mathcal{Q}_{i} = 2 s_{i} - 2 n_{i} ln \\\\bigg( 1 + \\\\frac{s_{i}}{b_{i}} \\\\bigg).\\n\\\\end{equation}\\nBy defining $w_{i} = \\\\ln \\\\left( 1 + \\\\frac{s_{i}}{b_{i}} \\\\right)$, we can calculate the expected value for the distributions of kackground only ($b$) and signal + background ($s+b$), respectively.\\n\\\\begin{eqnarray}\\n< -2ln \\\\mathcal{Q}_{i} >_{b} & = & 2s_{i} - 2(b_{i}) w_{i} {} \\\\nonumber \\\\\\\\\\n< -2ln \\\\mathcal{Q}_{i} >_{s+b} & = & 2s_{i} - 2(s_{i} + b_{i}) w_{i} {} \\n\\\\end{eqnarray}\\nIn this way, we can quantify the number of standard deviations between the expected number of background events and signal + background events across all channels.\\n\\\\begin{eqnarray}\\nZ_{0} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 b_{i}w_{i}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} b_{i} w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{B}w} = \\\\frac{S}{\\\\sqrt{B}}. {}\\n\\\\end{eqnarray}\\nWhere the standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){b}$ is calculated as: \\n\\\\begin{eqnarray}\\n\\\\sigma_{b}^{2} & = & E( (x- E(x))^{2} ) {} \\\\nonumber \\\\\\\\ \\n& = & E( (2 s_{i} - 2 n_{i} w_{i} - 2s_{i} + 2b_{i} w_{i})^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}E( n_{i}^{2} - 2n_{i}b_{i} + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( E(n_{i}^{2}) - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( \\\\sigma_{n}^{2} + E(n_{i})^{2} - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4b_{i}w_{i}^{2}. {}\\n\\\\end{eqnarray}\\nWith $\\\\sigma_{b}^{2} = \\\\sigma_{n}^{2} = E(n_{i}) = b_{i}$, which follows a Poisson distribution. Similarly, the number of standard deviations for the signal + background distribution is:\\n\\\\begin{eqnarray}\\nZ_{1} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{s+b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 (s_{i}+b_{i})w_{i}^{2}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} (s_{i}+b_{i}) w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{S+B}w} = \\\\frac{S}{\\\\sqrt{S+B}}. {}\\n\\\\end{eqnarray}\\nThe standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){s+b}$ is calculated in a similar way:\\n\\\\begin{equation}\\n\\\\sigma_{s+b}^{2} = E( (x- E(x))^{2} ) = 4(s_{i}+b_{i})w_{i}^{2}.\\n\\\\end{equation}\\nOn the other hand, in the second scenario, the test statistic $q_{\\\\mu}$, in the context of single-channel counting experiments, is defined from the Poisson likelihood function:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\frac{(\\\\mu s +b)^{n}}{n!}e^{-(\\\\mu s + b)}.\\n\\\\end{equation}\\nThe profile likelihood for the null hypothesis $\\\\mu = 0$ is given by:\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2ln\\\\frac{\\\\mathcal{L}(0)}{\\\\mathcal{L}(\\\\hat{\\\\mu})} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }. \\n\\\\end{cases}\\n\\\\end{equation}\\nIn this case, the maximum likelihood parameter is $\\\\hat{\\\\mu} = \\\\frac{n - b}{s}$. Using the Asimov data, where $n = s + b$, and Wilks' approximation~\\\\cite{conway2005calculation}, we have:\\n\\\\begin{equation}\\nZ_{0} = \\\\sqrt{q_{0}} = \\n\\\\begin{cases} \\n\\\\sqrt{2((s+b)ln(1+\\\\frac{s}{b}) - s)} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nFinally, by expanding the logarithmic function under the condition $s \\\\ll b$, we obtain the Gaussian approximation of the significance:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\frac{s}{\\\\sqrt{b}}.\\n\\\\end{equation}\\n\\\\end{document}\", \"\\\\section{Upper Limits including systematic uncertainties, Bayesian approach}\\nThe inclusion of systematic effects in the calculation of upper limits, experimental sensitivity, and observation requires a Bayesian approach. This strategy extends the likelihood function by including typically Gaussian distributions to model effects such as efficiency, luminosity, Monte Carlo event estimation, and others~\\\\cite{lista2016practical,cranmer2015practical,junk1999confidence}. In particular, to establish the reconstruction efficiency of background events, a nuisance parameter (\\\\( \\\\epsilon \\\\)) centered on the expected value of \\\\( b \\\\) can be included. Thus, the parameter of the Poisson distribution is defined by:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu,\\\\epsilon) = \\\\mu s + \\\\epsilon b,\\n\\\\end{equation}\\nwhere the efficiency follows a binomial distribution \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\). The standard deviation of the likelihood adjusts the uncertainty of the number of background events across the observable spectrum, typically ranging from 5 to 20\\\\\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon) = \\\\prod_{i=1}^{N-channels}\\n\\\\frac{ e^{-(\\\\mu s_{i} + \\\\epsilon b_{i})} (\\\\mu s_{i} + \\\\epsilon b_{i})^{n_{i}} }{n_{i}!}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^{2}}} e^{ -\\\\frac{ (1-\\\\epsilon)^{2} } {2\\\\sigma^{2}} }.\\n\\\\end{equation}\\nThe non-informative prior distribution naturally extends to:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu,\\\\epsilon) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\ \\\\text{and} \\\\ 0 < \\\\epsilon < \\\\epsilon^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nUsing Bayes' theorem, the posterior distribution is obtained.\\n\\\\begin{equation}\\nP(\\\\mu, \\\\epsilon / \\\\bm{x}) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon)}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThis means that to establish the upper limits of \\\\( \\\\mu \\\\) or the experimental sensitivity, the posterior must be marginalized to find the profile \\\\( P(\\\\mu | \\\\bm{x}) \\\\). This is a standard probability calculation and requires numerical integration or sampling of the posterior distribution using, for example, the Markov Chain Monte Carlo (MCMC) algorithm~\\\\cite{raftery1992practical, wang2023recent}. In any case, the probability profile is given by:\\n\\\\begin{equation}\\nP(\\\\mu,\\\\bm{x}) = \\\\int_{0}^{\\\\infty} P(\\\\mu, \\\\epsilon / \\\\bm{x}) d\\\\epsilon = \\\\frac{ \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\epsilon}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThe marginalization process correctly propagates the effect of systematic uncertainty in the upper limits. In general, the correlation shifts the limit values to higher values, thereby restricting the sensitivity of a model in the experiment or the exclusion power in an experimental study~\\\\cite{conway2005calculation}. As mentioned previously, the expected and observed upper limits are defined over the marginal distribution:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = \\\\int_{0}^{\\\\mu_{up}} P(\\\\mu,x) d\\\\mu. = 0.95\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:19}] shows the posterior distribution as a function of the signal strength \\\\( \\\\mu \\\\) and the efficiency in estimating background events (\\\\( b \\\\)), for the channel with \\\\( n=105 \\\\), \\\\( b=100 \\\\), \\\\( s=10 \\\\), and a systematic uncertainty of \\\\( \\\\sigma=0.1 \\\\) for the background events~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/UpperLimitSystematic.ipynb}{Source code}}. Similarly, Figure~[\\\\ref{fig:20}] shows the marginal distribution obtained using the double Gaussian quadrature method~\\\\cite{golub1969calculation}.\\nBy varying the systematic uncertainty, it is possible to find the observed upper limits and the correlation effect between parameters. Table~[\\\\ref{tb:3}] shows the behavior of the observed upper limit as a function of \\\\( \\\\sigma \\\\) for the numerical approximation (Gaussian quadrature) and for the sampling generated by the Metropolis algorithm, as well as the correlation coefficient indicating how uncertainty limits the exclusion power of the model. It is important to note that for high-dimensional posterior distributions, there are no quadrature rules that allow for accurately estimating the marginal distribution. In such cases, the Metropolis algorithms and optimization methods have been widely applied~\\\\cite{atlas2012observation,cms2012observation}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & Gaussian quadrature & MCMC algorithm & Correlation coef\\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 & -0.32\\\\\\\\\\n0.10 & 3.34 & 3.31 & -0.54\\\\\\\\\\n0.15 & 4.09 & 4.13 & -0.68\\\\\\\\\\n0.20 & 4.91 & 4.66 & -0.77\\\\\\\\\\n0.25 & 5.79 & 5.80 & -0.88\\\\\\\\ \\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nIn particular, the \\\\texttt{emcee} package was used for sampling the extended posterior distribution shown above~\\\\cite{foreman2013emcee, Bocklund2019ESPEI}. Figure~[\\\\ref{fig:21}] shows the corner plot of the posterior distribution along with the marginal distributions associated with the signal strength \\\\( \\\\mu \\\\) and the reconstruction efficiency \\\\( \\\\epsilon \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/MetropolisSamplingBayes.ipynb}{Source code}}. Additionally, the maximum likelihood estimators of the posterior are shown; these parameters are required for the profile likelihood method presented in the following section.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Profile likelihood}\\nMost of the recent searches at LHC use the so-called {\\\\it profile likelihood}\\napproach for the treatment of nuisance parameters~\\\\cite{asymptotic}.\\nThe approach is based on the test statistic built as the following likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{L(\\\\vec{x};\\\\mu,\\\\hat{\\\\hat{\\\\theta}}(\\\\mu))}{L(\\\\vec{x};\\\\hat{\\\\mu},\\\\hat{\\\\theta})}\\\\,,\\n\\\\end{equation}\\nwhere in the denominator both $\\\\mu$ and $\\\\theta$ are fit simultaneously\\nas $\\\\hat{\\\\mu}$ and $\\\\hat{\\\\theta}$, respectively, and\\nin the numerator $\\\\mu$ is fixed, and $\\\\hat{\\\\hat{\\\\theta}}(\\\\mu)$ is the best fit of $\\\\theta$\\nfor the fixed value of $\\\\mu$. The motivation for the choice of Eq.~(\\\\ref{eq:profLike})\\nas the test statistic comes from Wilks' theorem that allows to approximate asymptotically\\n$-2\\\\ln\\\\lambda(\\\\mu)$ as a $\\\\chi^2$~\\\\cite{Wilks}.\\nIn general, Wilks' theorem applies if we have two hypotheses $H_0$ and $H_1$ that\\nare {\\\\it nested}, i.e.: they can be expressed as sets of nuisance parameters\\n$\\\\vec{\\\\theta}\\\\in\\\\Theta_0$ and $\\\\vec{\\\\theta}\\\\in\\\\Theta_1$, respectively, such that\\n$\\\\Theta_0\\\\subseteq\\\\Theta_1$. Given the likelihood function:\\n\\\\begin{equation}\\nL = \\\\prod_{i=1}^N L(\\\\vec{x}_i, \\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nif $H_0$ and $H_1$ are nested, then the following quantity,\\nfor $N\\\\rightarrow\\\\infty$, is distributed as a $\\\\chi^2$ with a number of degrees of freedom\\nequal to the difference of the $\\\\Theta_1$ and $\\\\Theta_0$ dimensionalities: \\n\\\\begin{equation}\\n\\\\chi_r^2 = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_0}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_1}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nIn case of a search for a new signal where the parameter of interest is $\\\\mu$,\\n$H_0$ corresponds to $\\\\mu = 0$ and $H_1$ to any $\\\\mu\\\\ge0$, Eq.~(\\\\ref{eq:wilks})\\ngives:\\n\\\\begin{equation}\\n\\\\chi_r^2(\\\\mu) = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu,\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\mu^\\\\prime,\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu^\\\\prime,\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nConsidering that the supremum is equivalent to the\\nbest fit value, the profile likelihood defined in Eq.~(\\\\ref{eq:profLike}) is obtained.\\nAs a concrete example of application of the profile likelihood,\\nconsider a signal with a Gaussian distribution over a background\\ndistributed according to an exponential distribution. A pseudoexperiment that was randomly-extracted\\naccordint to such a model is shown in Fig.~\\\\ref{fig:toyGplusB}, where a signal yield\\n$s=40$ was assumed on top of a background yield $b=100$, exponentially\\ndistributed in the range of the random variable $m$ from 100 to 150~GeV.\\nThe signal was assumed centered at 125~GeV with a standard deviation of 6~GeV,\\nreminding the Higgs boson invariant mass spectrum.\\nThe signal yields $s$ is fit from data.\\nAll parameters in the model are fixed, except the background yield,\\nwhich is assumed to be known with some level of uncertainty modeled\\nwith a log normal distribution whose corresponding nuisance parameter is called $\\\\beta$.\\nThe likelihood function for the model, which only depends on two parameters,\\n$s$ and $\\\\beta$, is, in case of a single measurement $m$:\\n\\\\begin{equation}\\nL(m;s,\\\\beta) = L_0(m;s,b_0 = be^\\\\beta) L_\\\\beta(\\\\beta;\\\\sigma_\\\\beta)\\\\,,\\n\\\\end{equation}\\nwhere:\\n\\\\begin{eqnarray}\\nL_0(m;s,b_0) & = & \\\\frac{e^{-(s+b_0)}}{n!}\\\\left(\\ns \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} e^{-{(m-\\\\mu)^2}/{2\\\\sigma^2}}+b_0\\\\lambda e^{-\\\\lambda m}\\n\\\\right)\\\\,, \\\\\\\\\\nL_\\\\beta(\\\\beta;\\\\sigma_\\\\beta) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma_\\\\beta}e^{-{\\\\beta^2}/{2\\\\sigma^2}}\\\\,.\\n\\\\end{eqnarray}\\nIf we measure a set values $\\\\vec{m}=(m_1,\\\\,\\\\cdots\\\\,m_N)$, the likelihood function is:\\n\\\\begin{equation}\\nL(\\\\vec{m};s,\\\\beta) = \\\\prod_{i=1}^N L(m_i;s,\\\\beta)\\\\,.\\n\\\\end{equation}\\nThe scan of $-\\\\ln\\\\lambda(s)$ is shown in Fig.~\\\\ref{fig:plScan}, where the profile likelihood\\nwas evaluated assuming $\\\\sigma_\\\\beta=0$ (no uncertainty on $b$, blue curve) or $\\\\sigma_\\\\beta=0.3$\\n(red curve). The minimum value of $-\\\\ln\\\\lambda(s)$ is equal to zero, since\\nat the minimum numerator and denominator in Eq.~(\\\\ref{eq:profLike}) are identical.\\nIntroducing the uncertainty on $\\\\beta$ (red curve) makes the curve broader.\\nThis causes an increase of the uncertainty on the estimate of $s$, whose uncertainty\\ninterval is obtained by intersecting the curve of the negative logarithm of the profile likelihood\\nwith an horizontal line at $-\\\\ln\\\\lambda(s) = 0.5$ (green line in Fig.~\\\\ref{fig:plScan}\\\\footnote{\\nThe plot in Fig.~\\\\ref{fig:plScan} was generated with the library {\\\\sc RooStats}\\nin {\\\\sc Root}~\\\\cite{Root}, which by default, uses $-\\\\ln\\\\lambda$ instead of $-2\\\\ln\\\\lambda$.\\n}).\\nIn order to evaluate the significance of the observed signal, Wilks' theorem can be\\nused. If we assume $\\\\mu=0$ (null hypothesis), the quantity $q_0 = -2\\\\ln\\\\lambda(0)$\\ncan be approximated with a $\\\\chi^2$ having one degree of freedom. Hence, the significance\\ncan be approximately evaluated as:\\n\\\\begin{equation}\\nZ\\\\simeq \\\\sqrt{q_0}\\\\,.\\n\\\\end{equation}\\n$q_0$ is twice the intercept of the curve in Fig.~\\\\ref{fig:plScan} with the vertical axis,\\nand gives an approximate significance of $Z\\\\simeq\\\\sqrt{2\\\\times6.66} = 3.66$,\\nin case of no uncertainty on $b$, and $Z\\\\simeq\\\\sqrt{2\\\\times3.93} = 2.81$, in case\\nthe uncertainty on $b$ is considered. \\nIn this example, the effect of background yield uncertainty reduces the\\nsignificance bringing it below the evidence level ($3\\\\sigma$).\\nThose numerical values can be verified\\nby running many pseudo experiments (toy Monte Carlo) assuming $\\\\mu=0$ and\\ncomputing the corresponding $p$-value. In complex cases, the computation\\nof $p$-values using toy Monte Carlo may become unpractical, and Wilks'\\napproximation provides a very convenient, and often rather precise,\\nalternative calculation.\\n\", \"\\\\section{Least squares: Basic idea}\\nAs a specific example, we will consider fitting a straight line $y = a + bx$ to some data, which consist of a series on $n$ data\\npoints, each of which specifies $(x_i, y_i \\\\pm \\\\sigma_i)$ i.e. at precisely known $x_i$, the $y$ co-ordinate is measured\\nwith an uncertainty $\\\\sigma_i$. The $\\\\sigma_i$ are assumed to be uncorrelated. The more general case could involve \\n\\\\begin{itemize}\\n\\\\item{a more complicated functional form than linear;}\\n\\\\item{multidimensional $x$ and/or $y$;}\\n\\\\item{correlations among the $\\\\sigma_i$; and}\\n\\\\item{uncertainties on the $x_i$ values.}\\n\\\\end{itemize} \\nIn Particle Physics, we often deal with a histogram of some physical quantity $x$ (e.g. mass, angle, \\ntransverse momentum, etc.), in which case $y$ is simply the number of counts for that $x$ bin. Another possiblity\\nis that $y$ and $x$ are both physical quantities e.g. we have a two-dimensional plot showing the recession velocities \\nof galaxies as a function their distance. \\nThere are two statistical issues: Are our data consistent with the theory i.e. a straight line? And what are \\nthe best estimates of the parameters, the intercept and the gradient? The former is a Goodness of Fit \\nissue, while the latter is Parameter Determination. The Goodness of Fit is more fundamental, in that\\nif the data are not consistent with the hypothesis, the parameter values are meaningless. However, we will first\\nconsider Parameter Detemination, since checking the quality of the fit requires us to use the best straight\\nline. \\nThe data statistic used for both questions is $S$, the weighted sum of squared discrepancies\\\\footnote{Many people \\nrefer to this as $\\\\chi^2$. I prefer S, because otherwise a discussion about whether or not $\\\\chi^2$ follows the\\nmathematical $\\\\chi^2$ distribution sounds confusing.}\\n\\\\begin{equation}\\nS = \\\\Sigma (y_i^{th} - y_i^{obs})^2/\\\\sigma_i^2 = \\\\Sigma (a + bx_i - y_i^{obs})^2 / \\\\sigma_i^2\\n\\\\end{equation}\\nwhere $y_i^{th} = a + bx_i$ is the predicted value of $y$ at $x_i$, and $y_i^{obs}$ is the observed value. In the \\nexpression for $S$, we regard the data $(x_i, y_i \\\\pm \\\\sigma_i)$ as being fixed, and the parameters $a$\\nand $b$ as being variable.\\nIf for specific values of $a$ and $b$ the predicted values of $y$ and the corresponding observed ones are all close \\n(as measured in terms of the \\nuncertainties $\\\\sigma$), then $S$ will be `small', while significant discrepancies result in large $S$. Thus, according\\nto the least squares method, the best values of the parameters are those that minimise $S$, and the width of the $S$\\ndistribution determines their uncertainties. For a good fit, the value of $S_{min}$ should be `small'. A more quantative\\ndiscussion of `small' appears below. \\nTo determine the best values of $a$ and $b$, we need to set the first derivatives of $S$ with respect to $a$ and $b$ both\\nequal to zero. This leads to two simultaneous linear equations for $a$ and $b$ \\\\footnote{The derivatives are linear in the \\nparameters, because the functional form is linear in them. This would also be true for more complicated situations \\nsuch as a higher order polynomial (Yes, with respect to the coefficients, a $10^{th}$ order polynomial is linear),\\na series of inverse powers, Fourier series, etc.} \\nwhich are readily solved, to yield\\n\\\\begin{equation}\\n\\\\begin{split}\\na&=\\\\frac{<x^2><y> - <xy><x>}{<x^2> - <x>^2} \\\\\\\\\\nb&=\\\\frac {<xy> - <x> <y>}{<x^2> - <x>^2} \\n\\\\end{split}\\n\\\\end{equation} \\nwhere $<f>\\\\ = \\\\Sigma (f_i/\\\\sigma_i^2) / \\\\Sigma (1/\\\\sigma_i^2)$ i.e it is the weighted average of the quantity inside the\\nbrackets. If the positions of the data points are such that $<x>\\\\ = 0$, then $a=\\\\ <y>$, i.e. the height of the\\nbest fit line at the weighted centre of gravity of the data points is just the weighted average of the $y$ values. \\nIt is also essential to calculate the uncertainties $\\\\sigma_a$ and $\\\\sigma_b$ on the parameters and their correlation\\ncoefficient $\\\\rho = cov/(\\\\sigma_x \\\\sigma_y)$, where $cov$ is their covariance. The elements of the inverse\\ncovariance matrix $M$ are given by\\n\\\\begin{equation}\\n\\\\begin{split}\\nM_{aa} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a^2} = \\\\Sigma (1/\\\\sigma_i^2) \\\\\\\\\\nM_{ab} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a \\\\ \\\\partial b} = \\\\Sigma(x_i/\\\\sigma_i^2) \\\\\\\\\\nM_{bb} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial b^2} = \\\\Sigma(x_i^2/\\\\sigma_i^2) \\\\\\\\\\n\\\\end{split}\\n\\\\end{equation}\\nThe covariance matrix is obtained by inverting $M$. Since the covariance is proportional to $-< x >$, if the \\ndata are centred around $x = 0$, the uncertainties on $a$ and $b$ will be uncorrelated. That is \\none reason why track parameters are usually specified at the centre of the track, rather than at its starting point.\\n\\\\subsection{Correlated uncertainties on data}\\nSo far we have considered that the uncertainties on the data are uncorrelated, but this is not always the case; correlations can \\narise from some common systematic. Then instead of the first equation of (\\\\ref{eqn:S}), we use\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (y_i^{th} - y_i^{obs})E_{ij} (y_j^{th} - y_j^{obs})\\n\\\\end{equation}\\nwhere the double summation is over $i$ and $j$, and $E$ is the inverse covariance matrix\\\\footnote{We use the symbol $E$ for the inverse covariance matrix of the measured variables $y$, and $M$ for that of the output parameters (e.g. $a$ and $b$\\nfor the straight line fit).}\\nfor the uncertainties on the \\n$y_i$. For the special case of uncorrelated uncertainties, the only non-zero elements of $E$ are the diagonal ones \\n$E_{ii} = 1/\\\\sigma_i^2$ and then \\neqn. (\\\\ref{correlated_S}) reduces to (\\\\ref{eqn:S}).\\nThis new equation for $S$ can then be minimised to give the best values of the parameters, and $S_{min}$ can be used in a Goodness of Fit test. As before, if $y^{th}$ is linear in the parameters, their best estimates can be obtained by solving simultaneous linear equations, without the need for a minimisation programme.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\nIn scientific research, experiments are designed to collect data, and theories or models are developed to explain those observations. In general, the falsification of theories is based on hypothesis testing. Hypothesis tests determine, with a given confidence level ($CL$), whether the observed data provide sufficient evidence to reject an initial hypothesis, called the null hypothesis ($H_{0}$), in favor of an alternative hypothesis ($H_{1}$). The null hypothesis ($H_{0}$) is considered true until observations indicate otherwise; in such a case, the initial explanation is rejected, and the new theory ($H_{1}$) is accepted~\\\\cite{sinervo2002signal}. Both the frequentist and Bayesian approaches applied here yield robust upper limit estimations, adaptable to various experimental setups, making them vital tools for model testing and exclusion.\\nIn high-energy physics (HEP), the null hypothesis ($H_{0}$) refers to all known physical processes, which are summarized in what is known as the Standard Model. The alternative hypothesis ($H_{1}$) represents potential models that could explain new observations that the accepted model cannot account for, such as supersymmetry, extra dimensions, among others~\\\\cite{cowan2011asymptotic,florez2016probing}.\\nAdditionally, hypothesis testing requires the selection of a confidence level in terms of statistical significance ($\\\\alpha$).\\n\\\\begin{equation}\\nCL = 1 - \\\\alpha.\\n\\\\end{equation}\\nWhere $\\\\alpha$ (type I error) is the probability of rejecting the null hypothesis when it is true. By convention, model exclusion in particle physics is done for a value of $\\\\alpha = 0.05$, which corresponds to a confidence level ($CL$) of $95\\\\\\n\\\\begin{equation}\\n\\\\alpha = \\\\int_{P}^{\\\\infty} \\\\frac{1}{2\\\\pi} e^{-x^{2}/2} dx.\\n\\\\end{equation}\\nWhere $P$ is the percentile of the distribution, for which the type I error is $\\\\alpha = 0.05$. Figure~[\\\\ref{fig:2}] shows the standard normal distribution; the shaded area represents the type I error for the percentile $P_{95} \\\\approx 1.645$, which corresponds to the model exclusion condition at the $3\\\\sigma$ level.\\nThe confidence level for an observation is significantly higher. In general, the discovery threshold is set at $5\\\\sigma$, where the type I error is $\\\\alpha = 2.86 \\\\times 10^{-7}$. Table~[\\\\ref{tb:1}] summarizes different confidence levels and their interpretation in high-energy physics (HEP)~\\\\cite{lista2016practical,cranmer2015practical,cowan2011asymptotic}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccl}\\n\\\\hline\\n$\\\\alpha$ & $CL [\\\\ \\\\hline\\n$0.1586$ & $84.13$ & $1\\\\sigma$ & $H_{1}$ no excluded \\\\\\\\\\n$0.05$ & $95.00$ & $1.645\\\\sigma$ & $H_{1}$ excluded (Model exclusion) \\\\\\\\\\n$0.0227$ & $97.72$ & $2\\\\sigma$ & $H_{1}$ excluded \\\\\\\\ \\n$1.349 \\\\times 10^{-3}$ & $99.86$ & $3\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$3.167 \\\\times 10^{-5}$ & $99.99$ & $4\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$2.8665 \\\\times 10^{-7}$ & $99.99997$ & $5\\\\sigma$ & $H_{0}$ excluded (Observation) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Significance at different observation points. The exclusion of the alternative hypothesis requires a result statistical consistent with the background-only hypothesis ($H_{0}$), while confirmation of the observation requires compatibility with the signal + background hypothesis ($H_{1}$).}\\n\\\\end{center}\\n\\\\end{table}\\n', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\nIn this lecture, we introduce the Bayesian approach to inference starting with a\\ndescription of its salient features and ending with a detailed example, again using the top quark\\ndiscovery data from D\\\\O. \\nThe main point to be understood about the Bayesian approach is that it is merely applied\\nprobability theory (see Sec.~\\\\ref{sec:prob}).\\nA method is Bayesian if\\n\\\\begin{itemize}\\n\\\\item it is based on the degree of belief interpretation of probability and\\n\\\\item it uses Bayes theorem\\n\\\\begin{align}\\np(\\\\theta, \\\\omega | D) & = \\\\frac{p(D|\\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega)}{p(D)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nD & = \\\\textrm{ observed data}, \\\\nonumber \\\\\\\\\\n\\\\theta & = \\\\textrm{ parameters of interest}, \\\\nonumber\\\\\\\\\\n\\\\omega & = \\\\textrm{ nuisance parameters}, \\\\nonumber\\\\\\\\\\np(\\\\theta, \\\\omega| D) & = \\\\textrm{posterior density}, \\\\nonumber\\\\\\\\\\n\\\\pi(\\\\theta, \\\\omega) & = \\\\emph{prior density (or prior for short)}. \\\\nonumber\\n\\\\end{align}\\n\\\\end{itemize}\\nfor \\\\emph{all} inferences. The result of a Bayesian inference is the posterior density\\n$p(\\\\theta, \\\\omega | D$ from which, if desired, various summaries can be extracted. The parameters can be discrete or continuous and nuisance parameters are eliminated by \\nmarginalization,\\n\\\\begin{align}\\np(\\\\theta | D) & = \\\\int p(\\\\theta, \\\\omega | D ) \\\\, d\\\\omega, \\\\\\\\\\n& \\\\propto \\\\int p(D | \\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega) \\\\, d\\\\omega. \\\\nonumber\\n\\\\end{align}\\nThe function $\\\\pi(\\\\theta, \\\\omega)$, called the prior, encodes whatever information we have\\nabout the parameters $\\\\theta$ and $\\\\omega$ independently of the data $D$. A key\\nfeature of the Bayesian approach is recursion; the use of\\nthe posterior density $p(\\\\theta, \\\\omega|D)$ or one, or more, of its marginals as the prior in a subsequent analysis. \\nThese simple rules yield an extremely powerful and general inference model. Why\\nthen is the Bayesian approach not more widely used in particle physics? The \\nanswer is partly historical: the frequentist approach was dominant at the dawn of particle physics. It is also partly the widespread perception that the Bayesian\\napproach is too subjective to be useful for scientific work. However, there is published\\nevidence\\nthat this view is mistaken, witness the success of Bayesian methods in \\nhigh-profile analyses in particle physics such as the discovery of single top quark\\nproduction at the Tevatron~\\\\cite{Abazov:2009ii, Aaltonen:2009jj}.\\n', \"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\", '\\\\section{Errors}\\n\\nEstimation gives you a value for the parameter(s) that we have called $a$. But you also---presumably---want to \\nknow something about the uncertainty on that estimate. The maximum likelihood method provides this.\\n', '\\\\section{Inference}\\n\\\\subsection{Approximate error evaluation for maximum likelihood estimates}\\nA parabolic approximation of $-2\\\\ln L$ around the minimum is equivalent to a Gaussian approximation,\\nwhich may be sufficiently accurate in many but not all cases. For a Gaussian model, $-2\\\\ln L$\\nis given by:\\n\\\\begin{equation}\\n-2\\\\ln L(\\\\vec{x};\\\\mu, \\\\sigma) = \\\\sum_{i=1}^n\\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2}+\\\\text{const.}\\\\,.\\n\\\\end{equation}\\nAn approximate estimate of the covariance matrix is obtained from the\\n$2^{\\\\mathrm{nd}}$ order partial derivatives with respect to the fit parameters at the minimum:\\n\\\\begin{equation}\\nV_{ij}^{-1} = -\\\\left.\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right|_{\\\\theta_k=\\\\hat{\\\\theta}_k,\\\\,\\\\forall k}\\\\,.\\n\\\\end{equation}\\nAnother approximation alternative to the parabolic one from Eq.~(\\\\ref{eq:errLikVar}) is the evaluation of the excursion\\nrange of $-2\\\\ln L$ around the minimum, as visualized in Fig.~\\\\ref{fig:likeScan}.\\nThe uncertainty interval can be determined as the range around the minimum of $-2\\\\ln L$ for which $-2\\\\ln L$ increases by $+1$ (or $+n^2$ for a $n\\\\sigma$ interval).\\nErrors can be asymmetric with this approach if the curve is asymmetric.\\nFor a Gaussian case the result is identical to the $2^{\\\\mathrm{nd}}$ order derivative matrix (Eq.~(\\\\ref{eq:errLikVar})).\\n', \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was not a strong motivation to ensure that the correlations were all correct. \\nA commonplace, and often implicit, belief is that, although correlations in the synthetic data may not be exactly the same as correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using synthetic data would be to train on real data. Unfortunately, while synthetic data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. Even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. There are two ways to proceed. First, we can try to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to gather truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the fraction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments in probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, as discussed above these simulations have different components. The short-distance simulation, which produces hundreds of particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely appealing, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. A proof-of-principle implementation of this idea is OmniFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then one can try to invert the mapping to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up by many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting (refining the event selection) on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background in some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, where some supervised training is used to guide a data-driven estimation technique, is a very promising area for future development of ML for particle physics. \\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The modified frequentist approach}\\nA {\\\\it modified frequentist approach}~\\\\cite{CLs} was proposed for the first time for the\\ncombination of the results of searches for the Higgs boson by the four LEP experiments, ALEPH, DELPHI, L3 and OPAL~\\\\cite{Higgs_at_LEP}.\\nGiven a test statistic $\\\\lambda(x)$ that depends on some observation $x$, its distribution should be determined\\nunder the two hypotheses\\n$H_1$ (signal plus background) and $H_0$ (background only). The following $p$-values can be used,\\nwhere we assume that the test statistic $\\\\lambda$ tends to have small values for $H_1$ and\\nlarger values for $H_0$:\\n\\\\begin{eqnarray}\\np_{s+b}& = & P(\\\\lambda(x | H_1) \\\\ge \\\\lambda^{\\\\mathrm{obs}} )\\\\,, \\\\\\\\\\np_b & = & P(\\\\lambda(x | H_0) \\\\le \\\\lambda^{\\\\mathrm{obs}} )\\\\,. \\n\\\\end{eqnarray}\\n$p_{s+b}$ and $p_b$ can be interpreted as follows:\\n\\\\begin{itemize}\\n\\\\item $p_{s+b}$ is the probability to obtain a result which is less compatible with the signal than the observed result, assuming the signal hypothesis;\\n\\\\item $p_b$ is the probability to obtain a result less compatible with the background-only hypothesis than the observed one, assuming background only.\\n\\\\end{itemize}\\nInstead of requiring, as for a frequentist upper limit, $p_{s+b} \\\\le \\\\alpha$,\\nthe modified approach introduces a new quantity, $\\\\mathrm{CL}_s$, defined as:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b}\\\\,,\\n}\\n\\\\end{equation}\\nand the upper limit is set by requiring $\\\\mathrm{CL}_s \\\\le \\\\alpha$.\\nFor this reason, the modified frequentist approach is also called {\\\\it ``$\\\\mathrm{CL}_s$ method''}.\\nIn practice, in most of the realistic cases, $p_b$ and $p_{s+b}$ are computed from\\nsimulated pseudoexperiments ({\\\\it toy Monte Carlo}) by approximating the probabilities\\ndefined in Eq.~(\\\\ref{eq:CLSpsb},~\\\\ref{eq:CLSpb}) with the fraction of the total number of pseudoexperiments\\nsatisfying their respective condition:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b} = \\\\frac{N(\\\\lambda_{s+b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}{N(\\\\lambda_{b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}\\\\,.\\n\\\\end{equation}\\nSince $1-p_b \\\\le 1$, then $\\\\mathrm{CL}s \\\\ge p_{s+b}$, hence upper limits computed with the $\\\\mathrm{CL}_s$ method are\\nalways {\\\\it conservative}.\\nIn case the distributions of the test statistic $\\\\lambda$ (or equivalently $-2\\\\ln\\\\lambda$) for the two hypotheses $H_0$ and $H_1$\\nare well separated (Fig.~\\\\ref{fig:CLs12}, left),\\nif $H_1$ is true, than $p_b$ will have a very high chance to be very small, hence $1-p_b \\\\simeq 1$ and $\\\\mathrm{CL}_s \\\\simeq p_{s+b}$. In this case\\n$\\\\mathrm{CL}_s$ and the purely frequentist upper limits coincide.\\nIf the two distributions instead largely overlap (Fig.~\\\\ref{fig:CLs12}, right), indicating that the experiment has poor sensitivity on the\\nsignal, in case $p_b$ is large, because of a statistical fluctuation, then $1 - p_b$ becomes small.\\nThis prevents $\\\\mathrm{CL}_s$ to become too small,\\ni.e.: it prevents to reject cases where the experiment has little sensitivity.\\nIf we apply the $\\\\mathrm{CL}_s$ method to the previous counting experiment, using \\nthe observed number of events $n^{\\\\mathrm{obs}}$ as test statistic,\\nthen $\\\\mathrm{CL}s$ can be written, considering that $n$ tends to be large in case of\\n$H_1$, for this case, as:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{P(n\\\\le n^{\\\\mathrm{obs}} | s+b)}{P(n \\\\le n^{\\\\mathrm{obs}} |b)}\\\\,.\\n\\\\end{equation}\\nExplicitating the Poisson distribution, the computation gives the same result as for the Bayesian case with a uniform prior\\n(Eq.~(\\\\ref{eq:Helene})). In many cases, the $\\\\mathrm{CL}_s$ upper \\nlimits give results that are very close, numerically, to Bayesian\\ncomputations performed assuming a uniform prior.\\nOf course, this does not allow to interpret $\\\\mathrm{CL}_s$ upper limits\\nas Bayesian upper limits.\\nConcerning the interpretation of $\\\\mathrm{CL}_s$, it's worth reporting from Ref~\\\\cite{CLs} the\\nfollowing statements:\\n\\\\begin{displayquote}\\n{\\\\it A specific modification of a purely classical statistical analysis is used to avoid excluding or discovering signals which the search is in fact not sensitive to.}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it The use of\\\\, $\\\\mathrm{CL}_s$ is a conscious decision not to insist on the frequentist concept of full coverage (to guarantee that the confidence interval doesn't include the true value of the parameter in a fixed fraction of experiments).}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it Confidence intervals obtained in this manner do not have the same interpretation as traditional frequentist confidence intervals nor as Bayesian credible intervals.}\\n\\\\end{displayquote}\\n\", \"\\\\section{Inference}\\n\\\\subsection{Likelihood function for binned samples}\\nSometimes data are available in form of a binned histogram. This may be convenient\\nwhen a large number of entries is available, and computing an unbinned likelihood function (Eq.~(\\\\ref{eq:unbinnedLikeFun}))\\nwould be too much computationally expansive.\\nIn most of the cases, each bin content is independent on any other bin and all obey Poissonian distributions,\\nassuming that bins contain event-counting information.\\nThe likelihood function can be written as product of Poissonisn PDFs corresponding to each bin\\nwhose number of entries is given by $n_i$ .\\nThe expected number of entries in each bin depends on some unknown parameters: $\\\\mu_i = \\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nThe function to be minimized, in order to fit $\\\\theta_1, \\\\cdots,\\\\theta_n$, is the following:\\n\\\\begin{eqnarray}\\n-2\\\\ln L(\\\\vec{n};\\\\vec{\\\\theta}) & = &\\n-2\\\\ln \\\\prod_{i=1}^{n_{\\\\mathrm{bins}}}\\\\mathrm{Poiss}(n_i;\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)) \\\\\\\\\\n& = & -2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\ln \\\\frac{\\ne^{-\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)^{n_i}\\n}{n_i!}\\\\\\\\\\n& = & 2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\left(\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)\\n-n_i\\\\ln\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m) +\\\\ln{n_i!}\\\\right)\\\\,.\\n\\\\end{eqnarray}\\nThe expected number of entries in each bin, $\\\\mu_i$, is often approximated by a continuous function $\\\\mu(x)$\\nevaluated at the center of the bin $x=x_i$.\\nAlternatively, $\\\\mu_i$ can be given by the superposition of other histograms ({\\\\it templates}),\\ne.g.: the sum of histograms obtained from different simulated processes.\\nThe overall yields of the considered processes may be left as free parameters in the fit in order to constrain\\nthe normalization of simulated processes from data, rather than relying on simulation prediction,\\nwhich may affected by systematic uncertainties.\\nThe distribution of the number of entries in each bin can be approximated,\\nfor sufficiently large number of entries,\\nby a Gaussian with standard deviation equal to $\\\\sqrt{n_i}$. \\nMaximizing $L$ is equivalent to minimize:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\frac{\\\\left(n_i-\\\\mu(x_i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\right)^2\\n}{n_i }\\n\\\\end{equation}\\nEquation~(\\\\ref{eq:NeymanChi2}) defines the so-called Neyman's $\\\\chi^2$ variable.\\nSometimes, the denominator $n_i$ is replaced by $\\\\mu_i = \\\\mu (x_i; \\\\theta_1, \\\\cdots, \\\\theta_m)$\\n(Pearson's $\\\\chi^2$) in order to avoid cases with $n_i$ equal to zero or very small.\\nAnalytic solutions exist in a limited number of simple cases, e.g.: if $\\\\mu$ is a linear function.\\nIn most of the realistic cases, the $\\\\chi^2$ minimization is performed numerically, as for\\nmost of the unbinned maximum likelihood fits.\\nBinned fits are, in many cases, more convenient with respect to unbinned fits because the number of input\\nvariables decreases from the total number of entries to the number of bins.\\nThis leads usually to simpler and faster numerical implementations,\\nin particular when unbinned fits become unpractical in cases of very large number of entries.\\nAnyway, for limited number of entries, a fraction of the information is lost when\\nmoving from an unbinned to a binned sample and a possible loss of precision may occur.\\nThe maximum value of the likelihood function obtained from an umbinned maximum likelihood fit doesn't in general\\nprovide information about the quality ({\\\\it goodness}) of the fit.\\nInstead, the minimum value of the $\\\\chi^2$ in a fit with a Gaussian underlying model\\nis distributed according to a known PDF given by:\\n\\\\begin{equation}\\nP(\\\\chi^2;n) =\\\\frac{2^{-{n}/{2}}}{\\\\Gamma\\\\left({n}/{2}\\\\right)}\\n\\\\chi^{n-2}e^{-\\\\frac{\\\\chi^2}{2}}\\\\,,\\n\\\\end{equation}\\nwhere $n$ is the {\\\\it number of degrees of freedom}, equal to the number of\\nbins minus the number of fit parameters.\\nThe cumulative distribution (Eq.~(\\\\ref{eq:cumulative})) of $P(\\\\chi^2; n)$ follows a uniform distribution between from 0 to 1,\\nand it is an example of {\\\\it p-value} (See Sec.~\\\\ref{sec:HypTest}).\\nIf the true PDF model deviates from the assumed distribution, the distribution of the $p$-value will be more peaked around zero\\ninstead of being uniformly distributed.\\nIt's important to note that $p$-values are not the ``probability of the fit hypothesis'',\\nbecause that would be a Bayesian probability, with a completely different meaning, and should be evaluated\\nin a different way.\\nIn case of a Poissonian distribution of the number of bin entries that may deviate from the Gaussian approximation,\\nbecause of small number of entries,\\na better alternative to the Gaussian-inspired Neyman's or Pearson's $\\\\chi^2$ has been proposed\\nby Baker and Cousins~\\\\cite{baker_cousins} using the following likelihood ratio\\nas alternative to Eq.~(\\\\ref{eq:PoisBinLik}):\\n\\\\begin{eqnarray}\\n\\\\chi^2_{\\\\lambda} & = & -2\\\\ln\\\\prod_i\\\\frac{L(n_i;\\\\mu_i)}{L(n_i;n_i)} = -2\\\\ln\\\\prod_i\\\\frac{e^{-\\\\mu_i}\\\\mu_i^{n_i}}{n_i!}\\n\\\\frac{n_i!}{e^{-{n_i}}n_i^{n_i}} \\\\\\\\\\n& = & 2\\\\sum_i\\\\left[\\n\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)- n_i + n_i\\\\ln\\\\left(\\n\\\\frac{n_i}{\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\n\\\\right) \\\\right]\\\\,.\\n\\\\end{eqnarray}\\nEquation~(\\\\ref{eq:BakCous}) gives the same minimum value as the Poisson likelihood function,\\nsince a constant term has been added to the log-likelihood function in Eq.~(\\\\ref{eq:PoisBinLik}),\\nbut in addition it provides goodness-of-fit information, since it asymptotically obeys a $\\\\chi^2$\\ndistribution with $n - m$ degrees of freedom. This is due to Wilks' theorem, discussed\\nin Sec.~\\\\ref{sec:profLik}.\\n\", '\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Overdensity estimation}\\nIn order to train the most powerful ML-based classifier to discriminate signal from background, one would ideally train a network in a supervised manner with labeled data. This relies on a signal hypothesis that is chosen a-priori. An early attempt at discriminating background from \"everything else\" in order to obtain some degree of model-independence, was demonstrated in Ref.~\\\\citen{antiqcd}. Targeting searches for new physics in hadronic final states, a classifier was trained to discriminate QCD jets from various potential signal jets using Monte Carlo simulation. The disadvantage of such an approach is the dependence on signal simulation and which signals are to be included in the training.\\nAlthough simulated particle physics data is highly accurate over several orders of magnitude in length scale, simulation is known to not fully accurately reproduce collider data and this disagreement affects the tagging performance. Using weakly- or self-supervised (see Section~\\\\ref{sec:selfsupervised}) methods, algorithms can be trained directly on the data itself which has the added benefit of not having to derive transfer factors when training on synthetic data and testing on real data.\\n\\\\subsubsection{Weakly supervised methods}\\nIn weakly supervised learning, impure or noisy data sources can be used to label signal and background data in such a way that models can be trained in a supervised manner. Such methods can be utilized for anomaly detection when the signal is unknown, but there exist datasets where both signal and background are expected to be present in some relative fraction. This can be achieved by placing weak assumptions on the signal and background processes using domain knowledge.\\nThe goal of the weakly supervised methods we will discuss here, is to learn an approximation of the likelihood ratio $R(x)$ between the underlying probability densities of background $p_\\\\text{bg}(x)$ and data (possibly including signal) $p_\\\\text{data}(x)$, as a function of some input variables $x$:\\n\\\\begin{equation} \\nR(x)=\\\\frac{p_\\\\text{data}(x)}{p_\\\\text{bg}(x)}.\\n\\\\end{equation}\\nThis likelihood ratio, if it could be learned exactly, would be the most powerful model-agnostic anomaly detector, as given \\n\\\\begin{equation}\\np_\\\\text{data}(x)=(1-\\\\epsilon)p_\\\\text{bg}(x)+\\\\epsilon p_\\\\text{sig}(x),\\n\\\\end{equation}\\nwhere $p_\\\\text{sig}(x)$ is the probability density of signal, it would be monotonically related to the signal-to-background LR for any signal present in the data. A strategy for learning a good approximation of the likelihood ratio $R(x)$, is to train a classifier between data from a signal enriched region and samples drawn from a (fully data-driven) background model. If the background model is accurate and the classifier is well-trained, this approaches the likelihood ratio $R(x)$ by the Neyman-Pearson Lemma~\\\\cite{neyman1933ix}.\\nHence, the aim is to test whether the signal region data contains a combination of signal and background data. In the event that there are signal events present in the signal region, the classifier can differentiate between the signal region data and the background template. The true signal events are expected to have higher classification scores than the true background data. A cut on this classifier score can then be used to enhance the significance of signal events, making it a useful anomaly detection metric.\\nIn Ref.~\\\\citen{Dery2017WeaklySC} a method referred to as Learning from Label Proportions~\\\\cite{LLP} was utilized to discriminate between quarks and gluons using impure data samples. Despite not having access to the per-instance labels, the class proportions could be derived using domain knowledge. A supervised task was then defined using the class proportions themselves as the target, although operating the algorithm at a per-instance level. This concept has been extended in in the Classification WithOut Labels (CWola)~\\\\cite{cwola} framework. In this setup, the class proportions themselves do not need to be known, and it is enough to have two datasets at hand with an unequal fraction of signal instances in each set. A standard classifier can then be trained to discriminate between the two mixed datasets, and this can be shown to be the optimal classifier to discriminate between signal and background instances. The larger the difference in signal fraction between is dataset, the better the classifier becomes. The challenge is being able to design such mixed datasets, especially for a model-independent setup.\\nThe CWola strategy has been demonstrated and deployed for various model-independent search setups. In Ref.~\\\\citen{cwolabumphunt}, the authors introduce the \\\\textit{CWola bumphunt}. In this setup, one attempts to look for new, heavy generic particles that resonate around the particle mass in the dijet invariant mass spectrum. Starting from the weak assumption that this is a localized, narrow resonance, two mixed samples are created in the following way: The region in the dijet invariant mass close to the particle mass is defined as the signal-enriched mixed sample, and the regions next to it are defined as background-enriched regions. This is illustrated in Fig.~\\\\ref{fig:fig1}. In this way, the dijet invariant mass sideband regions serve as the background samples; these can serve as a good model for the background if the input features are statistically independent from the dijet invariant mass. If there is a signal present in the signal-like region around the particle mass, the classifier learns to identify it, while in the absence of a signal the classifier will likely learn random noise as there would be no difference between the two groups of events. It is crucial that the features being used for classification are not correlated with the dijet invariant mass. Otherwise, the classifier will be able to differentiate background events in the signal region from those in the adjacent dijet invariant mass regions used as the background-enriched mixed sample. Background events within the signal region will then be classified as signal-like, which can introduce artificial sculpting of the dijet invariant mass distribution. Note that the above strategy only works for narrow resonances, if there is a significant amount of signal in both datasets, as would be the case for a broad resonance, the classification performance is reduced. This method was used to analyze data collected by the ATLAS experiment in the search for generic new heavy resonances decaying into jets in Ref.~\\\\citen{ATLAS:2020iwa}, a first of its kind using weak supervision for model-agnostic searches. The power of this analysis can be seen in Figure~\\\\ref{fig:atlascwola}. This plot shows 95\\\\\\nThis methodology can also be applied in other setups than for a dijet bumphunt. In Ref.~\\\\citen{cwola_monojet}, model-agnostic learning using the CWola method is harnessed in order to improve the sensitivity of searches for new physics models with anomalous jet dynamics and a mono-jet signature. Focusing on cases where a heavy new particle decays into two jets which hadronize partially in the dark sector (making them \\\\textit{semi-visible jets}), and where one of the jets become completely invisible and the other partially visible, anomaly detection is utilized to detect the semi-visible anomalous jet. The degree of visibility of this jet can vary, making it difficult to train a supervised algorithm for each visibility fraction. Rather, CWola is deployed to train a generic anomalous jet identification classifier. The dominant background for a mono jet search is the electroweak production of vector bosons and jets, where the vector boson further decays to neutrinos, $Z(\\\\nu\\\\nu)$+jets. The experimental signature is missing transverse energy and a jet, mimicking the signal signature. Taking advantage of the fact that the vector boson also can decay visibly into two leptons and in these cases the jet remains the same, a background enriched control CWola sample can be defined using $Z(\\\\ell\\\\ell)$+jets events. None of the signal should be present in events with a di-lepton and jet signature. A model-independent anomalous jet tagger is then trained supervised to discriminate between jets coming from a $(\\\\ell\\\\ell)+jet$ and a $(\\\\nu\\\\nu)$+jet sample. If the monojet signature is present, CWola guarantees that the best algorithm trained to distinguish between these two regions, is also the best algorithm to discriminate between a normal SM jet from the V+jet background, and a semi-visible anomalous jet.\\nThis illustrates how generally CWola can be used. The only requirement is that one is able to define regions of the data depleted and enriched in signal, and that the signal and background events are statistically equal in the two regions. In terms of model independence, some degree of signal assumption is needed in order to define appropriate mixed samples.\\nMethods can also be used to bootstrap CWola and further improve the classification performance. In \\\\citen{Amram:2020ykb}, a powerful and model-independent anomalous jet tagger is defined starting from the CWola hunting methodology, but defining the mixed samples for training differently. Targeting signals where both jets in the event are anomalous, the key idea is that for a resonance decaying to a pair of anomalous jets, one can use an initial self- or supervised classifier (like an autoencoder, see Section~\\\\ref{sec:selfsupervised}) \\nto tag an event as signal-like or background-like using one jet and then use that information to construct samples for training a classifier using the other jet with weak supervision. By using an autoencoder as an initial classifier, one can group events into a signal-like and background-like sample based on the anomaly score on one of the jets (assuming that if the one jet is anomalous, the other must be too). A classifier can then be trained for the other jet that has not been tagged, where the mixed samples are defined based on the anomaly score of the tagged jet.\\nAnother weakly-supervised method for over-density detection is ANODE~\\\\cite{nachman2020anode}. In ANODE, conditional neural density estimation is used in order to interpolate probability densities from a data sideband into the data signal region. This interpolation is used and compared to the probability density of the actual data observed in the signal region, and used to construct a likelihood ratio as in Eq.~\\\\ref{eq:weak-supervision-likelihood-ratio}. This implies having to learn both the interpolated likelihood of the background in the signal region, as well as the likelihood for data in the signal region (see Fig.~\\\\ref{fig:fig1}). An improvement on this method is CATHODE (Classifying Anomalies Through Outer Density Estimation)~\\\\cite{Hallin:2021wme}. In CATHODE, rather than directly constructing the likelihood ratio, one rather samples events from the trained background estimator after it is interpolated into the signal region. This avoids having to learn the likelihood of data in the signal region. Then, a classifier is trained to discriminate data in the signal region from the data samples from the interpolated density estimator. This algorithm was first demonstrated for searches for heavy particles decaying into two jets. CATHODE proceeds by first training a conditional normalizing flow~\\\\cite{rezende2016variational} on the dijet invariant mass sidebands and then interpolating this into the signal region; samples from this flow are used as the background model and should correctly take into account any correlations between the input features and the dijet invariant mass.\\nNormalizing flows are a type of generative model that learn to transform a simple probability distribution (usually a standard Gaussian distribution) into a more complex distribution that resembles the target distribution of the data. This is achieved by defining a sequence of invertible transformations that map samples from the simple distribution to samples from the target distribution. The resulting model can be used to generate new data samples, perform density estimation, and compute likelihoods. Invertibility is important, as it ensures that the transformation has a well-defined inverse, which is needed for density estimation and likelihood computation. The key challenge in designing normalizing flows is to ensure that the resulting distribution is both complex enough to capture the target distribution and easy to work with, in the sense that likelihood computation and sampling are efficient. Recent work has focused on designing more expressive and flexible transformations, such as coupling layers, which allow for the transformation to depend on only a subset of the input variables~\\\\cite{dinh2017realnvp}. CATHODE utilizes such a normalizing flow to estimate the background density, conditioned on the dijet invariant mass. The density can then be interpolated into the dijet invariant mass signal region, while accounting for all correlations between the input features. Finally, a classifier is trained to distinguish between the artificially generated background samples from the normalizing flow (trained in data sidebands) and actual samples from the data signal region, yielding an estimate of the likelihood ratio as an anomaly metric (following the CWola paradigm).\\nA similar method is CURTAINS~\\\\cite{Raine:2022hht}. This method also takes advantage of a conditioned invertible neural network to learn the distribution of background events in a sideband and then use that to transform datapoints to those of the target distribution in the signal region. CURTAINs use an optimal transport loss to train the network to minimize the distance between the model output and the target data. The goal is to approximate the optimal transport function between two points in feature space when moving along the resonant spectrum. As a result, instead of generating new samples to create a background template, CURTAINs transforms the data in the side-bands to equivalent data points with a mass in the signal region. This approach eliminates the need to match data encodings to an intermediate prior distribution, which is the case of CATHODE, as it can lead to mismodelling of underlying correlations between the observables in the data if the trained posterior is not in perfect agreement with the prior distribution. Additionally, CURTAINs can also be employed to transform side-band data into validation regions, rather than simply constructing the background template in the signal region, making the algorithm easier to validate and test. Once the CURTAINs density estimation algorithm has been trained, a similar approach as in CATHODE is taken. Specifically, the transformed data (from sideband to signal region) is assumed to represent a sample of pure background events, while the signal region data represents a mixture of signal and background. A CWola classifier is trained to discriminate between the two datasets based on this assumption. \\nIn Ref.~\\\\citen{klein2022flows,curtains2}, an improvement of the CURTAINs technique is introduced, where a maximum likelihood estimation is used instead of an optimal transport loss. This improves the fidelity of the transformed data and is significantly faster and easier to train.\\nMore recently, diffusion models~\\\\cite{10.5555/3045118.3045358}, emerging as potent tools for high-dimensional density estimation, have been explored both for overdensity estimation~\\\\cite{sengupta2023improving} and for outlier detection~\\\\cite{mikuni2023highdimensional}.\\nThere are also weakly supervised methods that take advantage of simulation in the training of density estimators. In Simulation Assisted Likelihood-free Anomaly Detection (SALAD), a reweighting function for reweighting simulation to match data in the data sidebands is trained. This (parametrized) reweighting function is then interpolated into the signal region. Finally, a classifier to discriminate between the two is trained to get the likelihood ratio. Another simulation-assisted technique is Flow-enhanced transportation for anomaly detection (FETA)~\\\\cite{feta}, a mixture of SALAD and CURTAINS. A normalizing flow is trained in the sideband to map MC simulation to data. This learned flow is then applied to simulation in the signal region to obtain an approximation of the background.\\nThere are caveats when deploying weakly supervised methods.\\nAsymptotically, a weakly supervised classifier will converge to the performance of a fully supervised one. But in practice, performance typically degrades with smaller samples sizes available for training and lower fractions of signal events in the data sample. However, one can still obtain signal versus background classifiers with reasonable performance even with signal fractions well below 1\\\\\\n', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The test statistics and estimators of $\\\\mu$ and $\\\\vec\\\\theta$}\\nThis definitions in this section are all relative to a given dataset $\\\\datasim$ and value of the global observables $\\\\globs$, thus we will suppress their appearance. The nomenclature follows from Ref.~\\\\cite{asimov}.\\nThe maximum likelihood estimates (MLEs) $\\\\hat\\\\mu$ and $\\\\hat{\\\\vec\\\\theta}$ and the values of the parameters that maximize the likelihood function $L(\\\\mu,\\\\vec\\\\theta)$ or, equivalently, minimize $-\\\\ln L(\\\\mu,\\\\vec\\\\theta)$. The dependence of the likelihood function on the data propagates to the values of the MLEs, so when needed the MLEs will be given subscripts to indicate the data set used. For instance, $\\\\hat{\\\\vec\\\\theta}_{\\\\rm obs}$ is the MLE of $\\\\vec\\\\theta$ derived from the observed data and global observables. \\nThe conditional maximum likelihood estimate (CMLEs) $\\\\hathatthetamu$ is the value of $\\\\vec\\\\theta$ that maximizes the likelihood function with $\\\\mu$ fixed; it can be seen as a multidimensional function of the single variable $\\\\mu$. Again, the dependence on $\\\\datasim$ and $\\\\globs$ is implicit. This procedure for choosing specific values of the nuisance parameters for a given value of $\\\\mu$, $\\\\datasim$, and $\\\\globs$ is often referred to as ``profiling''. Similarly, $\\\\hathatthetamu$ is often called ``the profiled value of $\\\\vec\\\\theta$''.\\nGiven these definitions, we can construct the profile likelihood ratio\\n\\\\begin{equation}\\n{\\\\lambda}({\\\\mu}) = \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } \\\\;,\\n\\\\end{equation}\\nwhich depends explicitly on the parameter of interest $\\\\mu$, implicitly on the data $\\\\datasim$ and global observables $\\\\globs$, and is independent of the nuisance parameters $\\\\vec\\\\theta$ (which have been eliminated via ``profiling'').\\nIn any physical theory the rate of signal events is non-negative, thus $\\\\mu\\\\ge 0$. However, it is often convenient to allow $\\\\mu<0$ (as long as the pdf $f_c(x_c | \\\\mu,\\\\vec\\\\theta)\\\\ge 0$ everywhere). In particular, $\\\\hat\\\\mu<0$ indicates a deficit of events signal-like with respect to the background only and the boundary at $\\\\mu=0$ complicates the asymptotic distributions. Ref.~\\\\cite{asimov} uses a trick that is equivalent to requiring $\\\\mu\\\\ge 0$ while avoiding the formal complications of a boundary, which is to allow $\\\\mu< 0$ and impose the constraint in the test statistic itself. In particular, one defines $\\\\tilde \\\\lambda(\\\\mu)$\\n\\\\begin{equation}\\n\\\\tilde{\\\\lambda}({\\\\mu}) = \\\\left\\\\{ \\n\\\\begin{array}{ll} \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } & \\\\hat{\\\\mu} \\\\ge 0 , \\\\\\\\*[0.3 cm]\\n\\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) } {L(0,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0)) } & \\\\hat{\\\\mu} < 0 \\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nThis is not necessary when ensembles of pseudo-experiments are generated with ``Toy'' Monte Carlo techniques, but since they are equivalent we will write $\\\\tilde\\\\lambda$ to emphasize the boundary at $\\\\mu=0$.\\nFor discovery the test statistic $\\\\tilde{q}_0$ is used to differentiate the background-only hypothesis $\\\\mu=0$ from the alternative hypothesis $\\\\mu>0$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{0} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} > 0\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} \\\\le 0 \\n\\\\end{array} \\\\right. \\n\\\\end{equation}\\nNote that $\\\\tilde{q}_0$ is test statistic for a one-sided alternative. Note also that if we consider the parameter of interest $\\\\mu\\\\ge 0$, then it is equivalent to the two-sided test (because there are no values of $\\\\mu$ less than $\\\\mu=0$. \\nFor limit setting the test statistic $\\\\tilde{q}_{\\\\mu}$ is used to differentiate the hypothesis of signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser rate $\\\\mu'<\\\\mu$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{\\\\mu} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} \\\\le \\\\mu\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\n\\\\end{array} \\\\right. \\\\quad = \\\\quad \\\\: \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{lll} - 2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(0, \\\\hat{\\\\hat{\\\\theta}}(0))} &\\n\\\\hat{\\\\mu} < 0 \\\\;, \\\\\\\\*[0.2 cm] -2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}})} &\\n0 \\\\le \\\\hat{\\\\mu} \\\\le \\\\mu \\\\;, \\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\\\;.\\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nNote that $\\\\tilde{q}_{\\\\mu}$ is a test statistic for a one-sided alternative; it is a test statistic for a one-sided upper limit. \\nThe test statistic $\\\\tilde{t}_\\\\mu$ is used to differentiate signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser or greater rate $\\\\mu' \\\\ne\\\\mu$.\\n\\\\begin{equation}\\n\\\\tilde{t}_{\\\\mu} = - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) \\\\; . \\n\\\\end{equation}\\nNote that $\\\\tilde{t}_\\\\mu$ is a test statistic for a two-sided alternative (as in the case of the Feldman-Cousins technique, though this is more general as it incorporates nuisance parameters). Note that if we consider the parameter of interest $\\\\mu\\\\ge 0$ and we the test at $\\\\mu=0$ then there is no ``other side'' and we have $\\\\tilde{t}_{\\\\mu=0} = \\\\tilde{q}_0$. Finally, if one relaxes the constraint $\\\\mu\\\\ge0$ then the two-sided test statistic is written $t_\\\\mu$ or, simply, $-2\\\\ln\\\\lambda(\\\\mu)$.\\n\", \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", '\\\\section{Quantum Anomaly Detection}\\n\\\\subsection{Background}\\nIn recent years, \\\\ac{QML} has emerged as a new paradigm for data processing at the intersection of machine learning and quantum information processing. Quantum computing has the potential to address real-world challenges that are difficult or even intractable for classical computers~\\\\cite{arute_quantum_2019, zhong_quantum_2020, madsen_quantum_2022}. \\nSuch problems include prime number factoring~\\\\cite{shor97}, a problem at the basis of classical modern-day encryption, search in unstructured databases~\\\\cite{grover1996fast}, solving systems of equations~\\\\cite{Harrow_Hassidim_Lloyd_2009}, and simulations of quantum systems, enabling first-principle computation of chemical properties in atomic, molecular, and nuclear systems~\\\\cite{Kandala2017, Barkoutsos2018, Kiss2022}\\\\footnote{Please also refer to references therein, references provided are non-exhaustive.}.\\nInitially, applications of quantum computing in \\\\ac{ML} focused on investigating speedups in computationally expensive subroutines of learning algorithms, such as optimization and matrix inversion~\\\\cite{Lloyd_Mohseni_Rebentrost_2014, Wiebe_Braun_Lloyd_2012, Rebentrost_Mohseni_Lloyd_2014}. Through this scope, replacing classical subroutines with quantum algorithms provide provable speedups in terms of runtime complexity of the \\\\ac{ML} training. Nevertheless, such proofs frequently require large and fault tolerant quantum hardware. Namely, quantum computers with error correction schemes that are able to arbitrarily suppress the inherent logical error rates~\\\\cite{Acharya2023}. Such devices do not exist yet. Currently available quantum computers are noisy and have limited number of qubits of small decoherence times~\\\\cite{preskill_nisq_2018}. Hence, the size of the quantum circuits and the number of operations that can be carried out at present are limited. Quantum algorithms with too many operations for the device at hand, can be rendered useless, or at least equivalent to a classical computation, by the inherent hardware noise.\\nLately, studies have also investigated the potential of quantum computing to enhance fundamentally the learning model itself~\\\\cite{Biamonte2017, Schuld2018, Schuld_2018_book, Havlicek2019}. \\\\ac{QML} models have been shown to generalise well with few training data~\\\\cite{caro_generalization_2022}, to provide advantages over classical algorithms for specific types of learning problems~\\\\cite{Liu2021rigorous, Kubler2021, Huang2021, pirnay22_superpol, muser2023}, and are able to identify patterns in data that cannot be recognised efficiently with classical methods~\\\\cite{huangQA2022}. Mirroring classical models for classification tasks, \\\\ac{QML} algorithms can be coarsely grouped into two categories: \\\\textit{kernel-based} methods and \\\\textit{variational} learning approaches\\\\cite{Cerezo2022}.\\nIn the former category a main example are \\\\ac{QSVM}, where a classical \\\\ac{SVM}~\\\\cite{svmVapnik} is equipped with a \\\\textit{quantum kernel}. The values of the kernel are evaluated on a quantum computer through measurements~\\\\cite{Schuld_2019, Havlicek2019}. During training, (Q)SVMs find the hyperplane that separates different classes of data, maximizing the margin between them. These models can create non-linear decision boundaries in the data input space when equipped with kernel functions~\\\\cite{svmVapnik}. The kernels are constructed by feature maps that transform the data into a higher dimensional feature space, in which the classes can be more effectively separated by a linear decision boundary. In the case of a quantum kernel, the data is mapped to the Hilbert space spanned by the qubit states. The dimensionality of this space grows exponentially with the number of qubits, and hence, such models are difficult to simulate classically. After constructing the quantum kernel matrix from the measurements the loss function of the model is minimized on a classical device using quadratic programming techniques. In particle physics, SVMs can be used for supervised classification tasks~\\\\cite{vaiciulis2003, sforza2013, sahin2016}, although their use is not as prevalent as deep learning approaches or ensemble models such as boosted decision trees. Additionally, kernel machines have been extended to an unsupervised setting~\\\\cite{one_class_svm}, where the training data is assumed to contain mostly background events and an upper bound on the expected anomaly contamination is set using a hyperparameter.\\nThe latter category encompasses parametrised quantum circuits, also referred to as \\\\ac{QNN} or variational quantum circuits. These circuits are composed of gates whose parameters can be tuned iteratively to minimize a loss function using classical gradient-based learning techniques~\\\\cite{Benedetti2019, Mitarai2018}. The output of the circuit is an expectation value of an operator, that is sampled from a quantum computer. This approach allows for the training of quantum circuits to perform specific tasks, such as classification and generative modeling. Specific architectures of QNNs have been shown to be universal function approximators~\\\\cite{Pérez-Salinas2020}, and that they can be expressed as a Fourier series expansion~\\\\cite{Schuld_Sweke_Meyer_2021}. Contrary to (quantum) kernel machines, the loss function landscape of QNNs is non-convex, which can lead to trainability issues similar to the ones of the vanishing gradient problem in classical neural networks~\\\\cite{Cerezo2021, Wang2021, Holmes2022}. Nevertheless, the authors of Ref.~\\\\citen{thanasilp2022exponential} argue that under certain conditions, kernel-based models can also manifest similar problems in training. Additionally, some works provide a unified view of \\\\ac{QML}~\\\\cite{Jerbi2023}, while others claim that the kernel-based learning is more natural for \\\\ac{QML}~\\\\cite{Schuld_qml_is_kernels_2021}.\\nIn most current applications, one can treat the quantum computer as a specialized processing unit, that is part of the overall computation. The hybrid algorithms discussed above aim to leverage the different strengths of classical and quantum processing units while mitigating their corresponding weaknesses.\\n\\\\subsection{Applications in High Energy Physics} \\nStudies have assessed the potential of quantum computing and variational algorithms for simulations of lattice field theories~\\\\cite{atas2021, mildenberger2022probing, funcke2023review}, as an alternative to \\\\ac{MC} techniques~\\\\cite{Kiss_MC2022, Delgado_Hamilton_2022, Chang2021, Bravo2022}, and for parton showering simulations~\\\\cite{nachman_qshower2021, bepari2022}. Research along these lines is motivated by the question of whether quantum algorithms can provide a natural platform for simulating fundamental physics~\\\\cite{dimeglio2023quantum}. An additional motive is the prospect of quantum computers providing a more favorable computational complexity than currently available classical methods. Furthermore, \\\\ac{QML} models have been developed for solving reconstruction problems in the context of collider experiments~\\\\cite{Tuysuz_2020, Grossi2020, magano2022, Lerjarza2022, duckett2022}. \\n\\\\subsection{Supervised classification} \\nIn terms of classification tasks in a model-dependent setting, quantum computing was first considered in Ref.~\\\\citen{Mott:2017}. Therein, the training of a classifier for $H\\\\to\\\\gamma\\\\gamma$ events was mapped to a quantum annealing task. \\nSince then, studies have mainly focused on the design and implementation of supervised \\\\ac{QML} algorithms based on different \\\\ac{QSVM} and \\\\ac{QNN} architectures that are able classify \\\\ac{HEP} events by discriminating the signal distribution from the background distribution~\\\\cite{terashi2021, blance_quantum_2021, qmlHiggs2021, Guan_2021, Heredge2021, Chen2020, Chen2021, wu_2021_kernel, wu_2021_qnn}. Such quantum models, are often developed and assessed via computationally expensive quantum simulations on classical processors using limited number of qubits; typically up to 20. In these simulations, the algorithms can be investigated in an ideal noiseless environment. After the architecture has been chosen and its hyperparameters have converged to values that lead to good performance on the learning task at hand, the \\\\ac{QML} algorithms are tested by running experiments on real hardware via cloud-based platforms. \\nThe developed quantum models are typically benchmarked against classical models of similar complexity, that are trained on the same small data sets. So far, the number of training data points is at the order of $10^2$ to $10^4$. \\\\ac{HEP} datasets are frequently high dimensional, with number of features exceeding the order of presently available number qubits, posing a challenge for direct input and processing by data encoding circuits on current quantum devices. To address this challenge, a set of reduced features is typically used as an input to the \\\\ac{QML} models. This representation of reduced dimensionality is obtained by manual selection of physical variables~\\\\cite{terashi2021, blance_quantum_2021}, Principle Component Analysis (PCA)~\\\\cite{wu_2021_qnn, wu_2021_kernel, Schuhmacher23, Peixoto2023}, or autoencoders~\\\\cite{qmlHiggs2021, wozniak_belis_puljak23}. In the case of autoencoders, the compression of \\\\ac{HEP} events can be regarded as more representative since these models can, at least approximately, retain non-linear correlations of the input features in their latent space. Such higher order correlations are removed by definition in the case of PCA, and are potentially lost in manual feature selection or feature extraction based on univariate discrimination metrics~\\\\cite{qmlHiggs2021}.\\nSo far, in most studies regarding supervised models, the performance of the quantum algorithms is competitive and matches the performance of their classical counterparts. Numerical evidence suggests that \\\\ac{QML} algorithms might outperform classical models when the training datasets are small~\\\\cite{terashi2021, wu_2021_kernel, Guan_2021, Gianelle2022}. However, such a property has not been proven in general, yet.\\n\\\\subsection{Unsupervised new-physics searches} \\nRecently, different strategies were proposed for new-physics searches at the \\\\ac{LHC} using \\\\ac{QML} in the context of anomaly detection~\\\\cite{Blance_Spannowsky_2021, Ngairangbam_2022, alvi2023, wozniak_belis_puljak23, Schuhmacher23, Bermot2023, Bordoni:2023lad}. In Ref.~\\\\citen{Blance_Spannowsky_2021}, Gaussian Boson Sampling (GBS) is used to create a lower dimensional representation of \\\\ac{BSM} events where the Higgs boson decays into two pseudoscalar particles. GBS is classically difficult to simulate and can be implemented using continuous variable devices such as photonic quantum computers. This procedure is combined with a quantum version of the K-means clustering algorithm, Q-means, to detect anomalies. \\nK-means is a method that aims to partition an unlabeled dataset into K clusters in the feature space. Each cluster center, also called a centroid, is defined as the mean of the datapoints that belong to that cluster. Each datapoint is assigned to the nearest centroid, according to some distance measure, typically the Euclidean metric, which serves us the loss function of the algorithm. The cluster assignment and the coordinates of the centroids are iteratively updated, according to the loss minimization procedure, until the datapoints have converged to specific stable clusters. In the case of Q-means, the datapoints are embedded in the quantum Hilbert space, where the distance calculation occurs depending on the chosen quantum circuit. Additionally, depending on the design of the quantum model the minimisation of the loss can be accomplished with a quantum or classical algorithm. K-means has been applied to \\\\ac{HEP} for jet clustering~\\\\cite{Chekanov2006, Thaler2012, Stewart2015}, while Q-means and its variants have been applied also for unsupervised detection of new-physics events~\\\\cite{Blance_Spannowsky_2021, wozniak_belis_puljak23}.\\nA quantum autoencoder (QAE) is considered in Ref.~\\\\citen{Ngairangbam_2022} using four physical variables as an input. The authors demonstrate, both in quantum simulation and hardware, that the QAE is a promising approach for \\\\ac{BSM} scenarios involving a resonant heavy Higgs decaying to a pair of top quarks, and a \\\\ac{SM} Higgs decaying to two dark matter particles. The authors of Ref.~\\\\citen{Bordoni:2023lad} employ QAEs for the detection of long-lived particles and adapt the proposed methods for execution on real quantum hardware. Additionally, different architectures of \\\\ac{QNN} have been investigated in Ref.~\\\\citen{alvi2023}, using low dimensional (simulated) datasets involving Higgs-like scalar particles signatures as anomalies in a semi-supervised setting. The authors do not identify any region where the tested \\\\ac{QML} models present an advantage in performance or in terms of the needed size of the trained dataset.\\nRef.~\\\\citen{Schuhmacher23} proposes a simulation-assisted new-physics search where supervised quantum and classical \\\\ac{SVM}s are trained on a dataset that contains \\\\ac{SM} processes as background and an artificial set of anomalous events obtained from the \\\\ac{SM}-distributed features as signal. Specifically, the authors generate the distributions of the signal samples by a so-called scrambling process, in which the feature distributions of the background are smeared by the normal distribution, preserving energy and momentum conservation. Furthermore, it is demonstrated that the considered models are able to generalize to real signals such as Higgs and Graviton production events.\\nIn Ref.~\\\\citen{Bermot2023}, a quantum Generative Adversarial Network is designed to extract an anomaly score for each data input. The authors benchmark the proposed model and verify its efficacy in data sets where they treat the Higgs boson production and Graviton production as anomalies, respectively. Additionally, generative modeling in the context of Hamiltonian learning has been investigated for semi-leptonic top and dijet event production~\\\\cite{araz2023}. The anomaly score in Ref.~\\\\citen{araz2023} is constructed using the different properties of the time evolution of quantum states that represent background and signal data.\\nA new-physics search in dijet topologies is addressed in Ref.~\\\\citen{wozniak_belis_puljak23}, where an unsupervised quantum kernel machine and quantum clustering methods are designed to define a metric of typicality for QCD jets. The dijet events are described by 600 features --100 particle constituents per jet and three features per particle-- and the examined anomalies include two different Graviton scenarios and a \\\\ac{BSM} scalar boson production with the final state. The authors develop a convolutional autoencoder to produce a compressed representation of the \\\\ac{HEP} events, addressing the challenge of directly processing realistic high-dimensional data on current quantum devices. Consequently, the quantum anomaly detection algorithms use as an input the latent representation of the data that is generated by the encoder and are trained using QCD background events. For the proposed kernel-based anomaly detection model, this work identifies an advantage in performance of the quantum model over its classical counterpart.\\n\\\\subsection{Discussion \\\\& Outlook}\\nIn \\\\ac{HEP} applications so far, the quantum models are not designed to explicitly manifest an inductive bias towards the structure of the chosen (simulated) particle physics datasets. In the aforementioned studies, the model architectures, i.e., quantum circuits used for the implementation of \\\\ac{QNN}s and feature maps for the kernel methods, are constructed following ansätze in the \\\\ac{QML} literature that have desired properties such as expressiveness and hardware efficiency. \\nMany \\\\ac{QML} algorithms have been inspired by classical model architectures, such as autoencoders~\\\\cite{Romero_2017}, convolutional neural networks~\\\\cite{cong2019}, equivariant models~\\\\cite{Nguyen:2022lww}, and graph neural networks~\\\\cite{verdon2019quantum}. Despite drawing inspiration from classical models, these quantum counterparts may exhibit distinct properties and inductive biases~\\\\cite{Kubler2021,Bowles2023}. The studies presented in Sections~\\\\ref{sec:quantum_supervised} and~\\\\ref{sec:quantum_unsupervised} compare the performance of their proposed models to their classical counterparts for the task at hand. However, beyond promising results in specific problems and datasets, identifying precisely in which applications QML models could provide consistent benefits such as enhancement in model performance, or computational speed-ups, still remains an open question and an active area of research. Furthermore, due to limitations in current hardware, the behavior of \\\\ac{QML} models in the regime that is comparable to current state-of-the-art deep learning models, i.e., having millions or even billions of training samples and model parameters, is unknown.\\nIn general, the exploration of \\\\ac{QML} strategies for \\\\ac{HEP} data is, at least partly, motivated by the question of whether quantum models can exploit correlations and information existing in particle physics datasets leading to advantages in performance. It is important to note, that no studies, so far, have used quantum models for supervised or unsupervised classification in real data from \\\\ac{HEP} experiments.\\nThe data measured by the detectors and stored for the analysis of \\\\ac{HEP} experiments is classical. However, a quantum field theory framework is essential to predict and properly explain the outcome of such experiments. Furthermore, remnants of the initial quantum mechanical process --particle interaction-- are still present in the data. Specifically, measuring spin correlations between particles~\\\\cite{top_correlations_CMS2019}, observing entanglement between particles produced in proton collisions~\\\\cite{ATLAS:2023_entanglement, Cervera2017, Severi2022, Fabbrichesi2023} and violation of Bell inequalities~\\\\cite{Fabbrichesi2021, Afik_Nova_2022, Ghosh2023} in \\\\ac{LHC} data has been established. Measuring these first-principle quantities highlights that data from particle physics experiments cannot be described by classical local hidden-variable theories. In conclusion, the topics discussed above represent an active field of research and hold promise for classical and quantum data analysis algorithms that can enhance our ability to probe for new-physics.\\n', \"\\\\section{Introduction}\\nIn a particle accelerator, fundamental physical processes unfold, producing a variety of particles that characterize the final state of each event. These particles are reconstructed through detectors, which assign kinematic and dynamic variables to each event, such as the particle's position, energy deposition, and transverse momentum. These physical observables are typically defined by counting events in various observation channels~\\\\cite{read2002presentation,cowan2014statistics}. For example, measuring the invariant mass of a system within the 100-300 GeV range, with a 10 GeV resolution, yields 20 observation channels, each characterized by its event count.\\nFrom a physical perspective, various processes can contribute to the event count in any given observation channel. These processes may include known phenomena, as predicted by the Standard Model, or new physical processes beyond the model's current scope~\\\\cite{feldman1998unified, lista2016practical}. A central aspect of experimental physics analysis is determining which known processes contribute to a specific observable and identifying potential new processes to explain any deviations from established theories.\\nTo evaluate whether reconstructed events align with existing theories or if they indicate the need for alternative explanations, inferential statistical tools, such as parameter estimation and hypothesis testing, are employed~\\\\cite{cowan2014statistics, cranmer2015practical,barlow2019practical}. In this report, we describe and implement, using Python and RooFit~\\\\cite{verkerke2006roofit,schott2012roostats}, a range of statistical methods used in high-energy physics to estimate the sensitivity of new models and to define exclusion limits for these models.\\nThe development of particle physics has led to three main fields: theory, phenomenology, which links theory to experiments, and experimental work. A primary goal of phenomenology is to guide and optimize experimental searches for new particles based on theoretical models~\\\\cite{conway2005calculation, cranmer2015practical}. This strategy generally involves evaluating a model's sensitivity, identifying a kinematic region (signal region) with the highest potential for discovering new physics. Additionally, exclusion limits are established, defining the parameter space where the model is ruled out based on the expected event counts, typically at a 95\\\\\\nIn the experimental phase, the upper limit is calculated based on observed event counts rather than expected ones. After observation, two outcomes are possible: (1) the data conform to the established model, or (2) the data exhibit a discrepancy that cannot be explained by statistical fluctuations alone. In the first scenario, where data align with existing theories~\\\\cite{cowan2014statistics, casadei2011statistical}, the observed and expected limits are similar, providing no substantial evidence to support new theories, thus excluding them within a specific parameter region.\\nIn the second scenario, any significant discrepancy between data and theory is evaluated using the $5\\\\sigma$ threshold (corresponding to a $p$-value of $2.5 \\\\times 10^{-7}$)~\\\\cite{cowan2014statistics,jme2010cms}, which measures the probability of observing such data (or more extreme results) under the assumption that the current theory is correct. If this threshold is exceeded, a discovery can be claimed. Figure[\\\\ref{fig:1}] illustrates the general research framework in high-energy physics, highlighting the role of statistical tools in excluding models or detecting new physics.\\nIn the statistical modeling of new physical theories and their observations, a key source of uncertainty arises from systematic errors. These errors encompass factors related to the characteristics of the accelerator, particle detectors, and intrinsic model parameters, such as parton distribution functions (PDFs)~\\\\cite{junk1999confidence, cranmer2015practical}. Systematic uncertainties typically reduce the ability to exclude models and to detect new physics. Therefore, incorporating systematic effects is essential for achieving more accurate estimates\\\\cite{barlow2002systematic, read2002presentation, lista2016practical}.\\nThis document presents various models for estimating upper limits and significance, organized into two conceptual categories: single-channel and multichannel experiments, both with and without sources of systematic uncertainty. First, for single-channel experiments without systematic effects, we describe the frequentist and Bayesian approaches traditionally applied in experiments such as the former LEP collider and, more recently, the Large Hadron Collider (LHC)~\\\\cite{read2002presentation, atlas2012observation}. Next, we extend these methods to multichannel experiments, where the increase in dimensionality necessitates optimization strategies and Monte Carlo methods. Finally, we examine systematic effects that require a Bayesian approach, as well as the derivation of profile likelihoods through optimization processes\\\\cite{conway2005calculation}.\\n\", '\\\\section{Introduction to Lecture 2}\\nThis lecture deals with two different methods for determining parameters, least squares and likelihood, \\nwhen a functional form is fitted\\nto our data. A simple example would be straight line fitting, where the parameters are the intercept and gradient\\nof the line. However the methods are much more general than this. Also there are other\\nmethods of extracting parameters; these include the more fundamental Bayesian and Frequentist methods,\\nwhich are dealt with in Lecture 3 . \\nThe least squares method also provides a measure of Goodness of Fit for the agreement between the theory with the \\nbest values of the parameters, and the data; this is dealt with in section \\\\ref{GofF}. The likelihood technique plays \\nan important role in the Bayes approach, and likelihood ratios are relevant for choosing between two hypotheses;\\nthis is covered in Lecture 4. \\n', \"\\\\section{Upper Limits for multi-channel experiments}\\nIn the multi-channel case, the likelihood associated with the signal strength \\\\( \\\\mu \\\\) for the complete observation is determined by the joint likelihood of all channels~\\\\cite{wang2023recent}:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\prod_{i}^{Channels} \\\\mathcal{L}_{i}(\\\\mu).\\n\\\\end{equation}\\nWhere it is assumed that the information in each channel is independently and identically distributed. The definitions of the estimators \\\\( \\\\mathcal{Q}(\\\\mu) \\\\) and \\\\( q_{\\\\mu} \\\\) for a single channel naturally extend to the multi-channel case using the properties of logarithms. Thus, the statistical estimators are fully defined as follows, respectively:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = \\\\sum_{i}^{Channels} \\\\mathcal{Q}_{i}(\\\\mu). \\n\\\\end{equation}\\n\\\\begin{equation}\\nq_{\\\\mu} = \\\\sum_{i}^{Channels} q_{\\\\mu,i}. \\n\\\\end{equation}\\nAs an example, synthetic data associated with a resonance near the measured mass of the Higgs boson \\\\( m_{H} = 125 \\\\ \\\\text{GeV} \\\\) were simulated, with an exponential background component characteristic of the invariant mass of a diphoton system. A total of 30 channels were simulated to numerically illustrate the discovery reported by the CMS collaboration in 2012~\\\\cite{cms2012observation}. Figure~[\\\\ref{fig:15}] shows the resonance data with a mass similar to the measured Higgs boson mass under an exponential background component (blue shaded area). Additionally, the alternative signal + background hypothesis is shown, which could be consistent with the observation.\\nAs in the 2012 search, several signal models with masses ranging from 100 to 160 GeV in steps of 6 GeV will be assumed. For each signal point, the expected upper limit (\\\\( n = b \\\\)), representing a measurement compatible with the background-only hypothesis, and the observed upper limit using the synthetic data will be calculated. Given the data observation, the upper limits allow for excluding the Higgs model at a specific mass or rejecting the background-only hypothesis in favor of the model with this new particle. Typically, this discrepancy is indicated by the difference between the expected and observed upper limits; when this difference is large, it must be quantified in terms of the \\\\( 5\\\\sigma \\\\) criterion to report a discovery~\\\\cite{lista2016practical}.\\nFigure~[\\\\ref{fig:16}] shows the upper limit values as a function of the hypothetical particle mass using the estimator \\\\( \\\\mathcal{Q} \\\\). The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are calculated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{\\\\mathcal{Q}(\\\\mu_{up})} \\\\)~\\\\cite{conway2005calculation}. Note that low- and high-mass models are excluded at a \\\\( 95\\\\ \\nOn the other hand, Figure~[\\\\ref{fig:17}] shows the upper limit values as a function of the hypothetical particle mass using the \\\\( q_{\\\\mu} \\\\) estimator. In the generation of each random experiment, \\\\( \\\\hat{\\\\mu} \\\\) is found using the \\\\texttt{Scipy.optimize} package. The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are estimated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{q_{\\\\mu_{up}}} \\\\). Note the consistency of the results using both estimators. The primary difference lies in the approach to incorporating systematic uncertainties in the estimation of upper limits, significance, and \\\\( 5\\\\sigma \\\\) tension, which is why the profile likelihood is currently used by the CMS and ATLAS collaborations for these estimations~\\\\cite{cms2012observation, atlas2012observation}. Finally, to estimate the discrepancy between the observation and the expected number of events, the p-value of the observation is calculated under the assumption that the background-only hypothesis is correct.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0,\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n \\\\) is the number of observed events.\\n\\\\begin{equation}\\np_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:18}] shows the local p-value as a function of the particle mass. The dashed lines indicate the \\\\( 3\\\\sigma \\\\) evidence region and the \\\\( 5\\\\sigma \\\\) discovery region. This graph illustrates the statistical behavior of the p-value in the search for the Higgs boson in 2012~\\\\cite{cms2012observation}.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Variations on test statistics}\\nA number of test statistics is proposed in Ref.~\\\\cite{asymptotic} that better\\nserve various purposes. Below the main ones are reported:\\n\\\\begin{itemize}\\n\\\\item {\\\\bf Test statistic for discovery:}\\n\\\\begin{equation}\\nq_0 = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(0), &\\\\hat{\\\\mu}\\\\ge 0\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} < 0\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIn case of a negative estimate of $\\\\mu$ ($\\\\hat{\\\\mu}<0$), the test statistic is set to zero in order to\\nconsider only positive $\\\\hat{\\\\mu}$ as evidence against the background-only hypothesis.\\nWithin an asymptotic approximation, the significance is given by: $Z\\\\simeq\\\\sqrt{q_0}$.\\n\\\\item {\\\\bf Test statistic for upper limit:}\\n\\\\begin{equation}\\nq_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(\\\\mu), &\\\\hat{\\\\mu}\\\\le \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} > \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIf the $\\\\hat{\\\\mu}$ estimate is larger than the assumed value for $\\\\mu$, an upward fluctuation occurred.\\nIn those cases, $\\\\mu$ is not excluded by setting the test statistic to zero.\\n\\\\item {\\\\bf Test statistic for Higgs boson search:}\\n\\\\begin{equation}\\n\\\\tilde q_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0))}, & \\\\hat{\\\\mu} < 0\\\\,,\\\\\\\\\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\vec{\\\\theta}}(\\\\mu))}, & 0\\\\le \\\\hat{\\\\mu} < \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} \\\\ge \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThis test statistics both protects against unphysical cases with $\\\\mu <0$\\nand, as the test statistic for upper limits, protects upper limits\\nin cases of an upward $\\\\hat{\\\\mu}$ fluctuation.\\n\\\\end{itemize}\\nA number of measurements performed at LEP and Tevatron used a\\ntest statistic based on the ratio of the likelihood function evaluated under\\nthe signal plus background hypothesis and under the background only hypothesis,\\ninspired by the Neyman--Pearson lemma:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|s+b)}{L(\\\\vec{x}|b)}\\\\,.\\n\\\\end{equation}\\nIn many LEP and Tevatron analyses, nuisance parameters were treated using the hybrid Cousins--Hyghland approach.\\nAlternatively, one could use a formalism similar to the profile likelihood, \\nsetting $\\\\mu=0$ in the denominator and $\\\\mu=1$ in the numerator, and minimizing\\nthe likelihood functions with respect to the nuisance parameters:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu=1, \\\\hat{\\\\hat{\\\\theta}}(1))}{L(\\\\vec{x}|\\\\mu=0,\\\\hat{\\\\hat{\\\\theta}}(0))}\\\\,.\\n\\\\end{equation}\\nFor all the mentioned test statistics, asymptotic approximations exist and\\nare reported in Ref.~\\\\cite{asymptotic}. Those are based either on Wilks' theorem\\nor on Wald's approximations~\\\\cite{Wald}. If a value $\\\\mu$ is tested, and \\nthe data are supposed to be distributed according to another value of the signal strength $\\\\mu^\\\\prime$,\\nthe following approximation holds, asymptotically:\\n\\\\begin{equation}\\n-2\\\\ln\\\\lambda(\\\\mu) = \\\\frac{(\\\\mu-\\\\hat{\\\\mu})^2}{\\\\sigma^2} + {\\\\cal O}\\\\left(\\\\frac{1}{\\\\sqrt{N}}\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\mu}$ is distributed according to a Gaussian with average $\\\\mu^\\\\prime$ and\\nstandard deviation $\\\\sigma$. The covariance matrix for the nuisance parameters is\\ngiven, in the asymptotic approximation, by:\\n\\\\begin{equation}\\nV_{ij}^{-1} = \\\\left.\\\\left<\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right>\\\\right|_{\\\\mu=\\\\mu^\\\\prime}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu^\\\\prime$ is assumed as value for the signal strength.\\nIn some cases, asymptotic approximations (Eq.~(\\\\ref{eq:waldTestStat})) can be written in terms of an\\n{\\\\it Asimov dataset}~\\\\cite{Asimov}:\\n\\\\begin{displayquote}\\n{\\\\it We define the Asimov data set such that when one uses it to evaluate the estimators\\nfor all parameters, one obtains the true parameter values}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\nIn practice, an Asimov dataset is a single ``representative'' dataset obtained by replacing all\\nobservable (random) varibles with their expecteted value. In particular,\\nall yields in the data sample (e.g.: in a binned case) are replaced with their expected values, that may be non integer values.\\nThe median significance for different cases of test statistics can be computed in this\\nway without need of producing extensive sets of toy Monte Carlo. The implementation\\nof those asymptotic formulate is available in the {\\\\sc RooStats} library, released\\nas part an optional component {\\\\sc Root}~\\\\cite{Root}.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Limits in the presence of background}\\nThis is where it gets tricky.\\nTypically an experiment may observe $N_D$ events, with an expected background $N_B$ and efficiency $\\\\eta$, and wants to present results for $N_S={N_D-N_B \\\\over \\\\eta}$.\\nUncertainties in $\\\\eta$ and $N_B$ are handled by profiling or marginalising.\\nThe problem is that the \\n{\\\\it actual number} of background events is not $N_B$ but Poisson in $N_B$.\\nSo in a straightforward case, if you observe twelve events, with expected background 3.4 and $\\\\eta=1$\\nit is obviously sensible to say $N_S=8.6$\\n(though the error is $\\\\sqrt{12}$ not $\\\\sqrt{8.6}$)\\nBut suppose, with the same background, you see four events, three events or zero events.\\nCan you say $N_S=0.6$? or $-0.4$? Or $-3.4$???\\nWe will look at four methods of handling this, considering as an example the observation of three events with expected background 3.40 and wanting to present the 95\\\\ \\n\\\\subsubsection{Method 1: Pure frequentist}\\n$N_D-N_B$ is an unbiased estimator of $N_S$ and its properties are known.\\nQuote the result. Even if it is non-physical.\\nThe argument for doing so\\nis that\\nthis is needed for balance: if there is really no signal, approximately half of the experiments will give positive values and half negative. \\nIf the negative results are not published, but the positive ones are, the world average will be spuriously high.\\nFor a 95\\\\clearly one of them. So what?\\nA counter-argument is that if\\n$N_D<N_B$, we {\\\\it know} that the background has fluctuated downwards. But this cannot be incorporated \\ninto the formalism.\\nAnyway, the upper limit from 3 is 7.75, as $\\\\sum_0^3 e^{-7.75}7.75^r/r! = 0.05$, and the \\n95\\\\\\n\\\\subsubsection{Method 2: Go Bayesian}\\nAssign a uniform prior to $N_S$, for $N_S>0$, zero for $N_S<0$.\\nThe posterior is then just the likelihood, $P(N_S | N_D,N_B)=e^{-(N_S+N_B)}{(N_S+N_B)^{N_D} \\\\over N_D!}$.\\nThe required limit is obtained from integrating $\\\\int_0^{N_{hi}} P(N_S)\\\\, dN_S = 0.95$\\nwhere\\n$P(N_S)\\\\propto e^{-(N_s+3.40)}{(N_s+3.4)^3 \\\\over 3!}$; this is illustrated in Fig.~\\\\ref{fig:Bayeslimit}\\nand the value of the limit is \\n5.21.\\n\\\\subsubsection{Method 3: Feldman-Cousins}\\nThis---called `the unified approach' by Feldman and Cousins~\\\\cite{FC}---takes a step backwards\\nand considers the ambiguity in the use of confidence belts. \\nIn principle, if you decide to work at, say, 90\\\\This is shown in Fig.~\\\\ref{fig:FC1}.\\nIn practice, if you happen to get a low result you would quote an upper limit, but if you get a high result you would quote a central limit.\\nThis, which they call `flip-flopping', is illustrated in the plot by a break shown here for $r=10$. \\nNow the confidence belt is the green one for $r< 10$ and the red one for $r\\\\geq 10$. The\\nprobability of lying in the band is no longer 90\\\\Flip-flopping invalidates the Frequentist construction, leading to undercoverage. \\nThey show how to avoid this. You draw the plot slightly differently:\\n$r \\\\equiv N_D$ is still the horizontal variable, but as the vertical variable you use $N_S$. \\n(This means a different plot for any different $N_B$, whereas the previous Poisson plot is universal, but this is not a problem.)\\nThis is to be filled using $P(r;N_s)=e^{-(N_s+N_B)}{(N_S+N_B)^r \\\\over r!}$\\\\ .\\nFor each $N_S$ you define a region $R$ such that $\\\\sum_{r\\\\epsilon R}P(r;N_s) \\\\geq 90\\\\You have a choice of strategy that goes beyond `central' or `upper limit': one \\nplausible suggestion would be to\\nrank $r$ by probability and take them in order until the desired total probability content is achieved (which would, incidentally, give the shortest interval).\\nHowever this has the drawback that outcomes with $r < N_B$ will have small probabilities and be excluded for all $N_S$, so that, if such a result does occur, one cannot say anything constructive, just `This was unlikely'. \\nAn improved form of this suggestion is that for each $N_S$, considering each $r$ you compare $P(r;N_S)$ with the largest possible value obtained by varying $N_S$. This is easier than it sounds because this highest value is either at $N_S=r-N_B$ (if $r\\\\geq N_B$) or $N_S=0$ (if $r\\\\leq N_B$ ).\\nRank on the ratio $P(r;N_S)/P(r;N^{best}_S)$ and again take them in order till their sum gives the desired probability.\\nThis gives a band as shown in Fig.~\\\\ref{fig:FC2}, which has $N_B=3.4$. You can see that \\n`flip-flopping' occurs naturally: for small values of $r$ one just has an upper limit, whereas for larger values, above $r=7$, one obtains a lower limit as well. Yet there is a single band, and the coverage is\\ncorrect (i.e. it does not undercover).\\nIn the case we are considering, $r=3$, just an upper limit is given, at $4.86$. \\nLike other good ideas, this has not found universal favour. Two arguments are raised against the method.\\nFirst, that it deprives the physicist of the choice of whether to publish an upper limit or a range. \\nIt could be embarrassing if you\\nlook for something weird and are `forced' to publish a non-zero result. \\nBut this is actually the point, and in such cases one can always explain\\nthat the limits should not be taken as implying that the quantity actually is nonzero.\\nSecondly, if two experiments with different $N_B$ get the same small $N_D$, the one with the higher $N_B$ will quote a smaller limit on $N_S$. The worse experiment gets the better result, which is clearly unfair!\\nBut this is not comparing like with like: for a `bad' experiment with large background to get a small number of events is much less likely than it is for a `good' low background experiment.\\n\\\\subsubsection {Method 4: $CL_s$}\\nThis is a modification of the standard frequentist approach to include the \\nfact, as mentioned above, that a small observed signal implies a downward \\nfluctuation in background~\\\\cite{Read}. Although presented here using just numbers of events, the method is usually extended to use the full likelihood of the result, as will be discussed in Section~\\\\ref{subsection:Extension}.\\nDenote the (strict frequentist) \\nprobability of getting a result this small (or less) from $s+b$ events as \\n$CL_{s+b}$, and the equivalent probability from pure background as \\n$CL_b$ (so \\n$CL_b=CL_{s+b}$ for $s=0$).\\nThen introduce\\n\\\\begin{equation}\\nCL_s={CL_{s+b} \\\\over CL_b}\\n\\\\quad.\\n\\\\end{equation}\\nLooking at Fig.~\\\\ref{fig:CLS}, the $CL_{s+b}$ curve shows that if $s+b$ is small then the probability of getting three events or less is high, near 100\\\\the probability of only getting three events or less is only 5\\\\\\nThe point $s+b=3.4$ corresponds to $s=0$, at which the probability $CL_b$ is 56\\\\incorporate this by renormalizing the (blue) $CL_{s+b}$ curve to have a maximum of 100\\\\physically sensible region, dividing it by 0.56 to get the (green) $CL_s$ curve.\\nThis is treated in the same way as the $CL_{s+b}$ curve, reading off the point at $s+b=8.61$ where it falls to 5\\\\This is larger than the strict frequentist limit: the method over-covers (which, as we have seen, is allowed if not encouraged)\\nand is, in this respect `conservative'\\\\footnote{`Conservative' is a misleading word. It is used by people \\ndescribing their analyses to \\nimply safety and caution, whereas it usually entails cowardice and sloppy thinking.}. This is the same value as the Bayesian Method 2, as it makes the same assumptions. \\n$CL_s$ is not frequentist, just `frequentist inspired'. In terms of statistics there is perhaps little in its favour. But it has an intuitive appeal, and is widely used.\\n\\\\subsubsection{Summary so far}\\nGiven three observed events, and an expected background of 3.4 events, what is\\nthe 95\\\\Possible answers are shown in table~\\\\ref{tab:summary}.\\n\\\\begin{table}[h]\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nStrict Frequentist & 4.35 \\\\\\\\\\nBayesian (uniform prior) & 5.21 \\\\\\\\\\nFeldman-Cousins & 4.86 \\\\\\\\\\n$CL_s$ & 5.21 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{ Upper limits from different methods}\\n\\\\end{table}\\nWhich is `right'? Take your pick!\\nAll are correct. (Well, not wrong.). The golden rule is to say what you are doing, and if possible give the raw numbers. \\n\\\\subsubsection{Extension: not just counting numbers}\\nThese examples have used \\nsimple counting experiments. But a simple number does not (usually) exploit the full information.\\nConsider the illustration in Fig.~\\\\ref{fig:beyondsimple}. One is searching for (or putting an upper limit on) some broad resonance around 7~GeV. One could count the number of events inside some window\\n(perhaps 6 to 8~GeV?) and subtract the estimated background. This might work with high statistics, as in the left, but would be pretty useless with small numbers, as in the right. It is clearly not optimal \\njust to count an event as `in', whether it is at 7.0 or 7.9, and to treat an event as `out', if it is at 8.1 or \\n10.1.\\nIt is better to calculate the \\nLikelihood $\\\\ln L_{s+b}=\\\\sum_i \\\\ln{N_s S(x_i)+N_b B(x_i)} \\\\quad;\\\\quad \\\\ln{ L_b}=\\\\sum_i \\\\ln{N_b B(x_i)}$.\\nThen, for example using $CL_s$, you can work with $L_{s+b}/L_b$, or $-2 \\\\ln{(L_{s+b}/L_b)}$.\\nThe confidence/probability quantities can be found from simulations, or sometimes from data.\\n\\\\subsubsection{Extension: From numbers to masses}\\nLimits on numbers of events can readily be translated into limits on branching ratios,\\n$BR={N_s \\\\over N_{total}}$,\\nor limits on cross sections,\\n$\\\\sigma={N_s \\\\over \\\\int {\\\\cal L} dt}$\\\\ .\\nThese may translate to limits on other, theory, parameters.\\nIn the Higgs search (to take an example) the cross section depends on the mass, $M_H$---and so does the detection efficiency---which may require changing strategy (hence different backgrounds). This leads to the need\\nto basically repeat the analysis for all (of many) $M_H$ values. This can be presented in two ways. \\nThe first is shown in Fig.~\\\\ref{fig:significanceplot}, taken from Ref.~\\\\cite{ATLAS1}. For each $M_H$ (or whatever is being studied) you search for a signal and plot the $CL_s$ (or whatever limit method you prefer) significance \\nin a {\\\\it Significance Plot}. \\nSmall values indicate that it is unlikely to get a signal this large just from background.\\nOne often also plots the expected (from MC) significance, assuming the signal hypothesis is true. This is a measure of a `good experiment'. In this case there is a discovery level \\ndrop at $M_H \\\\approx 125$~GeV, which exceeds the expected significance, though not by much: ATLAS were lucky but not incredibly lucky.\\nThe second method is---for some reason---known as the green-and-yellow plot.\\nThis is basically the same data, but fixing $CL$ at a chosen value: in Fig.~\\\\ref{fig:greenandyellow} it is 95\\\\ You find the limit on signal strength, at this confidence level, and interpret it as \\na limit on the cross section $\\\\sigma / \\\\sigma_{SM}$.\\nAgain, as well as plotting the actual data one also plots the expected (from MC) limit, with variations.\\nIf there is no signal, 68\\\\ \\nSo this figure shows the experimental result as a black line. Around 125~GeV the 95\\\\is more than the Standard Model prediction indicating a discovery. There are peaks between 200 and 300~GeV, but they do not approach the SM value, indicating that they are just fluctuations. The value rises at 600~GeV, but the green (and yellow) bands rise also, showing that the experiment is not\\nsensitive for such high masses: basically it sees nothing but would expect to see nothing.\", '\\\\section{Inference}\\n\\\\subsection{Frequentist inference}\\nAssigning a probability level to an unknown parameter makes no sense in the frequentist approach\\nsince unknown parameters are not random variables.\\nA frequentist inference procedure should determine a central value and an uncertainty interval that depend\\non the observed measurements without introducing any subjective element.\\nSuch central value and interval extremes are random variables themselves.\\nThe function that returns the central value given an observed measurement is called {\\\\it estimator}.\\nThe parameter value provided by an estimator is also called {\\\\it best fit} value.\\nDifferent estimator choices are possible, the most frequently adopted is the {\\\\it maximum likelihood\\nestimator} because of its statistical properties discussed in Sec.~\\\\ref{sec:estimatorProperties}.\\nRepeating the experiment will result each time in a different data sample\\nand, for each data sample, the estimator returns a different central value $\\\\hat{\\\\theta}$.\\nAn uncertainty interval $[\\\\hat{\\\\theta} -\\\\delta, \\\\hat{\\\\theta} +\\\\delta]$ can be associated to\\nthe estimator value $\\\\hat{\\\\theta}$. In some cases, as for the Bayesian inference, an\\nasymmetric interval choice is also possible with frequentist inference:\\n$[\\\\hat{\\\\theta} -\\\\delta^-, \\\\hat{\\\\theta} +\\\\delta^+]$.\\nSome of the intervals obtained with this method contain the fixed and unknown true\\nvalue of $\\\\theta$, corresponding to a fraction equal to 68.3\\\\large number of experiments. This property is called {\\\\it coverage}.\\nThe simplest example of frequentist inference \\nassumes a Gaussian PDF (Eq.~(\\\\ref{eq:GaussianPDF})) with a known $\\\\sigma$ and an unknown $\\\\mu$.\\nA single experiment provides a measurement $x$, and we can estimate $\\\\mu$ as $\\\\hat{\\\\mu} = x$.\\nThe distribution of $\\\\hat{\\\\mu}$ is the original Gaussian because $\\\\hat{\\\\mu}$ is just equal to $x$.\\nA fraction of 68.3\\\\estimate $\\\\hat{\\\\mu}$ within: $\\\\mu - \\\\sigma < \\\\hat{\\\\mu} < \\\\mu + \\\\sigma$. This means that we can quote:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mu = x \\\\pm \\\\sigma\\\\,.\\n}\\n\\\\end{equation}\\n', \"\\\\section{Conceptual building blocks for modeling}\\n\\\\subsection{Probability densities and the likelihood function}\\nThis section specifies my notations and conventions, which I have chosen with some care.\\\\footnote{As in the case of relativity, notational conventions can make some properties of expressions manifest and help identify mistakes. For example, $g_{\\\\mu\\\\nu}x^\\\\mu y^\\\\nu$ is manifestly Lorentz invariant and $x^\\\\mu + y_\\\\nu$ is manifestly wrong.} Our statistical claims will be based on the outcome of an experiment. When discussing frequentist probabilities, one must consider ensembles of experiments, which may either be real, based on computer simulations, or mathematical abstraction.\\nFigure~\\\\ref{fig:hierarchy} establishes a hierarchy that is fairly general for the context of high-energy physics. Imagine the search for the Higgs boson, in which the search is composed of several ``channels'' indexed by $c$. Here a channel is defined by its associated event selection criteria, not an underlying physical process. In addition to the number of selected events, $n_c$, each channel may make use of some other measured quantity, $x_c$, such as the invariant mass of the candidate Higgs boson. The quantities will be called ``observables'' and will be written in roman letters e.g. $x_c$. The notation is chosen to make manifest that the observable $x$ is frequentist in nature. Replication of the experiment many times will result in different values of $x$ and this ensemble gives rise to a \\\\emph{probability density function} (pdf) of $x$, written $f(x)$, which has the important property that it is normalized to unity\\n\\\\[\\n\\\\int f(x) \\\\;dx\\\\;= 1\\\\;.\\n\\\\]\\nIn the case of discrete quantities, such as the number of events satisfying some event selection, the integral is replaced by a sum. Often one considers a parametric family of pdfs\\n\\\\[\\nf(x | \\\\alpha) \\\\;,\\n\\\\]\\nread ``$f$ of $x$ given $\\\\alpha$'' and, henceforth, referred to as a \\\\emph{probability model} or just \\\\emph{model}. The parameters of the model typically represent parameters of a physical theory or an unknown property of the detector's response. The parameters are not frequentist in nature, thus any probability statement associated with $\\\\alpha$ is Bayesian.\\\\footnote{Note, one can define a conditional distribution $f(x|y)$ when the joint distribution $f(x,y)$ is defined in a frequentist sense.} In order to make their lack of frequentist interpretation manifest, model parameters will be written in greek letters, e.g.: $\\\\mu, \\\\theta, \\\\alpha, \\\\nu$. \\\\footnote{While it is common to write $s$ and $b$ for the number of expected signal and background, these are parameters \\\\emph{not} observables, so I will write $\\\\nu_S$ and $\\\\nu_B$. This is one of few notational differences to Ref.~\\\\cite{asimov}.}\\nFrom the full set of parameters, one is typically only interested in a few: the \\\\emph{parameters of interest}. The remaining parameters are referred to as \\\\emph{nuisance parameters}, as we must account for them even though we are not interested in them directly.\\nWhile $f(x)$ describes the probability density for the observable $x$ for a single event, we also need to describe the probability density for a dataset with many events, $\\\\data = \\\\{x_1,\\\\dots,x_{n}\\\\}$. If we consider the events as independently drawn from the same underlying distribution, then clearly the probability density is just a product of densities for each event. However, if we have a prediction that the total number of events expected, call it $\\\\nu$, then we should also include the overall Poisson probability for observing $n$ events given $\\\\nu$ expected. Thus, we arrive at what statisticians call a marked Poisson model,\\n\\\\begin{equation}\\n\\\\F(\\\\data|\\\\nu,\\\\alpha) = \\\\Pois(n|\\\\nu) \\\\prod_{e=1}^n f(x_e|\\\\alpha) \\\\; ,\\n\\\\end{equation}\\nwhere I use a bold $\\\\F$ to distinguish it from the individual event probability density $f(x)$. In practice, the expectation is often parametrized as well and some parameters simultaneously modify the expected rate and shape, thus we can write $\\\\nu\\\\rightarrow\\\\nu(\\\\alpha)$. In \\\\texttt{RooFit} both $f$ and $\\\\F$ are implemented with a \\\\texttt{RooAbsPdf}; where \\\\texttt{RooAbsPdf::getVal(x)} always provides the value of $f(x)$ and depending on \\\\texttt{RooAbsPdf::extendMode()} the value of $\\\\nu$ is accessed via \\\\texttt{RooAbsPdf::expectedEvents()}.\\nThe \\\\emph{likelihood function} $L(\\\\alpha)$ is numerically equivalent to $f(x|\\\\alpha)$ with $x$ fixed -- or $\\\\F(\\\\data|\\\\alpha)$ with \\\\data\\\\ fixed. The likelihood function should not be interpreted as a probability density for $\\\\alpha$. In particular, the likelihood function does not have the property that it normalizes to unity\\n\\\\[\\n\\\\cancelto{\\\\mathrm{Not ~True!}}{\\\\int L(\\\\alpha) \\\\;d\\\\alpha = 1}\\\\; .\\n\\\\]\\nIt is common to work with the log-likelihood (or negative log-likelihood) function. In the case of a marked Poisson, we have what is commonly referred to as an extended likelihood~\\\\cite{Barlow1990496}\\n\\\\begin{eqnarray}\\\\nonumber\\n-\\\\ln L( \\\\alpha) &=& \\\\underbrace{\\\\nu(\\\\alpha) - n \\\\ln \\\\nu(\\\\alpha)}_{\\\\rm extended~term} - \\\\sum_{e=1}^n \\\\ln f(x_e) + \\\\underbrace{~\\\\ln n! ~}_{\\\\mathrm{constant}}\\\\; .\\n\\\\end{eqnarray}\\nTo reiterate the terminology, \\\\emph{probability density function} refers to the value of $f$ as a function of $x$ given a fixed value of $\\\\alpha$; \\\\emph{likelihood function} refers to the value of $f$ as a function of $\\\\alpha$ given a fixed value of $x$; and \\\\emph{model} refers to the full structure of $f(x|\\\\alpha)$.\\nProbability models can be constructed to simultaneously describe several channels, that is several disjoint regions of the data defined by the associated selection criteria. I will use $e$ as the index over events and $c$ as the index over channels. Thus, the number of events in the $c^{\\\\rm th}$ channel is $n_c$ and the value of the $e^{\\\\rm th}$ event in the $c^{\\\\rm th}$ channel is $x_{ce}$. In this context, the data is a collection of smaller datasets: \\\\mbox{$\\\\datasim=\\\\{\\\\data_1, \\\\dots, \\\\data_{c_{\\\\rm max}}\\\\}=\\\\{\\\\{x_{c=1,e=1}\\\\dots x_{c=1,e=n_c}\\\\}, \\\\dots \\\\{x_{c=c_{\\\\rm max},e=1}\\\\dots x_{c=c_{\\\\rm max},e=n_{c_{\\\\rm max}}} \\\\}\\\\}$}. In \\\\texttt{RooFit} the index $c$ is referred to as a \\\\texttt{RooCategory} and it is used to inside the dataset to differentiate events associated to different channels or categories. The class \\\\texttt{RooSimultaneous} associates the dataset $\\\\data_c$ with the corresponding marked Poisson model. The key point here is that there are now multiple Poisson terms. Thus we can write the combined (or simultaneous) model \\n\\\\begin{equation}\\n\\\\F_{\\\\textrm{sim}}(\\\\datasim|\\\\alpha) = \\\\prod_{c\\\\in\\\\rm channels} \\\\left[ \\\\Pois(n_c|\\\\nu(\\\\alpha)) \\\\prod_{e=1}^{n_c} f(x_{ce}|\\\\alpha) \\\\right] \\\\; ,\\n\\\\end{equation}\\nremembering that the symbol product over channels has implications for the structure of the dataset.\\n\\\\subsection{Auxiliary measurements} \\nAuxiliary measurements or control regions can be used to estimate or reduce the effect of systematic uncertainties. The signal region and control region are not fundamentally different. In the language that we are using here, they are just two different channels. \\nA common example is a simple counting experiment with an uncertain background. In the frequentist way of thinking, the true, unknown background in the signal region is a nuisance parameter, which I will denote $\\\\nu_B$.\\\\footnote{Note, you can think of a counting experiment in the context of Eq.~\\\\ref{Eq:markedPoisson} with $f(x)=1$, thus it reduces to just the Poisson term.} If we call the true, unknown signal rate $\\\\nu_S$ and the number of events in the signal region $n_{\\\\rm SR}$ then we can write the model $\\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)$. As long as $\\\\nu_B$ is a free parameter, there is no ability to make any useful inference about $\\\\nu_S$. Often we have some estimate for the background, which may have come from some control sample with $n_{\\\\rm CR}$ events. If the control sample has no signal contamination and is populated by the same background processes as the signal region, then we can write $\\\\Pois(n_{\\\\rm CR}|\\\\tau \\\\nu_B)$, where $n_{\\\\rm CR}$ is the number of events in the control region and $\\\\tau$ is a factor used to extrapolate the background from the signal region to the control region. Thus the total probability model can be written $\\\\F_{\\\\rm sim}(n_{\\\\rm SR},n_{\\\\rm CR} | \\\\nu_S, \\\\nu_B) = \\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)\\\\cdot \\\\Pois(n_{\\\\rm CR}|\\\\tau\\\\nu_B)$. This is a special case of Eq.~\\\\ref{Eq:simultaneous} and is often referred to as the ``on/off' problem~\\\\cite{Cousins:2008zz}.\\nBased on the control region alone, one would estimate (or `measure') $\\\\nu_B = n_{\\\\rm CR}/\\\\tau$. Intuitively the estimate comes with an `uncertainty' of $\\\\sqrt{n_{\\\\rm CR}}/\\\\tau$. We will make these points more precise in Sec.~\\\\ref{S:estimation}, but the important lesson here is that we can use auxiliary measurements (ie. $n_{\\\\rm CR}$) to describe our uncertainty on the nuisance parameter $\\\\nu_B$ statistically. Furthermore, we have formed a statistical model that can be treated in a frequentist formalism -- meaning that if we repeat the experiment many times $n_{\\\\rm CR}$ will vary and so will the estimate of $\\\\nu_B$. It is common to say that auxiliary measurements `constrain' the nuisance parameters. In principle the auxiliary measurements can be every bit as complex as the main signal region, and there is no formal distinction between the various channels.\\nThe use of auxiliary measurements is not restricted to estimating rates as in the case of the on/off problem above. One can also use auxiliary measurements to constrain other parameters of the model. To do so, one must relate the effect of some common parameter $\\\\alpha_p$ in multiple channels (ie. the signal region and a control regions). This is implicit in Eq.~\\\\ref{Eq:simultaneous}.\\n\\\\subsection{Frequentist and Bayesian reasoning}\\nThe intuitive interpretation of measurement of $\\\\nu_B$ to be $n_{\\\\rm CR}/\\\\tau \\\\pm \\\\sqrt{n_{\\\\rm CR}}/\\\\tau$ is that the parameter $\\\\nu_B$ has a distribution centered around $n_{\\\\rm CR}/\\\\tau$ with a width of $\\\\sqrt{n_{\\\\rm CR}}/\\\\tau$. With some practice you will be able to immediately identify this type of reasoning as Bayesian. It is manifestly Bayesian because we are referring to the probability distribution of a parameter. The frequentist notion of probability of an event is defined as the limit of its relative frequency in a large number of trials. The large number of trials is referred to as an ensemble. In particle physics the ensemble is formed conceptually by repeating the experiment many times. The true values of the parameters, on the other hand, are states of nature, not the outcome of an experiment. The true mass of the $Z$ boson has no frequentist probability distribution. The existence or non-existence of the Higgs boson has no frequentist probability associated with it. There is a sense in which one can talk about the probability of parameters, which follows from Bayes's theorem:\\n\\\\begin{equation}\\nP(A|B) = \\\\frac{P(B|A) P(A)}{P(B)} \\\\; .\\n\\\\end{equation}\\nBayes's theorem is a theorem, so there's no debating it. It is not the case that Frequentists dispute whether Bayes's theorem is true. The debate is whether the necessary probabilities exist in the first place. If one can define the joint probability $P(A,B)$ in a frequentist way, then a Frequentist is perfectly happy using Bayes theorem. Thus, the debate starts at the very definition of probability.\\nThe Bayesian definition of probability clearly can't be based on relative frequency. Instead, it is based on a degree of belief. Formally, the probability needs to satisfy Kolmogorov's axioms for probability, which both the frequentist and Bayesian definitions of probability do. One can quantify degree of belief through betting odds, thus Bayesian probabilities can be assigned to hypotheses on states of nature. In practice human's bets are not generally not `coherent' (see `dutch book'), thus this way of quantifying probabilities may not satisfy the Kolmogorov axioms.\\nMoving past the philosophy and accepting the Bayesian procedure at face value, the practical consequence is that one must supply prior probabilities for various parameter values and/or hypotheses. In particular, to interpret our example measurement of $n_{\\\\rm CR}$ as implying a probability distribution for $\\\\nu_B$ we would write\\n\\\\begin{equation}\\n\\\\pi(\\\\nu_B | n_{\\\\rm CR}) \\\\propto f(n_{\\\\rm CR} | \\\\nu_B) \\\\eta(\\\\nu_B) \\\\; ,\\n\\\\end{equation}\\nwhere $\\\\pi(\\\\nu_B | n_{\\\\rm CR})$ is called the \\\\textit{posterior} probability density, $f(n_{\\\\rm CR} | \\\\nu_B)$ is the likelihood function, and $\\\\eta(\\\\nu_B)$ is the \\\\textit{prior} probability. Here I have suppressed the somewhat curious term $P(n_{\\\\rm CR})$, which can be thought of as a normalization constant and is also referred to as the \\\\textit{evidence}. The main point here is that one can only invert `the probability of $n_{\\\\rm CR}$ given $\\\\nu_B$' to be `the probability of $\\\\nu_B$ given $n_{\\\\rm CR}$' if one supplies a prior. Humans are very susceptible to performing this logical inversion accidentally, typically with a uniform prior on $\\\\nu_B$. Furthermore, the prior degree of belief cannot be derived in an objective way. There are several formal rules for providing a prior based on formal rules (see Jefferey's prior and Reference priors), though these are not accurately described as representing a degree of belief. Thus, that style of Bayesian analysis is often referred to as objective Bayesian analysis.\\n{\\\\flushleft{Some useful and amusing quotes on Bayesian and Frequentist reasoning:}}\\n\\\\begin{quote}\\n{\\\\em ``Using Bayes's theorem doesn't make you a Bayesian, \\\\textbf{always} using Bayes's theorem makes you a Bayesian.''} --unknown\\n\\\\end{quote}\\n\\\\begin{quote}\\n{\\\\em\\n``Bayesians address the questions everyone is interested in by using assumptions that no one believes.\\nFrequentist use impeccable logic to deal with an issue that is of no interest to anyone.''}- Louis Lyons\\n\\\\end{quote}\\n\\\\subsection{Consistent Bayesian and Frequentist modeling of constraint terms} \\nOften a detailed probability model for an auxiliary measurement are not included directly into the model. If the model for the auxiliary measurement were available, it could and should be included as an additional channel as described in Sec.~\\\\ref{S:AuxMeas}. The more common situation for background and systematic uncertainties only has an estimate, ``central value'', or best guess for a parameter $\\\\alpha_p$ and some notion of uncertainty on this estimate. In this case one typically resorts to including idealized terms into the likelihood function, here referred to as ``constraint terms'', as surrogates for a more detailed model of the auxiliary measurement. I will denote this estimate for the parameters as $a_p$, to make it manifestly frequentist in nature. In this case there is a single measurement of $a_p$ per experiment, thus it is referred to as a ``global observable'' in \\\\roostats. The treatment of constraint terms is somewhat \\\\emph{ad hoc} and discussed in more detail in Section~\\\\ref{S:ConstraintExamples}. I make it a point to write constraint terms in a manifestly frequentist form $f(a_p | \\\\alpha_p)$. \\nProbabilities on parameters are legitimate constructs in a Bayesian setting, though they will always rely on a prior. In order to distinguish Bayesian pdfs from frequentist ones, greek letters will be used for their distributions. For instance, a generic Bayesian pdf might be written $\\\\pi(\\\\alpha)$. In the context of a main measurement, one might have a prior for $\\\\alpha_p$ based on some estimate $a_p$. In this case, the prior $\\\\pi(\\\\alpha_p )$ is really a posterior from some previous measurement. It is desirable to write with the help of Bayes theorem\\n\\\\begin{equation}\\n\\\\pi(\\\\alpha_p | a_p) \\\\propto L( \\\\alpha_p ) \\\\eta(\\\\alpha_p) = f(a_p|\\\\alpha_p) \\\\eta(\\\\alpha_p)\\\\; ,\\n\\\\end{equation}\\nwhere $\\\\eta(\\\\alpha_p)$ is some more fundamental prior.\\\\footnote{Glen Cowan has referred to this more fundamental prior as an 'urprior', which is based on the German use of 'ur' for forming words with the sense of `proto-, primitive, original'.} By taking the time to undo the Bayesian reasoning into an objective pdf or likelihood and a prior we are able to write a model that can be used in a frequentist context. Within \\\\roostats, the care is taken to separately track the frequentist component and the prior; this is achieved with the \\\\texttt{ModelConfig} class.\\nIf one can identify what auxiliary measurements were performed to provide the estimate of $\\\\alpha_p$ and its uncertainty, then it is not a logical fallacy to approximate it with a constraint term, it is simply a convenience. However, not all uncertainties that we deal result from auxiliary measurements. In particular, some theoretical uncertainties are not statistical in nature. For example, uncertainty associated with the choice of renormalization and factorization scales and missing higher-order corrections in a theoretical calculation are not statistical. Uncertainties from parton density functions are a bit of a hybrid as they are derived from data but require theoretical inputs and make various modeling assumptions. In a Bayesian setting there is no problem with including a prior on the parameters associated to theoretical uncertainties. In contrast, in a formal frequentist setting, one should not include constraint terms on theoretical uncertainties that lack a frequentist interpretation. That leads to a very cumbersome presentation of results, since formally the results should be shown as a function of the uncertain parameter. In practice, the groups often read Eq.~\\\\ref{eq:urprior} to arrive at an effective frequentist constraint term.\\nI will denote the set of parameters with constraint terms as $\\\\mathbb{S}$ and the global observables $\\\\mathcal{G}=\\\\{a_p\\\\}$ with $p\\\\in\\\\mathbb{S}$. By including the constraint terms explicitly (instead of implicitly as an additional channel) we arrive at the total probability model, which we will not need to generalize any further:\\n\\\\begin{equation}\\n\\\\F_{\\\\textrm{tot}}(\\\\datasim, \\\\mathcal{G}|\\\\alpha) = \\\\prod_{c\\\\in\\\\rm channels} \\\\left[ \\\\Pois(n_c|\\\\nu_c(\\\\alpha)) \\\\prod_{e=1}^{n_c} f_c(x_{ce}|\\\\alpha) \\\\right] \\\\cdot \\\\prod_{p \\\\in \\\\mathbb{S}} f_p(a_p | \\\\alpha_p)\\\\; .\\n\\\\end{equation}\\n\", '\\\\section{Worked example: Lifetime determination}\\nHere we consider an experiment which has resulted in $N$ observed decay times $t_i$ of a particle\\nwhose lifetime $\\\\tau$ we want to determine. The probability density for observing a decay at time $t$ \\nis \\n\\\\begin{equation}\\np(t;\\\\tau) = (1/\\\\tau) \\\\ e^{-t/\\\\tau}\\n\\\\end{equation} \\nNote the essential normalisation factor $1/\\\\tau$; without this the likelihood method does not work.\\nIt should be realised that realistic situations are more complicated than this. For example, we ignore\\nthe possibility of backgrounds, time resolution which smears the expected values of $t$, acceptance or \\nefficiency effects which vary with $t$, etc., but this enables us to estimate $\\\\tau$ and its uncertainty\\n$\\\\sigma_{\\\\tau}$ analytically. In real practical cases, it is almost always necessary to calculate the \\nlikelihood as a function of $\\\\tau$ numerically. \\nFrom equation \\\\ref{exp} we calculate the log-likelihood as\\n\\\\begin{equation}\\n\\\\ln L(\\\\tau) = \\\\ln[\\\\Pi\\\\ (1/\\\\tau) e^{-t_i/\\\\tau}] \\\\ \\\\ = \\\\ \\\\ \\\\Sigma (-\\\\ln \\\\tau - t_i/\\\\tau)\\n\\\\end{equation}\\nDifferentiating $\\\\ln L(\\\\tau)$ with respect to $\\\\tau$ and setting the derivative to zero then yields\\n\\\\begin{equation}\\n\\\\tau = \\\\Sigma t_i/N\\n\\\\end{equation}\\nThis equation has an appealing feature, as it can be read as ``The mean lifetime is equal to the mean lifetime\",\\nwhich sounds as if it must be true. However, what it really says is not quite so trivial: ``Our\\nbest estimate of the lifetime parameter $\\\\tau$ is equal to the mean of the $N$ observed decay times in our \\nexperiment.\"\\nWe next calculate $\\\\sigma_{\\\\tau}$ from the second derivative of $\\\\ln L$, and obtain \\n\\\\begin{equation}\\n\\\\sigma_{\\\\tau} = \\\\tau/\\\\sqrt N\\n\\\\end{equation}\\nThis exhibits a common feature that the uncertainty of our parameter estimate decreases as $1/\\\\sqrt N$ as we \\ncollect more and more data. However, a potential problem arises from the fact that our estimated uncertainty\\nis proportional to our estimate of the parameter. This is relevant if we are trying to combine different experimental\\nresults on the lifetime of a particle. For combining procedures which weight each result by $1/\\\\sigma^2$, a \\nmeasurement where the fluctuations in the observed times result in a low estimate of $\\\\tau$\\nwill tend to be over-weighted (compare the section on `Combining Experiments\\' in Lecture 1), \\nand so the weighted average would be biassed \\ndownwards. This shows that it is better to combine different experiments at the data level, rather than \\nsimply trying to use their results.\\nOne final point to note about our simplified example is that the likelihood $L(\\\\tau)$ depends on the \\nobservations only via the {\\\\bf sum} of the times $\\\\Sigma t_i$ i.e. their {\\\\bf distribution} is \\nirrelevant. Thus the likelihood distributions for two experiments having the same number of events and the \\nsame sum of observed decay times, but with one having the decay times consistent with an exponential \\ndistribution and the other having something completely different (e.g. all decays occur at the same time), \\nwould have identical likelihood\\nfunctions. This provides an example of the fact that the unbinned likelihood function does not in general provide\\nuseful information on Goodness of Fit. \\n', \"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\", \"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\", '\\\\section{Inference}\\n\\\\subsection{Maximum likelihood estimates}\\nThe maximum likelihood method takes as best-fit values of the unknown parameter\\nthe values that maximize the likelihood function (defined Sec.~\\\\ref{sec:likeFun}).\\nThe maximization of the likelihood function can be performed analytically only in the simplest cases,\\nwhile a numerical treatment is needed in most of the realistic cases.\\n{\\\\sc Minuit}~\\\\cite{minuit} is historically the most widely used minimization software engine in High Energy Physics.\\n\\\\subsubsection{Extended likelihood function}\\nGiven a sample of $N$ measurements of the variables $\\\\vec{x}=(x_1, \\\\cdots, x_n)$, the likelihood function expresses the probability\\ndensity evaluated for our sample as a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\n\\\\prod_{i=1}^Nf(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nThe size $N$ of the sample is in many cases also a random variable. In those cases,\\nthe {\\\\it extended likelihood function} can be defined as:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) \\\\prod_{i=1}^N f(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,,\\n\\\\end{equation}\\nwhere $P(N;\\\\theta_1,\\\\cdots,\\\\theta_m)$ is the distribution of $N$, and in practice is always a Poissonian \\nwhose expected rate parameter is a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) = \\\\frac{\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)^N e^{-\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)}}{N!}\\\\,.\\n\\\\end{equation}\\nIn many cases, either with a standard or an extended likelihood function,\\nit may be convenient to use $-\\\\ln L$ or $-2\\\\ln L$ in the numerical treatment\\nrather than $L$, because\\nthe product of the various terms is transformed into the sum of the logarithms of\\nthose terms, which may have advantages in the computation.\\nFor a Poissonian process that is given by the sum of a signal plus a background process,\\nthe extended likelihood function may be written as:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\n\\\\frac{(s+b)^N e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nf_sP_s(x_i;\\\\vec{\\\\theta}) + f_b P_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $s$ and $b$ are the signal and background expected yields, respectively,\\n$f_s$ and $f_b$ are the fraction of signal and background events, namely:\\n\\\\begin{eqnarray}\\nf_s & = & \\\\frac{s}{s+b} \\\\,,\\\\\\\\\\nf_b & = & \\\\frac{b}{s+b} \\\\,,\\n\\\\end{eqnarray}\\nand $P_s$ and $P_b$ are the PDF of the variable $x$ for signal and background,\\nrespectively.\\nReplacing $f_s$ and $f_b$ into Eq.~(\\\\ref{eq:extLikSB}) gives:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) = \\\\frac{e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIt may be more convenient to use the negative logarithm of Eq.~(\\\\ref{eq:extLikInt}),\\nthat should be minimize in order to determine the best-fit values of $s$, $b$ and $\\\\vec{\\\\theta}$:\\n\\\\begin{equation}\\n-\\\\ln L(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\ns + b -\\\\sum_{i=1}^N\\\\ln\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right) +\\\\ln N!\\\\,.\\n\\\\end{equation}\\nThe last term $\\\\ln N!$ is a constant with respect to the fit parameters,\\nand can be omitted in the minimization.\\nIn many cases, instead of using $s$ as parameter of interest,\\nthe {\\\\it signal strength} $\\\\mu$ is introduced, defined by the following equation:\\n\\\\begin{equation}\\ns = \\\\mu s_0\\\\,,\\n\\\\end{equation}\\nwhere $s_0$ is the theory prediction for the signal yield $s$.\\n$\\\\mu=1$ corresponds to the nominal value of the theory prediction for the signal yield.\\nAn example of unbinned maximum likelihood fit is given in Fig.~\\\\ref{fig:sbFit},\\nwhere the data are fit with a model inspired to Eq.~(\\\\ref{eq:extLikInt}), with\\n$P_s$ and $P_b$ taken as a Gaussian and an exponential distribution, respectively.\\nThe observed variable has been called $m$ in that case because the spectrum resembles an invariant mass peak,\\nand the position of the peak at 3.1~GeV reminds a $\\\\mathrm{J}/\\\\psi$ particle.\\nThe two PDFs can be written as:\\n\\\\begin{eqnarray}\\nP_s(m) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}e^{-\\\\frac{(m-\\\\mu)^2}{2\\\\sigma^2}}\\\\,,\\\\\\\\\\nP_b(m) & = & \\\\lambda e^{-\\\\lambda m}\\\\,.\\n\\\\end{eqnarray}\\nThe parameters $\\\\mu$, $\\\\sigma$ and $\\\\lambda$ are fit together with the\\nsignal and background yields $s$ and $b$. While $s$ is our {\\\\it parameter of interest},\\nbecause we will eventually determine a production cross section or branching fraction from\\nits measurement, the other additional parameters, that are not directly\\nrelated to our final measurement, are said {\\\\it nuisance parameters}.\\nIn general, nuisance parameters are needed to model background yield,\\ndetector resolution and efficiency, various parameters modeling the\\nsignal and background shapes, etc. Nuisance parameters are also important\\nto model {\\\\it systematic uncertainties}, as will be discussed more in\\ndetails in the following sections.\\n', \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\", \"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\", \"\\\\section{Estimation}\\nWhat statisticians call `estimation',\\nphysicists would generally call `measurement'.\\nSuppose you\\nknow the probability (density) function $P(x;a)$ \\nand you \\ntake a set of data $\\\\{x_i\\\\}$. What is the best value for $a$? (Sometimes one wants to estimate a property (e.g. the mean) rather than a parameter, but \\nthis is relatively uncommon, and the methodology is the same.) \\n$x_i$ may be single values, or pairs, or higher-dimensional.\\nThe unknown\\n$a$ may be a single parameter or several. If it has more than one component, these are sometimes split into `parameters of interest' and `nuisance parameters'.\\nThe {\\\\em estimator} is defined very broadly:\\nan estimator $\\\\hat a(x_1\\\\dots x_N)$ is a function of the data that gives a value for the parameter $a$. There is no `correct' estimator, but some are better than others. A perfect estimator would be:\\n\\\\begin{itemize}\\n\\\\item\\nConsistent. $\\\\hat a(x_1 \\\\dots x_N) \\\\to a$ as $ N \\\\to \\\\infty $,\\n\\\\item\\nUnbiased: $\\\\langle \\\\hat a \\\\rangle = a $,\\n\\\\item\\nEfficient: $\\\\langle (\\\\hat a - a)^2 \\\\rangle$ is as small as possible,\\n\\\\item \\nInvariant: $\\\\hat f(a) = f(\\\\hat a)$.\\n\\\\end{itemize}\\nNo estimator is perfect---these 4 goals are incompatible. In particular the second and the fourth; if\\nan estimator $\\\\hat a$ is unbiased for $a$ then\\n$\\\\sqrt{\\\\hat a}$ is not an unbiased estimator of $\\\\sqrt a$.\\n\\\\subsection{Bias}\\nSuppose we estimate the mean by taking the obvious\\\\footnote{Note the difference between $\\\\langle x \\\\rangle$ which is an average over a PDF and $\\\\overline x$ \\nwhich denotes the average over a particular sample: both are called `the mean $x$'.} $\\\\hat \\\\mu = \\\\xbar$\\n$\\\\left< \\\\hat \\\\mu \\\\right> = \\\\left< {1 \\\\over N } \\\\sum x_i \\\\right> = {1 \\\\over N } \\\\sum \\\\mu = \\\\mu$. \\nSo there is no bias. This expectation value of this estimator of $\\\\mu$ is just $\\\\mu$ itself. By contrast suppose\\nwe estimate the variance by the apparently obvious\\n$\\\\hat V = \\\\xsqbar-\\\\xbar^2$.\\nThen $\\\\left< \\\\hat V \\\\right> = \\\\left< \\\\xsqbar \\\\right> - \\\\left< \\\\xbar^2 \\\\right>$.\\nThe first term is just $\\\\left< x^2 \\\\right>$. To make sense of the second term, note that $\\\\left< x \\\\right> = \\\\left< \\\\xbar \\\\right>$ and add and subtract $\\\\left< x \\\\right>^2$ to get\\n$\\\\left< \\\\hat V \\\\right> = \\\\left< x^2 \\\\right> - \\\\left< x \\\\right>^2 - (\\\\left< \\\\xbar^2 \\\\right>- \\\\left< \\\\xbar \\\\right>^2)$\\n$\\\\left< \\\\hat V \\\\right> =V(x)-V(\\\\xbar)=V-{V \\\\over N}={N-1 \\\\over N} V$.\\nSo the estimator is biased! $\\\\hat V$ will, on average, give too small a value.\\nThis bias, like any known bias, can be corrected for. \\nUsing $\\\\hat V = {N \\\\over N-1} (\\\\xsqbar-\\\\xbar^2)$ corrects the bias. The familiar estimator for\\nthe standard deviation follows:\\n$\\\\hat \\\\sigma=\\\\sqrt{\\\\sum_i (x_i-\\\\xbar)^2 \\\\over N-1}$. \\n(Of course this gives a biased estimate of $\\\\sigma$. But $V$ is generally more important in this context.)\\n\\\\subsection {Efficiency}\\nSomewhat surprisingly, there is a limit to the efficiency of an estimator: the\\n{\\\\em minimum variance bound} (MVB),\\nalso known as the {\\\\em Cramér-Rao bound}.\\nFor any unbiased estimator $\\\\hat a(x)$, the variance is bounded \\n\\\\begin{equation}V(\\\\hat a)\\\\geq\\n-{1 \\\\over \\\\left< {d^2 \\\\ln L \\\\over da^2}\\\\right>}\\n={1 \\\\over \\\\left<\\n\\\\left({d \\\\ln L \\\\over da }\\\\right) ^2\\n\\\\right>}\\n\\\\quad.\\n\\\\end{equation}\\n$L$ is the likelihood (as introduced in Section~\\\\ref{sec:likelihood}) of a sample of independent measurements, i.e. the\\nprobability for the whole data sample for a particular value of $a$.\\nIt is just the product of the individual probabilities:\\n$L(a;x_1,x_2,...x_N)=P(x_1;a)P(x_2;a)...P(x_N;a)$.\\nWe will write $L(a;x_1,x_2,...x_N)$ as $L(a;x)$ for simplicity.\\n\\\\begin{proof}{Proof of the MVB}\\nUnitarity requires $\\\\int P(x;a)\\\\, dx = \\\\int L(a;x) \\\\, dx =1$\\nDifferentiate wrt $a$: \\\\qquad \\\\begin{equation}\\n0=\\\\int {dL \\\\over da} \\\\, dx = \\\\int L {d \\\\ln L \\\\over da} \\\\, dx = \\\\left< {d \\\\ln L \\\\over da} \\\\right>\\n\\\\end{equation}\\nIf $\\\\hat a$ is unbiased \\n$\\\\left< \\\\hat a\\\\right> = \\\\int \\\\hat a(x) P(x;a) \\\\, dx = \\\\int \\\\hat a(x) L(a;x) \\\\, dx =a$\\nDifferentiate wrt $a$: \\\\qquad $1=\\\\int \\\\hat a(x) {dL \\\\over da} \\\\, dx = \\\\int \\\\hat a L {d \\\\ln L \\\\over da} \\\\, dx $\\nSubtract Eq.~\\\\ref{eq:one} multiplied by $a$, and get $\\\\int (\\\\hat a - a){d \\\\ln L \\\\over da} L dx =1$\\nInvoke the Schwarz inequality $\\\\int u^2 \\\\, dx \\\\int v^2 \\\\, dx \\\\geq \\\\left( \\\\int u v \\\\, dx \\\\right)^2 $ with $u\\\\equiv (\\\\hat a - a) \\\\sqrt L, v\\\\equiv {d \\\\ln L \\\\over da} \\\\sqrt L$\\nHence $\\\\int (\\\\hat a - a)^2 L \\\\, dx \\\\int \\\\left( {d \\\\ln L \\\\over da}\\\\right)^2 L \\\\, dx \\\\geq 1$\\n\\\\begin{equation} \\n\\\\left< (\\\\hat a - a)^2 \\\\right> \\\\geq 1/\\\\left<\\\\left( {d ln L \\\\over da }\\\\right)^2 \\\\right>\\n\\\\end{equation}\\n\\\\end{proof}\\nDifferentiating Eq.~\\\\ref{eq:one} again gives\\n${ d \\\\over da} \\\\int L { d \\\\ln L \\\\over da} \\\\, dx = \\\\int {d L \\\\over da} \\\\, {d \\\\ln L \\\\over da} \\\\, dx + \\\\int L {d^2 \\\\ln A \\\\over da^2} \\\\, dx\\n=\\n\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>+\\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>=0$,\\nhence \\n$\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>= - \\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>$.\\nThis is the {\\\\em Fisher information} referred to in Section~\\\\ref{sec:Jeffreys}. Note how it is intrinsically positive.\\n\\\\subsection{Maximum likelihood estimation}\\nThe {\\\\em maximum likelihood} (ML) estimator just does what it says: $a$ is adjusted to maximise the\\nlikelihood of the sample\\n(for practical reasons one actually maximises the log likelihood, which is a sum rather than a product).\\n\\\\begin{equation}\\n{\\\\rm Maximise } \\\\ln L = \\\\sum_i \\\\ln {P(x_i;a)}\\n\\\\quad,\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\left. {d \\\\ln L \\\\over d a } \\\\right|_{\\\\hat a}=0\\n\\\\quad.\\n\\\\end{equation}\\nThe \\nML estimator is very commonly used. It is not only simple and intuitive, it has lots of nice properties.\\n\\\\begin{itemize}\\n\\\\item\\nIt is consistent.\\n\\\\item\\nIt is biased, but bias falls like $1/N$.\\n\\\\item\\nIt is efficient for the large $N$.\\n\\\\item\\nIt is invariant---doesn't matter if you reparametrize $a$. \\n\\\\end{itemize}\\nA particular maximisation problem may be solved in 3 ways, depending on the complexity\\n\\\\begin{enumerate}\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} algebraically,\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} numerically, and\\n\\\\item Solve Eq.~\\\\ref{eq:logL} numerically.\\n\\\\end{enumerate}\\n\\\\subsection{Least squares}\\n{\\\\em Least squares estimation} follows from maximum likelihood estimation.\\nIf you have \\nGaussian measurements of $y$ taken at various $x$ values, with measurement error $\\\\sigma$, and a prediction $y=f(x;a)$\\nthen the Gaussian probability\\n\\\\centerline{$P(y;x,a)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-(y-f(x,a))^2/2 \\\\sigma^2}$}\\ngives the log likelihood\\n\\\\centerline{$\\\\ln L = - \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over 2 \\\\sigma_i^2} + {\\\\rm constants}$.}\\nTo maximise $\\\\ln L$, you minimise $\\\\chi^2 = \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over \\\\sigma_i^2} $, hence the name `least squares'.\\nDifferentiating gives the {\\\\em normal equations}:\\n$\\\\sum { \\\\left(y_i - f(x_i;a)\\\\right) \\\\over \\\\sigma_i^2}f'(x_i;a) =0$.\\nIf $f(x;a)$ is linear in $a$ then these can be solved exactly. Otherwise an iterative method has to be used.\\n\\\\subsection{Straight line fits}\\nAs a particular instance of least squares estimation, suppose the function is $y=mx+c$, and assume all $\\\\sigma_i$ are the same (the extension to the general case is straightforward).\\nThe normal equations are then $\\\\sum (y_i - m x_i -c) x_i = 0$ and $\\\\sum (y_i-m x_i - c ) =0$\\\\ , for which the solution, shown in Fig.~\\\\ref{fig:slfit}, is\\n\\\\noindent $m={\\\\overline{xy} - \\\\overline x \\\\ , \\\\overline y \\\\over \\\\xsqbar - \\\\xbar^2}$\\\\ , $c=\\\\overline y - m \\\\xbar$ \\\\ .\\nStatisticians call this {\\\\em regression}. Actually there is a subtle difference, as shown in Fig.~\\\\ref{fig:regression}.\\nThe straight line fit considers well-defined $x$ values and $y$ values with measurement errors---if it were not for those\\nerrors then presumably the values would line up perfectly, with no scatter. The scatter in regression is not caused by measurement errors, but by the fact that the variables are linked only loosely. \\nThe history of regression started with Galton, who measured the heights of fathers and their (adult) sons.\\nTall parents tend to have tall children so there is a correlation. Because the height of a son depends\\nnot just on his paternal genes but on many factors (maternal genes, diet, childhood illnesses $\\\\dots$), the points\\ndo not line up exactly---and using a high accuracy laser interferometer to do the measurements, rather than a simple\\nruler, would not change anything. \\nGalton, incidentally, used this to show that although \\ntall fathers tend to have tall sons, they are not that tall. An outstandingly tall father will have (on average) quite tall children, and only tallish grandchildren. He called this \\n`Regression towards mediocrity', hence the name.\\nIt is also true that tall sons tend to have tall fathers---but not that tall---and only tallish grandfathers. Regress works in both directions!\\nThus for regression there is always an ambiguity as to whether to plot $x$ against $y$ or $y$ against $x$.\\nFor a straight line fit as we usually meet them this does not arise: one variable is precisely specified and we call that one $x$, and the one with measurement errors is $y$. \\n\\\\subsection{Fitting histograms}\\nWhen fitting a histogram the error is given by Poisson statistics for the number of events in each bin.\\nThere are \\n4 methods of approaching this problem---in order of increasing accuracy and decreasing speed. It is assumed that the bin width $W$ is narrow, so that $f(x_i,a)=\\\\int_{x_i}^{x_i+W} P(x,a)\\\\, dx$ can be approximated by \\n$f_i(x_i;a)=P(x_i;a) \\\\times W$. $W$ is almost always the same for all bins,\\nbut the rare cases of variable bin width can easily be included.\\n\\\\begin{enumerate}\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2 \\\\over n_i}$. This is the simplest but clearly breaks if $n_i=0$.\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2\\\\over f_i}$ . Minimising the Pearson $\\\\chi^2$ (which {\\\\em is}\\nvalid here) avoids the division-by-zero problem. It assumes that the Poisson distribution can be approximated by a Gaussian.\\n\\\\item Maximise $\\\\ln L = \\\\sum \\\\ln(e^{-f_i} f_i^{n_i} / n_i!) \\\\sim \\\\sum n_i \\\\ln f_i - f_i$. This, known as {\\\\em binned maximum likelihood}, remedies that assumption.\\n\\\\item Ignore bins and maximise the total likelihood. Sums run over $N_{events}$ not $N_{bins}$. So if you have large data samples this is much slower. You have to use it for sparse data, but of course in such cases the sample is small and the\\ntime penalty is irrelevant.\\n\\\\end{enumerate}\\nWhich method to use is something you have to decide on a case by case basis. \\nIf you have bins with zero entries then the first method is ruled out\\n(and removing such bins from the fit introduces bias so this should not be done).\\nOtherwise, in my experience, the improvement in adopting a more complicated method tends to be small.\\n\", \"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Reconstruction}\\nRegression algorithms are another type of supervised learning,\\nproviding continuous outputs which can have any numerical value within\\na range. They can be deployed for reconstruction purposes in HEP\\ne.g. when we want to make precise determinations of continuous\\nquantities like hit positions, track momenta or jet energies.\\nAt the intensity frontier advanced detectors collect record amounts of\\nluminosity at what would be considered ``medium'' energies by today's\\nstandards. One example is the Beijing Electron Positron Collider\\n(BEPCII) running at center of mass energies 2.0--4.6 GeV. The BESIII\\nexperiment has collected record size data samples in this\\n$\\\\tau$--charm region. Advanced ML techniques have been applied for\\nmany tasks~\\\\cite{BESIII}. One of them is cluster reconstruction for\\nthe cylindrical triple-GEM inner tracker, part of the 2019 upgrade to\\nthe aging inner drift chamber. The goal is to measure the drift\\ncathode layer position of ionizing particles from the readouts of the\\nanode strips, which is the first reconstruction step. Two methods are\\navailable: weighted by electric charge average position of the anode\\nstrips (Q~method), or time measurement using the drift gap as kind of\\nmicro time projection chamber (T~method). The two methods can be\\ncombined to improve the position resolution, but this combination is\\nmade difficult by the correlations to the incident angle. Here ML\\ntechniques come to the rescue: a XGBoost regressor is developed to\\nreconstruct the initial particle positions from the Q and T\\nreadouts. Substantial improvements over the charge centroid method are\\nreported.\\nML techniques are entering in full force the ``sister'' field of\\nparticle astrophysics. One development in the field of very high\\nenergy gamma-ray astronomy is the Cherenkov Telescope Array (CTA)\\nwhich will ultimately consists of 19 telescopes in the Northern and 99\\ntelescopes in the Southern hemisphere to cover the full sky. A\\ncolossal amount of data in the multi-petabyte range per year is\\nexpected. The telescope arrays are operated as a single instrument to\\nobserve extensive air showers originating from gammas or charged\\nparticles, and aim to separate them and measure basic characteristics\\nas energy, direction and impact point of the original particle. An\\nexploratory regression study~\\\\cite{GammaLearn} in this direction uses\\nCNN with the hope to extract more information directly from the raw\\ndata and outperform traditional approaches based on human-selected\\nfeatures.\\nThe main difficulty is that conventional CNNs are developed to process\\nrectangular images with regular pixel grids. The telescope outputs\\nhere have hexagonal pixels forming hexagonal images. One, not very\\nsatisfying approach is resampling, converting the image to a standard\\none, potentially losing some information about the neighbors. This\\nanalysis takes the more difficult route of reimplementing the\\nconvolutional and pooling operations of CNNs by building matrices of\\nneighbor indices, rearranging the data accordingly and then applying\\nthe general methods for convolution, or for pooling with different\\nfunctions depending on the task ({\\\\it softmax, average, max}).\\nThe next difficulty is to combine images from several telescopes to\\nobtain stereoscopic information. Traditional DL methods\\nonly deal with single images, sequentially in time. This is solved by\\nadding a convolution block for each telescope in the array, and\\nfeeding them all to the dense fully connected part of the network. The\\nexploratory study with four telescopes shows promise in the\\nmeasurements of energy, direction and impact point for incoming\\nparticles; additional work is needed to outperform traditional methods\\nand solve technical details before applying the developed algorithm on\\nreal data.\\nTracking detectors form the core of most collider experiments, and\\nsuccessful track reconstruction is mission critical for achieving\\ntheir goals. Reconstructing tracks is a combinatorial problem,\\ni.e. finding the measurements (hits) belonging to individual particles\\nentering the detectors from an often huge set of possible\\ncombinations. With the transition to the High-Luminosity LHC (HL-LHC)\\nthe complexity of this task will increase substantially. Traditional\\napproaches like track following (inside-out or outside-in) and Kalman\\nfilters do not scale favorably to very high hit densities, and are\\ntypically custom implemented for each experiment with large amount of\\nhuman efforts.\\nThe TrackML~\\\\cite{TrackML} project has the ambition to stimulate new\\napproaches and the development of new algorithms by exposing data from\\na virtual, but realistic HL-LHC tracking detector to data science and\\ncomputer experts outside of the HEP community. Production of\\ntop-antitop quark pairs is selected for the signal events, which are\\nthen merged with 200 soft interactions (pile-up events). Fast\\nsimulation is used to generate hits in the tracker from charged\\ntracks. The magnetic field is inhomogeneous, energy loss, hadronic\\ninteractions and multiple scattering are parameterized. The silicon\\ntracker consists of three parts: innermost pixel detector, followed by\\ntwo layers of short and long silicon strips providing hermetic\\ncoverage up to $|\\\\eta|\\\\ <\\\\ 3$. For each collision, about ten thousand\\ncharged particles, originating approximately from the center of the\\ndetector, produce about ten precise hits per track in three\\ndimensions.\\nThe challenge, running on the Kaggle platform~\\\\cite{TrackMLKaggle} and\\non Codalab in 2018--2019, is split in two phases: accuracy and\\nthroughput. The first phase is scored by a specially developed metric,\\nwhich puts high priority on efficiency of finding real hits belonging\\nto a particle and low fake rates. At least 50\\\\originate from the same simulated truth particle, with hits on the\\ninnermost layers, key for good vertex resolution, and on the outermost\\nlayers, key for long lever arms and thus for good momentum resolution,\\ngetting highest weights in the overall score. A random solution will\\nget a score of zero and a perfect reconstruction of all events in the\\ntest dataset, consisting of 125 simulated events, will get a score of\\none.\\nThe challenge attracted more than 650 participants. In the accuracy\\nphase the participants provide their reconstruction of the test\\ndataset to Kaggle where it is scored. In the throughput phase the\\nparticipants provide their algorithms and software, and it is run in a\\nconsistent environment (Docker containers on two i686 processor cores\\nand 4GB of memory) to measure both accuracy and runtime, which will be\\nvery important to handle the enormous datasets expected from the\\nHL-LHC.\\nWinners~\\\\cite{TrackMLWin} of the accuracy phase are teams (with\\nscores): top-quarks(0.92219), outrunner(0.90400) and Sergey\\nGorbunov(0.89416). While training can consume lots of computer\\nresources, where machine learning really shines is the speed of\\nreconstruction once the algorithms are trained. Winners of the\\nthroughput phase are teams sgorbuno (Sergei Gorbunov), fastrack\\n(Dmitry Emelyanov) and cloudkitchen (Marcel Kunze), who were able to\\ncombine high accuracy scores with speeds well below ten seconds per\\nevent, and even below one second for the first two.\\nThe TrackML challenge shows that ML techniques like representation\\nlearning, combinatorial optimization, clustering and even time series\\nprediction can be applied to tracking. The best solutions offer a\\nsynergy between model-based and data-based approaches, combining the\\nbest of both worlds: physical track models and machine learning, with\\nsensible trade-offs between complexity and performance.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Non-profile frequentist estimator}\\nThe modified frequentist method is generalized through the Neyman-Pearson Lemma, a fundamental statistical result stating that the most powerful test for hypothesis comparison is based on minimizing the Type II error. This error occurs when the null hypothesis (\\\\( H_0 \\\\)), which assumes only background, is not rejected despite being false. The Neyman-Pearson Lemma indicates that, given a significance level, the most efficient statistical test to discriminate between two hypotheses is based on the likelihood ratio~\\\\cite{cranmer2015practical,lista2016practical,cowan2011asymptotic}.\\n\\\\begin{equation}\\nR(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)}.\\n\\\\end{equation}\\nThe formula has an asymptotic approximation to a \\\\(\\\\chi^{2}\\\\) distribution when expressed in terms of logarithms~\\\\cite{lista2016practical}. Generally, this formula is expressed as follows:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = - 2 Ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)} \\\\bigg). \\n\\\\end{equation}\\nThis expression is known as the log-likelihood ratio, and it allows for generalization to experiments with multiple channels. Consider, for example, a single-channel experiment measuring the mass of a hypothetical particle \\\\( m(\\\\rho) \\\\) within the range of 100 to 200 GeV. Suppose the observation is \\\\( n = 105 \\\\), the expected number of background events is \\\\( b = 100 \\\\), and the model for this new particle predicts \\\\( s = 10 \\\\). The statistical estimator based on this distribution model is expressed as follows:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & -2Ln \\\\bigg( \\\\frac{ e^{-(\\\\mu s + b)}(\\\\mu s + b)^{n} }{ e^{-b}b^{n} } \\\\bigg) {} \\\\nonumber \\\\\\\\\\n& = & 2 \\\\bigg( \\\\mu s - n Ln \\\\bigg( 1 + \\\\frac{\\\\mu s} {b} \\\\bigg) \\\\bigg). {}\\n\\\\end{eqnarray}\\nFigure~[\\\\ref{fig:9}] shows the single-channel experiment, where the error bars represent the Poisson uncertainty, \\\\( \\\\epsilon = \\\\sqrt{n} \\\\), associated with the observed number of events. Additionally, it describes the behavior of the estimator as a function of the signal strength, \\\\( \\\\mu \\\\). This reveals that the likelihood ratio defines a convex optimization problem with a unique global minimum, corresponding to the best fit of the model under the null hypothesis to describe the observation. Notably, both hypotheses can fit the observed data, making it essential to determine the set of theories that can be excluded based on the measurement of \\\\( n \\\\) or to assess whether there is sufficient evidence to claim the discovery of the particle in question~\\\\cite{cms2012observation,atlas2022detailed}.\\nThe best-fit value is obtained by differentiating the log-likelihood function with respect to the parameter of interest and evaluating the result at zero. In this case, the minimum can be calculated exactly using elementary methods.\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{n-b}{s} = 0.5\\n\\\\end{equation}\\nEstimating the best model is fundamental in current statistical estimators. In general, obtaining the best fit in experiments with multiple channels and systematic uncertainties requires advanced optimization processes and sampling techniques, which will be described later. Moreover, calculating upper limits involves sampling the distributions of the estimator under both the null and alternative hypotheses. The next section will address the sampling of the estimator and the definition of the confidence level for the signal, known as \\\\( CL_s \\\\).\\n\\\\subsubsection{Sampling of the log-likelihood estimator}\\nTo obtain the upper limit using the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), the estimator can be sampled under both the null and alternative hypotheses; these distributions are labeled \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), respectively. To calculate \\\\( f(\\\\mathcal{Q}|0) \\\\), a random number is generated following a Poisson distribution with \\\\( \\\\mu = 0 \\\\), representing the number of observed events under the null hypothesis. Similarly, the distribution \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\) is obtained using a specific value of \\\\( \\\\mu \\\\)~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:10}] shows a schematic of the shapes of the distributions of the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\). The value \\\\( Q_{obs} \\\\) corresponds to the estimator for the observed number of events \\\\( n \\\\). Typically, the background-only distribution is found to the right of \\\\( Q_{obs} \\\\), while the signal + background distribution is found to the left of \\\\( Q_{obs} \\\\). The degree of agreement between the observation and the models is evaluated through the confidence level, represented by the shaded areas in the plot~\\\\cite{cowan2014statistics}.\\nThe green shaded area represents the p-value of the observation under the background-only hypothesis (\\\\( H_{0} \\\\)) and is expressed as:\\n\\\\begin{equation}\\np_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q},\\n\\\\end{equation}\\nThe p-value of the null hypothesis is related to the well-known power of the test, which is the confidence level, denoted as \\\\( \\\\beta \\\\):\\n\\\\begin{equation}\\n\\\\beta = CL_{b} = 1 - p_{0} = 1 - \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nOn the other hand, the p-value for the signal + background hypothesis (\\\\( H_{1} \\\\)) is represented by the yellow shaded area, which directly corresponds to the confidence level of the \\\\( H_{1} \\\\) hypothesis:\\n\\\\begin{equation}\\np_{\\\\mu} = CL_{s+b} = \\\\int_{\\\\mathcal{Q}_{observed}}^{\\\\infty} f(\\\\mathcal{Q}/\\\\mu) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThus, the statistical significance \\\\( \\\\alpha \\\\) is a particular case of the p-value of the observation under the null hypothesis (\\\\( p_{0} \\\\)) and constitutes evidence in favor of \\\\( H_{1} \\\\) against \\\\( H_{0} \\\\). Consequently, maximizing the significance is a tool for optimizing the search window. In various statistical studies, it is suggested that stronger evidence in favor of \\\\( H_{1} \\\\) over \\\\( H_{0} \\\\) is reflected in~\\\\cite{lista2016practical,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{p_{\\\\mu}}{\\\\beta} = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}. \\n\\\\end{equation}\\nThis completely defines the confidence level of the signal. Using these definitions, the upper limit of the signal strength is obtained through the following strategy: 1) vary the signal strength \\\\( \\\\mu \\\\), 2) sample the distributions \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), 3) calculate the p-values corresponding to the observed estimator, and 4) determine the confidence level \\\\( CL_s(\\\\mu) \\\\). In this way, the upper limit \\\\( \\\\mu^{up} \\\\) for exclusion is given by:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = 0.05\\n\\\\end{equation}\\nThis process is typically computationally expensive due to the sampling of distributions for each value of \\\\( \\\\mu \\\\). In the case of multiple channels and nuisance parameter estimation, parallelization is required to obtain results efficiently. This is crucial, as optimizing the search window at the phenomenological level necessitates maximizing statistical significance or other statistical metrics~\\\\cite{florez2016probing,allahverdi2016distinguishing}.\\nIn the experiment related to the invariant mass channel \\\\( m(\\\\rho) \\\\), we have two key estimates. The first is the expected upper limit, which is assumed under the hypothesis that the observation consists of background nuisance only. In this case, the expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.19 \\\\). This implies that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Exp}} \\\\cdot s = 2.19 \\\\times 10 = 21.9 \\\\) events would be excluded, provided that \\\\( n = b \\\\) events are measured. In the second case, the observed upper limit corresponds to the actual data observation. This observed upper limit is estimated as \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.57 \\\\), meaning that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Obs}} \\\\cdot s = 2.57 \\\\times 10 = 25.7 \\\\) events would be excluded based on the observation. Figure~[\\\\ref{fig:11}] shows the search for the confidence level for both the expected and observed limits~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LEP/UpperLimitLnQ.ipynb}{Source code}}. Additionally, the distribution of the statistical estimator for \\\\( \\\\mu = \\\\mu_{up}^{\\\\text{Obs}} \\\\) is presented as an illustrative example. Finally, results obtained using the professional \\\\texttt{RooFit} package, used by the CMS and ATLAS collaborations for upper limit estimation, are included~\\\\cite{verkerke2006roofit,schott2012roostats}.\\nAn important feature in this type of analysis is that when the upper limits are statistically consistent, as in this case, there is insufficient evidence to claim a discovery. In this scenario, we would accept that the null hypothesis \\\\( H_{0} \\\\) (background-only hypothesis) adequately describes the observation. The first sign of tension between the observed data and the background-only hypothesis arises when the expected and observed upper limits differ significantly. From a statistical perspective, this discrepancy must be evaluated, and to consider the observation of a new phenomenon, the difference must exceed the \\\\( 5\\\\sigma \\\\) threshold~\\\\cite{lista2016practical,cranmer2015practical,cms2012observation}.', '\\\\section{Discoveries and upper limits}\\n\\\\subsection{Feldman--Cousins intervals}\\nA solution to the flip-flopping problem was developed by Feldman and Cousins~\\\\cite{feldman_cousins}.\\nThey proposed to select confidence interval based on a likelihood-ratio criterion.\\nGiven a value $\\\\theta=\\\\theta_0$ of the parameter of interest, the\\nFeldman--Cousins confidence interval is defined as:\\n\\\\begin{equation}\\nR_\\\\mu = \\\\left\\\\{x : \\\\frac{L(x;\\\\theta_0)}{L(x;\\\\hat{\\\\theta})} >k_\\\\alpha\\\\right\\\\}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\theta}$ is the best-fit value for $\\\\theta$ and the constant $k_\\\\alpha$\\nshould be set in order to ensure the desired confidence level $1-\\\\alpha$.\\nAn example of the confidence belt computed with the Feldman--Cousins approach\\nis shown in Fig.~\\\\ref{fig:fcBelt}\\nfor the Gaussian case illustrated in Fig.~\\\\ref{fig:flipFlop}.\\nWith the Feldman--Cousins approach, the confidence interval smoothly changes\\nfrom a fully asymmetric one, which leads to an upper limit, for low values of $x$, to\\nan asymmetric interval for higher values of $x$ interval, then finally a symmetric interval\\n(to a very good approximation) is obtained for large values of $x$, recovering\\nthe usual result as in Fig.~\\\\ref{fig:flipFlop}.\\nEven for the simplest Gaussian case, the computation of Feldman--Cousins intervals\\nrequires numerical treatment and for complex cases their computation may be very CPU intensive.\\n', \"\\\\section{Errors}\\n\\\\subsection{Errors in 2 or more dimensions}\\nFor 2 (or more) dimensions, one plots the log likelihood and defines regions using contours in $\\\\Delta \\\\ln L$ (or $\\\\Delta \\\\chi^2\\\\equiv - 2 \\\\Delta \\\\ln L$). An example is given in Fig.~\\\\ref{fig:CMS}.\\nThe link between the $\\\\Delta \\\\ln L$ values and the significance changes. \\nIn 1D, there is a 68\\\\In 2D, a 1$\\\\sigma$ square would give a probability $0.68^2 = 47\\\\If one rounds off the corners and draws a 1$\\\\sigma$ contour \\nat $\\\\Delta \\\\ln L=-\\\\half$ this falls to 39\\\\To retrieve the full 68\\\\ $\\\\Delta \\\\chi^2=2.27$.\\nFor 95\\\\\\nThe necessary value is obtained from the $\\\\chi^2$ distribution---described later. It can be found by the R function {\\\\tt qchisq(p,n)} or the Root function {\\\\tt TMath::ChiSquareQuantile(p,n)}, where the desired \\nprobability {\\\\tt p} and number of degrees of freedom {\\\\tt n} are the arguments given.\\n\\\\subsubsection{Nuisance parameters}\\nIn the example of Fig.~\\\\ref{fig:CMS}, both $C_V$ and $C_F$ are interesting.\\nBut in many cases one is interested only in one (or some) of the quantities and the others\\nare `nuisance parameters' that one would like to remove, reducing the dimensionality of the \\nquoted result. There are two methods of doing this, one (basically) frequentist and one Bayesian.\\nThe frequentist uses the {\\\\it profile likelihood} technique. Suppose that there are two parameters,\\n$a_1$ and $a_2$, where $a_2$ is a nuisance parameter, and so one wants to reduce the \\njoint likelihood function $L(x;a_1,a_2)$ to some function $\\\\hat L(a_1)$.\\nTo do this one scans across the values of $a_1$ and inserts $\\\\hat{\\\\hat a}_2(a_1)$, the value of $a_2$ which maximises the likelihood for that particular $a_1$\\n\\\\begin{equation}\\n\\\\hat L (x,a_1)=L(a_1,\\\\hat{\\\\hat a}_2(a_1))\\n\\\\end{equation}\\n\\\\noindent and the location of the maximum and the $\\\\Delta \\\\ln L=\\\\-\\\\half$ errors are\\nread off as usual. \\nTo see why this works---though this is not a very rigorous motivation---suppose one had a likelihood function\\nas shown in Fig.~\\\\ref{fig:profile}.\\nThe horizontal axis is for the parameter of interest, $a_1$, and the vertical for the nuisance parameter $a_2$.\\nDifferent values of $a_2$ give different results (central and errors) for $a_1$.\\nIf it is possible to transform to $a_2'(a_1,a_2)$ so that $L$ factorises, then we can write\\n$L(a_1,a_2')=L_1(a_1)L_2(a_2')$: this is shown in the plot on the right.\\nWe suppose that this is indeed possible. In the case here, and other not-too-complicated cases, it clearly is, although\\nit will not be so in more complicated topologies with multiple peaks. \\nThen using the transformed graph, \\nwhatever the value of $a_2'$, one would get the same result for $a_1$.\\nThen one can present this result for $a_1$, independent of anything about $a_2'$.\\nThere is no need to factorise explicitly: the\\npath of central $a_2'$ value as a function of $a_1$ (the central of the 3 lines on the right hand plot)\\nis the path of the peak, and that path can be located in the first plot (the transformation only stretches the $a_2$ axis, it does not change the heights).\\nThe Bayesian method uses the technique called {\\\\it marginalisation}, which \\njust integrates over $a_2$.\\nFrequentists can not do this as they are not allowed to integrate likelihoods over the parameter:\\n$\\\\int P(x;a)\\\\, dx$ is fine, but $\\\\int P(x;a)\\\\, da$ is off limits.\\nNevertheless this can be a very helpful alternative to profiling, specially for many nuisance parameters.\\nBut if you use it you must be aware that this is strictly Bayesian.\\nReparametrizing $a_2$ (or choosing a different prior) will give different results for $a_1$.\\nIn many cases, where the effect of the nuisance parameter is small, this does not\\nhave a big effect on the result.\\n\", \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\", \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{The Profile Likelihood}\\nAs noted in Sec.~\\\\ref{sec:likelihood}, likelihood functions can be used to estimate the parameters\\non which they depend. The method of choice to do so, in a frequentist analysis, is called \\\\textbf{maximum\\nlikelihood}, a method first used by Karl Frederick Gauss, \\\\emph{The Prince of Mathematics}, but developed into a formidable statistical tool in the 1930s by \\nSir Ronald A. Fisher~\\\\cite{Fisher}, perhaps the most influential statistician of the twentieth century.\\nFisher showed that a good way to estimate the parameters of a likelihood function is to pick\\nthe value that maximizes it. Such estimates are called \\\\textrm{maximum likelihood estimates} (MLE). In general, a function into which data can be inserted to yield an MLE of\\na parameter is called a maximum likelihood estimator. For simplicity, we shall use the \\nsame abbreviation MLE\\nto mean both the estimate and the estimator and we shall not be too\\npicky about distinguishing the two. The D\\\\O\\\\ top quark\\ndiscovery example illustrates the\\nmethod. \\n\\\\begin{quote}\\n\\\\paragraph*{Example: Top Quark Discovery Revisited}\\nWe start by listing\\n\\\\begin{align*}\\n& \\\\textbf{the knowns} \\\\\\\\\\n& D = N, B \\\\text{ where} \\\\\\\\\\n& N = 17 \\\\textrm{ observed events} \\\\\\\\\\n& B = 3.8 \\\\textrm{ estimated background events with uncertainty } \\\\delta B = 0.6 \\\\\\\\\\n&\\\\textbf{and the unknowns} \\\\\\\\\\n& b \\\\quad\\\\textrm{mean background count}\\\\\\\\\\n& s \\\\quad\\\\textrm{mean signal count}.\\n\\\\end{align*} \\nNext, we construct a probability model for the data $D = N, B$ assuming that \\n$N$ and $B$ are statistically independent. Since this is a counting\\nexperiment, we shall assume that $p(x| s, b)$ is a Poisson distribution with mean\\ncount $s + b$. In the absence of details about how the background $B$ was arrived\\nat, the standard assumption is that data of the form $y \\\\pm \\\\delta y$ can be modeled\\nwith a Gaussian (or normal) density. However, we can do a bit better. Background estimates are usually based on auxiliary experiments, either real or simulated, that define control regions. \\nSuppose that the observed count in the control region is $Q$ and the mean count is $b k$, where $k$ (ideally) is the known scale factor between the control and signal regions. We can model these data with a\\nPoisson distribution with count $Q$ and mean $b k$. But, we are given $B$ and $\\\\delta B$ rather than $Q$ and $k$, so we need a model to relate the two pairs of numbers. \\nThe simplest model is $B = Q / k$ and $\\\\delta B = \\\\sqrt{Q} / k$ from which we can infer an effective count $Q$ using $Q = (B / \\\\delta B)^2$. What of the scale factor $k$? Well, since it\\nis not given, it must be estimated. The obvious estimate is $Q / B = B / \\\\delta B^2$.\\nWith these assumptions, our likelihood function is\\n\\\\begin{eqnarray}\\np(D | s, b) & = & \\\\textrm{Poisson}(N, s + b) \\\\, \\\\textrm{Poisson}(Q, bk), \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nQ & = & (B / \\\\delta B)^2 = 41.11,\\\\nonumber\\\\\\\\\\nk & = & B / \\\\delta B^2 = 10.56. \\\\nonumber\\n\\\\end{eqnarray}\\nThe first term in Eq.~(\\\\ref{eq:toplh}) is the likelihood for the count $N = 17$, while\\nthe second term is the likelihood for $B = 3.8$, or equivalently the count $Q$. The\\nfact that $Q$ is not an integer causes no difficulty: we merely write\\nthe Poisson distribution as\\n$(bk)^Q \\\\exp(-bk) / \\\\Gamma(Q+1)$, which permits continuation to non-integer counts $Q$.\\nThe maximum likelihood estimators \\nfor $s$ and $b$ are found by maximizing Eq.~(\\\\ref{eq:toplh}), that is, by solving the equations \\n\\\\begin{align}\\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial s} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{s} = N - B, \\\\nonumber\\\\\\\\ \\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial b} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{b} = B, \\\\nonumber\\n\\\\end{align}\\nas expected.\\nA more complete analysis would account for the uncertainty in\\n$k$. One way is to introduce two more control regions with observed counts $V$ and $W$ and mean counts $v$ and $w k$, respectively, and extend \\nEq.~(\\\\ref{eq:toplh}) with two more Poisson distributions.\\n\\\\end{quote}\\n\\\\bigskip\\nThe maximum likelihood method is the most widely used method for\\nestimating parameters because it generally leads to reasonable estimates. But the\\nmethod has features, or encourages practices, which, somewhat uncharitably, we label the\\ngood, the bad, and the ugly!\\n\\\\begin{itemize}\\n\\\\item \\\\emph{The Good}\\n\\\\begin{itemize}\\n\\\\item Maximum likelihood estimators are consistent: the RMS goes to zero as more\\nand more data are included in the likelihood. This is an extremely important property,\\nwhich basically says it makes sense to take more data because we shall get more accurate results. One would not knowingly use an inconsistent estimator!\\n\\\\item If an unbiased estimator for a parameter exists the maximum\\nlikelihood method will find it.\\n\\\\item Given the MLE for $s$, the MLE for any function $y = g(s)$ of $s$ is,\\nvery conveniently, just $\\\\hat{y} = g(\\\\hat{s})$. This is a very nice practical feature which\\nmakes it possible to maximize the likelihood using the most convenient parameterization of it and then transform back to the parameter of interest at the end. \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Bad (according to some!)}\\n\\\\begin{itemize}\\n\\\\item In general, MLEs are biased.\\\\\\\\ \\\\\\\\\\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 7:} Show this\\\\\\\\\\nHint: Taylor expand $y = g(\\\\hat{s} + h)$ about the MLE $\\\\hat{s}$,\\\\\\\\\\nthen consider its ensemble average. }} \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Ugly (according to some!)}\\n\\\\begin{itemize}\\n\\\\item The fact that most MLEs are biased encourages the routine application of bias correction, which can waste data and, sometimes, yield absurdities.\\n\\\\end{itemize}\\n\\\\end{itemize}\\n\\\\noindent\\nHere is an example of the seriously ugly.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nFor a discrete probability distribution $p(k)$, the \\\\textbf{moment generating function} is the ensemble average\\n\\\\begin{align*}\\nG(x) & = < e^{xk} > \\\\\\\\\\n& = \\\\sum_{k} e^{xk} \\\\, p(k).\\n\\\\end{align*}\\nFor the binomial, with parameters $p$ and $n$, this is\\n\\\\begin{align*}\\nG(x) & = (e^x p + 1 - p)^n, \\\\quad \\\\framebox{\\\\textbf{Exercise 8a:} Show this}\\n\\\\end{align*}\\nwhich is useful for calculating \\\\textbf{moments}\\n\\\\begin{align*}\\n\\\\mu_r & = \\\\left. \\\\frac{d^rG}{dx^r}\\\\right |_{x=0} = \\\\sum_k k^r \\\\, p(k),\\n\\\\end{align*}\\ne.g., $\\\\mu_2 = (np)^2 + np - np^2$ for the binomial distribution.\\nGiven that $k$ events out $n$ pass a set of cuts, the MLE of the event selection efficiency is\\nthe obvious estimate $\\\\hat{p} = k / n$. The equally obvious estimate of $p^2$ is $( k / n)^2$.\\nBut,\\n\\\\begin{align*}\\n< ( k / n)^2 > & = p^2 + V / n , \\\\quad \\\\framebox{\\\\textbf{Exercise 8b:} Show this}\\n\\\\end{align*}\\nso $(k / n)^2$ is a biased estimate of $p^2$ with positive bias $V / n$. The unbiased estimate of $p^2$ is\\n\\\\begin{align*}\\nk(k-1) / [ n (n - 1)] , \\\\quad \\\\framebox{\\\\textbf{Exercise 8c:} Show this}\\n\\\\end{align*}\\nwhich, for a single success, i.e., $k = 1$, yields the sensible estimate $\\\\hat{p} = 1 / n$, but\\nthe less than helpful one $\\\\hat{p^2} = 0!$\\n\\\\end{quote}\\n\\\\bigskip\\nIn order to infer a value for the parameter of interest, for example, \\nthe signal $s$ in\\nour 2-parameter likelihood function in Eq.~(\\\\ref{eq:toplh}), the likelihood\\nmust be reduced to one involving the parameter of interest only, here $s$, \\nby somehow getting rid of all the \\\\textbf{nuisance} parameters, here the background\\nparameter $b$. A nuisance parameter is simply a parameter that is not of current interest.\\nIn a strict frequentist calculation, this reduction to the parameter of interest must be done\\nin such a way as to respect the frequentist principle: \\\\emph{coverage probability $\\\\geq$ confidence level}. In general, this is very difficult to do exactly.\\nIn practice, we replace all nuisance parameters by their \\\\textbf{conditional maximum likelihood\\nestimates} (CMLE). The CMLE is the maximum likelihood estimate conditional on\\na \\\\emph{given} value of the current parameter (or parameters) of interest. In the top discovery example, we construct an estimator of $b$ as a function of $s$, $\\\\hat{b}(s)$, and\\nreplace $b$ in the likelihood $p(D | s, b)$ by $\\\\hat{b}(s)$ to yield a function\\n$p_{PL}(D | s)$ called the \\\\textbf{profile likelihood}.\\n\\\\begin{quote}\\n\\\\emph{Since the profile likelihood entails an approximation, namely, replacing unknown parameters by their conditional estimates, it is not the likelihood but rather an approximation to it. Consequently, \\nthe frequentist principle is not guaranteed to be satisfied exactly.}\\n\\\\end{quote}\\nThis does not seem to be much progress. However, things are much better than they may appear because of\\nan important theorem proved by Wilks in 1938. If certain conditions are met, roughly that the\\nMLEs do not occur on the boundary of the parameter space and the likelihood becomes\\never more Gaussian as the data become more numerous --- that is, in the so-called\\n\\\\textbf{asymptotic limit}, then if the true density of $x$ is $p(x| s, b)$ the random number\\n\\\\begin{align}\\nt(x, s) & = -2 \\\\ln \\\\lambda(x, s), \\\\\\\\\\n\\\\textrm{where } \\\\lambda(x, s) & = \\\\frac{p_{PL}(x | s)}{ p_{PL}(x | \\\\hat{s})}, \\n\\\\end{align}\\nhas a probability density that converges to a $\\\\chi^2$ density with one degree of\\nfreedom. More generally, if the numerator of $\\\\lambda$ contains $m$ free parameters the\\nasymptotic density of $t$ is a $\\\\chi^2$ density with $m$ degrees of freedom. Therefore, we may take $t(D, s)$ to be a $\\\\chi^2$ variate, at least\\napproximately, and solve $t(D, s) = n^2$ for $s$ to get \\napproximate $n$-standard deviation confidence intervals. In particular, if we solve $t(D, s) = 1$, we\\nobtain approximate 68\\\\Wilks' theorem provides the main justification for using the profile likelihood. \\nWe again use the top discovery example to illustrate the procedure.\\n\\\\begin{quote} \\n\\\\paragraph*{Example: Top Quark Discovery Revisited Again}\\nThe conditional MLE of $b$ is found to be\\n\\\\begin{align}\\n\\\\hat{b}(s) & = \\\\frac{g + \\\\sqrt{g^2 + 4 (1 + k) Q s}}{2(1+k)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\ng & = N + Q - (1+k) s.\\\\nonumber\\n\\\\end{align}\\nThe likelihood $p(D | s, b)$ is shown in Fig.~\\\\ref{fig:toppl}(a) together with\\nthe graph of $\\\\hat{b}(s)$. The mode (i.e. the peak) occurs at $s = \\\\hat{s} = N - B$.\\nBy solving $$-2 \\\\ln \\\\frac{p_{PL}(17 | s)}{ p_{PL}(17 | 17 - 3.8)} = 1$$ for $s$ we get two solutions\\n$s = 9.4$ and $s = 17.7$. Therefore, we can make the statement\\n$s \\\\in [9.4, 17.7]$ at approximately 68\\\\$-\\\\ln \\\\lambda(17, s)$ created using \\nthe {\\\\tt RooFit}~\\\\cite{RooFit} and {\\\\tt RooStats}~\\\\cite{RooStats} packages.\\n\\\\smallskip\\n\\\\framebox{\\\\textbf{Exercise 9:} Verify this interval using the {\\\\tt RooFit/RooStats} package}\\n\\\\medskip\\nIntervals constructed this way are not guaranteed to\\nsatisfy the frequentist principle. In practice, however, \\ntheir coverage is very good for the typical probability models\\nused in particle physics, even for modest amounts of\\ndata. This is illustrated in Fig.~\\\\ref{fig:wilks}, which shows how rapidly the density of $t(x, s)$ \\nconverges to a $\\\\chi^2$ density for the probability distribution $p(x, y| s, b) = \\\\textrm{Poisson}(x|s+b) \\\\textrm{Poisson}(y | b)$\\\\footnote{It was the difficulty of extracting information\\nfrom this distribution that compelled the \\nauthor (against his will) to repair his parlous knowledge of statistics~\\\\cite{Fidecaro:1985cm}!}.\\nThe figure also shows what happens if we impose the restriction $\\\\hat{s} \\\\geq 0$, that is,\\nwe forbid negative signal estimates.\\n\\\\end{quote}\\n\", '\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Likelihood}\\nLet us assume that $p(x|\\\\theta)$ is a \\\\textbf{probability density function} (pdf) such that\\n$P(A| \\\\theta) = \\\\int_A p(x|\\\\theta) \\\\, dx$ is the probability of the statement $A = x \\\\in R_x$,\\nwhere $x$ denotes possible data, $\\\\theta$\\nthe parameters that\\ncharacterize the probability model, and $R_x$ is a finite set. If $x$ is discrete, then both\\n$p(x|\\\\theta)$ and $P(A|\\\\theta)$ are probabilities. The \\\\textbf{likelihood function} is\\nsimply the probability model $p(x|\\\\theta)$ evaluated at the data $x_O$ actually obtained, i.e., the function $p(x_O|\\\\theta)$.\\nThe following are examples of likelihoods.\\n\\\\begin{quote}\\n\\\\paragraph*{Example 1}\\nIn 1995, CDF and D\\\\O\\\\ discovered the top quark~\\\\cite{Abe:1995hr, Abachi:1995iq} at Fermilab. The D\\\\O\\\\\\nCollaboration found $x = D$ events ($D = 17$). For a counting experiment, the datum can be modeled using\\n\\\\begin{align*}\\np(x | d) & = \\\\textrm{Poisson}(x, d) \\\\quad \\\\textrm{probability to get $x$ events}\\n\\\\\\\\ p(D | d) & = \\\\textrm{Poisson}(D, d) \\\\quad \\\\textrm{likelihood of observation $D$ events}\\n\\\\\\\\ & = d^{D} \\\\exp(-d) / D!\\n\\\\end{align*}\\nWe shall analyze this example in detail in Lectures 2 and 3.\\n\\\\paragraph*{Example 2}\\nFigure~\\\\ref{fig:CI} shows the transverse momentum spectrum of jets in\\n$p p \\\\rightarrow \\\\textrm{jet} + X$ events measured by the CMS Collaboration~\\\\cite{Chatrchyan:2013muj}. The spectrum has $K = 20$\\nbins with total count $N$ that was modeled using the likelihood\\n\\\\begin{align*}\\np(D | p) & = \\\\textrm{Multinomial}(D, N, p), \\\\quad D = D_1,\\\\cdots,D_K, \\\\quad p = p_1,\\\\cdots,p_K\\n\\\\\\\\ \\\\sum_{i=1}^K D_i & = N. \\n\\\\end{align*}\\nThis is an example of a \\\\emph{binned} likelihood.\\n\\\\framebox{\\\\parbox{0.7\\\\textwidth}{\\\\textbf{Exercise 6a:} Show that a multi-Poisson likelihood can be\\\\ written as the\\\\\\\\ product of a multinomial and a Poisson with count $N$}}\\n\\\\paragraph*{Example 3}\\nFigure~\\\\ref{fig:type1a} shows a plot of the distance modulus versus redshift for\\n$N = 580$ Type 1a supernovae~\\\\cite{Suzuki:2011hu}. These heteroscedastic data\\\\footnote{Data in which each item, $x_i$, or group of items has a different uncertainty.} $\\\\{z_i, x_i \\\\pm \\\\sigma_i \\\\}$ are modeled\\nusing the likelihood\\n\\\\begin{align*}\\np(D | \\\\Omega_M, \\\\Omega_\\\\Lambda, Q) & = \\\\prod_{i=1}^N \\\\textrm{Gaussian}(x_i, \\\\mu_i, \\\\sigma_i),\\n\\\\end{align*}\\nwhich is an example of an \\\\emph{un-binned} likelihood. The cosmological model is\\nencoded in the distance modulus function $\\\\mu_i$, which depends on the redshift $z_i$\\nand the matter density and cosmological constant parameters $\\\\Omega_M$ and $\\\\Omega_\\\\Lambda$, respectively. (See Ref.~\\\\cite{Dungan:2009fp} for an accessible introduction to the analysis of these data.)\\n\\\\paragraph*{Example 4}\\nThe discovery of a neutral Higgs boson in 2012 by ATLAS~\\\\cite{Aad:2012tfa} and CMS~\\\\cite{Chatrchyan:2012ufa} in the di-photon final state ($p p \\\\rightarrow H \\\\rightarrow \\\\gamma\\\\gamma$)\\nmade use of an un-binned likelihood of the form,\\n\\\\begin{align*}\\np(x| s, m, w, b) & = \\\\exp[-(s + b)] \\\\prod_{i=1}^N [ s f_s(x_i | m, w) + b f_b(x_i) ] \\n\\\\\\\\\\n\\\\textrm{where } x & = \\\\textrm{di-photon masses} \\\\\\\\\\nm & = \\\\textrm{mass of boson} \\\\\\\\\\nw & = \\\\textrm{width of resonance} \\\\\\\\\\ns & = \\\\textrm{expected (i.e., mean) signal count} \\\\\\\\\\nb & = \\\\textrm{expected background count} \\\\\\\\\\nf_s & = \\\\textrm{signal probability density} \\\\\\\\\\nf_b & = \\\\textrm{background probability density} \\\\\\\\ \\\\\\\\\\n& \\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 6b:} Show that a binned multi-Poisson\\\\\\\\ likelihood yields an un-binned likelihood of this\\\\\\\\ form as the bin widths go to zero}}\\n\\\\end{align*}\\n\\\\end{quote}\\n\\\\bigskip\\nThe likelihood function is arguably the most important quantity in a statistical analysis\\nBecause it can be used to answer questions such as the following.\\n\\\\begin{enumerate}\\n\\\\item How do I estimate a parameter?\\n\\\\item How do I quantify its accuracy?\\n\\\\item How do I test an hypothesis?\\n\\\\item How do I quantify the significance of a result?\\n\\\\end{enumerate}\\nWriting down the likelihood function requires:\\n\\\\begin{enumerate}\\n\\\\item identifying all that is \\\\emph{known}, e.g., the observations,\\n\\\\item identifying all that is \\\\emph{unknown}, e.g., the parameters,\\n\\\\item constructing a probability model for \\\\emph{both}.\\n\\\\end{enumerate}\\nMany analyses in particle physics do not use likelihood functions explicitly. However, it is worth spending time to think about them because doing so encourages a deeper reflection on what is being done, a more systematic approach to the statistical analysis,\\nand ultimately leads to better answers. \\nBeing explicit about what is and is not known in an analysis problem may seem a pointless exercise;\\nsurely these things are obvious. Consider the D\\\\O\\\\ top quark discovery data~\\\\cite{Abachi:1995iq},\\n$D = 17$ events observed with a background estimate of $B = 3.8 \\\\pm 0.6$ events. The\\nuncertainty in 17 is invariably said to be $\\\\sqrt{17} = 4.1$. Not so! The count 17 is perfectly\\nknown: it is 17. What we are uncertain about is the mean count $d$, that is, the parameter\\nof the probability model, which we take to be a Poisson distribution.\\nThe $\\\\pm 4.1$ must somehow be a statement not about 17 but rather about the\\nunknown parameter $d$. We shall explain what the $\\\\pm 4.1$ means in Lecture 2.', '\\\\section{Inference}\\n\\\\subsection{Estimate of Gaussian parameters}\\nIf we have $n$ independent measurements $\\\\vec{x}=(x_1,\\\\cdots,x_n)$ all modeled (exactly or approximatively)\\nwith the same Gaussian PDF,\\nwe can write the negative of twice the logarithm of the likelihood function as follows:\\n\\\\begin{equation}\\n-2\\\\ln L(\\\\vec{x}; \\\\mu) = \\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2} + n(\\\\ln 2\\\\pi + 2\\\\ln\\\\sigma)\\\\,.\\n\\\\end{equation}\\nThe first term, $\\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2}$, is an example of $\\\\chi^2$ variable (see Sec.~\\\\ref{sec:binnedSamples}).\\nAn analytical minimization of $-2\\\\ln L$ with respect to $\\\\mu$,\\nassuming $\\\\sigma^2$ is known, gives the {\\\\it arithmetic mean} as maximum likelihood estimate of $\\\\mu$:\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i\\\\,.\\n\\\\end{equation}\\nIf $\\\\sigma^2$ is also unknown, the maximum likelihood estimate of $\\\\sigma^2$ is:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2} = \\\\frac{1}{n} \\\\sum_{i=1}^m(x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\nThe estimate in Eq.~(\\\\ref{eq:sigma2MLestimate}) can be demonstrated to have an unpleasant feature, called {\\\\it bias}, that\\nwill be discussed in Sec.~\\\\ref{sec:bias}.\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Treatment of nuisance parameters}\\nNuisance parameters have been introduced in Sec.~\\\\ref{sec:extLikFun}.\\nUsually, signal extraction procedures (either parameter fits or upper limits determinations) determine,\\ntogether with parameters of interest, also nuisance parameters that model effects\\nnot strictly related to our final measurement, like\\nbackground yield and shape, detector resolution, etc.\\nNuisance parameters are also used to model sources of systematic\\nuncertainties.\\nOften, the true value of a nuisance parameter is not known, but we may have some\\nestimate from sources that are external to our problem.\\nIn those cases, we can refer to {\\\\it nominal values} of the nuisance parameter and\\ntheir uncertainty. Nominal values of nuisance parameters are random variables\\ndistributed according to some PDF that depend on their true value.\\nA Gaussian distribution is the simplest assumption for nominal values of nuisance parameters.\\nAnyway, this may give negative values corresponding to the\\nleftmost tail, which are not suitable for\\nnon-negative quantities like cross sections.\\nFor instance, we may have an estimate of some background yield $b$ given by:\\n\\\\begin{equation}\\nb = \\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $L_{\\\\mathrm{int}}$ is the estimate of the integrated luminosity (assumed to be known\\nwith negligible uncertainty), $\\\\sigma_b$ is the background cross section evaluated\\nfrom theory, and $\\\\beta$ is a nuisance parameter, whose nominal value is equal to one, representing the\\nuncertainty on the cross-section evaluation. If the uncertainty on $\\\\beta$ is large, one may\\nhave a negative value of $\\\\beta$ with non-negligible probability, hence an unphysical negative value of\\nthe background yield $b$.\\nA safer assumption in such cases is to take a log normal distribution for the uncertain non-negative\\nquantities:\\n\\\\begin{equation}\\nb = e^\\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\beta$ is again distributed according to a normal distribution with nominal value equal to zero,\\nin this case.\\nUnder the Bayesian approach, nuisance parameters don't require a special treatment.\\nIf we have a parameter of interest $\\\\mu$ and a nuisance parameter $\\\\theta$,\\na Bayesian posterior will be obtained as (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(\\\\mu,\\\\theta|\\\\vec{x}) = \\\\frac{\\nL(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\pi(\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta^\\\\prime\\n}\\\\,.\\n\\\\end{equation}\\n$P(\\\\mu|\\\\vec{x})$ can be obtained as marginal PDF of $\\\\mu$ by integrating $P(\\\\mu,\\\\theta|\\\\vec{x})$ over $\\\\theta$:\\n\\\\begin{equation}\\nP(\\\\mu|\\\\vec{x}) = \\\\int P(\\\\mu,\\\\theta|\\\\vec{x}),\\\\mathrm{d}\\\\theta =\\n\\\\frac{\\n\\\\int L(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\\\,\\\\mathrm{d}\\\\theta\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta)\\\\pi(\\\\mu^\\\\prime,\\\\theta)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta\\n}\\\\,.\\n\\\\end{equation}\\nIn the frequentist approach, one possibility is to introduce in the likelihood\\nfunction a model for a data sample that can constrain the nuisance parameter.\\nIdeally, we may have a control sample $\\\\vec{y}$, complementary to the main\\ndata sample $\\\\vec{x}$, that only depends on the nuisance parameter, and\\nwe can write a global likelihood function as:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_y(\\\\vec{y};\\\\theta)\\\\,.\\n\\\\end{equation}\\nUsing control regions to constrain nuisance parameters is usually a good method\\nto reduce systematic uncertainties. Anyway, it may not always be\\nfeasible and in many cases we may just have information abut the nominal\\nvalue $\\\\theta^{\\\\mathrm{nom}}$ of $\\\\theta$ and its distribution obtained from a complementary measurement:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_\\\\theta(\\\\theta^{\\\\mathrm{nom}};\\\\theta)\\\\,.\\n\\\\end{equation}\\n$L_\\\\theta$ may be a Gaussian or log normal distribution in the easiest cases.\\nIn order to achieve a likelihood function that does not depend on\\nnuisance parameters, for many measurements at LEP or Tevatron a method\\nproposed by Cousins and Highland was adopted~\\\\cite{Cousins_Highlands}\\nwhich consists of integrating the likelihood function over the nuisance\\nparameters, similarly to what is done in the Bayesian approach (Eq.~(\\\\ref{eq:BayesianNuisance})).\\nFor this reason, this method was also called hybrid.\\nAnyway the Cousins--Highland does not guarantee to provide exact coverage,\\nand was often used as pragmatic solution in the frequentist context.\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote', '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\nThe simulation narrative is probably the easiest to explain and produces statistical models with the strongest logical connection to physical theory being tested. We begin with an relation that every particle physicists should know for the rate of events expected from a specific physical process\\n\\\\begin{equation}\\n\\\\textrm{rate} = \\\\textrm{(flux)} \\\\times \\\\textrm{(cross section)} \\\\times\\\\textrm{(efficiency)} \\\\times \\\\textrm{(acceptance)} \\\\;,\\n\\\\end{equation}\\nwhere the cross section is predicted from the theory, the flux is controlled by the accelerator\\\\footnote{In some cases, like cosmic rays, the flux must be estimated since the accelerator is quite far away.}, and the efficiency and acceptance are properties of the detector and event selection criteria. It is worth noting that the equation above is actually a repackaging of a more fundamental relationship. In fact the fundamental quantity that is predicted from first principles in quantum theory is the \\\\textit{scattering probability} \\\\mbox{$P(i\\\\to f)=|\\\\langle i|f\\\\rangle|^2/ (\\\\langle i|i\\\\rangle\\\\langle f | f \\\\rangle)$} inside a box of size $V$ over some time interval $T$, which is then repackaged into the Lorentz invariant form above~\\\\cite{Sredniki}.\\nIn the simulation narrative the efficiency and acceptance are estimated with computer simulations of the detector. Typically, a large sample of events is generated using Monte Carlo techniques~\\\\cite{MonteCarlo}. The Monte Carlo sampling is performed separately for the hard (perturbative) interaction (e.g. \\\\texttt{MadGraph}), the parton shower and hadronization process (e.g. \\\\texttt{Pythia} and \\\\texttt{Herwig}), and the interaction of particles with the detector (e.g. \\\\texttt{Geant}). Note, the efficiency and acceptance depend on the physical process considered, and I will refer to each such process as a \\\\textit{sample} (in reference to the corresponding sample of events generated with Monte Carlo techniques).\\nTo simplify the notation, I will define the effective cross section, $\\\\sigma_{\\\\rm eff.}$ to be the product of the total cross section, efficiency, and acceptance. Thus, the total number of events expected to be selected for a given scattering process, $\\\\nu$, is the product of the time-integrated flux or time-integrated luminosity, $\\\\lambda$, and the effective cross section\\n\\\\begin{equation}\\n\\\\nu = \\\\lambda \\\\sigma_{\\\\rm eff.}\\\\;.\\n\\\\end{equation}\\nI use $\\\\lambda$ here instead of the more common $L$ to avoid confusion with the likelihood function and because when we incorporate uncertainty on the time-integrated luminosity it will be a parameter of the model for which I have chosen to use greek letters. \\nIf we did not need to worry about detector effects and we could measure the final state perfectly, then the distribution for any observable $x$ would be given by\\n\\\\begin{equation}\\n\\\\textrm{(idealized)}\\\\hspace{2em}f(x) = \\\\frac{1}{\\\\sigma_{\\\\rm eff.}} \\\\frac{d\\\\sigma_{\\\\rm eff.}}{dx}\\\\;.\\\\hspace{5em}\\n\\\\end{equation}\\nOf course, we do need to worry about detector effects and we incorporate them with the detector simulation discussed above. From the Monte Carlo sample of events\\\\footnote{Here I only consider unweighted Monte Carlo samples, but the discussion below can be generalized for weighted Monte Carlo samples.} $\\\\{x_1, \\\\dots, x_N\\\\}$ we can estimate the underlying distribution $f(x)$ simply by creating a histogram. If we want we can write the histogram based on $B$ bins centered at $x_b$ with bin width $w_b$ explicitly as\\n\\\\begin{equation}\\n\\\\textrm{(histogram)} \\\\hspace{2em}f(x) \\\\approx h(x) = \\\\sum_{i=1}^N \\\\sum_{b=1}^B \\\\frac{ \\\\theta(|x_i-x_b|/w_b) }{N} \\\\frac{\\\\theta(|x -x_b|/w_b)}{w_b}\\\\;, \\\\end{equation}\\nwhere the first Heaviside function accumulates simulated events in the bin and the second selects the bin containing the value of $x$ in question. Histograms are the most common way to estimate a probability density function based on a finite sample, but there are other possibilities. The downsides of histograms as an estimate for the distribution $f(x)$ is that they are discontinuous and have dependence on the location of the bin boundaries. A particularly nice alternative is called kernel estimation~\\\\cite{Cranmer:2000du}. In this approach, one places a kernel of probability $K(x)$ centered around each event in the sample:\\n\\\\begin{equation} \\n\\\\textrm{(kernel estimate)}\\\\hspace{2em}f(x) \\\\approx \\\\hat{f}_0(x) = \\\\frac{1}{N} \\\\sum_{i=1}^N K\\\\left( \\\\frac{x-x_i}{h} \\\\right)\\\\;.\\\\hspace{1em}\\n\\\\end{equation}\\nThe most common choice of the kernel is a Gaussian distribution, and there are results for the optimal width of the kernel $h$. Equation~\\\\ref{eq:keys} is referred to as the fixed kernel estimate since $h$ is common for all the events in the sample. A second order estimate or adaptive kernel estimation provides better performance when the distribution is multimodal or has both narrow and wide features~\\\\cite{Cranmer:2000du}.\\n', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The distribution of the test statistic and $p$-values}\\nThe test statistic should be interpreted as a single real-valued number that represents the outcome of the experiment. More formally, it is a mapping of the data to a single real-valued number: \\\\mbox{$\\\\tilde{q}_\\\\mu: \\\\datasim,\\\\globs \\\\rightarrow \\\\mathbb{R}$}. For the observed data the test statistic has a given value, eg. $\\\\tilde{q}_{\\\\mu,\\\\rm obs}$. If one were to repeat the experiment many times the test statistic would take on different values, thus, conceptually, the test statistic has a distribution. Similarly, we can use our model to generate pseudo-experiments using Monte Carlo techniques or more abstractly consider the distribution. Since the number of expected events $\\\\nu(\\\\mu,\\\\vec\\\\theta)$ and the distributions of the discriminating variables $f_c(x_c|\\\\mu,\\\\vec\\\\theta)$ explicitly depend on $\\\\vec\\\\theta$ the distribution of the test statistic will also depend on $\\\\vec\\\\theta$. Let us denote this distribution \\n\\\\begin{equation}\\nf(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\;,\\n\\\\end{equation}\\nand we have analogous expressions for each of the test statistics described above.\\nThe $p$-value for a given observation under a particular hypothesis ($\\\\mu,\\\\vec\\\\theta$) is the probability for an equally or more `extreme' outcome than observed assuming that hypothesis \\n\\\\begin{equation}\\np_{\\\\mu,\\\\vec\\\\theta} = \\\\int_{\\\\tilde{q}_{\\\\mu,\\\\rm obs}}^\\\\infty f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\, d\\\\tilde{q}_\\\\mu\\\\;.\\n\\\\end{equation}\\nThe logic is that small $p$-values are evidence against the corresponding hypothesis. In Toy Monte Carlo approaches, the integral above is really carried out in the space of the data $\\\\int d\\\\datasim d\\\\globs$.\\nThe immediate difficulty is that we are interested in $\\\\mu$ but the $p$-values depend on both $\\\\mu$ and $\\\\vec\\\\theta$. In the frequentist approach the hypothesis $\\\\mu=\\\\mu_0$ would not be rejected unless the $p$-value is sufficiently small \\\\textit{for all} values of $\\\\vec\\\\theta$. Equivalently, one can use the supremum $p$-value for over all $\\\\vec\\\\theta$ to base the decision to accept or reject the hypothesis at $\\\\mu=\\\\mu_0$.\\n\\\\begin{equation}\\np^{\\\\rm sup}_{\\\\mu} = \\\\sup_{\\\\vec\\\\theta}\\\\; p_{\\\\mu,\\\\vec\\\\theta} \\n\\\\end{equation}\\nThe key conceptual reason for choosing the test statistics based on the profile likelihood ratio is that asymptotically (ie. when there are many events) the distribution of the profile likelihood ratio \\\\mbox{$\\\\lambda(\\\\mu=\\\\mu_{\\\\rm true})$} is independent of the values of the nuisance parameters. This follows from Wilks's theorem. In that limit $p^{\\\\rm sup}_{\\\\mu} = p_{\\\\mu,\\\\vec\\\\theta}$ for all $\\\\vec\\\\theta$. \\nThe asymptotic distributions \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu, \\\\vec\\\\theta)$} and \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu', \\\\vec\\\\theta)$} are known and described in Sec.~\\\\ref{sec:asymptotic}. For results based on generating ensembles of pseudo-experiements using Toy Monte Carlo techniques does not assume the form of the distribution $f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta)$, but knowing that it is approximately independent of $\\\\vec\\\\theta$ means that one does not need to calculate $p$-values for all $\\\\vec\\\\theta$ (which is not computationally feasible). Since there may still be some residual dependence of the $p$-values on the choice of $\\\\vec\\\\theta$ we would like to know the specific value of $\\\\vec\\\\theta^{\\\\rm sup}$ that produces the supremum $p$-value over $\\\\vec\\\\theta$. Since larger $p$-values indicate better agreement of the data with the model, it is not surprising that choosing $\\\\vec\\\\theta^{\\\\rm sup}=\\\\hathatthetamu$ is a good estimate of $\\\\vec\\\\theta^{\\\\rm sup}$. This has been studied in detail by statisticians, and is called the Hybrid Resampling method and is referred to in physics as the `profile construction'~\\\\cite{Feldman,Cranmer,hybridResampling,Bodhi}.\\nBased on the discussion above, the following $p$-value is used to quantify consistency with the hypothesis of a signal strength of $\\\\mu$:\\n\\\\begin{equation}\\np_{\\\\mu}=\\\\int_{\\\\tilde q_{\\\\mu,\\\\rm obs}}^{\\\\infty} f(\\\\tilde q_\\\\mu|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu,\\\\textrm{obs})) \\\\,d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nA standard 95\\\\\\nTo calculate the $CL_s$ upper limit, we define $p'_\\\\mu$ as a ratio of p-values,\\n\\\\begin{equation}\\np'_\\\\mu=\\\\frac{p_\\\\mu}{1-p_b} \\\\; ,\\n\\\\end{equation}\\nwhere $p_b$ is the $p$-value derived from the same test statistic under the background-only hypothesis\\n\\\\begin{equation}\\np_b=1-\\\\int_{\\\\tilde q_{\\\\mu,obs}}^\\\\infty f(\\\\tilde q_\\\\mu|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nThe $CL_s$ upper-limit on $\\\\mu$ is denoted $\\\\mu_{up}$ and obtained by solving for $p'_{\\\\mu_{up}}=5\\\\It is worth noting that while confidence intervals produced with the ``CLs'' method over cover, a value of $\\\\mu$ is regarded as excluded at the 95\\\\ \\nFor the purposes discovery one is interested in compatibility of the data with the background-only hypothesis. Statistically, a discovery corresponds to rejecting the background-only hypothesis. This compatibility is based on the following $p$-value\\n\\\\begin{equation}\\np_0=\\\\int_{\\\\tilde q_{0,obs}}^\\\\infty f(\\\\tilde q_0|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_0 \\\\;.\\n\\\\end{equation}\\nThis $p$-value is also based on the background-only hypothesis, but the test statistic $\\\\tilde q_0$ is suited for testing the background-only while the test statistic $\\\\tilde{q}_\\\\mu$ in Eq.~\\\\ref{eq:pb} is suited for testing a hypothesis with signal.\\nIt is customary to convert the background-only $p$-value into the quantile (or ``sigma'') of a unit Gaussian. This conversion is purely conventional and makes no assumption that the test statistic $q_0$ is Gaussian distributed. The conversion is defined as:\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1-p_0) ;\\\\,\\n\\\\end{equation}\\nwhere $\\\\Phi^{-1}$ is the inverse of the cumulative distribution for a unit Gaussian. One says the significance of the result is $Z\\\\sigma$ and the standard discovery convention is $5\\\\sigma$, corresponding to $p_0=2.87 \\\\cdot 10^{-7}$.\\n\", \"\\\\section{Experimental sensitivity using the $\\\\mathcal{Q}$ estimator}\\nThe experimental sensitivity to detect a new physics signal depends on multiple factors, including the accelerator's integrated luminosity, data quality, detector triggers, simulations, and accurate estimation of events associated with known physics (background)~\\\\cite{barlow2002systematic}. Given the vast parameter space and variety of theories, there arises a need to identify a specific search region to efficiently focus experimental efforts towards generating new discoveries~\\\\cite{casadei2011statistical}.\\nPhenomenology in high-energy physics (HEP) defines this search region as the signal region, determined by the expected number of new physics events for certain observables, such as invariant mass, transverse mass, etc. A common strategy to identify this region involves maximizing the statistical significance of observing \\\\( n = b + \\\\mu s \\\\), with \\\\( \\\\mu = 1 \\\\) events consistent with the new theory, assuming the background-only hypothesis (\\\\( H_{0} \\\\)) is true. This expected number of events is referred to as Asimov data~\\\\cite{lista2016practical,cowan2011asymptotic}, and it is used to determine the parameter space window of the theory measurable in the experiment with a specific luminosity. The statistical estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), with expected value \\\\( n = s + b \\\\), is given by:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & 2( \\\\mu s - nLn(1 + \\\\frac{\\\\mu s}{b}) ) {} \\\\nonumber \\\\\\\\\\n\\\\mathcal{Q}(1) & = & 2( s - (s+b)Ln(1 + \\\\frac{s}{b}) ). {}\\n\\\\end{eqnarray}\\nFrom this value, \\\\( \\\\mathcal{Q}_{\\\\text{obs}}(1) \\\\) is calculated, which allows estimating the significance in the context of a distribution corresponding to background-only hypothesis:\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{\\\\text{obs}}(1)} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThe optimal signal region for the experimental search is determined by finding the expected number of new physics events that maximizes the statistical significance. Thus:\\n\\\\begin{equation}\\ns_{best} = \\\\max_{s} \\\\alpha(s).\\n\\\\end{equation}\\nThis value of \\\\( \\\\alpha \\\\) is converted into units of standard deviations from a \\\\( \\\\mathcal{N}(0,1) \\\\) distribution. For now, this estimate assumes that the number of background events is well-determined, with no associated statistical or systematic error. Systematic effects on the background can modify both the signal region and the upper limits of theoretical predictions, and must be taken into account in phenomenological studies. Figure~[\\\\ref{fig:12}] illustrates the distribution \\\\( f(\\\\mathcal{Q},0) \\\\) with \\\\( Q_{obs}(1) \\\\), which is used to estimate the significance of the new physics model with \\\\( s=10 \\\\) and \\\\( b=100 \\\\) background events. The value of \\\\( p_{0} \\\\) is \\\\( \\\\alpha = 0.1705 \\\\), which corresponds to \\\\( Z_{0} = 0.952 \\\\) standard deviations. This result can also be approximated using the estimator~\\\\cite{florez2016probing,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nZ_{0} = \\\\frac{s}{\\\\sqrt{s+b}} \\\\approx 0.953.\\n\\\\end{equation}\\nThis estimator is considered an approximate estimation of the signal significance and is valid when \\\\( s \\\\ll b \\\\) (Appendix~\\\\ref{sec:AppendixB}). It is worth noting that this estimator has been widely used in optimizing search regions beyond the Standard Model in the context of phenomenology and experimental analysis~\\\\cite{florez2016probing,allahverdi2016distinguishing,cms2012observation,atlas2012observation}.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Bayesian `credible intervals'}\\nA Bayesian has no problems saying `It will probably rain tomorrow'\\nor `The probability that $124.85 < M_H < 125.33$~GeV is 68\\\\downside, of course, is that another Bayesian can say `It will probably not rain tomorrow' and \\n`The probability that $124.85 < M_H < 125.33~GeV$ is 86\\\\cannot resolve their subjective difference in any objective way.\\nA Bayesian has a prior belief PDF $P(a)$ and defines a region $R$ such that $\\\\int_R P(a)\\\\, da = CL$.\\nThere is the same ambiguity regarding choice of content (68\\\\even if their meaning is different.\\nThere are two happy coincidences.\\nThe first is that \\nBayesian credible intervals on Gaussians, with a flat prior, are the same as Frequentist confidence intervals. If \\nF quotes 68\\\\their results will agree.\\nThe second is that although the Frequentist Poisson upper limit is given by $\\\\sum_{r=0}^{r=r_{data}} e^{-a_{hi}} a_{hi}^r / r!$ whereas the \\nBayesian Poisson flat prior upper limit is given by $\\\\int_0^{a_{hi}} e^{-a} a^{r_{data}} / r_{data}!\\\\, da$,\\nintegration by parts of the Bayesian formula gives a series which is same as the Frequentist limit.\\nA Bayesian will also say : `I see zero events---the probability is 95\\\\ This is (I think) a coincidence---it does not apply for lower limits. But it does avoid heated discussions as to which value to publish.\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Measurement as parameter estimation}\\nOne of the most common tasks of the working physicist is to estimate some model parameter. We do it so often, that we often don't realize it. For instance, the sample mean $\\\\bar{x} = \\\\sum_{e=1}^n x_e / n$ is an estimate for the mean, $\\\\mu$, of a Gaussian probability density $f(x|\\\\mu,\\\\sigma) =\\\\Gauss(x|\\\\mu,\\\\sigma)$. More generally, an \\\\textit{estimator} $\\\\hat{\\\\alpha}(\\\\data)$ is some function of the data and its value is used to estimate the true value of some parameter $\\\\alpha$. There are various abstract properties such as variance, bias, consistency, efficiency, robustness, etc~\\\\cite{James}. The bias of an estimator is defined as $B(\\\\hat\\\\alpha) = E[ \\\\hat\\\\alpha ]-\\\\alpha$, where $E$ means the expectation value of \\\\mbox{$E[ \\\\hat\\\\alpha ]=\\\\int\\\\hat\\\\alpha(x) f(x)dx$} or the probability-weighted average. Clearly one would like an unbiased estimator. The variance of an estimator is defined as $var[\\\\hat\\\\alpha] = E[ (\\\\alpha - E[\\\\hat{\\\\alpha}] )^2 ]$; and clearly one would like an estimator with the minimum variance. Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cram\\\\'er-Rao bound (that is the inverse of the Fisher information, which we will refer to again later). \\nThe most widely used estimator in physics is the maximum likelihood estimator (MLE). It is defined as the value of $\\\\alpha$ which maximizes the likelihood function $L(\\\\alpha)$. Equivalently this value, $\\\\hat{\\\\alpha}$, maximizes $\\\\log L(\\\\alpha)$ and minimizes $-\\\\log L(\\\\alpha)$. The most common tool for finding the maximum likelihood estimator is \\\\texttt{Minuit}, which conventionally minimizes $-\\\\log L(\\\\alpha)$ (or any other function)~\\\\cite{James:1975dr}. The jargon is that one `fits' the function and the maximum likelihood estimate is the `best fit value'. \\nWhen one has a multi-parameter likelihood function $L(\\\\vec{\\\\alpha})$, then the situation is slightly more complicated. The maximum likelihood estimate for the full parameter list, $\\\\hat{\\\\vec{\\\\alpha}}$, is clearly defined. The various components $\\\\hat{\\\\alpha}_p$ are referred to as the \\\\textit{unconditional maximum likelihood estimates}. In the physics jargon, one says all the parameters are `floating'. One can also ask about maximum likelihood estimate of $\\\\alpha_p$ is with some other parameters $\\\\vec{\\\\alpha}_o$ fixed; this is called the \\\\textit{conditional maximum likelihood estimate} and is denoted $\\\\hat{\\\\hat{\\\\alpha}}_p(\\\\vec{\\\\alpha}_o)$. These are important quantities for defining the profile likelihood ratio, which we will discuss in more detail later. The concept of variance of the estimates is also generalized to the covariance matrix $cov[\\\\alpha_p, \\\\alpha_{p'}] = E[(\\\\hat\\\\alpha_p - \\\\alpha_p)(\\\\hat\\\\alpha_{p'}- \\\\alpha_{p'})]$ and is often denoted $\\\\Sigma_{pp'}$. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie. $cov[\\\\alpha_p, \\\\alpha_{p}] = var[\\\\alpha_p]$.\\nIn the case of a Poisson model $\\\\Pois(n|\\\\nu)$ the maximum likelihood estimate of $\\\\nu$ is simply \\\\mbox{$\\\\hat{\\\\nu}=n$}. Thus, it follows that the variance of the estimator is $var[\\\\hat{\\\\nu}]=var[n]=\\\\nu$. Thus if the true rate is $\\\\nu$ one expects to find estimates $\\\\hat{\\\\nu}$ with a characteristic spread around $\\\\nu$; it is in this sense that the measurement has a estimate has some uncertainty or `error' of $\\\\sqrt{n}$. We will make this statement of uncertainty more precise when we discuss frequentist confidence intervals.\\nWhen the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution.\\\\footnote{There are various conditions that must be met for this to be true, but skip the fine print in these lectures. There are two conditions that are most often violated in particle physics, which will be addressed later.} This does not depend on the pdf $f(x)$ having a Gaussian form. For small samples this isn't the case, but this limiting distribution is often referred to as an \\\\textit{asymptotic distribution}.\\nFurthermore, under most circumstances in particle physics, the maximum likelihood estimate approaches the minimum variance or Cram\\\\'er-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by\\n\\\\begin{equation}\\n\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = E\\\\left[- \\\\frac{\\\\partial^2 \\\\log f(x|\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\middle| \\\\;\\\\vec\\\\alpha \\\\right ] \\\\;,\\n\\\\end{equation}\\nwhere I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the true value $\\\\vec\\\\alpha$. The right side of Eq.~\\\\ref{Eq:expfisher} is called the (expected) Fisher information matrix. Remember that the expectation involves an integral over the observables. Since that integral is difficult to perform in general, one often uses the observed Fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data\\n\\\\begin{equation}\\n\\\\tilde\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = - \\\\frac{\\\\partial^2 \\\\log L(\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\; .\\n\\\\end{equation}\\nThis is what \\\\texttt{Minuit}'s \\\\texttt{Hesse} algorithm\\\\footnote{The matrix is called the Hessian, hence the name.} calculates to estimate the covariance matrix of the parameters.\\n\", '\\\\section{Introduction}\\nParticle physics is the study of the subatomic constituents of matter: How many are there? What are their properties? How do they interact? There are two basic approaches to answering these questions: a theoretical one and an experimental one. On the theoretical side, we can ask: what possible subatomic particles could there be? Remarkably, there are constraints due to theoretical consistency of the underlying theory that strongly limit the types of particles possible. For example, there is a direct logical path from the requirement that things do not start appearing out of nowhere (``unitarity\\'\\') to the Pauli exclusion principle, which keeps matter from imploding. To the dismay of many theorists, however, there seem to be many more self-consistent theories than the one describing nature, and so experiments are essential. \\nThe state-of-the art particle experiment is the Large Hadron Collider (LHC) at CERN on the border between France and Switzerland. \\nIts major success so far is finding the Higgs boson in 2012. The LHC collides protons together at close to the speed of light. This energy is then converted into mass via $E=mc^2$ thereby forming new particles.\\nUsually these particles last for only fractions of a second (the lifetime of the Higgs boson is $10^{-22}$ s); hence, the art of modern experimental particle physics involves finding indications that a particle was made even though we never actually see it. The experimental challenge is complicated by the fact that particles of interest are usually quite rare and look nearly identical to much more common backgrounds. For example, only one in every billion proton collisions at LHC produces a Higgs boson, and only one in ten thousand of these is easy to see. Finding new particles in modern experiments is like finding a particular piece of hay in a haystack.\\nLuckily, hay-in-a-haystack problems are exactly what modern machine learning excels at solving. \\nThere are two aspects of particle physics that make it unique, or at least highly atypical, as compared to other fields where machine learning is applied. First, particle physics is governed by quantum mechanics. Of course, everything is governed by quantum mechanics, but in particle physics the inherent uncertainty of the quantum mechanical world affects the nature of the truth we might hope to learn. Just like how \\nSchr\\\\\"odinger\\'s cat can be alive and dead at the same time, a collision at the LHC can both produce a Higgs boson and not produce a Higgs boson at the same time. In fact, there is quantum mechanical interference between the signal process (protons collide and a Higgs boson is produced) and a background process (protons collide without producing a Higgs). The question ``Was there a Higgs boson in this event?\\'\\' is unanswerable. To be a little more precise, for a given number of particles $n$ produced, the probability distribution for signal and background, differential in the momenta of the particles produced (the phase space) has the form\\n\\\\begin{equation}\\nd P_{\\\\text{data}}^n = |{\\\\mathcal M}_S +{\\\\mathcal M}_B|^2 dp_1 \\\\cdots dp_n \\n\\\\end{equation}\\nHere, ${\\\\mathcal M}_S$ and ${\\\\mathcal M}_B$ are the quantum-mechanical amplitudes ($S$-matrix elements, which are complex numbers) for producing signal and background, and the cross term ${\\\\mathcal M}_S {\\\\mathcal M}_B^\\\\star + {\\\\mathcal M}_B {\\\\mathcal M}_S^\\\\star$ represents the interference. This interference term can be positive (constructive interference) or negative (destructive interference).\\nAlthough an individual event cannot be assigned a truth label, the probability of finding a certain set of particles showing up in the detector depends on whether the Higgs boson exists:\\nfinding the Higgs boson amounts to excluding the background-only hypothesis (Eq.~\\\\eqref{Pdata1} with ${\\\\mathcal M}_S=0$). \\nIn practice, the probability distribution of signal is often strongly peaked, due to a resonance for example, in some small region of phase space. In such regions, background can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_S$. In complementary regions, signal can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_B$. Thus it is commonplace to approximate the full probability distribution with a mixture model. If we sum over possible numbers $n$ of particles and integrate over the momenta in the observable region, we can then write\\n\\\\begin{equation}\\nP_{\\\\text{data}} = \\\\alpha_S \\\\, P_{\\\\text{S}} + \\\\alpha_B \\\\, P_{\\\\text{B}}\\n\\\\end{equation}\\nwith $\\\\alpha_S + \\\\alpha_B=1$. \\nThat is, we treat the probability distribution of the data as a linear combination of the probability distributions for signal and background. The goal is then to determine the coefficients $\\\\alpha_S$ and $\\\\alpha_B$, or often more succinctly, whether $\\\\alpha_S$ is non-zero. \\nEach measured event gives us a number of particles $n$ and point in $n$-particle phase space $\\\\{p_i\\\\}$, with some uncertainty or binning $dp_1\\\\cdots d p_n$,\\ndrawn from the true probability distribution $d P^n_{\\\\text{data}}$.\\nOnly after many draws can we hope to constrain $\\\\alpha_S$. Even within the mixture model approximation, there is still no truth label for individual events. \\nThis is different from, say, distinguishing cats from dogs (or alive cats from dead cats) in an image database. For cats and dogs, even if the distributions overlap, there is a correct answer ($\\\\alpha_S=1$ or $0$ for each event). For particle physics, where the distributions overlap, a particle is both signal {\\\\it and} background.\\nThe second way in which particle physics differs from typical machine learning applications is that particle physics has remarkably accurate simulation tools for producing synthetic data for training. These tools have been developed by experts over more than 40 years. Together, they describe the evolution of a particle collision over 20 orders of magnitude in length. The smallest scale the LHC probes is around $10^{-18}$ m, one thousandth the size of a proton. Here the physics is described by perturbative quantum field theory; particles interact rather weakly and first-principles calculations are accurate. The Higgs boson has a size (Compton wavelength) of $10^{-17}$ m, so it is only at these small distances that we have any hope of examining it. Between $10^{-18}$ m and $10^{-15}$ m, a semi-classical Markov model is used to turn a handful of primordial particles into hundreds of quarks and gluons. Between $10^{-15}$ m and $10^{-6}$ m, the quarks and gluons turn into a zoo of metastable subatomic particles that subsequently decay into hundreds of ``stable\\'\\' particles: pions, protons, neutrons, electrons and photons. These then start interacting with detector components and propagating through the material, as described by other excellent parameterized models. The detector model is accurate from $10^{-6}$ m to the $100$ m size of the LHC detectors (the ATLAS detector at the LHC is 46 meters long). The result is a progression from an order-10 dimensional phase space at the shortest distances, to an order-$10^3$ dimensional phase space at intermediate scales, to an order-$10^8$ dimensional phase space of electronic detector readouts channels. Combined, these simulation tools give a phenomenally robust (but embarrassingly sparse) sampling from this hundred million dimensional space. Around one trillion events have been recorded at the LHC, and a comparable number of events have been simulated, providing hundreds of petabytes of actual and synthetic data to analyze. The first stage of the simulations, up to the stable particle level (the $10^3$ dimensional space) is relatively fast: one million events can be generated in an hour or so on a laptop.\\nThe second stage of the simulation, through the detector, is much slower, taking seconds or even minutes per event. Conveniently, for many applications, the first stage of the simulation is sufficient. \\nNo human being, and as yet no machine, can visualize a hundred million dimensional distribution. So the typical analysis pipeline is to take all of the low-level outputs and aggregate them into a single composite feature, such as the total energy of the particles in some region. Ideally, a histogram of this feature would exhibit a resonance peak or some other salient indication of signal. We additionally want this feature to have a simple physical interpretation, so that we can cross-check the distribution against our intuition. For the Higgs boson, the ``golden discovery channel\\'\\' was two muons (or electrons) and two antimuons (or positrons). A Feynman diagram describing this process looks like\\n\\\\begin{equation}\\n{\\n\\\\parbox{10mm} {\\n\\\\includegraphics[width=0.6\\\\columnwidth,trim = {100 0 00 0}]{hmmmm.pdf}\\n}\\n}\\n\\\\end{equation}\\nThe invariant mass $m=\\\\sqrt{(E_1+E_2+E_3+E_4)^2 - (\\\\vec{p}_1+\\\\vec{p}_2+\\\\vec{p}_3+\\\\vec{p}_4)^2}$, with $E_i$ the energies and $\\\\vec{p}_i$ the momenta,\\nof the four observed particles is a powerful way to discover the signal.\\nFor the Higgs boson signal, the probability density of this feature has a peak at the Higgs boson mass of 125 GeV where the background is very small. Unfortunately, only one in every $10^{13}$ proton collisions will give such a signal. If we do not demand that our signal be background-free, and we also do not demand having any physical interpretation such as we have for a feature like mass, \\nthen we can ask: what feature is the {\\\\it optimal} way of statistically discriminating a signal from its background? Such questions, when supplemented with the enormous amounts of easily produced synthetic data, are ideally suited to modern machine learning methodology.\\n', '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Incorporating Monte Carlo statistical uncertainty on the histogram templates}\\nThe histogram based approach described above are based Monte Carlo simulations of full detector simulation. These simulations are very computationally intensive and often the histograms are sparsely populated. In this case the histograms are not good descriptions of the underlying distribution, but are estimates of that distribution with some statistical uncertainty. Barlow and Beeston outlined a treatment of this situation in which each bin of each sample is given a nuisance parameter for the true rate, which is then fit using both the data measurement and the Monte Carlo estimate~\\\\cite{Barlow:1993dm}. This approach would lead to several hundred nuisance parameters in the current analysis. Instead, the \\\\texttt{HistFactory} employs a lighter weight version in which there is only one nuisance parameter per bin associated with the total Monte Carlo estimate and the total statistical uncertainty in that bin. If we focus on an individual bin with index $b$ the contribution to the full statistical model is the factor\\n\\\\begin{equation}\\n\\\\Pois(n_b | \\\\nu_b(\\\\vec\\\\alpha) + \\\\gamma_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)) \\\\, \\\\Pois(m_b | \\\\gamma_b \\\\tau_b) \\\\;,\\n\\\\end{equation}\\nwhere $n_b$ is the number of events observed in the bin, $\\\\nu_b(\\\\vec\\\\alpha)$ is the number of events expected in the bin where Monte Carlo statistical uncertainties need not be included (either because the estimate is data driven or because the Monte Carlo sample is sufficiently large), $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)$ is the number of events estimated using Monte Carlo techniques where the statistical uncertainty needs to be taken into account. Both expectations include the dependence on the parameters $\\\\vec\\\\alpha$. The factor $\\\\gamma_b$ is the nuisance parameter reflecting that the true rate may differ from the Monte Carlo estimate $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) $ by some amount. If the total statistical uncertainty is $\\\\delta_b$, then the relative statistical uncertainty is given by $\\\\nu_b^{\\\\rm MC}/\\\\delta_b$. This corresponds to a total Monte Carlo sample in that bin of size $m_b = (\\\\delta_b/\\\\nu_b^{\\\\rm MC})^2$. Treating the Monte Carlo estimate as an auxiliary measurement, we arrive at a Poisson constraint term $ \\\\Pois(m_b | \\\\gamma_b \\\\tau_b)$, where $m_b$ would fluctuate about $\\\\gamma_b \\\\tau_b$ if we generated a new Monte Carlo sample. Since we have scaled $\\\\gamma$ to be a factor about 1, then we also have $\\\\tau_b=(\\\\nu_b^{\\\\rm MC}/\\\\delta_b)^2$; however, $\\\\tau_b$ is treated as a fixed constant and does not fluctuate when generating ensembles of pseudo-experiments.\\nIt is worth noting that the conditional maximum likelihood estimate $\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha)$ can be solved analytically with a simple quadratic expression.\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha) = \\\\frac{-B + \\\\sqrt{B^2 - 4 AC}}{2A} \\\\;,\\n\\\\end{equation}\\nwith\\n\\\\begin{equation}\\nA = \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)^2 + \\\\tau_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nB= \\\\nu_b(\\\\vec\\\\alpha) \\\\tau + \\\\nu_b(\\\\vec\\\\alpha) \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - n_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - m_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nC= m_b \\\\nu_b(\\\\vec\\\\alpha) \\\\;.\\n\\\\end{equation}\\nIn a Bayesian technique with a flat prior on $\\\\gamma_b$, the posterior distribution is a gamma distribution. Similarly, the distribution of $\\\\hat\\\\gamma_b$ will take on a skew distribution with an envelope similar to the gamma distribution, but with features reflecting the discrete values of $m_b$. Because the maximum likelihood estimate of $\\\\gamma_b$ will also depend on $n_b$ and $\\\\hat{\\\\vec\\\\alpha}$, the features from the discrete values of $m_b$ will be smeared. This effect will be more noticeable for large statistical uncertainties where $\\\\tau_b$ is small and the distribution of $\\\\hat\\\\gamma_b$ will have several small peaks. For smaller statistical uncertainties where $\\\\tau_b$ is large the distribution of $\\\\hat\\\\gamma_b$ will be approximately Gaussian.', '\\\\section{Errors}\\n\\\\subsection{Errors from likelihood}\\nFor large $N$, the $\\\\ln L(a,x)$ curve is a parabola, as shown in Fig.~\\\\ref{fig:ML}.\\nAt the maximum, a Taylor expansion gives $\\\\ln L(a)=\\\\ln L(\\\\hat a)+{1 \\\\over 2} (a-\\\\hat a)^2 {d^2 \\\\ln L \\\\over da^2}$ $\\\\dots$\\nThe maximum likelihood estimator saturates the MVB, so \\n\\\\begin{equation}\\nV_{\\\\hat a}=-1/\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>\\n\\\\qquad \\\\sigma_{\\\\hat a}=\\\\sqrt{-{1 \\\\over {d^2 \\\\ln L \\\\over da^2}}}\\n\\\\quad.\\n\\\\end{equation}\\nWe approximate the expectation value $\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>$ by the actual value in this case\\n$ \\\\left. d^2 \\\\ln L \\\\over da^2 \\\\right|_{a=\\\\hat a}$ (for a discussion of the introduced inaccuracy, see Ref.~\\\\cite{DeltaML}).\\nThis can be read off the curve, as also shown in Fig.~\\\\ref{fig:ML}. The maximum gives the estimate.\\nYou then draw a line $1\\\\over 2$ below that (of course nowadays this is done within the code, not with pencil and ruler, but the visual image is still valid). This line $\\\\ln L(a)=\\\\ln L(\\\\hat a)-{1 \\\\over 2}$ intersects the likelihood curve at the points\\n$a=\\\\hat a \\\\pm \\\\sigma_{\\\\hat a}$. \\nIf you are working with $\\\\chi^2$, $L\\\\propto e^{-{1 \\\\over 2}\\\\chi^2}$ so the line is $\\\\Delta \\\\chi^2=1$.\\nThis gives $\\\\sigma$, or 68\\\\\\n', '\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Outlier detection}\\nThe above methods focus on detecting new physics as overdensities in very specific regions of the kinematic phase space; this paradigm is similar to a traditional bump hunt, often performed in HEP searches for novel particles. However, new physics signatures are equally likely to manifest themselves as unexpected events in the tail of distributions. This type of events may be identified using out-of-distribution detection algorithms. The prime example of such an algorithm is the auto-encoder\\\\cite{lecun1987phd, ballard1987modular, hinton1993autoencoders}, which is especially popular in high-energy physics applications\\\\cite{Radovic:2018dip, albertsson2019machine, Jawahar:2021vyu, tsan2021particle, Finke_2021, Laguarta:2023evo, Vaslin:2023lig, Anzalone:2023ugq, Bohm:2023ihd}.\\n\\\\subsubsection{Self-supervised methods}\\nSelf-supervised learning\\\\cite{balestriero2023cookbook} is a form of unsupervised learning where the data provides the supervision. In general, a part of the data is initially withheld from the model and the task of the network is to reproduce this data. Consequently, the network learns a meaningful representation of the data to solve this problem. The self-supervised learning workflow usually involves two stages: first, generating a set of supervisory signals from the input data; and second, employing these signals for a supervised~task. Self-supervised learning can be seen as a hybrid approach that lies somewhere between unsupervised and supervised learning. In high-energy physics, the most used type of self-supervised model is by far the auto-encoder.\\nThe standard Auto-Encoder (AE) model consists of two neural networks: the encoder and the decoder. The encoder maps the input data to a \\\\textit{latent space} of a lower dimensionality. For example, a particle that is represented by 64 features (transverse momentum, azimuthal angle, etc.) is reduced to a 16 feature representation. In contrast, the objective of the decoder is to reconstruct the input features from the latent space features. The ultimate goal of the AE training is to minimise the difference between the input and reconstructed data. This difference can be quantified by employing various loss functions. The Mean Squared Error (MSE) loss function is the most basic example of quantifying the input-output discrepancy:\\n\\\\begin{equation}\\nL_\\\\mathrm{MSE} = (x - f(z,\\\\theta))^2 \\n\\\\end{equation}\\nwhere $x$ is the input data, $z$ is the latent space data, and $\\\\theta$ are the weights of the decoder. This reconstruction loss is propagated through both the decoder and the encoder. Thus, the latent space and the reconstructed data evolve simultaneously.\\nThe extent to which the auto-encoder latent space follows a statistical distribution is referred to as the latent space \\\\textit{regularity}. The latent space of the standard auto-encoder does not follow any particular distribution. The regularity of the standard AE depends on the input features, the dimension of the latent space, and the encoder architecture. Thus, the encoder will shape the latent space such that it facilitates the reconstruction task, thus minimising the MSE loss from \\\\autoref{eq:vanillaloss}. In contrast, the Variational Auto-encoder (VAE)\\\\cite{kingma2014autoencoding} is an extension of the conventional auto-encoder described above, which models the latent representation to approximate a given probability distribution. This is typically a Gaussian distribution, described by a mean and a variance; however, many alternatives exist\\\\cite{joo2019dirichlet, patrini2019sinkhorn, Cerri_2019, Dillon_2021, Cheng_2023}, and the choice of latent space distribution ultimately depends on the task. \\nThe main idea of variational inference is to deﬁne a parametrised family of distributions and to search within it for the best approximation of the chosen prior distribution. The ``best approximation\\'\\' is defined as the element of the aforementioned family of distributions that minimises a pre-deﬁned function that measures the dissimilarity between the trial approximation and the prior. The function that is most commonly employed for this task is the Kullback-Leibler\\\\cite{Joyce2011} (KL) divergence, defined as\\n\\\\begin{equation}\\n\\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma}) = -\\\\frac{1}{2}\\\\sum_i \\\\left ( \\\\log(\\\\sigma_i^2) - \\\\sigma_i^2 -\\\\mu_i^2 +1 \\\\right)~,\\n\\\\end{equation}\\nfor the specific case of comparing a parametrised Gaussian distribution $\\\\mathrm{N}(\\\\vec{\\\\mu},\\\\vec{\\\\sigma})$ with $\\\\mathrm{N}(1, 0)$. A broader discussion on the KL divergence is found in Ref.\\\\,\\\\citen{paisley2012variational}. Note that the KL divergence is a somewhat unstable dissimilarity metric. Hence, more robust alternatives exist, such as the Wasserstein distance, which led to the creation of an AE architecture with the same name\\\\cite{tolstikhin2019wasserstein}. Variations on the Wasserstein AE have also been applied in a high-energy physics context\\\\cite{Komiske_2019, Komiske_2020}.\\nThe VAE loss consists of two components: the reconstruction loss, conventionally the MSE, and the KL divergence term. The latter encourages the VAE to produce a latent space that follows a well-defined prior distribution, regularising the model. Thus, the VAE loss can be written schematically as\\n\\\\begin{equation}\\n{\\\\cal L} = (1-\\\\beta) \\\\mathrm{MSE}(\\\\mathrm{Output}, \\\\mathrm{Input}) + \\\\beta \\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma})~,\\n\\\\end{equation}\\nwhere MSE labels the reconstruction loss, $\\\\mathrm{D_\\\\mathrm{{KL}}}$ is the KL regularization term, and $\\\\beta\\\\in[0, 1]$ is a hyperparameter that balances the effect of the two loss components.\\nThe weakly supervised methods from the previous sections aim to learn the likelihood ratio and thus can identify anomalies. In contrast, self-supervised models only learn the probability density of the background. Hence, an event may be labeled as anomalous if its probability to be associated with the learned latent distribution is very low. Additionally, the learned distribution exists in a lower dimensional embedded space. This stops the model from memorizing the input and is a form of lossy compression. Therefore, the model is generally capable of reconstructing events it is frequently exposed to during its training, but it fails at reconstructing events that are rare in the training set. The difference between the input data and its reconstructed counterpart may then be used to define an anomaly score: a high MSE is expected for anomalous data and a low MSE is expected for typical events. An illustration of this paradigm is shown in Figure~\\\\ref{fig:ae}. There exist several studies in HEP where AEs and VAEs are used for detecting new physics as outliers in the data~\\\\cite{Farina:2018fyg, Heimel:2018mkt,Blance:2019ibf,Hajer:2018kqm,Roy:2019jae,Cheng_2023}. For~example, this type of workflow was used to search for new physics in the two-body invariant mass spectrum of two jets or a jet and a lepton with the ATLAS Experiment in Ref.~\\\\citen{ATLAS:2023ixc}. Therein, a selection on an auto-encoder output is used to suppress the background and define signal regions with a high signal-to-background ratio. The auto-encoder output for data and for a range of potential new physics signatures is shown in Figure~\\\\ref{fig:atlasae}.\\nAs mentioned in the beginning of this section, autoencoders are efficient for event-by-event outlier detection and are not expected to perform well in finding overdensities. This makes them complimentary to the weakly supervised methods. Furthermore, an additional problem that auto-encoders have is discussed in Ref.~\\\\citen{obstructions}. In the aforementioned work it is demonstrated that the connection between large MSE and anomalies is not completely clear: for data sets with a nontrivial topology, there will always be points that wrongly are classified as anomalous. Conventionally, this can be mitigated by using VAEs and classifying anomalous events using the regularized latent space. An alternative method of circumventing this issue is based on the so called normalised AE \\\\cite{yoon2023autoencoding}, which is located at the boundary between self-supervised and unsupervised learning. This newer type of AE architecture uses energy-based models as an alternative to the likelihood ratio or the MSE. Thus, the normalised AE avoids classifying genuinely complex albeit standard events as anomalous. For more details on this last kind of AE and its possible application to HEP, see Ref.\\\\,\\\\citen{dillon2023normalized}. As mentioned earlier, diffusion models are also being explored as an alternative method to perform density estimation, similar to variational autoencoders, utilizing the learned density as a permutation-invariant anomaly detection score ~\\\\cite{mikuni2023highdimensional}.\\nAs mentioned earlier, diffusion models are also increasingly being investigated as an alternative approach for density estimation. This method parallels the use of variational autoencoders, leveraging the learned density to create a permutation-invariant score for anomaly detection, as detailed in Ref.~\\\\citen{mikuni2023highdimensional}.\\n\\\\subsubsection{Unsupervised Methods}\\nUnsupervised anomaly detection methods usually perform some type of data clustering. They include models such as Support Vector Machines~\\\\cite{boser1992training}, Isolation Forests~\\\\cite{isoforest}, and Gaussian Mixture Models\\\\cite{vanBeekveld:2020txa, Kuusela_2012}. An application using SVMs for anomaly detection in particle physics is discussed in Section~\\\\ref{sec:qml}. An example of unsupervised clustering for collider physics is presented in Ref.~\\\\citen{ucluster}. Therein, the Unsupervised Clustering algorithm, or UCluster, uses an attention-based Graph Neural Network known as \"ABC net\"~\\\\cite{abcnet} to create a latent space in which points sharing similar properties are placed close to each other. This is achieved by combining a clustering objective and a classification task during training. The produced embedding is shown to be capable of clustering together events that contain a new physics signal. A benefit of this method is that it naturally provides a way of performing background estimation. For each identified cluster, the nearest cluster within the embedding space can be used as a background model. The anomalous signal remains\\nlocalized in a particular cluster. Therefore, the nearest clusters are signal free, as shown in Figure~\\\\ref{fig:ucluster}. \\nA model-independent search method, based on Gaussian Mixture Models (GMMs), is introduced in Ref.~\\\\citen{Kuusela_2012}. Within this methodology, a GMM is being used to model the background. Then, in order to avoid any dependence on a signal hypothesis, deviations from this model are identified by fitting a mixture of the background model and a number of additional Gaussians to the observed data. This allows to search for any potential deviation from the background expectation without developing a model for the signal a priori.\\nFinally, decision trees have also been explored in anomaly detection for searches. For example, in Ref.~\\\\citen{roche2023nanosecond}, a tree-based autoencoder is trained through a self-supervised paradigm on background data and then evaluated on the ADC challenge data~\\\\cite{adcchallenge}. Their unsupervised counterpart, isolation forests, have been less prominent in particle physics, but they have been applied for accelerator control~\\\\cite{Halilovic:2665985}.\\nA key challenge with outlier detection methods, as discussed in Ref.~\\\\citen{golling2023massive}, is their tendency to generate anomaly scores closely correlated to the variable of interest. This may lead to undesired sculpting effects, complicating bump-hunt like searches. To address this, strategies such as decorrelating the latent space from the variable of interest or tailoring the anomaly metric to be conditional on the jet mass \\\\cite{Cheng_2023} should be explored. However, efforts in these areas remain limited.\\n', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Ensemble of pseudo-experiments generated with ``Toy'' Monte Carlo}\\nThe $p$-values in the procedure described above require performing several integrals. In the case of the asymptotic approach, the distributions for $\\\\tilde q_\\\\mu$ and $\\\\tilde q_0$ are known and the integral is performed directly. When the distributions are not assumed to take on their asymptotic form, then they must be constructed using Monte Carlo methods. In the ``toy Monte Carlo'' approach one generates pseudo-experiments in which the number of events in each channel $n_c$, the values of the discriminating variables $\\\\{x_{ec}\\\\}$ for each of those events, and the auxiliary measurements (global observables) $a_p$ are all randomized according to $\\\\F_{\\\\rm tot}$. We denote the resulting data $\\\\data_{\\\\rm toy}$ and global observables $\\\\globs_{\\\\rm toy}$. By doing this several times one can build an ensemble of pseudo-experiments and evaluate the necessary integrals. Recall that Monte Carlo techniques can be viewed as a form of numerical integration.\\nThe fact that the auxiliary measurements $a_p$ are randomized is unfamiliar in particle physics. The more familiar approach for toy Monte Carlo is that the nuisance parameters are randomized. This requires a distribution for the nuisance parameters, and thus corresponds to a Bayesian treatment of the nuisance parameters. The resulting $p$-values are a hybrid Bayesian-Frequentist quantity with no consistent definition of probability. To maintain a strictly frequentist procedure, the corresponding operation is to randomize the auxiliary measurements. \\nWhile formally this procedure is well motivated, as physicists we also know that our models can have deficiencies and we should check that the distribution of the auxiliary measurements does not deviate too far from our expectations. In Section~\\\\ref{Sec:crossChecks} we show the distribution of the auxiliary measurements and the corresponding $\\\\hat{\\\\vec\\\\theta}$ from the toy Monte Carlo technique. \\nTechnically, the pseudo-experiments are generated with the \\\\texttt{RooStats} \\\\texttt{ToyMCSampler}, which is used by the higher-level tool \\\\texttt{FrequentistCalculator}, which is in turn used by \\\\texttt{HypoTestInverter}.\\n\", \"\\\\section{Errors}\\n\\\\subsection {Combining errors}\\nHaving obtained---by whatever means---errors $\\\\sigma_x, \\\\sigma_y...$ \\nhow does one combine them to get errors on derived quantities $f(x,y...), g(x,y,...)$?\\nSuppose $f=Ax+By+C$, with $A,B$ and $C$ constant.\\nThen it is easy to show that \\n\\\\begin{align}\\nV_f&= \\\\notag\\n\\\\left< (f - \\\\left< f\\\\right>)^2\\\\right>\\\\\\\\&= \\\\notag\\n\\\\left< (Ax+By+C - \\\\left< Ax+By+C\\\\right>)^2\\\\right> \\\\\\\\\\n&= \\\\notag\\nA^2(\\\\left<x^2\\\\right> - \\\\left<x\\\\right>^2)\\n+B^2(\\\\left<y^2\\\\right> - \\\\left<y\\\\right>^2)\\n+2AB(\\\\left<xy\\\\right> - \\\\left<x\\\\right>\\\\left< y \\\\right>)\\\\\\\\\\n&=A^2 V_x + B^2 V_y + 2AB\\\\, {\\\\rm Cov}_{xy}\\n\\\\quad.\\n\\\\end{align}\\nIf $f$ is not a simple linear function of $x$ and $y$ then one can use a first order Taylor expansion to\\napproximate it about a central value $f_0(x_0,y_0)$\\n\\\\begin{equation}\\nf(x,y)\\\\approx f_0 \\n+ \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) (x-x_0)\\n+ \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) (y-y_0)\\n\\\\end{equation}\\n\\\\noindent and application of Eq.~\\\\ref{eq:COE0} gives\\n\\\\begin{equation}\\nV_f=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 V_x+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 V_y+\\n2 \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) {\\\\rm Cov}_{xy}\\n\\\\end{equation}\\n\\\\noindent writing the more familiar $\\\\sigma^2$ \\ninstead of $V$ this is equivalent to\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n+ 2 \\\\rho \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) \\\\sigma_x \\\\sigma_y\\n\\\\quad.\\n\\\\end{equation}\\nIf $x$ and $y$ are independent, which is often but not always the case, this reduces to what is often known as the\\n`combination of errors' formula\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n\\\\quad.\\n\\\\end{equation}\\nExtension to more than two variables is trivial: an extra squared term is added for each and\\nan extra covariance term for each of the variables (if any) with which it is correlated.\\nThis can be expressed in language as {\\\\it errors add in quadrature}. This is a friendly fact, as\\nthe result is smaller than you would get from arithmetic addition. If this puzzles you, it may be helpful to think \\nof this as allowing for the possibility that a positive fluctuation in one variable may be cancelled by a negative fluctuation in\\nthe other. \\nThere are a couple of special cases we need to consider. \\nIf $f$ is a simple product, $f=Axy$, then Eq.~\\\\ref{eq:COE2} gives\\n$$\\\\sigma_f^2=(Ay)^2 \\\\sigma_x^2+ (Ax)^2 \\\\sigma_y^2 \\\\ ,$$\\nwhich, dividing by $f^2$, can be written as\\n\\\\begin{equation}\\n\\\\left({ \\\\sigma_f \\\\over f }\\\\right)^2 =\\\\left( {\\\\sigma_x \\\\over x }\\\\right)^2 +\\n\\\\left({ \\\\sigma_y \\\\over y }\\\\right)^2.\\n\\\\end{equation}\\nFurthermore this also applies if $f$ is a simple quotient, \\n$f=Ax/y$ or $Ay/x$ or even $A/(xy)$.\\nThis is very elegant, but it should not be overemphasised. Equation~\\\\ref{eq:COE3}\\nis not fundamental: it only applies in certain cases (products or quotients). Equation~\\\\ref{eq:COE2} is\\nthe fundamental one, and Eq.~\\\\ref{eq:COE3} is just a special case of it.\\nFor example: if you measure the radius of a cylinder as $r=123 \\\\pm 2$ mm and the height as $h=456 \\\\pm 3$ mm\\nthen the volume $\\\\pi r^2 h$ is $\\\\pi \\\\times 123^2 \\\\times 456 = 21673295 \\\\ {\\\\rm mm}^3$ \\nwith error $\\\\sqrt{(2 \\\\pi r h)^2 \\\\times \\\\sigma_r^2 + ( \\\\pi r^2 )^2 \\\\times \\\\sigma_h^2}=719101$,\\nso one could write it as $v=(216.73 \\\\pm 0.72) \\\\times 10^5\\\\ {\\\\rm mm}^3$.\\nThe surface area $2 \\\\pi r^2 + 2\\\\pi r h$ is $ 2 \\\\pi \\\\times 123^2 + 2 \\\\pi \\\\times 123 \\\\times 456 = 447470\\\\ {\\\\rm mm}^2$\\nwith error $\\\\sqrt{(4\\\\pi r + 2 \\\\pi h)^2 \\\\sigma_r^2 + (2 \\\\pi r)^2 \\\\sigma_h^2 }= 9121 \\\\ {\\\\rm mm}^2 $---so one could write the result \\nas $a=(447.5 \\\\pm 9.1) \\\\times 10^3 \\\\ {\\\\rm mm}^2$.\\nA full error analysis has to include the treatment of the covariance terms---if only to show that they can be ignored.\\nWhy should the $x$ and $y$ in Eq.~\\\\ref{eq:COE1} be correlated? \\nFor direct measurements very often (but not always) they will not be.\\nHowever the interpretation of results is generally a multistage process. \\nFrom raw numbers of events one computes branching ratios (or cross sections...), from which one computes matrix elements (or particle masses...). Many quantities of interest to theorists are expressed as ratios of experimental numbers. \\nAnd in this interpretation there is plenty of scope for correlations to creep into the analysis.\\nFor example, an experiment might measure a cross section $\\\\sigma(pp \\\\to X) $ from a number of observed events $N$ in the decay channel $X \\\\to \\\\mu^+\\\\mu^-$. One would \\nuse a formula\\n$$\\\\sigma={N \\\\over B \\\\eta {\\\\cal L}} \\\\ ,$$\\nwhere $\\\\eta$ is the efficiency for detecting and reconstructing \\nan event, $B$ is the branching ratio for $X \\\\to \\\\mu^+\\\\mu^-$, and ${\\\\cal L}$ is the integrated luminosity.\\nThese will all have errors, and the above prescription can be applied.\\nHowever it might also use the $X \\\\to e^+e^-$ channel and then use\\n$$\\\\sigma'={N' \\\\over B' \\\\eta' {\\\\cal L}} \\\\ .$$\\nNow $\\\\sigma$ and $\\\\sigma'$ are clearly correlated; even though $N$ and $N'$ are \\nindependent, the same ${\\\\cal L}$ appears in both. If the estimate of ${\\\\cal L}$ is on the high side, that will push both $\\\\sigma$ and $\\\\sigma'$ downwards, and vice versa. \\nOn the other hand, if a second experiment did the same measurement it would have its own $N$, $\\\\eta$ and ${\\\\cal L}$, but would be correlated with the first\\nthrough using the same branching ratio (taken, presumably, from the Particle Data Group).\\nTo calculate correlations between results we need the equivalent of Eq.~\\\\ref{eq:COE0}\\n\\\\begin{align}\\n{\\\\rm Cov}_{fg} &= \\\\notag \\\\left< (f-\\\\langle f \\\\rangle )(g-\\\\langle g \\\\rangle ) \\\\right> \\\\\\\\\\n&=\\\\left({\\\\partial f \\\\over \\\\partial x} \\\\right) \\\\left( { \\\\partial g \\\\over \\\\partial x} \\\\right) \\\\sigma_x^2\\n\\\\quad,\\n\\\\end{align} \\nThis can all be combined in the general formula which encapsulates all of the ones above\\n\\\\begin{equation}\\n{\\\\bf V_f} = {\\\\bf G V_x \\\\tilde G}\\n\\\\quad,\\n\\\\end{equation}\\nwhere ${\\\\bf V_x}$ is the covariance matrix of the primary quantities (often, as pointed out earlier, this is diagonal),\\n${\\\\bf V_f}$ is the covariance matrix of secondary quantities, and \\n\\\\begin{equation}\\nG_{ij}={\\\\partial f_i \\\\over \\\\partial x_j}\\n\\\\quad.\\n\\\\end{equation}\\nThe {\\\\bf G} matrix is rectangular but need not be square. \\nThere may be more---or fewer---derived quantities than primary quantities.\\nThe matrix algebra of ${\\\\bf G}$ and its transpose ${\\\\bf \\\\tilde G}$ \\nensures that the numbers of rows and columns match for Eq.~\\\\ref{eq:COE4}.\\nTo show how this works, we go back to our earlier example of a cylinder.\\n$v$ and $a$ are correlated: if $r$ or $h$ fluctuate upwards (or downwards), that makes both volume and area larger\\n(or smaller). The matrix ${\\\\bf G}$ is\\n\\\\begin{equation}\\n{\\\\bf G}=\\\\left( \\n\\\\begin{matrix}\\n2 \\\\pi r h & \\\\pi r^2 \\\\\\\\\\n2 \\\\pi (2 r + h) &\\n2 \\\\pi r\\n\\\\end{matrix}\\n\\\\right)\\n=\\\\left( \\n\\\\begin{matrix}\\n352411 & 47529 \\\\\\\\\\n4411 & 773\\n\\\\end{matrix}\\n\\\\right)\\n\\\\quad,\\n\\\\end{equation} \\n\\\\noindent the variance matrix $V_x$ is\\n\\\\begin{equation*}\\n{\\\\bf V_x}=\\\\left( \\n\\\\begin{matrix}\\n4 & 0\\\\\\\\\\n0& 9\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\n\\\\noindent and Eq.~\\\\ref{eq:COE4} gives\\n\\\\begin{equation*}\\n{\\\\bf V_f}=\\\\left( \\n\\\\begin{matrix}\\n517.1 \\\\times 10^9 & 6.548 \\\\times 10^9\\\\\\\\\\n6.548 \\\\times 10^9 & 83.20 \\\\times 10^6\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\nfrom which one obtains, as before,\\n$\\\\sigma_v= 719101, \\\\sigma_a=9121$ but also $\\\\rho=0.9983$.\\nThis can be used to provide a useful example of why correlation matters. Suppose\\nyou want to know the volume to surface ratio, $z=v/a$, of this cylinder. \\nDivision gives $z=21673295/447470=48.4352$ mm.\\nIf we just use Eq.~\\\\ref{eq:COE2} for the error, this gives $\\\\sigma_z=1.89$ mm. \\nIncluding the correlation term, as in Eq.~\\\\ref{eq:COE2a}, reduces this to\\n$0.62$ mm---three times smaller. It makes a big difference.\\nWe can also check that this is correct, because the ration ${v \\\\over a}$ can be written as\\n$\\\\pi r^2 h \\\\over 2 \\\\pi r^2 + 2 \\\\pi r h$, and applying the uncorrelated errors of the original $r$ and $h$ to this also gives\\nan error of $0.62$ mm.\\nAs a second, hopefully helpful, example we consider a simple straight line fit, $y=mx+c$.\\nAssuming that all the $N$ $y$ values are measured with the same error $\\\\sigma$,\\nleast squares estimation gives the well known results\\n\\\\begin{equation}\\nm={\\\\overline{xy} - \\\\overline x \\\\, \\\\overline y \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\qquad\\nc={\\\\overline{y}\\\\, \\\\overline{x^2} - \\\\overline {xy} \\\\, \\\\overline x \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\quad.\\n\\\\end{equation}\\nFor simplicity we write $D=1/(\\\\overline{x^2}-\\\\overline x^2)$. The differentials are\\n\\\\begin{equation*}\\n{\\\\partial m \\\\over \\\\partial y_i}={D \\\\over N} (x_i-\\\\overline x) \\\\qquad\\n{\\\\partial c \\\\over \\\\partial y_i}={D \\\\over N} (\\\\overline{x^2}- x_i\\\\overline x)\\n\\\\quad,\\n\\\\end{equation*}\\n\\\\noindent from which, remembering that the $y$ values are uncorrelated,\\n\\\\begin{align*}\\nV_m=\\\\sigma^2\\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)^2=\\\\sigma^2 {D \\\\over N}\\n\\\\\\\\\\nV_c=\\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (\\\\overline{x^2}- x_i\\\\overline x)^2 =\\\\sigma^2 \\\\overline {x^2} {D \\\\over N}\\\\\\\\\\n{\\\\rm Cov}_{mc}= \\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)(\\\\overline{x^2}-x_i \\\\overline x)=-\\\\sigma^2 \\\\overline x {D \\\\over N}\\n\\\\end{align*}\\n\\\\noindent from which the correlation between $m$ and $c$ is just $\\\\rho=- \\\\overline x / \\\\sqrt{\\\\overline{x^2}}$.\\nThis makes sense. Imagine you're fitting a straight line through a set of points with a range of positive $x$ values (so $\\\\overline x$ is positive). If the rightmost point happened to be a bit higher, that would push the slope $m$ up and the intercept $c$ down. Likewise if the leftmost point happened to be too high that would push the slope down and the intercept up. There is a negative correlation between the two fitted quantities.\\nDoes it matter? Sometimes. Not if you're just interested in the slope---or the constant. But suppose you intend to use them to find the expected value of $y$ at some extrapolated $x$. Equation~\\\\ref{eq:COE2a} gives\\n\\\\begin{equation*}\\ny=m x + c \\\\pm \\\\sqrt {x^2 \\\\sigma_m^2 + \\\\sigma_c^2 + 2 x \\\\rho \\\\sigma_m \\\\sigma_c}\\n\\\\end{equation*}\\nand if, for a typical case where $\\\\overline x$ is positive so $\\\\rho$ is negative, you leave out the correlation term you will overestimate your error.\\nThis is an educational example because this correlation can be avoided. Shifting to a co-ordinate system in which\\n$\\\\overline x$ is zero ensures that the quantities are uncorrelated. This is \\nequivalent to rewriting the well-known $y=mx+c$ formula as $y=m(x-\\\\overline x)+c'$, where\\n$m$ is the same as before and $c'=c+m \\\\overline x$. $m$ and $c'$ are now uncorrelated, and \\nerror calculations involving them become a lot simpler. \\n\", \"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Consistent Bayesian and Frequentist modeling}\\nThe variational estimates $\\\\eta^\\\\pm$ and $\\\\sigma^\\\\pm$ typically correspond to so called ``$\\\\pm 1\\\\sigma$ variations'' in the source of the uncertainty. Here we are focusing on the source of the uncertainty, not its affect on rates and shapes. For instance, we might say that the jet energy scale has a 10\\\\\\nIt is often advocated that a ``log-normal'' or ``gamma'' distribution for $\\\\alpha_p$ is more appropriate than a gaussian constraint~\\\\cite{CousinsLogNormal}. This is particularly clear in the case of bounded parameters and large uncertainties. Here we must take some care to build a probability model that can maintain a consistent interpretation in Bayesian a frequentist settings. Table~\\\\ref{tab:constraints} summarizes a few consistent treatments of the frequentist pdf, the likelihood function, a prior, and the resulting posterior.\\n\\\\begin{table}[*htb]\\n\\\\center\\n\\\\begin{tabular}{llll}\\nPDF & Likelihood $\\\\propto$ & Prior $\\\\pi_0$ & Posterior $\\\\pi$ \\\\\\\\ \\\\hline\\n$G(a_p | \\\\alpha_p, \\\\sigma_p)$ & $G(\\\\alpha_p | a_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\alpha_p)\\\\propto$ const & $G(\\\\alpha_p | a_p, \\\\sigma_p)$ \\\\\\\\\\n$\\\\Pois(n_p | \\\\tau_p \\\\beta_p)$ & $\\\\PGamma(\\\\beta_p | A=\\\\tau_p; B=1+n_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto$ const & $\\\\PGamma(\\\\beta_p | A=\\\\tau_p; B=1+n_p)$ \\\\\\\\\\n$\\\\LN(n_p | \\\\beta_p, \\\\sigma_p)$ & $ \\\\beta_p \\\\cdot \\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto $ const & $\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ \\\\\\\\\\n$\\\\LN(n_p | \\\\beta_p, \\\\sigma_p)$ & $\\\\beta_p \\\\cdot\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$ & $\\\\pi_0(\\\\beta_p) \\\\propto 1/\\\\beta_p $ & $\\\\LN(\\\\beta_p | n_p, \\\\sigma_p)$\\\\\\\\\\n\\\\end{tabular}\\n\\\\caption{Table relating consistent treatments of PDF, likelihood, prior, and posterior for nuisance parameter constraint terms.}\\n\\\\end{table}\\nFinally, it is worth mentioning that the uncertainty on some parameters is not the result of an auxiliary measurement -- so the constraint term idealization, it is not just a convenience, but a real conceptual leap. This is particularly true for theoretical uncertainties from higher-order corrections or renormalizaiton and factorization scale dependence. In these cases a formal frequentist analysis would not include a constraint term for these parameters, and the result would simply depend on their assumed values. As this is not the norm, we can think of reading Table~\\\\ref{tab:constraints} from right-to-left with a subjective Bayesian prior $\\\\pi(\\\\alpha)$ being interpreted as coming from a fictional auxiliary measurement.\\n\\\\subsubsubsection{Gaussian Constraint}\\nThe Gaussian constraint for $\\\\alpha_p$ corresponds to the familiar situation. It is a good approximation of the auxiliary measurement when the likelihood function for $\\\\alpha_p$ from that auxiliary measurement has a Gaussian shape. More formally, it is valid when the maximum likelihood estimate of $\\\\alpha_p$ (eg. the best fit value of $\\\\alpha_p$) has a Gaussian distribution. Here we can identify the maximum likelihood estimate of $\\\\alpha_p$ with the global observable $a_p$, remembering that it is a number that is extracted from the data and thus its distribution has a frequentist interpretation. \\n\\\\begin{equation}\\nG(a_p | \\\\alpha_p, \\\\sigma_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma_p^2}} \\\\exp \\\\left[ -\\\\frac{(a_p - \\\\alpha_p)^2}{2\\\\sigma_p^2} \\\\right]\\n\\\\end{equation}\\nwith $\\\\sigma_p=1$ by default.\\nNote that the PDF of $a_p$ and the likelihood for $\\\\alpha_p$ are positive for all values. \\n\\\\subsubsubsection{Poisson (``Gamma'') constraint}\\nWhen the auxiliary measurement is actually based on counting events in a control region (eg. a Poisson process), a more accurate to describe the auxiliary measurement with a Poisson distribution. It has been shown that the truncated Gaussian constraint can lead to undercoverage (overly optimistic) results, which makes this issue practically relevant~\\\\cite{Cousins:2008zz}. Table~\\\\ref{tab:constraints} shows that a Poisson PDF together with a uniform prior leads to a gamma posterior, thus this type of constraint is often called a ``gamma'' constraint. This is a bit unfortunate since the gamma distribution is manifestly Bayesian and with a different choice of prior, one might not arrive at a gamma posterior. When dealing with the Poisson constraint, it is no longer convenient to work with our conventional scaling for $\\\\alpha_p$ which can be negative. Instead, it is more natural to think of the number of events measured in the auxiliary measurement $n_p$ and the mean of the Poisson parameter. This information is not usually available, instead one usually has some notion of the relative uncertainty in the parameter $\\\\sigma_p^{\\\\rm rel}$ (eg. a the jet energy scale is known to 10\\\\\\\\begin{equation}\\n\\\\Pois(n_p | \\\\tau_p \\\\alpha_p) =\\\\frac{ (\\\\tau_p \\\\alpha_p)^{n_p} \\\\; e^{-\\\\tau_p \\\\alpha_p} } {n_p!}\\n\\\\end{equation}\\nHere we can use the fact that Var$[n_p]=\\\\sqrt{\\\\tau_p\\\\alpha_p}$ and reverse engineer the nominal auxiliary measurement \\n\\\\begin{equation}\\nn_p^0 = \\\\tau_p = (1/\\\\sigma_{p}^{\\\\rm rel})^2\\\\; .\\n\\\\end{equation}\\nwhere the superscript $0$ is to remind us that $n_p$ will fluctuate in repeated experiments but $n_p^0$ is the value of our measured estimate of the parameter.\\nOne important thing to keep in mind is that there is only one constraint term per nuisance parameter, so there must be only one $\\\\sigma_p^{rel}$ per nuisance parameter. This $\\\\sigma_p^{rel}$ is related to the fundamental uncertainty in the source and we cannot infer this from the various response terms $\\\\eta_{ps}^\\\\pm$ or $\\\\sigma_{pub}^\\\\pm$. \\nAnother technical difficulty is that the Poisson distribution is discrete. So if one were to say the relative uncertainty was 30\\\\\\\\begin{equation}\\n\\\\PGamma(\\\\alpha_p | A=\\\\tau_p, B=n_p-1) = A (A \\\\alpha_p)^{B} e^{-A \\\\alpha_p} / \\\\Gamma(B)\\\\;.\\n\\\\end{equation}\\nThis approach works fine for likelihood fits, Bayesian calculations, and frequentist techniques based on asymptotic approximations, but it does not offer a consistent treatment of the pdf for the global observable $n_p$ that is needed for techniques based on Monte Carlo sampling. \\n\\\\subsubsubsection{Log-normal constraint}\\nFrom Eadie et al., ``The log-normal distribution represents a random variable whose logarithm follows a normal distribution. It provides a model for the error of a process involving many small multiplicative errors (from the Central Limit Theorem). It is also appropriate when the value of an observed variable is a random proportion of the previous observation.''~\\\\cite{Eadie:qy,CousinsLogNormal}. This logic of multiplicative errors applies to the the measured value, not the parameter. Thus, it is natural to say that there is some auxiliary measurement (global observable) with a log-normal distribution. As in the gamma/Poisson case above, let us again say that the global observable is $n_p$ with a nominal value\\n\\\\begin{equation}\\nn_p^0 = \\\\tau_p = (1/\\\\sigma_{p}^{\\\\rm rel})^2\\\\; .\\n\\\\end{equation}\\nThen the conventional choice for the corresponding log-normal distribution is\\n\\\\begin{equation}\\n\\\\LN(n_p | \\\\alpha_p, \\\\kappa_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right]\\n\\\\end{equation}\\nwhile the likelihood function is (blue curve in Fig.~\\\\ref{fig:lognormal}(a)).\\n\\\\begin{equation}\\nL( \\\\alpha_p) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right];.\\n\\\\end{equation}\\nTo get to the posterior for $\\\\alpha_p$ given $n_p$ we need an ur-prior $\\\\eta(\\\\alpha_p$)\\n\\\\begin{equation}\\n\\\\pi( \\\\alpha_p) \\\\propto \\\\eta(\\\\alpha_p) \\\\; \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\ln \\\\kappa}\\\\frac{1}{n_p} \\\\exp \\\\left[ -\\\\frac{\\\\ln(n_p/ \\\\alpha_p)^2}{2(\\\\ln \\\\kappa_p)^2} \\\\right]\\n\\\\end{equation}\\nIf $\\\\eta(\\\\alpha_p)$ is uniform, then the posterior looks like the red curve in Fig.~\\\\ref{fig:lognormal}(b). However, when paired with an ``ur-prior'' $\\\\eta(\\\\alpha_p) \\\\propto 1/\\\\alpha_p$ (green curve in Fig.~\\\\ref{fig:lognormal}(b)), this results in a posterior distribution that is also of a log-normal form for $\\\\alpha_p$ (blue curve in Fig.~\\\\ref{fig:lognormal}(b)).\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Frequentist upper limit}\\nIn practical terms, the simplest approach for making estimates is to consider single-channel experiments, where we have an observed value ($n$), an expected number of background events ($b$), and an expected number of signal events ($s$). Since the measurement process involves counting events in the channel, we will model it using a Poisson distribution.\\nFor $H_{0}$, the mean of the distribution will be $\\\\lambda = b$, meaning that the observation is explained solely by background events. For $H_{1}$, the mean is given by $\\\\lambda(\\\\mu) = \\\\mu s + b$, where $\\\\mu$ is known as the signal strength and measures the agreement between $H_{1}$ and the observation. In this way, to find the confidence level of both hypotheses with respect to the observation, we use the cumulative Poisson distribution, which is given by:\\n\\\\begin{equation}\\nF_{P}(\\\\lambda) = \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda(\\\\mu))^{i}e^{-\\\\lambda(\\\\mu)}}{(i)!}.\\n\\\\end{equation}\\nWhere $\\\\mu = 0$ for $H_{0}$ and $\\\\mu = 1$ for $H_{1}$. This cumulative distribution has a direct relation with the cumulative $\\\\chi^2(x; k)$ distribution, with $k = 2(n + 1)$ degrees of freedom and $x = 2\\\\lambda$ (see Appendix \\\\ref{sec:AppendixA}). Therefore, the confidence level $CL = 0.95$ is given by~\\\\cite{lista2016practical}:\\n\\\\begin{eqnarray}\\n1-\\\\alpha & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) {} \\\\nonumber \\\\\\\\ \\n0.95 & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)). {}\\n\\\\end{eqnarray}\\nThe signal strength $\\\\mu$ allows us to statistically assess the degree of agreement between the alternative hypothesis and the observation. In general, we can find the upper limit of the signal strength, i.e., the point at which the alternative hypothesis can no longer explain the observation. This means excluding all models with $\\\\mu > \\\\mu_{up}$ at a $3\\\\sigma$ confidence level. By inverting the relation~(\\\\ref{eq:4}), it is possible to calculate the upper limit for all new theories, given the observation $n$, an expected number of background events $b$, and new physics events $s$. Thus, we obtain:\\n\\\\begin{equation}\\n\\\\mu_{up}= \\\\frac{1}{s} (\\\\frac{1}{2}F^{-1}_{\\\\chi^2}(1-\\\\alpha;k=2(n+1)) - b).\\n\\\\end{equation}\\nWhere $\\\\mu_{up}$ is the upper limit at $95\\\\\\nIn the general case, when the number of observations is different from 0, the upper bounds of the model are determined by varying both the observations and the expected background~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:4}] illustrates the behavior of the upper bound $\\\\mu_{up}$ for a given $n$, as a function of the expected nuisance level $b$~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Frequentist/FrequentistUpperLimitScan.ipynb}{Source code}}.\\nIt is important to note that as the number of observations decreases, the upper bound value as a function of the background component tends to negative values. A value of $\\\\mu_{up} < 0$ implies the exclusion of the null hypothesis in the absence of signal events $s$. This scenario presents a contradiction, as it represents a non-physical condition known as the coverage problem of the statistical estimator. To address this issue, variants of the frequentist method have been proposed that correct the coverage problem while adhering to solid probabilistic principles. These approaches include Bayesian methods and a modified version of the frequentist method~\\\\cite{lista2016practical,cranmer2015practical,conway2005calculation}.\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Upper limits for event counting experiments}\\nThe simplest search for a new signal consists of counting the number of events\\npassing a specified selection.\\nThe number of selected events $n$ is distributed according to a Poissonian distribution\\nwhere the expected value, in case of presence of signal plus background ($H_1$)\\nis $s + b$, and for background only ($H_0$) is $b$.\\nAssume we count $n$ events, we then want to compare the two hypotheses $H_1$ and $H_0$.\\nAs simplest case, we can assume that $b$ is known with negligible uncertainty.\\nIf not, uncertainty on its estimate must be taken into account.\\nThe likelihood function for this case can be written as:\\n\\\\begin{equation}\\nL(n;s) = \\\\frac{(s+b)^n}{n!}e^{-(s+b)}\\\\,.\\n\\\\end{equation}\\n$H_0$ corresponds to the case $s=0$.\\nUsing the Bayesian approach, an upper limit\\n$s^{\\\\mathrm{up}}$ on $s$ can be determined by requiring that the posterior probability\\ncorresponding to the interval $[0,s^{\\\\mathrm{up}}]$ is equal to the confidence\\nlevel $1-\\\\alpha$:\\n\\\\begin{equation}\\n1-\\\\alpha = \\\\int_0^{s^{\\\\mathrm{up}}} P(s|n)\\\\,\\\\mathrm{d}s =\\n\\\\frac{\\n\\\\displaystyle\\\\int_0^{s\\\\mathrm{up}} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}{\\n\\\\displaystyle\\\\int_0^{+\\\\infty} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}\\\\,.\\n\\\\end{equation}\\nThe choice of a uniform prior, $\\\\pi(s)=1$, simplifies the computation and\\nEq.~(\\\\ref{eq:poisBayesUL}) reduces to~\\\\cite{Helene}:\\n\\\\begin{equation}\\n\\\\alpha = e^{-s^{\\\\mathrm{up}}}\\\\frac{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{(s^{\\\\mathrm{up}}+b)^m}{m!}\\n}{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{b^m}{m!}\\n}\\\\,.\\n\\\\end{equation}\\nUpper limits obtained with Eq.~(\\\\ref{eq:Helene}) are shown in Fig.~\\\\ref{fig:Helene}.\\nIn the case $b=0$, the results obtained in Eq.~(\\\\ref{eq:BayesianPoissonUL90}) and~(\\\\ref{eq:BayesianPoissonUL95}) are\\nagain recovered.\\nFrequentist upper limits for a counting experiment can be easily computed\\nin case of negligible background ($b=0$). If zero events are observed ($n=0$),\\nthe likelihood function simplifies to:\\n\\\\begin{equation}\\nL(n=0;s) = \\\\mathrm{Poiss}(0;s) = e^{-s}\\\\,.\\n\\\\end{equation}\\nThe inversion of the fully asymmetric Neyman belt reduces to:\\n\\\\begin{equation}\\nP(n\\\\le0;s^{\\\\mathrm{up}}) = P(n=0;s^{\\\\mathrm{up}}) = \\\\alpha \\\\implies s^{\\\\mathrm{up}} = -\\\\ln\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhich lead to results that are numerically identical to the Bayesian computation:\\n\\\\begin{eqnarray}\\ns & < & s^{\\\\mathrm{up}} = 2.303\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.1\\\\,\\\\text{(90\\\\ s & < & s^{\\\\mathrm{up}} = 2.996\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.05\\\\,\\\\text{(95\\\\\\\\end{eqnarray}\\nIn spite of the numerical coincidence, the interpretation of frequentist and Bayesian upper limits remain very different.\\nUpper limits from Eq.~(\\\\ref{eq:FreqPoissonUL90}) and~(\\\\ref{eq:FreqPoissonUL95})\\nanyway suffer from the flip-flopping problem and the coverage is spoiled when deciding to switch\\nfrom an upper limit to a central value depending on the observed significance level.\\nFeldman--Cousins intervals cure the flip-flopping issue and ensure the correct coverage\\n(or may overcover for discrete variables).\\nUpper limits at 90\\\\are reported in Fig.~\\\\ref{fig:fcUL}.\\nThe ``ripple'' structure is due to the discrete nature of Poissonian counting.\\nIt's evident from the figure that, even for $n = 0$, the upper limit decrease as $b$ increases (apart from ripple effects).\\nThis means that if two experiment are designed for an expected background of --say-- 0.1 and 0.01, the\\n``worse'' experiment (i.e.: the one which expects 0.1 events) achieves the best upper limit in case\\nno event is observed ($n=0$), which is the most likely outcome if no signal is present.\\nThis feature was noted in the 2001 edition of the PDG~\\\\cite{PDG2001}\\n\\\\begin{displayquote}\\n{\\\\it The intervals constructed according to the unified procedure [Feldman--Cousins] for a Poisson variable $n$ consisting\\nof signal and background have the property that for $n = 0$ observed events, the upper limit decreases for increasing\\nexpected background. This is counter-intuitive, since it is known that if $n = 0$ for the experiment in question,\\nthen no background was observed, and therefore one may argue that the expected background should not be relevant.\\nThe extent to which one should regard this feature as a drawback is a subject of some controversy.}\\n\\\\end{displayquote}\\nThis counter-intuitive feature of frequentist upper limits is one of the reasons that led to the use in High-Energy Physics of\\na modified approach, whose main feature is that is also prevents rejecting cases where the experiment has little sensitivity\\ndue to statistical fluctuation, as will be described in next Section.\\n\", \"\\\\section{Inference}\\n\\\\subsection{Bayesian inference}\\nOne example of inference is the use of Bayes theorem to determine the posterior PDF\\nof an unknown parameter $\\\\theta$ given an observation $x$:\\n\\\\begin{equation}\\nP(\\\\theta|x) = \\\\frac{L(x;\\\\theta)\\\\pi(\\\\theta)}{\\n\\\\int L(x;\\\\theta)\\\\pi(\\\\theta)\\\\,\\\\mathrm{d}\\\\theta}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\pi(\\\\theta)$ is the prior PDF.\\nThe posterior $P(\\\\theta|x)$ contains all the information we can obtain from $x$\\nabout $\\\\theta$.\\nOne example of possible outcome for $P(\\\\theta|x)$ is shown in Fig.~\\\\ref{fig:BayesIntCentralSym}\\nwith two possible choices of uncertainty interval (left and right plots).\\nThe most probable value, $\\\\hat{\\\\theta}$, also called {\\\\it mode}, shown as dashed line\\nin both plots, can be taken as central value\\nfor the parameter $\\\\theta$.\\nIt's worth noting that if $\\\\pi(\\\\theta)$ is assumed to be a constant,\\n$\\\\hat{\\\\theta}$ corresponds to the maximum of \\nthe likelihood function ({\\\\it maximum likelihood estimate}, see Sec.~\\\\ref{sec:maxLik}).\\nDifferent choices of 68.3\\\\can be taken. A central interval $[\\\\theta_1,\\\\theta_2]$,\\nrepresented in the left plot in Fig.~\\\\ref{fig:BayesIntCentralSym}\\nas shaded area, is obtained in order to have equal areas under the two extreme tails:\\n\\\\begin{eqnarray}\\n\\\\int_{-\\\\infty}^{\\\\theta_1} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\n\\\\int^{+\\\\infty}_{\\\\theta_2} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & \\\\frac{\\\\alpha}{2}\\\\,, \\n\\\\end{eqnarray}\\nwhere $\\\\alpha = 1 - 68.3\\\\ \\nAnother example of a possible coice of 68.3\\\\where a symmetric interval is taken, corresponding to:\\n\\\\begin{eqnarray}\\n\\\\int_{\\\\hat{\\\\theta}-\\\\delta}^{\\\\hat{\\\\theta}+\\\\delta} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\,.\\\\\\\\\\n\\\\end{eqnarray}\\nTwo extreme choices of fully asymmetric probability intervals are shown in Fig.~\\\\ref{fig:BayesIntHiLo},\\nleading to an upper (left) or lower (right) limit to the parameter $\\\\theta$.\\nFor upper or lower limits, usually a 90\\\\of the usual 68.3\\\\ \\nThe intervals in Fig.~\\\\ref{fig:BayesIntHiLo} are chosen such that:\\n\\\\begin{eqnarray}\\n\\\\int_{-\\\\infty}^{\\\\theta^{\\\\mathrm{up}}} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\quad \\\\text{(left plot)}\\\\,,\\\\\\\\\\n\\\\int^{+\\\\infty}_{\\\\theta^{\\\\mathrm{lo}}} P(\\\\theta|x)\\\\,\\\\mathrm{d}\\\\theta & = & {1-\\\\alpha}\\\\quad \\\\text{(right plot)}\\\\,,\\\\\\\\\\n\\\\end{eqnarray}\\nwhere in this case $\\\\alpha = 0.1$.\\n\\\\subsubsection{Example of Bayiesian inference: Poissonian counting}\\nIn a counting experiment, i.e.: the only information relevant to measure the yield of our signal is the\\nnumber of events $n$ that pass a given selection, a Poissonian can be used\\nto model the distribution of $n$ with an expected number of events $s$:\\n\\\\begin{equation}\\nP(n;s) = \\\\frac{s^n e^{-s}}{n!}\\\\,.\\n\\\\end{equation}\\nIf a particular value of $n$ is measured, the posterior PDF of $s$ is (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(s|n) = \\\\frac{\\\\displaystyle\\n\\\\frac{s^n e^{-s}}{n!} \\\\pi(s)\\n}{\\\\displaystyle\\n\\\\int_0^{\\\\infty} \\\\frac{s^{\\\\prime n} e^{-s^{\\\\prime}}}{n!} \\\\pi(s^\\\\prime)\\\\,\\\\mathrm{d}s^\\\\prime\\n}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\pi(s)$ is the assumed prior for $s$. If we take $\\\\pi(s)$ to be uniform, performing the\\nintegration gives a denominator equal to one, hence:\\n\\\\begin{equation}\\nP(s|n) =\\\\frac{s^n e^{-s}}{n!}\\\\,.\\n\\\\end{equation}\\nNote that though Eqs.~(\\\\ref{eq:PoissonBayesInference}) and~(\\\\ref{eq:PoissonBayesInferencePost}) lead to the\\nsame expression, the former is a probability for the discrete random variable $n$, the latter is a\\nposterior PDF of the unknown parameter $s$.\\nFrom Eq.~(\\\\ref{eq:PoissonBayesInference}), the mode $\\\\hat{s}$ is equal to $n$, but\\n$\\\\left<s\\\\right> = n+1$, due to the asymmetric distribution of $s$, and\\n$\\\\mathbbm{V}\\\\mathrm{ar}[s] = n+1$, while the variance of $n$ for a Poissonian\\ndistribution is $\\\\sqrt{s}$ (Sec.~\\\\ref{sec:Poissonian}).\\nFigure~\\\\ref{fig:PoissonBayesPost} shows two cases of posterior PDF of $s$, for the cases $n=5$ (left)\\nand for $n=0$ (right). In the case $n=5$, a central value $\\\\hat{s}=5$ can be taken as most probable value.\\nIn that plot, a central interval was chosen (Eq.~(\\\\ref{eq:BayesCentralLeft},~\\\\ref{eq:BayesCentralRight})).\\nFor the case $n=0$, the most probable value of $s$ is $\\\\hat{s}=0$. A fully asymmetric interval corresponding\\nto a probability $1-\\\\alpha$ leads to an upper limit:\\n\\\\begin{equation}\\ne^{-s^{\\\\mathrm{up}}} = \\\\alpha\\\\,,\\n\\\\end{equation}\\nwhich then leads to:\\n\\\\begin{eqnarray}\\ns & < & s^{\\\\mathrm{up}} = 2.303\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.1\\\\,\\\\text{(90\\\\ s & < & s^{\\\\mathrm{up}} = 2.996\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.05\\\\,\\\\text{(95\\\\\\\\end{eqnarray}\\n\", \"\\\\section{Outline}\\nThis series of five lectures deals with practical aspects of statistical issues that arise in typical \\nHigh Energy Physics analyses. The topics are:\\n\\\\begin{itemize}\\n\\\\item{Introduction. This is largely a reminder of topics which you should have encountered as \\nundergraduates. Some of them are looked at in novel ways, and will hopefully provide new insights.}\\n\\\\item{Least Squares and Likelihoods. We deal with two different methods for parameter determination.\\nLeast Squares is also useful for Goodness of Fit testing, while likelihood ratios play a crucial role in\\nchoosing between two hypotheses.}\\n\\\\item{Bayes and Frequentism. These are two fundamental and very different approaches to statistical\\nsearches. They disagree even in their views on `What is probability?'}\\n\\\\item{Searches for New Physics. Many statistical issues arise in searches for New Physics. These may \\nresult in discovery claims, or alternatively in exclusion of theoretical models in some region of their\\nparameter space (e.g. mass ranges).} \\n\\\\item{Learning to love the covariance matrix. This is relevant for dealing with the possible correlations\\nbetween uncertainties on two or more quantities. The covariance matrix takes care of all these\\ncorrelations, so that you do not have to worry about each situation separately.\\nThis was an unscheduled lecture which was included at the request of several students.}\\n\\\\end{itemize}\\nLectures 3 to 5 are not included in these proceedings but can be found elsewhere~\\\\cite{Lecture3,Lecture4,Lecture5}. \\nThe material in these lectures follows loosely that in my book \\\\cite{LL_book}, together with some \\nsignificant updates (see ref. \\\\cite{LL_book_update}). \\n\", \"\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Parametrizing the alternative hypothesis}\\nFor true model-independent searches, no assumption should be made about the alternative model. In Ref.~\\\\citen{Kuusela_2012}, the alternative hypothesis is parametrized as a mixture of the background model and a number of additional Gaussians. Another method is the New Physics Learning Machine (NPLM)~\\\\cite{nplm}. Here, the alternative hypothesis is being parametrized by the network itself: given a dataset and a reference sample (like Monte Carlo simulation or data from a data sideband), a neural network is constructed such that it parametrises the alternative model as small perturbations away from the reference. When this model is trained, it learns the maximum likelihood fit to the data by construction, since its loss incorporates the log likelihood of the data. Its output is the ratio between the best-fit data distribution and the reference distribution, which is used as a test statistic to select data that displays a high level of discrepancy with the reference model. This ratio measures the disagreement between the reference model and the data and can be used for hypothesis testing. An overview of the NPLM design in shown in Figure~\\\\ref{fig:nplm}. A drawback of this method is the difficulty of defining the reference sample $\\\\mathcal{R}$. For example, $\\\\mathcal{R}$ can be a taken from Monte Carlo simulation, with the caveat that this might be a less than optimal approximation to nature. Alternatively, the reference sample can be taken from a data sideband. However, in this case the difficulty is to find a region that is signal free, but still statistically identical to the data signal region. \\nIntegrating NPLM with techniques such as CURTAINs or CATHODE offers a potential method for creating the reference sample. This entails training a conditional density estimator with data from signal-free sidebands, enabling effective extrapolation into the signal region.\\nFor the technique to be effective and avoid generating false positives, it's crucial that the density estimation maintains a high degree of accuracy throughout the entire spectrum of the variable of interest. One challenge arises when integrating NPLM in its full power, which is capable of identifying overdensities across multiple dimensions simultaneously, with conditional density estimation. This integration demands conditioning on multiple variables at the same time, adding a layer of difficulty to the process.\\n\\\\noindent\\nA challenge in utilizing anomaly detection for discovering new physics lies in the inherent difficulty of optimizing these algorithms when the nature of the signal remains unknown a priori. Moreover, the sensitivity of various anomaly detection methods can vary considerably depending on the type of signal, as demonstrated in Figure~\\\\ref{fig:pvals} and elaborated in Ref.~\\\\citen{Harris:2881089}. The best one can hope to do is to monitor the performance on wide variety of different potential signals \", '\\\\section{Upper Limits with RooFit}\\nDue to the computational complexity of calculating upper limits and experimental sensitivity, high-level statistical packages have been developed for the accurate definition of distribution functions, integration methods, and Monte Carlo generation. Among these, \\\\texttt{RooFit}, developed by Stanford University, stands out for its application in the CMS and ATLAS collaborations~\\\\cite{verkerke2006roofit,schott2012roostats}. Extensive documentation of these implementations can be found in the \\\\href{https://roostatsworkbook.readthedocs.io/en/latest/docs-cls_toys.html}{RooFit Workbook}. This document provides access to the creation of the \\\\texttt{RooFit} workspace, as well as the code to obtain upper limits using the profile binned likelihood method and the asymptotic approximation~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/tree/main/RooStats}{Source code}}. The asymptotic approximation is widely discussed in other sources~\\\\cite{cowan2014statistics,cowan2011asymptotic}, so the conceptual part was omitted in this report. Figure~[\\\\ref{fig:25}] shows the upper limits for the mass point $m(H) = 124 \\\\ \\\\text{GeV}$ using the profile likelihood method and the asymptotic approximation. Note the agreement in the values of both methods, demonstrating a significant discrepancy between observation and the background-only hypothesis~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/tree/main/RooStats}{Source code}}.\\n', '\\\\section{Frequentist Statistical Procedures}\\n\\nHere I summarize the procedure used by the LHC Higgs combination group for computing frequentist $p$-values uses for \\nquantifying the agreement with the background-only hypothesis and for determining exclusion limits. \\nThe procedures are based on the profile likelihood ratio test statistic. \\nThe parameter of interest is the overall signal strength factor $\\\\mu$, which acts as a scaling to the total rate of signal events. We often write $\\\\mu=\\\\sigma/\\\\sigma_{SM}$, where $\\\\sigma_{SM}$ is the standard model production cross-section; however, it should be clarified that the same $\\\\mu$ factor is used for all production modes and could also be seen as a scaling on the branching ratios. The signal strength is called so that $\\\\mu=0$ corresponds to the background-only model and $\\\\mu=1$ is the standard model signal. It is convenient to separate the full list of parameters $\\\\vec\\\\alpha$ into the parameter of interest $\\\\mu$ and the nuisance parameters $\\\\vec\\\\theta$: $\\\\vec\\\\alpha=(\\\\mu,\\\\vec\\\\theta)$.\\nFor a given data set $\\\\datasim$ and values for the global observables $\\\\globs$ there is an associated likelihood function over $\\\\mu$ and $\\\\theta$ derived from combined model over all the channels including all the constraint terms in Eq.~\\\\ref{Eq:ftot}\\n\\\\begin{equation}\\nL(\\\\mu,\\\\vec\\\\theta;\\\\datasim,\\\\globs) = \\\\F_{\\\\rm tot}(\\\\datasim,\\\\globs|\\\\mu,\\\\vec\\\\theta) \\\\;.\\n\\\\end{equation}\\nThe notation $L(\\\\mu,\\\\vec\\\\theta)$ leaves the dependence on the data implicit, which can lead to confusion. Thus, we will explicitly write the dependence on the data when the identity of the dataset is important and only suppress $\\\\datasim,\\\\globs$ when the statements about the likelihood are generic.\\nWe begin with the definition of the procedure in the abstract and then describe three implementations of the method based on asymptotic distributions, ensemble tests (Toy Monte Carlo), and importance sampling.\\n', \"\\\\section{Discoveries and upper limits}\\n\\nThe process towards a discovery, from the point of view of data analysis,\\nproceeds starting with a test of our data sample against two hypotheses concerning the theoretical underlying model:\\n\\\\begin{itemize}\\n\\\\item $H_0$: the data are described by a model that contains background only;\\n\\\\item $H_1$: the data are described by a model that contains a new signal plus background.\\n\\\\end{itemize}\\nThe discrimination between the two hypotheses can be based on a test statistic $\\\\lambda$ whose distribution\\nis known under the two considered hypotheses.\\nWe may assume that $\\\\lambda$ tends to have (conventionally) large values if $H_1$ is true and small values if $H_0$ is true.\\nThis convention is consistent with using as test statistic the likelihood ratio $\\\\lambda =L(x|H_1)/L(x|H_0)$,\\nas in the Neyman--Pearson lemma (Eq.~(\\\\ref{eq:neymanPearsonLemma})).\\nUnder the frequentist approach, it's possible to compute a $p$-value equal to the probability that\\n$\\\\lambda$ is greater or equal to than the value $\\\\lambda^{\\\\mathrm{obs}}$ observed in data.\\nSuch $p$-value is usually converted into an equivalent probability computed as the area\\nunder the rightmost tail of a standard normal distribution:\\n\\\\begin{equation}\\np = \\\\int_Z^{+\\\\infty} \\\\frac{1}{\\\\sqrt{2\\\\pi}}e^{-{x^2}/{2}}\\\\,\\\\mathrm{d}x = 1 - \\\\Phi(Z)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\Phi$ is the cumulative (Eq.~(\\\\ref{eq:cumulative})) of a standard normal distribution.\\n$Z$ in Eq.~(\\\\ref{eq:significance}) is called {\\\\it significance level}.\\nIn literature conventionally a signal with a significance of at least 3 ($3\\\\sigma$ {\\\\it level})\\nis claimed as {\\\\it evidence}. It corresponds to a $p$-value of \\n$1.35\\\\times 10^{-3}$ or less. If the significance exceeds 5 ($5\\\\sigma$ {\\\\it level}), i.e.: the\\n$p$-value is below $2.9\\\\times10^{-7}$, one is allowed to claim the {\\\\it observation} of the new signal.\\nIt's worth noting that the probability that background produces a large test statistic is not equal to\\nthe probability of the null hypothesis (background only), which has only a Bayesian sense.\\nFinding a large significance level, anyway, is only part of the discovery process in the\\ncontext of the scientific method. Below a sentence is reported from a recent statement of the\\nAmerican Statistical Association:\\n\\\\begin{displayquote}\\n{\\\\it The p-value was never intended to be a substitute for scientific reasoning. Well-reasoned statistical arguments contain much more than the value of a single number and whether that number exceeds an arbitrary threshold. The ASA statement is intended to steer research into a `post p < 0.05 era'}~\\\\cite{pvalASA}.\\n\\\\end{displayquote}\\nThis was also remarked by the physicists community, for instance by Cowan {\\\\it et al.}:\\n\\\\begin{displayquote}\\n\\\\textit{It should be emphasized that in an actual scientific context, rejecting the background-only hypothesis in a statistical sense is only part of discovering a new phenomenon. One's \\\\textbf{degree of belief} that a new process is present will depend in general on other factors as well, such as the plausibility of the new signal hypothesis and the degree to which it can describe the data}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\n\", \"\\\\section{Likelihood}\\nThe likelihood function is very widely used in many statistics applications. In this \\nSection, we consider it just for Parameter Determination. An important feature of the \\nlikelihood approach is that it can be used with {\\\\bf unbinned} data, and\\nhence can be applied in situations where there are not enough individual observations\\nto construct a histogram for the $\\\\chi^2$ approach. \\nWe start by assuming that we wish to fit our data $x$, using a model $f(x;\\\\mu)$ \\nwhich has one or more free parameters $\\\\mu$, whose value(s) we need to determine. \\nThe function $f$ is known as the `probability distribution' ($pdf$) and \\nspecifies the probability (or probability density, for the data having continuous as\\nopposed to discrete values) for obtaining different values of the data, when the parameter(s)\\nare specified. Without this \\nit is impossible to apply the likelihood (or many other) approaches. \\nFor example $x$ could be observations of a variable of interest within some \\nrange, and $f$ could be\\nany function such as a straight line, with gradient and intercept as parameters.\\nBut we will start with an angular distribution\\n\\\\begin{equation}\\ny(\\\\cos\\\\theta;\\\\beta) = \\\\frac{d\\\\ p}{d\\\\cos\\\\theta} = N(1+\\\\beta \\\\cos^2\\\\theta)\\n\\\\end{equation}\\nHere $\\\\theta$ is the angle at which a particle is observed, $dp/d\\\\cos\\\\theta$ is the $pdf$\\nspecifying the probability density for observing a decay at any $\\\\cos\\\\theta$, $\\\\beta$ is\\nthe parameter we want to determine, and $N$ is the crucial nomalisation factor \\nwhich ensures that the probability of observing a given decay at any $\\\\cos\\\\theta$\\nin the whole range from $-1$ to $+1$ is unity. In this case $N = 1/(2(1+\\\\beta/3))$. \\nThe data consists of $N$ decays, with their individual observations $\\\\cos\\\\theta_i$.\\nAssuming temporarily that the value of the parameter $\\\\beta$ is specified,\\nthe probability density $y_1$ of observing the first decay at $\\\\cos\\\\theta_1$ is\\n\\\\begin{equation}\\ny_1 = N (1+\\\\beta \\\\cos^2\\\\theta_1) = 0.5 (1+\\\\beta \\\\cos^2\\\\theta_1)/(1 + \\\\beta/3),\\n\\\\end{equation}\\nand similarly for the rest of the $N$ observations. Since the individual observations\\nare independent, the overall probability $P(\\\\beta)$ of observing the complete data set\\nof $N$ events is given by the product of the individual probabilities\\n\\\\begin{equation}\\nP(\\\\beta) = \\\\Pi y_i = \\\\Pi \\\\ 0.5 (1+\\\\beta \\\\cos^2\\\\theta_i)/(1 + \\\\beta/3) \\n\\\\end{equation}\\nWe imagine that this is computed for all values of the parameter $\\\\beta$; \\nthen this is known as the likelihood function ${\\\\it L}(\\\\beta)$.\\nThe likelihood method then takes as the estimate of $\\\\beta$ that value which \\nmaximises the likelihood. That is, it is the value which maximises (with respect to \\n$\\\\beta$) the probability density of observing the given data set. Conversely \\nwe rule out values of $\\\\beta$ for which ${\\\\it L}(\\\\beta)$ is very small. The\\nuncertainty on $\\\\beta$ is related to the width of the ${\\\\it L}(\\\\beta)$ \\ndistribution (see later). \\nIt is often convenient to consider the logarithm of the likelihood\\n\\\\begin{equation} \\n{\\\\it l} = \\\\ln{\\\\it L} = \\\\Sigma \\\\ln y_i\\n\\\\end{equation}\\nOne reason for this is that, for a large number of observations \\nsome fraction could have small $y_i$. Then the likelihood, involving the product of the\\n$y_i$, could be very small and may underflow the computer's range for real numbers.\\nIn contrast, {\\\\it l} involves a sum rather than a product, and $\\\\ln y_i$ rather than \\n$y_i$, and so produces a gentler number.\\n\\\\subsection{Likelihood and $pdf$}\\nThe procedure for constructing the likelihood is first to write down the $pdf$, and then to insert into that \\nexpression the observed data values in order to evaluate their product, which is the likelihood. Thus both \\nthe $pdf$ and the likelihood involve the data $x$ and the parameter(s) $\\\\mu$. The difference is that the $pdf$ is a function of $x$ for fixed values of $\\\\mu$, while the likelihood is a function of $\\\\mu$ given the fixed observed \\ndata $x_{obs}$. \\nThus for a Poisson distribution, the probability of observing $n$ events when the rate $\\\\mu$ is specified is \\n\\\\begin{equation}\\nP(n;\\\\mu) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $n$, while the likelihood is\\n\\\\begin{equation}\\nL(\\\\mu;n) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $\\\\mu$ for the fixed observed number $n$.\\n\\\\subsection{Intuitive example: Location and width of peak}\\nWe consider a \\nsituation where we are studying a resonant state which would result in a bump in the mass distribution of its decay particles.\\nWe assume that the bump can be parametrised as a simple Breit-Wigner\\n\\\\begin{equation}\\ny(m;M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nwhere $y$ is the probability density of obtaining a mass $m$ if the location and width the state are $M_0$ and $\\\\Gamma$,\\nthe parameters we want to determine. It is essential that $y$ is normalised, i.e. its integral over all physical values of \\n$m$ is unity; hence the normalisation factor of $\\\\Gamma/(2\\\\pi)$. The data consists of $n$ observations of $m$, as shown in fig. \\\\ref{fig:L_for_Resonance}.\\nAssume for the moment that we know $M_0$ and $\\\\Gamma$. Then the probability density for observing the $i^{th}$\\nevent with mass $m_i$ is\\n\\\\begin{equation}\\ny_i(M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nSince the events are independent, the probability density for observing the whole data sample is\\n\\\\begin{equation}\\ny_{all}(M_0,\\\\Gamma) =\\\\Pi \\\\ \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nand this is known as the likelihood $L(M_0,\\\\Gamma)$. Then the best values for the parameters are taken as\\nthe combination that maximises the probability density for the whole data sample i.e. $L(M_0,\\\\Gamma)$. \\nParameter values for which $L$ is very small compared to its maximum value are rejected, and the uncertainties \\non the parameters are related to the width of the distribution of $L$; we will be more specific later.\\nThe curve in\\nfig. \\\\ref{fig:L_for_Resonance}(left) shows the expected probability distribution for fixed parameter values. The way $L$ is calculated involves\\nmultiplying the heights of the curve at all the observed $m_i$ values. If we now consider varying $M_0$, this moves the curve bodily to the left or right without changing its shape or normalisation. So to determine the best value of $M_0$, we need to find where to locate the curve so that the product of the heights is a maximum; it is plausibe that the peak will be located where the majority of events are to be found.\\nNow we will consider how the optimum value of $\\\\Gamma$ is obtained. A small $\\\\Gamma$ results in a narrow curve, so the masses in the tail will make an even smaller contribution to the product in eqn. \\\\ref{product}, and hence reduce the likelihood. But a large $\\\\Gamma$ is not good, because not only is the width larger, but because of the normalisation condition, the peak height is reduced, and so the observations in the peak region make a smaller contribution to the likelihood. The optimal \\n$\\\\Gamma$ involves a trade-off between these two effects.\\nOf course, in finding the optimal of values of the two parameters, in general it is necessary to find the maximum of the \\nlikelihood as a function of the two parameters, rather than maximising with respect to just one, and then with respect to the other and then stopping (see section \\\\ref{More_variables}).\\n\\\\subsection{Uncertainty on parameter}\\nWith a large amount of data, the likelihood as a function of a parameter $\\\\mu$ is \\noften approximately Gaussian. In that case, ${\\\\it l}$ is an upturned parabola. Then\\nthe following definitions of $\\\\sigma_\\\\mu$, the uncertainty on $\\\\mu_{best}$, \\nyield identical answers:\\n\\\\begin{itemize}\\n\\\\item{The RMS of the likelihood distribution.}\\n\\\\item{[$-\\\\frac{d^2 {\\\\it l}}{d \\\\mu^2}]^{-1/2}$. If you remember that \\nthe second derivative of the log likelihood function is involved because it \\ncontrols the width of the ${\\\\it l}$ distribution, a mneumonic helps \\nyou remember the formula for $\\\\sigma_\\\\mu$: Since $\\\\sigma_\\\\mu$ must have the \\nsame units as $\\\\mu$, the second derivative must appear to the power $-1/2$. But because the\\nlog of the likelihood has a maximum, the second derivative is negative, so the minus \\nsign is necessary before we take the square root.}\\n\\\\item{It is the distance in $\\\\mu$ from the maximum in order to decrease ${\\\\it l}$ by half a unit\\nfrom its maximum value. i.e.\\n\\\\begin{equation}\\n{\\\\it l} (\\\\mu_{best} + \\\\sigma_{\\\\mu}) = {\\\\it l}_{max} - 0.5 \\n\\\\end{equation}\\n}\\n\\\\end{itemize} \\nIn situations where the likelihood is not Gaussian in shape, these three definitions no longer agree.\\nThe third one is most commonly used in that case. Now the upper and lower ends of the intervals can \\nbe asymmetric with respect to the central value. It is a mistake to believe that this method \\nprovides intervals which have a $68\\\\parameter\\\\footnote{Unfortunately, this incorrect statement occurs in my book\\\\cite{LL_book}. It is \\ncorrected in a separate update\\\\cite{LL_book_update}.}.\\nSymmetric uncertainties are easier to work with than asymmetric ones. It is thus sometimes better to quote the \\nuncertainty on a function of the first variable you think of. For example, for a charged particle in a magnetic field,\\nthe reciprocal of the momentum has a nearly symmetric uncertainty. Especially for high\\nmomentum tracks, the upper uncertainty on the momentum can be much larger than the lower one \\ne.g. $1.0\\\\ ^{+1.5}_{-0.4}$ TeV.\\n\\\\subsection{Coverage}\\nAn important feature of any statistical method for estimating a range for some parameter $\\\\mu$ at a \\nspecified confidence level $\\\\alpha$ is its coverage $C$. If the procedure is applied many times, \\nthese ranges will vary because of statistical fluctuations in the observed data. Then $C$ is defined as\\nthe fraction of ranges which contain the true value $\\\\mu_{true}$; it can vary with $\\\\mu_{true}$. \\nIt is very\\nimportant to realise that coverage is a property of the {\\\\bf statistical procedure} and does not apply\\nto your particular measurement. An ideal plot of coverage as a function of $\\\\mu$ would have $C$ constant \\nat its nominal value $\\\\alpha$. For a Poisson counting experiment, figure \\\\ref{fig:PoissonCoverage} shows $C$ as a \\nfunction of the Poisson parameter $\\\\mu$, when the observed number of counts $n$ is used to determine a range \\nfor $\\\\mu$ via the change in log-likelihood being 0.5. The coverage is far from constant at small $\\\\mu$.\\nIf $C$ is smaller than $\\\\alpha$, this is known as undercoverage. Certainly frequentists would regard this \\nas unfortunate; it means that people reading an article containing parameters determined this way are \\nlikely to place more than justified reliance on the quoted range. Methods using the Neyman construction \\nto determine parameter ranges by construction do not have undercoverage. \\nCoverage involves a statement about $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$. This is to be interpreted as a\\nprobability statement about how often the ranges $\\\\mu_l$ to $\\\\mu_u$ contain the (unknown but constant) true\\nvalue $\\\\mu_{true}$. This is a frequentist statement; Bayesians do not want to consider the ensemble of possible\\nresults if the measurement procedure were to be repeated. Thus Bayesians would regard the statement\\nabout $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$ as describing what fraction of their estimated \\nposterior probability density for $\\\\mu_{true}$ would be \\nbetween the fixed values $\\\\mu_l$ and $\\\\mu_u$, derived from their actual measurement.\\n\\\\subsection{More than one parameter}\\nFor the case of just one parameter $\\\\mu$, the likelihood best estimate $\\\\hat{\\\\mu}$ is given \\nby the value of $\\\\mu$ which maximises $L$. Its uncertainty $\\\\sigma_\\\\mu$ is determined either from \\n\\\\begin{equation}\\n1/\\\\sigma_\\\\mu^2 = -d^2\\\\ln L/d\\\\mu^2 ;\\n\\\\end{equation} \\nof by finding how far $\\\\hat{\\\\mu}$ would have to be changed in order to reduce $\\\\ln L$ by 0.5.\\nWhen we have two or more parameters $\\\\beta_i$ the rule for finding the best estimates $\\\\hat{\\\\beta_i}$\\nis still to maximise $L$.\\nFor the uncertainties and their correlations, the generalisation of equation \\\\ref{error} is to construct\\nthe inverse covariance matrix ${\\\\bf M}$, whose elements are given by\\n\\\\begin{equation}\\nM_{ij} = -\\\\frac{\\\\partial^2 \\\\ln L} {\\\\partial \\\\beta_i\\\\ \\\\partial \\\\beta_j} \\n\\\\end{equation} \\nThen the inverse of $\\\\bf{M}$ is the covariance matrix, whose diagonal elements are the variances of $\\\\beta_i$,\\nand whose off-diagonal ones are the covariances.\\nAlternatively (and more common in practice), the uncertainty on a specific $\\\\beta_j$ can be obtained \\nby using the profile likelihood $L_{prof}(\\\\beta_j)$.\\nThis is the likelihood as a function of the specific $\\\\beta_j$, where for each value of $\\\\beta_j,\\\\ L$ has been remaximised\\nwith respect to all the other $\\\\beta$. Then $L_{prof}(\\\\beta_j)$ is used with the `reduce $\\\\ln L_{prof}$ = 0.5' rule\\nto obtain the uncertainty on $\\\\beta_j$. This is equivalent to determining the contour in $\\\\beta$-space where \\n$\\\\ln L = \\\\ln L_{max} - 0.5$, and finding the values $\\\\beta_{j,1}$ and $\\\\beta_{j,2}$ on the contour which are \\nfurthest from $\\\\hat{\\\\beta_j.}$ Then the (probably asymmetric) upper and lower uncertainties on $\\\\beta_j$ are \\ngiven by $\\\\beta_{j,2}- \\\\hat{\\\\beta_j}$ and $\\\\hat{\\\\beta_j} - \\\\beta_{j,1}$ respectively.\\nBecause these are likelihood methods of obtaining the intervals, these estimates of uncertainities provide only\\n{\\\\bf nominal} regions of 68\\\\the region within \\nthe contour described in the previous paragraph for the multidimensional $\\\\beta$ space will have less than 68\\\\overage. To achieve that, the $`0.5'$ in the rule for how much $\\\\ln L$ has to be reduced from its maximum \\nmust be replaced by a larger number, whose value depends on the dimensionality of $\\\\beta$.\\n\", \"\\\\section{Upper Limits using the profile binned likelihood}\\nThe profile binned likelihood method is chosen by particle physics collaborations to present phenomenology studies and experimental analyses. As shown above, this method is based on estimating the signal confidence level through a fully frequentist approach focused on parameter estimation~\\\\cite{lista2016practical,barlow2002systematic}. To include systematic effects, the likelihood function is extended with appropriate distribution functions to describe efficiency effects with a given width \\\\( \\\\sigma \\\\). This methodology requires maximizing the likelihood function and obtaining the background profile as a function of the signal strength \\\\( \\\\mu \\\\). The improvement lies in replacing the normalization of the posterior distribution with a multivariable optimization problem, which is computationally less costly and leads to limits that allow for better model exclusion. The statistical estimator is given by~\\\\cite{conway2005calculation,cms2012observation,atlas2012observation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = - 2 ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu, \\\\hat{\\\\hat{b}}(\\\\mu) )}{ \\\\mathcal{L}(\\\\hat{\\\\mu},\\\\hat{b}) } \\\\bigg).\\n\\\\end{equation}\\nwhere \\\\( \\\\hat{\\\\mu} \\\\) and \\\\( \\\\hat{b} \\\\) are the unconditional maximum likelihood estimators, and \\\\( \\\\hat{\\\\hat{b}}(\\\\mu) \\\\) is the conditional maximum likelihood estimator. Frequentist limits are generally less restrictive and allow for exploring regions of significance while adequately accounting for systematic effects. Similar to the Bayesian approach, maximizing the likelihood as a function of \\\\( \\\\mu \\\\) enables finding the profile likelihood that propagates the effect of the background parameters \\\\( \\\\epsilon \\\\).\\nSystematic effects lead to higher upper limits, which restrict the model exclusion power and experimental sensitivity. For example, in a single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), the \\\\texttt{optimize} package is used to find the best-fit parameters, and Monte Carlo methods are applied to sample the estimator \\\\( q_{\\\\mu} \\\\). The extended likelihood function is given by:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu, \\\\epsilon) = \\\\frac{ e^{ -(\\\\mu s + \\\\epsilon b) } (\\\\mu s + \\\\epsilon b)^{n} }{n!} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^{2}} } e^{ -\\\\frac{(1-\\\\epsilon)^{2}}{2\\\\sigma^{2}} },\\n\\\\end{equation}\\nwhere maximizing the likelihood function for the efficiency \\\\( \\\\epsilon \\\\) yields the conditional nuisance estimator \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\):\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) = \\\\frac{1}{2b} \\\\bigg[ ( b -\\\\mu s - \\\\sigma^{2}b^{2}) + \\\\sqrt{ (b + \\\\mu s - \\\\sigma^{2}b^{2})^{2} + 4 b^{2} \\\\sigma^{2}n} \\\\bigg]. \\n\\\\end{equation}\\nFor the single-channel experiment, Figure~[\\\\ref{fig:22}] shows the profile of the nuisance parameter for various values of \\\\( \\\\mu \\\\). The maximum value of the likelihood function shifts depending on the signal strength \\\\( \\\\mu \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/ProfileLikelihood/ProfileLikelihoodNuissance.ipynb}{Source code}}. The right plot shows the maximum likelihood estimate \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\) using both Equation~(\\\\ref{eq:profile}) and the optimization package. This result indicates the maximal behavior of the uncertainty for each value of the signal strength \\\\( \\\\mu \\\\), ensuring model exclusion as restrictive as allowed by the uncertainty \\\\( \\\\sigma \\\\). More generally, the multi-channel case requires a fully numerical procedure to find the profiles of the nuisance parameters and \\\\( q_{\\\\mu} \\\\)~\\\\cite{lista2016practical,cranmer2015practical}.\\nTo illustrate the behavior of the estimator \\\\( q_{\\\\mu} \\\\) as a function of the systematic uncertainty \\\\( \\\\sigma \\\\), a sweep over the signal strength \\\\( \\\\mu \\\\) is performed while optimizing the estimator for the current values of the observation, background component, and signal events. Figure~[\\\\ref{fig:23}] shows the profile binned likelihood for the single-channel experiment as a function of the width of the systematic uncertainty. A larger uncertainty leads to a higher upper limit, which can be quantified using Wald's approximation \\\\( Z(3\\\\sigma) = \\\\sqrt{q_{\\\\mu}} \\\\)~\\\\cite{cowan2011asymptotic}. \\nThe green dashed line represents the observed upper limit for each profile likelihood. For example, for \\\\( \\\\sigma = 0.20 \\\\), the observed upper limit is \\\\( \\\\mu_{up} \\\\approx 4.5 \\\\), which contrasts with the value obtained from the Bayesian approach (Table~[\\\\ref{tb:3}], \\\\( \\\\mu_{up} = 4.91 \\\\)), a smaller value. The right plot represents the search for the p-value while fixing the value of the systematic uncertainty. Obtaining the pseudo-data requires generating an observation consistent with the background-only hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\), and for the signal + background hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\mu s + \\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\) in each observable channel. For each generated value of \\\\( n \\\\) across all channels, the optimization of \\\\( (\\\\hat{\\\\mu}, \\\\hat{\\\\epsilon}, \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu)) \\\\) is performed.\\nTable~[\\\\ref{tb:4}] shows the upper limit values using the grouped profile likelihood method for several values of \\\\( \\\\sigma \\\\). In comparison with the upper limits obtained by the Bayesian method, consistency is observed within the statistical confidence levels inherent to the sampling.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\n$\\\\sigma$ & Profile likelihood Ratio & MCMC algorithm \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{2}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 \\\\\\\\\\n0.10 & 3.52 & 3.31 \\\\\\\\\\n0.20 & 4.57 & 4.66 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nFor the multi-channel case, results are very close between the upper limits without uncertainty and those found using the profile likelihood method. This behavior is attributed to the combined uncertainty of the background across the 30 channels, which does not significantly affect the confidence in the signal strength, especially in the resonance region~\\\\cite{cms2022portrait,atlas2012observation,cowan2011asymptotic}. Table~[\\\\ref{tb:5}] shows the upper limit values for several mass points \\\\( m(H) \\\\) and uncertainty \\\\( \\\\sigma \\\\).\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\nMass($H$)[GeV] & $\\\\sigma$ & Method: Profile likelihood ratio \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & $\\\\mu_{up}(95\\\\ 110 & 0.1 & 0.43 \\\\\\\\\\n110 & 0.2 & 0.45 \\\\\\\\\\n\\\\hline\\n124 & 0.1 & 1.45 \\\\\\\\\\n124 & 0.2 & 1.46 \\\\\\\\\\n\\\\hline\\n142 & 0.1 & 0.29 \\\\\\\\\\n142 & 0.2 & 0.31 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength for different mass points using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\n\\\\subsection{Experimental sensitivity with systematic effects}\\nIn the case of determining the experimental sensitivity for a specific model $s$, the Asimov data $n = s + b$ are used, and a modification is applied to the statistical estimator $q_{\\\\mu}$~\\\\cite{lista2016practical}.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nIn the Wald approximation, the significance is approximately given by:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\sqrt{q_{0}}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:24}] shows the profile likelihood for various values of the systematic uncertainty $\\\\sigma$. It demonstrates how statistical significance is impacted by uncertainty. In general, greater uncertainty in the estimation of background events leads to a loss of sensitivity in a potential experimental analysis aiming to validate a new hypothetical model.\\nThere are alternative methods to establish statistical significance~\\\\cite{cowan2011asymptotic}, such as:\\n\\\\begin{equation}\\nZ_{0}(\\\\sigma) = \\\\frac{s}{\\\\sqrt{s+(1+\\\\sigma)b}}. \\n\\\\end{equation}\\nHowever, in general, experimental sensitivity is overestimated because the maximal information of $\\\\hat{\\\\hat{b}}$ is not fully captured in the profile likelihood. Table~[\\\\ref{tb:6}] shows the statistical significance for various values of systematic uncertainty $\\\\sigma$. This effect reduces experimental sensitivity and should be calculated using the profile binned likelihood method. Note that the approximation $Z_{0} = s / \\\\sqrt{s + b}$ is no longer valid due to the convolution effect of counting and efficiency distributions.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & $Z$ & $Z_{0}=s/\\\\sqrt{s+b}$ & $Z_{0}(\\\\sigma)$ \\\\\\\\\\n\\\\hline\\n0.05 & 0.878 & 0.932 & 0.931 \\\\\\\\\\n0.1 & 0.695 & 0.932 & 0.912 \\\\\\\\\\n0.2 & 0.443 & 0.932 & 0.877 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Statistical significance as a function of systematic uncertainty for the case of a single-channel experiment. Note how the approximation $Z_{0}$ becomes invalid even for $s \\\\ll b$.}\\n\\\\end{center}\\n\\\\end{table}\\nAs shown in Table~[\\\\ref{tb:6}], when there is a 20\\\\\\n\"}},\n",
       "       {'entity_name': 'cramérrao bound', 'entity_type': 'statistics_concept', 'description': 'A lower bound on the variance of unbiased estimators, indicating the best possible precision that can be achieved in estimating a parameter.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Measurement as parameter estimation}\\nOne of the most common tasks of the working physicist is to estimate some model parameter. We do it so often, that we often don't realize it. For instance, the sample mean $\\\\bar{x} = \\\\sum_{e=1}^n x_e / n$ is an estimate for the mean, $\\\\mu$, of a Gaussian probability density $f(x|\\\\mu,\\\\sigma) =\\\\Gauss(x|\\\\mu,\\\\sigma)$. More generally, an \\\\textit{estimator} $\\\\hat{\\\\alpha}(\\\\data)$ is some function of the data and its value is used to estimate the true value of some parameter $\\\\alpha$. There are various abstract properties such as variance, bias, consistency, efficiency, robustness, etc~\\\\cite{James}. The bias of an estimator is defined as $B(\\\\hat\\\\alpha) = E[ \\\\hat\\\\alpha ]-\\\\alpha$, where $E$ means the expectation value of \\\\mbox{$E[ \\\\hat\\\\alpha ]=\\\\int\\\\hat\\\\alpha(x) f(x)dx$} or the probability-weighted average. Clearly one would like an unbiased estimator. The variance of an estimator is defined as $var[\\\\hat\\\\alpha] = E[ (\\\\alpha - E[\\\\hat{\\\\alpha}] )^2 ]$; and clearly one would like an estimator with the minimum variance. Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cram\\\\'er-Rao bound (that is the inverse of the Fisher information, which we will refer to again later). \\nThe most widely used estimator in physics is the maximum likelihood estimator (MLE). It is defined as the value of $\\\\alpha$ which maximizes the likelihood function $L(\\\\alpha)$. Equivalently this value, $\\\\hat{\\\\alpha}$, maximizes $\\\\log L(\\\\alpha)$ and minimizes $-\\\\log L(\\\\alpha)$. The most common tool for finding the maximum likelihood estimator is \\\\texttt{Minuit}, which conventionally minimizes $-\\\\log L(\\\\alpha)$ (or any other function)~\\\\cite{James:1975dr}. The jargon is that one `fits' the function and the maximum likelihood estimate is the `best fit value'. \\nWhen one has a multi-parameter likelihood function $L(\\\\vec{\\\\alpha})$, then the situation is slightly more complicated. The maximum likelihood estimate for the full parameter list, $\\\\hat{\\\\vec{\\\\alpha}}$, is clearly defined. The various components $\\\\hat{\\\\alpha}_p$ are referred to as the \\\\textit{unconditional maximum likelihood estimates}. In the physics jargon, one says all the parameters are `floating'. One can also ask about maximum likelihood estimate of $\\\\alpha_p$ is with some other parameters $\\\\vec{\\\\alpha}_o$ fixed; this is called the \\\\textit{conditional maximum likelihood estimate} and is denoted $\\\\hat{\\\\hat{\\\\alpha}}_p(\\\\vec{\\\\alpha}_o)$. These are important quantities for defining the profile likelihood ratio, which we will discuss in more detail later. The concept of variance of the estimates is also generalized to the covariance matrix $cov[\\\\alpha_p, \\\\alpha_{p'}] = E[(\\\\hat\\\\alpha_p - \\\\alpha_p)(\\\\hat\\\\alpha_{p'}- \\\\alpha_{p'})]$ and is often denoted $\\\\Sigma_{pp'}$. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie. $cov[\\\\alpha_p, \\\\alpha_{p}] = var[\\\\alpha_p]$.\\nIn the case of a Poisson model $\\\\Pois(n|\\\\nu)$ the maximum likelihood estimate of $\\\\nu$ is simply \\\\mbox{$\\\\hat{\\\\nu}=n$}. Thus, it follows that the variance of the estimator is $var[\\\\hat{\\\\nu}]=var[n]=\\\\nu$. Thus if the true rate is $\\\\nu$ one expects to find estimates $\\\\hat{\\\\nu}$ with a characteristic spread around $\\\\nu$; it is in this sense that the measurement has a estimate has some uncertainty or `error' of $\\\\sqrt{n}$. We will make this statement of uncertainty more precise when we discuss frequentist confidence intervals.\\nWhen the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution.\\\\footnote{There are various conditions that must be met for this to be true, but skip the fine print in these lectures. There are two conditions that are most often violated in particle physics, which will be addressed later.} This does not depend on the pdf $f(x)$ having a Gaussian form. For small samples this isn't the case, but this limiting distribution is often referred to as an \\\\textit{asymptotic distribution}.\\nFurthermore, under most circumstances in particle physics, the maximum likelihood estimate approaches the minimum variance or Cram\\\\'er-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by\\n\\\\begin{equation}\\n\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = E\\\\left[- \\\\frac{\\\\partial^2 \\\\log f(x|\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\middle| \\\\;\\\\vec\\\\alpha \\\\right ] \\\\;,\\n\\\\end{equation}\\nwhere I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the true value $\\\\vec\\\\alpha$. The right side of Eq.~\\\\ref{Eq:expfisher} is called the (expected) Fisher information matrix. Remember that the expectation involves an integral over the observables. Since that integral is difficult to perform in general, one often uses the observed Fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data\\n\\\\begin{equation}\\n\\\\tilde\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = - \\\\frac{\\\\partial^2 \\\\log L(\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\; .\\n\\\\end{equation}\\nThis is what \\\\texttt{Minuit}'s \\\\texttt{Hesse} algorithm\\\\footnote{The matrix is called the Hessian, hence the name.} calculates to estimate the covariance matrix of the parameters.\\n\"}},\n",
       "       {'entity_name': 'fisher information', 'entity_type': 'statistics_concept', 'description': 'A measure that quantifies the amount of information that an observable random variable carries about an unknown parameter upon which the probability depends. It is commonly used in the context of maximum likelihood estimation and is represented as a matrix that provides insights into the variance of maximum likelihood estimates.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Measurement as parameter estimation}\\nOne of the most common tasks of the working physicist is to estimate some model parameter. We do it so often, that we often don't realize it. For instance, the sample mean $\\\\bar{x} = \\\\sum_{e=1}^n x_e / n$ is an estimate for the mean, $\\\\mu$, of a Gaussian probability density $f(x|\\\\mu,\\\\sigma) =\\\\Gauss(x|\\\\mu,\\\\sigma)$. More generally, an \\\\textit{estimator} $\\\\hat{\\\\alpha}(\\\\data)$ is some function of the data and its value is used to estimate the true value of some parameter $\\\\alpha$. There are various abstract properties such as variance, bias, consistency, efficiency, robustness, etc~\\\\cite{James}. The bias of an estimator is defined as $B(\\\\hat\\\\alpha) = E[ \\\\hat\\\\alpha ]-\\\\alpha$, where $E$ means the expectation value of \\\\mbox{$E[ \\\\hat\\\\alpha ]=\\\\int\\\\hat\\\\alpha(x) f(x)dx$} or the probability-weighted average. Clearly one would like an unbiased estimator. The variance of an estimator is defined as $var[\\\\hat\\\\alpha] = E[ (\\\\alpha - E[\\\\hat{\\\\alpha}] )^2 ]$; and clearly one would like an estimator with the minimum variance. Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cram\\\\'er-Rao bound (that is the inverse of the Fisher information, which we will refer to again later). \\nThe most widely used estimator in physics is the maximum likelihood estimator (MLE). It is defined as the value of $\\\\alpha$ which maximizes the likelihood function $L(\\\\alpha)$. Equivalently this value, $\\\\hat{\\\\alpha}$, maximizes $\\\\log L(\\\\alpha)$ and minimizes $-\\\\log L(\\\\alpha)$. The most common tool for finding the maximum likelihood estimator is \\\\texttt{Minuit}, which conventionally minimizes $-\\\\log L(\\\\alpha)$ (or any other function)~\\\\cite{James:1975dr}. The jargon is that one `fits' the function and the maximum likelihood estimate is the `best fit value'. \\nWhen one has a multi-parameter likelihood function $L(\\\\vec{\\\\alpha})$, then the situation is slightly more complicated. The maximum likelihood estimate for the full parameter list, $\\\\hat{\\\\vec{\\\\alpha}}$, is clearly defined. The various components $\\\\hat{\\\\alpha}_p$ are referred to as the \\\\textit{unconditional maximum likelihood estimates}. In the physics jargon, one says all the parameters are `floating'. One can also ask about maximum likelihood estimate of $\\\\alpha_p$ is with some other parameters $\\\\vec{\\\\alpha}_o$ fixed; this is called the \\\\textit{conditional maximum likelihood estimate} and is denoted $\\\\hat{\\\\hat{\\\\alpha}}_p(\\\\vec{\\\\alpha}_o)$. These are important quantities for defining the profile likelihood ratio, which we will discuss in more detail later. The concept of variance of the estimates is also generalized to the covariance matrix $cov[\\\\alpha_p, \\\\alpha_{p'}] = E[(\\\\hat\\\\alpha_p - \\\\alpha_p)(\\\\hat\\\\alpha_{p'}- \\\\alpha_{p'})]$ and is often denoted $\\\\Sigma_{pp'}$. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie. $cov[\\\\alpha_p, \\\\alpha_{p}] = var[\\\\alpha_p]$.\\nIn the case of a Poisson model $\\\\Pois(n|\\\\nu)$ the maximum likelihood estimate of $\\\\nu$ is simply \\\\mbox{$\\\\hat{\\\\nu}=n$}. Thus, it follows that the variance of the estimator is $var[\\\\hat{\\\\nu}]=var[n]=\\\\nu$. Thus if the true rate is $\\\\nu$ one expects to find estimates $\\\\hat{\\\\nu}$ with a characteristic spread around $\\\\nu$; it is in this sense that the measurement has a estimate has some uncertainty or `error' of $\\\\sqrt{n}$. We will make this statement of uncertainty more precise when we discuss frequentist confidence intervals.\\nWhen the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution.\\\\footnote{There are various conditions that must be met for this to be true, but skip the fine print in these lectures. There are two conditions that are most often violated in particle physics, which will be addressed later.} This does not depend on the pdf $f(x)$ having a Gaussian form. For small samples this isn't the case, but this limiting distribution is often referred to as an \\\\textit{asymptotic distribution}.\\nFurthermore, under most circumstances in particle physics, the maximum likelihood estimate approaches the minimum variance or Cram\\\\'er-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by\\n\\\\begin{equation}\\n\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = E\\\\left[- \\\\frac{\\\\partial^2 \\\\log f(x|\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\middle| \\\\;\\\\vec\\\\alpha \\\\right ] \\\\;,\\n\\\\end{equation}\\nwhere I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the true value $\\\\vec\\\\alpha$. The right side of Eq.~\\\\ref{Eq:expfisher} is called the (expected) Fisher information matrix. Remember that the expectation involves an integral over the observables. Since that integral is difficult to perform in general, one often uses the observed Fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data\\n\\\\begin{equation}\\n\\\\tilde\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = - \\\\frac{\\\\partial^2 \\\\log L(\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\; .\\n\\\\end{equation}\\nThis is what \\\\texttt{Minuit}'s \\\\texttt{Hesse} algorithm\\\\footnote{The matrix is called the Hessian, hence the name.} calculates to estimate the covariance matrix of the parameters.\\n\", \"\\\\section{Probability}\\n\\\\subsection {Bayesian probability}\\nThe Bayesian definition of probability is that $P_A$ represents your belief in $A$.\\n1 represents certainty, 0 represents total disbelief.\\nIntermediate values can be calibrated by asking whether you would prefer to bet on $A$, or on a white ball being drawn from an urn containing a mix of white and black balls. \\nThis avoids the limitations of frequentist probability---coins, dice, kaons, rain tomorrow, existence of supersymmetry (SUSY) can all have probabilities assigned to them.\\nThe drawback is that your value for $P_A$ may be different from mine, or anyone else's. It is\\nalso called subjective probability.\\nBayesian probability makes great use of Bayes' theorem, in the form\\n\\\\begin{equation}P(Theory|Data)= {P(Data|Theory) \\\\over P(Data) } \\\\times P(Theory) \\\\quad.\\n\\\\end{equation}\\n$P(Theory)$ is called the {\\\\em prior}: your initial belief in $Theory$. $P(Data|Theory)$ is the {\\\\em Likelihood}:\\nthe probability of getting $Data$ if $Theory$ is true. $P(Theory|Data)$ is the {\\\\em Posterior}: your belief in $Theory$ \\nin the light of a particular $Data$ being observed.\\nSo this all works very sensibly. If the data observed is predicted by the theory, your belief in that theory is boosted,\\nthough this is moderated by the probabilty that the data could have arisen anyway. Conversely, if data is observed which \\nis disfavoured by the theory, your belief in that theory is weakened.\\nThe process can be chained. \\nThe posterior from a first experiment can be taken as the prior for a second experiment, and so on. \\nWhen you write out the factors you find that the order doesn't matter. \\n\\\\subsubsection{Prior distributions}\\nOften, though, the theory being considered is not totally defined: it may contain a parameter (or several parameters)\\nsuch as a mass, coupling constant, or decay rate. Generically we will call this $a$, with the proviso that it may\\nbe multidimensional. \\nThe prior is now not a single number $P(Theory)$ \\nbut a probability distribution $P_0(a)$. \\n$\\\\int_{a_1}^{a_2} P_0(a)\\\\, da $ is your prior belief that $a$ lies between $a_1$ and $a_2$.\\n$\\\\int_{-\\\\infty}^{\\\\infty} P_0(a)\\\\, da$ is your original $P(Theory)$. This is generally taken as 1, which is valid provided the possibility that the theory that is false is matched by some value of $a$---for example if the coupling constant for a hypothetical particle is zero, that accommodates any belief that it might not exist. Bayes' theorem then runs:\\n\\\\begin{equation} P_1(a;x) \\\\propto L(a;x) P_0(a)\\n\\\\quad.\\n\\\\end{equation}\\nIf the range of $a$ is infinite, $P_0(a)$ may be vanishingly small (this is called an `improper prior'). \\nHowever this is not a problem. Suppose, for example, that all we know about $a$ is that it is non-negative, and we\\nare genuinely equally open to its having any value. We write $P_0(a)$ as $C$, so $\\\\int_{a_1}^{a_2} P_0(a)\\\\, da =C(a_2-a_1)$.\\nThis probability is vanishingly small: if you were offered the choice of a bet on $a$ lying within the range $[a_1,a_2]$\\nor of drawing a white ball from an urn containing 1 white ball and $N$ black balls, you would choose the latter, however large $N$ was. However it is not zero: if the urn contained $N$ black balls, but no white ball, your betting choice would change. After a measurement you have\\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) C da'} C$, and the factors of $C$ can be cancelled (which, and this is the point, you could {\\\\em not} do if $C$ were exactly zero) giving \\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) da'} $ or,\\n$\\nP_1(a;x) \\\\propto L(a;x) $,\\nand you can then just normalize $P_1(a)$ to 1.\\nFigure~\\\\ref{fig:bayes1} shows Eq.~\\\\ref{eq:bayes} at work. Suppose $a$ is known to lie between 0 and 6, and\\nthe prior distribution is taken as flat, as shown in the left hand plot. A measurement of $a$ gives a result \\n$4.4 \\\\pm 1.0$~, as shown in the central plot. The product of the two gives (after normalization) the posterior, as shown in the right hand plot.\\n\\\\subsubsection{Likelihood}\\nThe likelihood---the number $P(Data|Theory)$---is now generalised to the function $L(a,x)$, where $x$ is the observed value of the data. Again, $x$ may be multidimensional, but in what follows it is not misleading to ignore that.\\nThis can be confusing. For example, anticipating Section~\\\\ref{sec:poisson}, the probability of getting $x$ counts from a Poisson process with mean $a$ is\\n\\\\begin{equation}\\nP(x,a)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWe also write\\n\\\\begin{equation}\\nL(a,x)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWhat's the difference? Technically there is none. These are identical joint functions of two variables ($x$ and $a$)\\nto which we have just happened to have given different names. Pragmatically we regard Eq.~\\\\ref{eq:lone}\\nas describing the probability of getting various different $x$ from some fixed $a$, whereas Eq.~\\\\ref{eq:ltwo}\\ndescribes the likelihood for various different $a$ from some given $x$. \\nBut be careful with the term `likelihood'. If $P(x_1,a)>P(x_2,a)$ then $x_1$ is more probable (whatever you mean by that) than\\n$x_2$. If $L(a_1,x)>L(a_2,x)$ it does not mean that $a_1$ is more likely (however you define that) than $a_2$.\\n\\\\subsubsection{Shortcomings of Bayesian probability}\\nThe big problem with Bayesian probability is that it is subjective.\\nYour $P_0(a)$ and my $P_0(a)$ may be different---so how can we compare results?\\nScience does, after all, take pride in being objective: it handles real facts, not opinions.\\nIf you present a Bayesian result from your search for the $X$ particle this embodies\\nthe actual experiment and your irrational prior prejudices. I am interested in your experiment but not\\nin your irrational prior prejudices---I have my own---and it is unhelpful if you combine the two.\\nBayesians sometimes ask about the right prior they should use. \\nThis is the wrong question. The prior is what you believe, and only you know that.\\nThere is an argument made for taking the prior as uniform. This is sometimes\\ncalled the \\n`Principle of ignorance' and justified as being impartial. But this is misleading, even dishonest. \\nIf $P_0(a)$ is taken as constant, favouring no particular value, then it is not constant for $a^2$ or $\\\\sqrt a$ or $\\\\ln a$,\\nwhich are equally valid parameters. \\nIt is true that with lots of data, $P_1(a)$ decouples from $P_0(a)$.\\nThe final result depends only on the measurements.\\nBut this is not the case with little data---and that's the situation we're usually in---when doing statistics properly matters.\\nAs an example, suppose you make a Gaussian measurement (anticipating slightly Section~\\\\ref{sec:measurement}).\\nYou consider a prior flat in $a$ and a prior flat in $\\\\ln a$. This latter is quite sensible---it says you expect a \\nresult between 0.1 and 0.2 as being equally likely as a result between 1 and 2, or 10 and 20.\\nThe posteriors are shown in Fig.~\\\\ref{fig:differentpriors}.\\nFor an `accurate' result of $3\\\\pm 0.5$ the posteriors are very close. For an `intermediate' result\\nof $4.0 \\\\pm 1.0$ there is an appreciable difference in the peak value and the shape. For a `poor'\\nmeasurement of $5.0 \\\\pm 2.0$ the posteriors are {\\\\em very} different.\\nSo you should never just quote results from a single prior. \\nTry several forms of prior and examine the spread of results. If they are pretty much the same\\nyou are vindicated. This is called\\n`robustness under choice of prior' and it is standard practice for statisticians. If they are different\\nthen the data are telling you about the limitations of your results.\\n\\\\subsubsection {Jeffreys' prior}\\nJeffreys~\\\\cite{Jeffreys} suggested a technique now known as the Jeffreys' or {\\\\em objective prior}: that\\nyou should \\nchoose a prior flat in a transformed variable $a'$ for which the Fisher information, ${\\\\cal I} =-\\\\left< {\\\\partial^2 L(x;a)\\n\\\\over \\\\partial a^2}\\\\right> $ is constant. \\nThe Fisher information (which is important in maximum likelihood estimation, as described in Section~\\\\ref{sec:ML})\\nis a measure of how much a measurement tells you about the parameter: a large ${\\\\cal I}$ has a likelihood function with a sharp peak and will tell you (by some measure) a lot about $a$; a small ${\\\\cal I}$ has a featureless likelihood function\\nwhich will not be useful. Jeffrey's principle is that the prior should not favour or disfavour particular values of the parameter.\\nIt is equivalently---and more conveniently---used as taking a prior in the original $a$ which is proportional to\\n$\\\\sqrt{\\\\cal I}$.\\nIt has not been universally adopted for various reasons. Some practitioners like to be able to include their own\\nprior belief into the analysis. It also makes the prior dependent on the experiment (in the form of the likelihood function). \\nThus if ATLAS and CMS searched for the same new $X$ particle they would use different priors for $P_0(M_X)$, \\nwhich is (to some people) absurd.\\nSo it is not universal---but when you are selecting a bunch of priors to test robustness---the Jefferys' prior \\nis a strong contender for inclusion.\\n\", \"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\", \"\\\\section{Estimation}\\nWhat statisticians call `estimation',\\nphysicists would generally call `measurement'.\\nSuppose you\\nknow the probability (density) function $P(x;a)$ \\nand you \\ntake a set of data $\\\\{x_i\\\\}$. What is the best value for $a$? (Sometimes one wants to estimate a property (e.g. the mean) rather than a parameter, but \\nthis is relatively uncommon, and the methodology is the same.) \\n$x_i$ may be single values, or pairs, or higher-dimensional.\\nThe unknown\\n$a$ may be a single parameter or several. If it has more than one component, these are sometimes split into `parameters of interest' and `nuisance parameters'.\\nThe {\\\\em estimator} is defined very broadly:\\nan estimator $\\\\hat a(x_1\\\\dots x_N)$ is a function of the data that gives a value for the parameter $a$. There is no `correct' estimator, but some are better than others. A perfect estimator would be:\\n\\\\begin{itemize}\\n\\\\item\\nConsistent. $\\\\hat a(x_1 \\\\dots x_N) \\\\to a$ as $ N \\\\to \\\\infty $,\\n\\\\item\\nUnbiased: $\\\\langle \\\\hat a \\\\rangle = a $,\\n\\\\item\\nEfficient: $\\\\langle (\\\\hat a - a)^2 \\\\rangle$ is as small as possible,\\n\\\\item \\nInvariant: $\\\\hat f(a) = f(\\\\hat a)$.\\n\\\\end{itemize}\\nNo estimator is perfect---these 4 goals are incompatible. In particular the second and the fourth; if\\nan estimator $\\\\hat a$ is unbiased for $a$ then\\n$\\\\sqrt{\\\\hat a}$ is not an unbiased estimator of $\\\\sqrt a$.\\n\\\\subsection{Bias}\\nSuppose we estimate the mean by taking the obvious\\\\footnote{Note the difference between $\\\\langle x \\\\rangle$ which is an average over a PDF and $\\\\overline x$ \\nwhich denotes the average over a particular sample: both are called `the mean $x$'.} $\\\\hat \\\\mu = \\\\xbar$\\n$\\\\left< \\\\hat \\\\mu \\\\right> = \\\\left< {1 \\\\over N } \\\\sum x_i \\\\right> = {1 \\\\over N } \\\\sum \\\\mu = \\\\mu$. \\nSo there is no bias. This expectation value of this estimator of $\\\\mu$ is just $\\\\mu$ itself. By contrast suppose\\nwe estimate the variance by the apparently obvious\\n$\\\\hat V = \\\\xsqbar-\\\\xbar^2$.\\nThen $\\\\left< \\\\hat V \\\\right> = \\\\left< \\\\xsqbar \\\\right> - \\\\left< \\\\xbar^2 \\\\right>$.\\nThe first term is just $\\\\left< x^2 \\\\right>$. To make sense of the second term, note that $\\\\left< x \\\\right> = \\\\left< \\\\xbar \\\\right>$ and add and subtract $\\\\left< x \\\\right>^2$ to get\\n$\\\\left< \\\\hat V \\\\right> = \\\\left< x^2 \\\\right> - \\\\left< x \\\\right>^2 - (\\\\left< \\\\xbar^2 \\\\right>- \\\\left< \\\\xbar \\\\right>^2)$\\n$\\\\left< \\\\hat V \\\\right> =V(x)-V(\\\\xbar)=V-{V \\\\over N}={N-1 \\\\over N} V$.\\nSo the estimator is biased! $\\\\hat V$ will, on average, give too small a value.\\nThis bias, like any known bias, can be corrected for. \\nUsing $\\\\hat V = {N \\\\over N-1} (\\\\xsqbar-\\\\xbar^2)$ corrects the bias. The familiar estimator for\\nthe standard deviation follows:\\n$\\\\hat \\\\sigma=\\\\sqrt{\\\\sum_i (x_i-\\\\xbar)^2 \\\\over N-1}$. \\n(Of course this gives a biased estimate of $\\\\sigma$. But $V$ is generally more important in this context.)\\n\\\\subsection {Efficiency}\\nSomewhat surprisingly, there is a limit to the efficiency of an estimator: the\\n{\\\\em minimum variance bound} (MVB),\\nalso known as the {\\\\em Cramér-Rao bound}.\\nFor any unbiased estimator $\\\\hat a(x)$, the variance is bounded \\n\\\\begin{equation}V(\\\\hat a)\\\\geq\\n-{1 \\\\over \\\\left< {d^2 \\\\ln L \\\\over da^2}\\\\right>}\\n={1 \\\\over \\\\left<\\n\\\\left({d \\\\ln L \\\\over da }\\\\right) ^2\\n\\\\right>}\\n\\\\quad.\\n\\\\end{equation}\\n$L$ is the likelihood (as introduced in Section~\\\\ref{sec:likelihood}) of a sample of independent measurements, i.e. the\\nprobability for the whole data sample for a particular value of $a$.\\nIt is just the product of the individual probabilities:\\n$L(a;x_1,x_2,...x_N)=P(x_1;a)P(x_2;a)...P(x_N;a)$.\\nWe will write $L(a;x_1,x_2,...x_N)$ as $L(a;x)$ for simplicity.\\n\\\\begin{proof}{Proof of the MVB}\\nUnitarity requires $\\\\int P(x;a)\\\\, dx = \\\\int L(a;x) \\\\, dx =1$\\nDifferentiate wrt $a$: \\\\qquad \\\\begin{equation}\\n0=\\\\int {dL \\\\over da} \\\\, dx = \\\\int L {d \\\\ln L \\\\over da} \\\\, dx = \\\\left< {d \\\\ln L \\\\over da} \\\\right>\\n\\\\end{equation}\\nIf $\\\\hat a$ is unbiased \\n$\\\\left< \\\\hat a\\\\right> = \\\\int \\\\hat a(x) P(x;a) \\\\, dx = \\\\int \\\\hat a(x) L(a;x) \\\\, dx =a$\\nDifferentiate wrt $a$: \\\\qquad $1=\\\\int \\\\hat a(x) {dL \\\\over da} \\\\, dx = \\\\int \\\\hat a L {d \\\\ln L \\\\over da} \\\\, dx $\\nSubtract Eq.~\\\\ref{eq:one} multiplied by $a$, and get $\\\\int (\\\\hat a - a){d \\\\ln L \\\\over da} L dx =1$\\nInvoke the Schwarz inequality $\\\\int u^2 \\\\, dx \\\\int v^2 \\\\, dx \\\\geq \\\\left( \\\\int u v \\\\, dx \\\\right)^2 $ with $u\\\\equiv (\\\\hat a - a) \\\\sqrt L, v\\\\equiv {d \\\\ln L \\\\over da} \\\\sqrt L$\\nHence $\\\\int (\\\\hat a - a)^2 L \\\\, dx \\\\int \\\\left( {d \\\\ln L \\\\over da}\\\\right)^2 L \\\\, dx \\\\geq 1$\\n\\\\begin{equation} \\n\\\\left< (\\\\hat a - a)^2 \\\\right> \\\\geq 1/\\\\left<\\\\left( {d ln L \\\\over da }\\\\right)^2 \\\\right>\\n\\\\end{equation}\\n\\\\end{proof}\\nDifferentiating Eq.~\\\\ref{eq:one} again gives\\n${ d \\\\over da} \\\\int L { d \\\\ln L \\\\over da} \\\\, dx = \\\\int {d L \\\\over da} \\\\, {d \\\\ln L \\\\over da} \\\\, dx + \\\\int L {d^2 \\\\ln A \\\\over da^2} \\\\, dx\\n=\\n\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>+\\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>=0$,\\nhence \\n$\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>= - \\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>$.\\nThis is the {\\\\em Fisher information} referred to in Section~\\\\ref{sec:Jeffreys}. Note how it is intrinsically positive.\\n\\\\subsection{Maximum likelihood estimation}\\nThe {\\\\em maximum likelihood} (ML) estimator just does what it says: $a$ is adjusted to maximise the\\nlikelihood of the sample\\n(for practical reasons one actually maximises the log likelihood, which is a sum rather than a product).\\n\\\\begin{equation}\\n{\\\\rm Maximise } \\\\ln L = \\\\sum_i \\\\ln {P(x_i;a)}\\n\\\\quad,\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\left. {d \\\\ln L \\\\over d a } \\\\right|_{\\\\hat a}=0\\n\\\\quad.\\n\\\\end{equation}\\nThe \\nML estimator is very commonly used. It is not only simple and intuitive, it has lots of nice properties.\\n\\\\begin{itemize}\\n\\\\item\\nIt is consistent.\\n\\\\item\\nIt is biased, but bias falls like $1/N$.\\n\\\\item\\nIt is efficient for the large $N$.\\n\\\\item\\nIt is invariant---doesn't matter if you reparametrize $a$. \\n\\\\end{itemize}\\nA particular maximisation problem may be solved in 3 ways, depending on the complexity\\n\\\\begin{enumerate}\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} algebraically,\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} numerically, and\\n\\\\item Solve Eq.~\\\\ref{eq:logL} numerically.\\n\\\\end{enumerate}\\n\\\\subsection{Least squares}\\n{\\\\em Least squares estimation} follows from maximum likelihood estimation.\\nIf you have \\nGaussian measurements of $y$ taken at various $x$ values, with measurement error $\\\\sigma$, and a prediction $y=f(x;a)$\\nthen the Gaussian probability\\n\\\\centerline{$P(y;x,a)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-(y-f(x,a))^2/2 \\\\sigma^2}$}\\ngives the log likelihood\\n\\\\centerline{$\\\\ln L = - \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over 2 \\\\sigma_i^2} + {\\\\rm constants}$.}\\nTo maximise $\\\\ln L$, you minimise $\\\\chi^2 = \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over \\\\sigma_i^2} $, hence the name `least squares'.\\nDifferentiating gives the {\\\\em normal equations}:\\n$\\\\sum { \\\\left(y_i - f(x_i;a)\\\\right) \\\\over \\\\sigma_i^2}f'(x_i;a) =0$.\\nIf $f(x;a)$ is linear in $a$ then these can be solved exactly. Otherwise an iterative method has to be used.\\n\\\\subsection{Straight line fits}\\nAs a particular instance of least squares estimation, suppose the function is $y=mx+c$, and assume all $\\\\sigma_i$ are the same (the extension to the general case is straightforward).\\nThe normal equations are then $\\\\sum (y_i - m x_i -c) x_i = 0$ and $\\\\sum (y_i-m x_i - c ) =0$\\\\ , for which the solution, shown in Fig.~\\\\ref{fig:slfit}, is\\n\\\\noindent $m={\\\\overline{xy} - \\\\overline x \\\\ , \\\\overline y \\\\over \\\\xsqbar - \\\\xbar^2}$\\\\ , $c=\\\\overline y - m \\\\xbar$ \\\\ .\\nStatisticians call this {\\\\em regression}. Actually there is a subtle difference, as shown in Fig.~\\\\ref{fig:regression}.\\nThe straight line fit considers well-defined $x$ values and $y$ values with measurement errors---if it were not for those\\nerrors then presumably the values would line up perfectly, with no scatter. The scatter in regression is not caused by measurement errors, but by the fact that the variables are linked only loosely. \\nThe history of regression started with Galton, who measured the heights of fathers and their (adult) sons.\\nTall parents tend to have tall children so there is a correlation. Because the height of a son depends\\nnot just on his paternal genes but on many factors (maternal genes, diet, childhood illnesses $\\\\dots$), the points\\ndo not line up exactly---and using a high accuracy laser interferometer to do the measurements, rather than a simple\\nruler, would not change anything. \\nGalton, incidentally, used this to show that although \\ntall fathers tend to have tall sons, they are not that tall. An outstandingly tall father will have (on average) quite tall children, and only tallish grandchildren. He called this \\n`Regression towards mediocrity', hence the name.\\nIt is also true that tall sons tend to have tall fathers---but not that tall---and only tallish grandfathers. Regress works in both directions!\\nThus for regression there is always an ambiguity as to whether to plot $x$ against $y$ or $y$ against $x$.\\nFor a straight line fit as we usually meet them this does not arise: one variable is precisely specified and we call that one $x$, and the one with measurement errors is $y$. \\n\\\\subsection{Fitting histograms}\\nWhen fitting a histogram the error is given by Poisson statistics for the number of events in each bin.\\nThere are \\n4 methods of approaching this problem---in order of increasing accuracy and decreasing speed. It is assumed that the bin width $W$ is narrow, so that $f(x_i,a)=\\\\int_{x_i}^{x_i+W} P(x,a)\\\\, dx$ can be approximated by \\n$f_i(x_i;a)=P(x_i;a) \\\\times W$. $W$ is almost always the same for all bins,\\nbut the rare cases of variable bin width can easily be included.\\n\\\\begin{enumerate}\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2 \\\\over n_i}$. This is the simplest but clearly breaks if $n_i=0$.\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2\\\\over f_i}$ . Minimising the Pearson $\\\\chi^2$ (which {\\\\em is}\\nvalid here) avoids the division-by-zero problem. It assumes that the Poisson distribution can be approximated by a Gaussian.\\n\\\\item Maximise $\\\\ln L = \\\\sum \\\\ln(e^{-f_i} f_i^{n_i} / n_i!) \\\\sim \\\\sum n_i \\\\ln f_i - f_i$. This, known as {\\\\em binned maximum likelihood}, remedies that assumption.\\n\\\\item Ignore bins and maximise the total likelihood. Sums run over $N_{events}$ not $N_{bins}$. So if you have large data samples this is much slower. You have to use it for sparse data, but of course in such cases the sample is small and the\\ntime penalty is irrelevant.\\n\\\\end{enumerate}\\nWhich method to use is something you have to decide on a case by case basis. \\nIf you have bins with zero entries then the first method is ruled out\\n(and removing such bins from the fit introduces bias so this should not be done).\\nOtherwise, in my experience, the improvement in adopting a more complicated method tends to be small.\\n\"}},\n",
       "       {'entity_name': 'asymptotic distribution and wilks theorem', 'entity_type': 'statistics_concept', 'description': \"A collection of statistical concepts that describe the behavior of likelihood ratio test statistics as sample sizes become large. This includes the asymptotic distribution, which approaches a Gaussian distribution, and Wilks' theorem, which states that under certain conditions, the distribution of the likelihood ratio statistic converges to a chi-squared distribution. Wald's theorem generalizes this by providing the asymptotic distribution when the true parameter value differs from the tested value. These concepts are essential for hypothesis testing in frequentist statistics, particularly in deriving critical values and approximating significance levels.\", 'relevant_passages': {\"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{The Profile Likelihood}\\nAs noted in Sec.~\\\\ref{sec:likelihood}, likelihood functions can be used to estimate the parameters\\non which they depend. The method of choice to do so, in a frequentist analysis, is called \\\\textbf{maximum\\nlikelihood}, a method first used by Karl Frederick Gauss, \\\\emph{The Prince of Mathematics}, but developed into a formidable statistical tool in the 1930s by \\nSir Ronald A. Fisher~\\\\cite{Fisher}, perhaps the most influential statistician of the twentieth century.\\nFisher showed that a good way to estimate the parameters of a likelihood function is to pick\\nthe value that maximizes it. Such estimates are called \\\\textrm{maximum likelihood estimates} (MLE). In general, a function into which data can be inserted to yield an MLE of\\na parameter is called a maximum likelihood estimator. For simplicity, we shall use the \\nsame abbreviation MLE\\nto mean both the estimate and the estimator and we shall not be too\\npicky about distinguishing the two. The D\\\\O\\\\ top quark\\ndiscovery example illustrates the\\nmethod. \\n\\\\begin{quote}\\n\\\\paragraph*{Example: Top Quark Discovery Revisited}\\nWe start by listing\\n\\\\begin{align*}\\n& \\\\textbf{the knowns} \\\\\\\\\\n& D = N, B \\\\text{ where} \\\\\\\\\\n& N = 17 \\\\textrm{ observed events} \\\\\\\\\\n& B = 3.8 \\\\textrm{ estimated background events with uncertainty } \\\\delta B = 0.6 \\\\\\\\\\n&\\\\textbf{and the unknowns} \\\\\\\\\\n& b \\\\quad\\\\textrm{mean background count}\\\\\\\\\\n& s \\\\quad\\\\textrm{mean signal count}.\\n\\\\end{align*} \\nNext, we construct a probability model for the data $D = N, B$ assuming that \\n$N$ and $B$ are statistically independent. Since this is a counting\\nexperiment, we shall assume that $p(x| s, b)$ is a Poisson distribution with mean\\ncount $s + b$. In the absence of details about how the background $B$ was arrived\\nat, the standard assumption is that data of the form $y \\\\pm \\\\delta y$ can be modeled\\nwith a Gaussian (or normal) density. However, we can do a bit better. Background estimates are usually based on auxiliary experiments, either real or simulated, that define control regions. \\nSuppose that the observed count in the control region is $Q$ and the mean count is $b k$, where $k$ (ideally) is the known scale factor between the control and signal regions. We can model these data with a\\nPoisson distribution with count $Q$ and mean $b k$. But, we are given $B$ and $\\\\delta B$ rather than $Q$ and $k$, so we need a model to relate the two pairs of numbers. \\nThe simplest model is $B = Q / k$ and $\\\\delta B = \\\\sqrt{Q} / k$ from which we can infer an effective count $Q$ using $Q = (B / \\\\delta B)^2$. What of the scale factor $k$? Well, since it\\nis not given, it must be estimated. The obvious estimate is $Q / B = B / \\\\delta B^2$.\\nWith these assumptions, our likelihood function is\\n\\\\begin{eqnarray}\\np(D | s, b) & = & \\\\textrm{Poisson}(N, s + b) \\\\, \\\\textrm{Poisson}(Q, bk), \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nQ & = & (B / \\\\delta B)^2 = 41.11,\\\\nonumber\\\\\\\\\\nk & = & B / \\\\delta B^2 = 10.56. \\\\nonumber\\n\\\\end{eqnarray}\\nThe first term in Eq.~(\\\\ref{eq:toplh}) is the likelihood for the count $N = 17$, while\\nthe second term is the likelihood for $B = 3.8$, or equivalently the count $Q$. The\\nfact that $Q$ is not an integer causes no difficulty: we merely write\\nthe Poisson distribution as\\n$(bk)^Q \\\\exp(-bk) / \\\\Gamma(Q+1)$, which permits continuation to non-integer counts $Q$.\\nThe maximum likelihood estimators \\nfor $s$ and $b$ are found by maximizing Eq.~(\\\\ref{eq:toplh}), that is, by solving the equations \\n\\\\begin{align}\\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial s} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{s} = N - B, \\\\nonumber\\\\\\\\ \\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial b} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{b} = B, \\\\nonumber\\n\\\\end{align}\\nas expected.\\nA more complete analysis would account for the uncertainty in\\n$k$. One way is to introduce two more control regions with observed counts $V$ and $W$ and mean counts $v$ and $w k$, respectively, and extend \\nEq.~(\\\\ref{eq:toplh}) with two more Poisson distributions.\\n\\\\end{quote}\\n\\\\bigskip\\nThe maximum likelihood method is the most widely used method for\\nestimating parameters because it generally leads to reasonable estimates. But the\\nmethod has features, or encourages practices, which, somewhat uncharitably, we label the\\ngood, the bad, and the ugly!\\n\\\\begin{itemize}\\n\\\\item \\\\emph{The Good}\\n\\\\begin{itemize}\\n\\\\item Maximum likelihood estimators are consistent: the RMS goes to zero as more\\nand more data are included in the likelihood. This is an extremely important property,\\nwhich basically says it makes sense to take more data because we shall get more accurate results. One would not knowingly use an inconsistent estimator!\\n\\\\item If an unbiased estimator for a parameter exists the maximum\\nlikelihood method will find it.\\n\\\\item Given the MLE for $s$, the MLE for any function $y = g(s)$ of $s$ is,\\nvery conveniently, just $\\\\hat{y} = g(\\\\hat{s})$. This is a very nice practical feature which\\nmakes it possible to maximize the likelihood using the most convenient parameterization of it and then transform back to the parameter of interest at the end. \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Bad (according to some!)}\\n\\\\begin{itemize}\\n\\\\item In general, MLEs are biased.\\\\\\\\ \\\\\\\\\\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 7:} Show this\\\\\\\\\\nHint: Taylor expand $y = g(\\\\hat{s} + h)$ about the MLE $\\\\hat{s}$,\\\\\\\\\\nthen consider its ensemble average. }} \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Ugly (according to some!)}\\n\\\\begin{itemize}\\n\\\\item The fact that most MLEs are biased encourages the routine application of bias correction, which can waste data and, sometimes, yield absurdities.\\n\\\\end{itemize}\\n\\\\end{itemize}\\n\\\\noindent\\nHere is an example of the seriously ugly.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nFor a discrete probability distribution $p(k)$, the \\\\textbf{moment generating function} is the ensemble average\\n\\\\begin{align*}\\nG(x) & = < e^{xk} > \\\\\\\\\\n& = \\\\sum_{k} e^{xk} \\\\, p(k).\\n\\\\end{align*}\\nFor the binomial, with parameters $p$ and $n$, this is\\n\\\\begin{align*}\\nG(x) & = (e^x p + 1 - p)^n, \\\\quad \\\\framebox{\\\\textbf{Exercise 8a:} Show this}\\n\\\\end{align*}\\nwhich is useful for calculating \\\\textbf{moments}\\n\\\\begin{align*}\\n\\\\mu_r & = \\\\left. \\\\frac{d^rG}{dx^r}\\\\right |_{x=0} = \\\\sum_k k^r \\\\, p(k),\\n\\\\end{align*}\\ne.g., $\\\\mu_2 = (np)^2 + np - np^2$ for the binomial distribution.\\nGiven that $k$ events out $n$ pass a set of cuts, the MLE of the event selection efficiency is\\nthe obvious estimate $\\\\hat{p} = k / n$. The equally obvious estimate of $p^2$ is $( k / n)^2$.\\nBut,\\n\\\\begin{align*}\\n< ( k / n)^2 > & = p^2 + V / n , \\\\quad \\\\framebox{\\\\textbf{Exercise 8b:} Show this}\\n\\\\end{align*}\\nso $(k / n)^2$ is a biased estimate of $p^2$ with positive bias $V / n$. The unbiased estimate of $p^2$ is\\n\\\\begin{align*}\\nk(k-1) / [ n (n - 1)] , \\\\quad \\\\framebox{\\\\textbf{Exercise 8c:} Show this}\\n\\\\end{align*}\\nwhich, for a single success, i.e., $k = 1$, yields the sensible estimate $\\\\hat{p} = 1 / n$, but\\nthe less than helpful one $\\\\hat{p^2} = 0!$\\n\\\\end{quote}\\n\\\\bigskip\\nIn order to infer a value for the parameter of interest, for example, \\nthe signal $s$ in\\nour 2-parameter likelihood function in Eq.~(\\\\ref{eq:toplh}), the likelihood\\nmust be reduced to one involving the parameter of interest only, here $s$, \\nby somehow getting rid of all the \\\\textbf{nuisance} parameters, here the background\\nparameter $b$. A nuisance parameter is simply a parameter that is not of current interest.\\nIn a strict frequentist calculation, this reduction to the parameter of interest must be done\\nin such a way as to respect the frequentist principle: \\\\emph{coverage probability $\\\\geq$ confidence level}. In general, this is very difficult to do exactly.\\nIn practice, we replace all nuisance parameters by their \\\\textbf{conditional maximum likelihood\\nestimates} (CMLE). The CMLE is the maximum likelihood estimate conditional on\\na \\\\emph{given} value of the current parameter (or parameters) of interest. In the top discovery example, we construct an estimator of $b$ as a function of $s$, $\\\\hat{b}(s)$, and\\nreplace $b$ in the likelihood $p(D | s, b)$ by $\\\\hat{b}(s)$ to yield a function\\n$p_{PL}(D | s)$ called the \\\\textbf{profile likelihood}.\\n\\\\begin{quote}\\n\\\\emph{Since the profile likelihood entails an approximation, namely, replacing unknown parameters by their conditional estimates, it is not the likelihood but rather an approximation to it. Consequently, \\nthe frequentist principle is not guaranteed to be satisfied exactly.}\\n\\\\end{quote}\\nThis does not seem to be much progress. However, things are much better than they may appear because of\\nan important theorem proved by Wilks in 1938. If certain conditions are met, roughly that the\\nMLEs do not occur on the boundary of the parameter space and the likelihood becomes\\never more Gaussian as the data become more numerous --- that is, in the so-called\\n\\\\textbf{asymptotic limit}, then if the true density of $x$ is $p(x| s, b)$ the random number\\n\\\\begin{align}\\nt(x, s) & = -2 \\\\ln \\\\lambda(x, s), \\\\\\\\\\n\\\\textrm{where } \\\\lambda(x, s) & = \\\\frac{p_{PL}(x | s)}{ p_{PL}(x | \\\\hat{s})}, \\n\\\\end{align}\\nhas a probability density that converges to a $\\\\chi^2$ density with one degree of\\nfreedom. More generally, if the numerator of $\\\\lambda$ contains $m$ free parameters the\\nasymptotic density of $t$ is a $\\\\chi^2$ density with $m$ degrees of freedom. Therefore, we may take $t(D, s)$ to be a $\\\\chi^2$ variate, at least\\napproximately, and solve $t(D, s) = n^2$ for $s$ to get \\napproximate $n$-standard deviation confidence intervals. In particular, if we solve $t(D, s) = 1$, we\\nobtain approximate 68\\\\Wilks' theorem provides the main justification for using the profile likelihood. \\nWe again use the top discovery example to illustrate the procedure.\\n\\\\begin{quote} \\n\\\\paragraph*{Example: Top Quark Discovery Revisited Again}\\nThe conditional MLE of $b$ is found to be\\n\\\\begin{align}\\n\\\\hat{b}(s) & = \\\\frac{g + \\\\sqrt{g^2 + 4 (1 + k) Q s}}{2(1+k)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\ng & = N + Q - (1+k) s.\\\\nonumber\\n\\\\end{align}\\nThe likelihood $p(D | s, b)$ is shown in Fig.~\\\\ref{fig:toppl}(a) together with\\nthe graph of $\\\\hat{b}(s)$. The mode (i.e. the peak) occurs at $s = \\\\hat{s} = N - B$.\\nBy solving $$-2 \\\\ln \\\\frac{p_{PL}(17 | s)}{ p_{PL}(17 | 17 - 3.8)} = 1$$ for $s$ we get two solutions\\n$s = 9.4$ and $s = 17.7$. Therefore, we can make the statement\\n$s \\\\in [9.4, 17.7]$ at approximately 68\\\\$-\\\\ln \\\\lambda(17, s)$ created using \\nthe {\\\\tt RooFit}~\\\\cite{RooFit} and {\\\\tt RooStats}~\\\\cite{RooStats} packages.\\n\\\\smallskip\\n\\\\framebox{\\\\textbf{Exercise 9:} Verify this interval using the {\\\\tt RooFit/RooStats} package}\\n\\\\medskip\\nIntervals constructed this way are not guaranteed to\\nsatisfy the frequentist principle. In practice, however, \\ntheir coverage is very good for the typical probability models\\nused in particle physics, even for modest amounts of\\ndata. This is illustrated in Fig.~\\\\ref{fig:wilks}, which shows how rapidly the density of $t(x, s)$ \\nconverges to a $\\\\chi^2$ density for the probability distribution $p(x, y| s, b) = \\\\textrm{Poisson}(x|s+b) \\\\textrm{Poisson}(y | b)$\\\\footnote{It was the difficulty of extracting information\\nfrom this distribution that compelled the \\nauthor (against his will) to repair his parlous knowledge of statistics~\\\\cite{Fidecaro:1985cm}!}.\\nThe figure also shows what happens if we impose the restriction $\\\\hat{s} \\\\geq 0$, that is,\\nwe forbid negative signal estimates.\\n\\\\end{quote}\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Variations on test statistics}\\nA number of test statistics is proposed in Ref.~\\\\cite{asymptotic} that better\\nserve various purposes. Below the main ones are reported:\\n\\\\begin{itemize}\\n\\\\item {\\\\bf Test statistic for discovery:}\\n\\\\begin{equation}\\nq_0 = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(0), &\\\\hat{\\\\mu}\\\\ge 0\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} < 0\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIn case of a negative estimate of $\\\\mu$ ($\\\\hat{\\\\mu}<0$), the test statistic is set to zero in order to\\nconsider only positive $\\\\hat{\\\\mu}$ as evidence against the background-only hypothesis.\\nWithin an asymptotic approximation, the significance is given by: $Z\\\\simeq\\\\sqrt{q_0}$.\\n\\\\item {\\\\bf Test statistic for upper limit:}\\n\\\\begin{equation}\\nq_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(\\\\mu), &\\\\hat{\\\\mu}\\\\le \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} > \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIf the $\\\\hat{\\\\mu}$ estimate is larger than the assumed value for $\\\\mu$, an upward fluctuation occurred.\\nIn those cases, $\\\\mu$ is not excluded by setting the test statistic to zero.\\n\\\\item {\\\\bf Test statistic for Higgs boson search:}\\n\\\\begin{equation}\\n\\\\tilde q_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0))}, & \\\\hat{\\\\mu} < 0\\\\,,\\\\\\\\\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\vec{\\\\theta}}(\\\\mu))}, & 0\\\\le \\\\hat{\\\\mu} < \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} \\\\ge \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThis test statistics both protects against unphysical cases with $\\\\mu <0$\\nand, as the test statistic for upper limits, protects upper limits\\nin cases of an upward $\\\\hat{\\\\mu}$ fluctuation.\\n\\\\end{itemize}\\nA number of measurements performed at LEP and Tevatron used a\\ntest statistic based on the ratio of the likelihood function evaluated under\\nthe signal plus background hypothesis and under the background only hypothesis,\\ninspired by the Neyman--Pearson lemma:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|s+b)}{L(\\\\vec{x}|b)}\\\\,.\\n\\\\end{equation}\\nIn many LEP and Tevatron analyses, nuisance parameters were treated using the hybrid Cousins--Hyghland approach.\\nAlternatively, one could use a formalism similar to the profile likelihood, \\nsetting $\\\\mu=0$ in the denominator and $\\\\mu=1$ in the numerator, and minimizing\\nthe likelihood functions with respect to the nuisance parameters:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu=1, \\\\hat{\\\\hat{\\\\theta}}(1))}{L(\\\\vec{x}|\\\\mu=0,\\\\hat{\\\\hat{\\\\theta}}(0))}\\\\,.\\n\\\\end{equation}\\nFor all the mentioned test statistics, asymptotic approximations exist and\\nare reported in Ref.~\\\\cite{asymptotic}. Those are based either on Wilks' theorem\\nor on Wald's approximations~\\\\cite{Wald}. If a value $\\\\mu$ is tested, and \\nthe data are supposed to be distributed according to another value of the signal strength $\\\\mu^\\\\prime$,\\nthe following approximation holds, asymptotically:\\n\\\\begin{equation}\\n-2\\\\ln\\\\lambda(\\\\mu) = \\\\frac{(\\\\mu-\\\\hat{\\\\mu})^2}{\\\\sigma^2} + {\\\\cal O}\\\\left(\\\\frac{1}{\\\\sqrt{N}}\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\mu}$ is distributed according to a Gaussian with average $\\\\mu^\\\\prime$ and\\nstandard deviation $\\\\sigma$. The covariance matrix for the nuisance parameters is\\ngiven, in the asymptotic approximation, by:\\n\\\\begin{equation}\\nV_{ij}^{-1} = \\\\left.\\\\left<\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right>\\\\right|_{\\\\mu=\\\\mu^\\\\prime}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu^\\\\prime$ is assumed as value for the signal strength.\\nIn some cases, asymptotic approximations (Eq.~(\\\\ref{eq:waldTestStat})) can be written in terms of an\\n{\\\\it Asimov dataset}~\\\\cite{Asimov}:\\n\\\\begin{displayquote}\\n{\\\\it We define the Asimov data set such that when one uses it to evaluate the estimators\\nfor all parameters, one obtains the true parameter values}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\nIn practice, an Asimov dataset is a single ``representative'' dataset obtained by replacing all\\nobservable (random) varibles with their expecteted value. In particular,\\nall yields in the data sample (e.g.: in a binned case) are replaced with their expected values, that may be non integer values.\\nThe median significance for different cases of test statistics can be computed in this\\nway without need of producing extensive sets of toy Monte Carlo. The implementation\\nof those asymptotic formulate is available in the {\\\\sc RooStats} library, released\\nas part an optional component {\\\\sc Root}~\\\\cite{Root}.\\n\", \"\\\\section{Appendix B: Statistical significance for small values of signal}\\nThere are two approaches to assess the significance of a potential new physics signal: using the estimator $\\\\mathcal{Q}$ or through the test statistic $q_{\\\\mu}$. In the first approach, a Gaussian approximation is obtained by evaluating the statistical estimator at its central values.\\n\\\\begin{equation}\\n-2ln \\\\mathcal{Q}_{i} = 2 s_{i} - 2 n_{i} ln \\\\bigg( 1 + \\\\frac{s_{i}}{b_{i}} \\\\bigg).\\n\\\\end{equation}\\nBy defining $w_{i} = \\\\ln \\\\left( 1 + \\\\frac{s_{i}}{b_{i}} \\\\right)$, we can calculate the expected value for the distributions of kackground only ($b$) and signal + background ($s+b$), respectively.\\n\\\\begin{eqnarray}\\n< -2ln \\\\mathcal{Q}_{i} >_{b} & = & 2s_{i} - 2(b_{i}) w_{i} {} \\\\nonumber \\\\\\\\\\n< -2ln \\\\mathcal{Q}_{i} >_{s+b} & = & 2s_{i} - 2(s_{i} + b_{i}) w_{i} {} \\n\\\\end{eqnarray}\\nIn this way, we can quantify the number of standard deviations between the expected number of background events and signal + background events across all channels.\\n\\\\begin{eqnarray}\\nZ_{0} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 b_{i}w_{i}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} b_{i} w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{B}w} = \\\\frac{S}{\\\\sqrt{B}}. {}\\n\\\\end{eqnarray}\\nWhere the standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){b}$ is calculated as: \\n\\\\begin{eqnarray}\\n\\\\sigma_{b}^{2} & = & E( (x- E(x))^{2} ) {} \\\\nonumber \\\\\\\\ \\n& = & E( (2 s_{i} - 2 n_{i} w_{i} - 2s_{i} + 2b_{i} w_{i})^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}E( n_{i}^{2} - 2n_{i}b_{i} + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( E(n_{i}^{2}) - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4w_{i}^{2}( \\\\sigma_{n}^{2} + E(n_{i})^{2} - 2b_{i}E(n_{i}) + b_{i}^{2} ) {} \\\\nonumber \\\\\\\\\\n& = & 4b_{i}w_{i}^{2}. {}\\n\\\\end{eqnarray}\\nWith $\\\\sigma_{b}^{2} = \\\\sigma_{n}^{2} = E(n_{i}) = b_{i}$, which follows a Poisson distribution. Similarly, the number of standard deviations for the signal + background distribution is:\\n\\\\begin{eqnarray}\\nZ_{1} & = & \\\\frac{< -2ln \\\\mathcal{Q}_{i} >_{b} - < -2ln \\\\mathcal{Q}_{i} >_{s+b}}{\\\\sigma_{s+b}} {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} 2(s_{i} - b_{i}w_{i}) - 2(s_{i} - (s_{i}+b_{i})w_{i}) }{ \\\\sqrt{ \\\\sum_{i} 4 (s_{i}+b_{i})w_{i}^{2}} } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{ \\\\sum_{i} s_{i} w_{i} }{ \\\\sqrt{ \\\\sum_{i} (s_{i}+b_{i}) w_{i}^{2} } } {} \\\\nonumber \\\\\\\\\\n& = & \\\\frac{Sw}{\\\\sqrt{S+B}w} = \\\\frac{S}{\\\\sqrt{S+B}}. {}\\n\\\\end{eqnarray}\\nThe standard error of the distribution $(-2\\\\ln \\\\mathcal{Q}{i}){s+b}$ is calculated in a similar way:\\n\\\\begin{equation}\\n\\\\sigma_{s+b}^{2} = E( (x- E(x))^{2} ) = 4(s_{i}+b_{i})w_{i}^{2}.\\n\\\\end{equation}\\nOn the other hand, in the second scenario, the test statistic $q_{\\\\mu}$, in the context of single-channel counting experiments, is defined from the Poisson likelihood function:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\frac{(\\\\mu s +b)^{n}}{n!}e^{-(\\\\mu s + b)}.\\n\\\\end{equation}\\nThe profile likelihood for the null hypothesis $\\\\mu = 0$ is given by:\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2ln\\\\frac{\\\\mathcal{L}(0)}{\\\\mathcal{L}(\\\\hat{\\\\mu})} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }. \\n\\\\end{cases}\\n\\\\end{equation}\\nIn this case, the maximum likelihood parameter is $\\\\hat{\\\\mu} = \\\\frac{n - b}{s}$. Using the Asimov data, where $n = s + b$, and Wilks' approximation~\\\\cite{conway2005calculation}, we have:\\n\\\\begin{equation}\\nZ_{0} = \\\\sqrt{q_{0}} = \\n\\\\begin{cases} \\n\\\\sqrt{2((s+b)ln(1+\\\\frac{s}{b}) - s)} & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nFinally, by expanding the logarithmic function under the condition $s \\\\ll b$, we obtain the Gaussian approximation of the significance:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\frac{s}{\\\\sqrt{b}}.\\n\\\\end{equation}\\n\\\\end{document}\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Measurement as parameter estimation}\\nOne of the most common tasks of the working physicist is to estimate some model parameter. We do it so often, that we often don't realize it. For instance, the sample mean $\\\\bar{x} = \\\\sum_{e=1}^n x_e / n$ is an estimate for the mean, $\\\\mu$, of a Gaussian probability density $f(x|\\\\mu,\\\\sigma) =\\\\Gauss(x|\\\\mu,\\\\sigma)$. More generally, an \\\\textit{estimator} $\\\\hat{\\\\alpha}(\\\\data)$ is some function of the data and its value is used to estimate the true value of some parameter $\\\\alpha$. There are various abstract properties such as variance, bias, consistency, efficiency, robustness, etc~\\\\cite{James}. The bias of an estimator is defined as $B(\\\\hat\\\\alpha) = E[ \\\\hat\\\\alpha ]-\\\\alpha$, where $E$ means the expectation value of \\\\mbox{$E[ \\\\hat\\\\alpha ]=\\\\int\\\\hat\\\\alpha(x) f(x)dx$} or the probability-weighted average. Clearly one would like an unbiased estimator. The variance of an estimator is defined as $var[\\\\hat\\\\alpha] = E[ (\\\\alpha - E[\\\\hat{\\\\alpha}] )^2 ]$; and clearly one would like an estimator with the minimum variance. Unfortunately, there is a tradeoff between bias and variance. Physicists tend to be allergic to biased estimators, and within the class of unbiased estimators, there is a well defined minimum variance bound referred to as the Cram\\\\'er-Rao bound (that is the inverse of the Fisher information, which we will refer to again later). \\nThe most widely used estimator in physics is the maximum likelihood estimator (MLE). It is defined as the value of $\\\\alpha$ which maximizes the likelihood function $L(\\\\alpha)$. Equivalently this value, $\\\\hat{\\\\alpha}$, maximizes $\\\\log L(\\\\alpha)$ and minimizes $-\\\\log L(\\\\alpha)$. The most common tool for finding the maximum likelihood estimator is \\\\texttt{Minuit}, which conventionally minimizes $-\\\\log L(\\\\alpha)$ (or any other function)~\\\\cite{James:1975dr}. The jargon is that one `fits' the function and the maximum likelihood estimate is the `best fit value'. \\nWhen one has a multi-parameter likelihood function $L(\\\\vec{\\\\alpha})$, then the situation is slightly more complicated. The maximum likelihood estimate for the full parameter list, $\\\\hat{\\\\vec{\\\\alpha}}$, is clearly defined. The various components $\\\\hat{\\\\alpha}_p$ are referred to as the \\\\textit{unconditional maximum likelihood estimates}. In the physics jargon, one says all the parameters are `floating'. One can also ask about maximum likelihood estimate of $\\\\alpha_p$ is with some other parameters $\\\\vec{\\\\alpha}_o$ fixed; this is called the \\\\textit{conditional maximum likelihood estimate} and is denoted $\\\\hat{\\\\hat{\\\\alpha}}_p(\\\\vec{\\\\alpha}_o)$. These are important quantities for defining the profile likelihood ratio, which we will discuss in more detail later. The concept of variance of the estimates is also generalized to the covariance matrix $cov[\\\\alpha_p, \\\\alpha_{p'}] = E[(\\\\hat\\\\alpha_p - \\\\alpha_p)(\\\\hat\\\\alpha_{p'}- \\\\alpha_{p'})]$ and is often denoted $\\\\Sigma_{pp'}$. Note, the diagonal elements of the covariance matrix are the same as the variance for the individual parameters, ie. $cov[\\\\alpha_p, \\\\alpha_{p}] = var[\\\\alpha_p]$.\\nIn the case of a Poisson model $\\\\Pois(n|\\\\nu)$ the maximum likelihood estimate of $\\\\nu$ is simply \\\\mbox{$\\\\hat{\\\\nu}=n$}. Thus, it follows that the variance of the estimator is $var[\\\\hat{\\\\nu}]=var[n]=\\\\nu$. Thus if the true rate is $\\\\nu$ one expects to find estimates $\\\\hat{\\\\nu}$ with a characteristic spread around $\\\\nu$; it is in this sense that the measurement has a estimate has some uncertainty or `error' of $\\\\sqrt{n}$. We will make this statement of uncertainty more precise when we discuss frequentist confidence intervals.\\nWhen the number of events is large, the distribution of maximum likelihood estimates approaches a Gaussian or normal distribution.\\\\footnote{There are various conditions that must be met for this to be true, but skip the fine print in these lectures. There are two conditions that are most often violated in particle physics, which will be addressed later.} This does not depend on the pdf $f(x)$ having a Gaussian form. For small samples this isn't the case, but this limiting distribution is often referred to as an \\\\textit{asymptotic distribution}.\\nFurthermore, under most circumstances in particle physics, the maximum likelihood estimate approaches the minimum variance or Cram\\\\'er-Rao bound. In particular, the inverse of the covariance matrix for the estimates is asymptotically given by\\n\\\\begin{equation}\\n\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = E\\\\left[- \\\\frac{\\\\partial^2 \\\\log f(x|\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\middle| \\\\;\\\\vec\\\\alpha \\\\right ] \\\\;,\\n\\\\end{equation}\\nwhere I have written explicitly that the expectation, and thus the covariance matrix itself, depend on the true value $\\\\vec\\\\alpha$. The right side of Eq.~\\\\ref{Eq:expfisher} is called the (expected) Fisher information matrix. Remember that the expectation involves an integral over the observables. Since that integral is difficult to perform in general, one often uses the observed Fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data\\n\\\\begin{equation}\\n\\\\tilde\\\\Sigma_{pp'}^{-1}(\\\\vec\\\\alpha) = - \\\\frac{\\\\partial^2 \\\\log L(\\\\vec{\\\\alpha})}{\\\\partial\\\\alpha_p \\\\partial_{p'}} \\\\; .\\n\\\\end{equation}\\nThis is what \\\\texttt{Minuit}'s \\\\texttt{Hesse} algorithm\\\\footnote{The matrix is called the Hessian, hence the name.} calculates to estimate the covariance matrix of the parameters.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Profile likelihood}\\nMost of the recent searches at LHC use the so-called {\\\\it profile likelihood}\\napproach for the treatment of nuisance parameters~\\\\cite{asymptotic}.\\nThe approach is based on the test statistic built as the following likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{L(\\\\vec{x};\\\\mu,\\\\hat{\\\\hat{\\\\theta}}(\\\\mu))}{L(\\\\vec{x};\\\\hat{\\\\mu},\\\\hat{\\\\theta})}\\\\,,\\n\\\\end{equation}\\nwhere in the denominator both $\\\\mu$ and $\\\\theta$ are fit simultaneously\\nas $\\\\hat{\\\\mu}$ and $\\\\hat{\\\\theta}$, respectively, and\\nin the numerator $\\\\mu$ is fixed, and $\\\\hat{\\\\hat{\\\\theta}}(\\\\mu)$ is the best fit of $\\\\theta$\\nfor the fixed value of $\\\\mu$. The motivation for the choice of Eq.~(\\\\ref{eq:profLike})\\nas the test statistic comes from Wilks' theorem that allows to approximate asymptotically\\n$-2\\\\ln\\\\lambda(\\\\mu)$ as a $\\\\chi^2$~\\\\cite{Wilks}.\\nIn general, Wilks' theorem applies if we have two hypotheses $H_0$ and $H_1$ that\\nare {\\\\it nested}, i.e.: they can be expressed as sets of nuisance parameters\\n$\\\\vec{\\\\theta}\\\\in\\\\Theta_0$ and $\\\\vec{\\\\theta}\\\\in\\\\Theta_1$, respectively, such that\\n$\\\\Theta_0\\\\subseteq\\\\Theta_1$. Given the likelihood function:\\n\\\\begin{equation}\\nL = \\\\prod_{i=1}^N L(\\\\vec{x}_i, \\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nif $H_0$ and $H_1$ are nested, then the following quantity,\\nfor $N\\\\rightarrow\\\\infty$, is distributed as a $\\\\chi^2$ with a number of degrees of freedom\\nequal to the difference of the $\\\\Theta_1$ and $\\\\Theta_0$ dimensionalities: \\n\\\\begin{equation}\\n\\\\chi_r^2 = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_0}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_1}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nIn case of a search for a new signal where the parameter of interest is $\\\\mu$,\\n$H_0$ corresponds to $\\\\mu = 0$ and $H_1$ to any $\\\\mu\\\\ge0$, Eq.~(\\\\ref{eq:wilks})\\ngives:\\n\\\\begin{equation}\\n\\\\chi_r^2(\\\\mu) = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu,\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\mu^\\\\prime,\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu^\\\\prime,\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nConsidering that the supremum is equivalent to the\\nbest fit value, the profile likelihood defined in Eq.~(\\\\ref{eq:profLike}) is obtained.\\nAs a concrete example of application of the profile likelihood,\\nconsider a signal with a Gaussian distribution over a background\\ndistributed according to an exponential distribution. A pseudoexperiment that was randomly-extracted\\naccordint to such a model is shown in Fig.~\\\\ref{fig:toyGplusB}, where a signal yield\\n$s=40$ was assumed on top of a background yield $b=100$, exponentially\\ndistributed in the range of the random variable $m$ from 100 to 150~GeV.\\nThe signal was assumed centered at 125~GeV with a standard deviation of 6~GeV,\\nreminding the Higgs boson invariant mass spectrum.\\nThe signal yields $s$ is fit from data.\\nAll parameters in the model are fixed, except the background yield,\\nwhich is assumed to be known with some level of uncertainty modeled\\nwith a log normal distribution whose corresponding nuisance parameter is called $\\\\beta$.\\nThe likelihood function for the model, which only depends on two parameters,\\n$s$ and $\\\\beta$, is, in case of a single measurement $m$:\\n\\\\begin{equation}\\nL(m;s,\\\\beta) = L_0(m;s,b_0 = be^\\\\beta) L_\\\\beta(\\\\beta;\\\\sigma_\\\\beta)\\\\,,\\n\\\\end{equation}\\nwhere:\\n\\\\begin{eqnarray}\\nL_0(m;s,b_0) & = & \\\\frac{e^{-(s+b_0)}}{n!}\\\\left(\\ns \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} e^{-{(m-\\\\mu)^2}/{2\\\\sigma^2}}+b_0\\\\lambda e^{-\\\\lambda m}\\n\\\\right)\\\\,, \\\\\\\\\\nL_\\\\beta(\\\\beta;\\\\sigma_\\\\beta) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma_\\\\beta}e^{-{\\\\beta^2}/{2\\\\sigma^2}}\\\\,.\\n\\\\end{eqnarray}\\nIf we measure a set values $\\\\vec{m}=(m_1,\\\\,\\\\cdots\\\\,m_N)$, the likelihood function is:\\n\\\\begin{equation}\\nL(\\\\vec{m};s,\\\\beta) = \\\\prod_{i=1}^N L(m_i;s,\\\\beta)\\\\,.\\n\\\\end{equation}\\nThe scan of $-\\\\ln\\\\lambda(s)$ is shown in Fig.~\\\\ref{fig:plScan}, where the profile likelihood\\nwas evaluated assuming $\\\\sigma_\\\\beta=0$ (no uncertainty on $b$, blue curve) or $\\\\sigma_\\\\beta=0.3$\\n(red curve). The minimum value of $-\\\\ln\\\\lambda(s)$ is equal to zero, since\\nat the minimum numerator and denominator in Eq.~(\\\\ref{eq:profLike}) are identical.\\nIntroducing the uncertainty on $\\\\beta$ (red curve) makes the curve broader.\\nThis causes an increase of the uncertainty on the estimate of $s$, whose uncertainty\\ninterval is obtained by intersecting the curve of the negative logarithm of the profile likelihood\\nwith an horizontal line at $-\\\\ln\\\\lambda(s) = 0.5$ (green line in Fig.~\\\\ref{fig:plScan}\\\\footnote{\\nThe plot in Fig.~\\\\ref{fig:plScan} was generated with the library {\\\\sc RooStats}\\nin {\\\\sc Root}~\\\\cite{Root}, which by default, uses $-\\\\ln\\\\lambda$ instead of $-2\\\\ln\\\\lambda$.\\n}).\\nIn order to evaluate the significance of the observed signal, Wilks' theorem can be\\nused. If we assume $\\\\mu=0$ (null hypothesis), the quantity $q_0 = -2\\\\ln\\\\lambda(0)$\\ncan be approximated with a $\\\\chi^2$ having one degree of freedom. Hence, the significance\\ncan be approximately evaluated as:\\n\\\\begin{equation}\\nZ\\\\simeq \\\\sqrt{q_0}\\\\,.\\n\\\\end{equation}\\n$q_0$ is twice the intercept of the curve in Fig.~\\\\ref{fig:plScan} with the vertical axis,\\nand gives an approximate significance of $Z\\\\simeq\\\\sqrt{2\\\\times6.66} = 3.66$,\\nin case of no uncertainty on $b$, and $Z\\\\simeq\\\\sqrt{2\\\\times3.93} = 2.81$, in case\\nthe uncertainty on $b$ is considered. \\nIn this example, the effect of background yield uncertainty reduces the\\nsignificance bringing it below the evidence level ($3\\\\sigma$).\\nThose numerical values can be verified\\nby running many pseudo experiments (toy Monte Carlo) assuming $\\\\mu=0$ and\\ncomputing the corresponding $p$-value. In complex cases, the computation\\nof $p$-values using toy Monte Carlo may become unpractical, and Wilks'\\napproximation provides a very convenient, and often rather precise,\\nalternative calculation.\\n\", \"\\\\section{Upper Limits using the profile binned likelihood}\\nThe profile binned likelihood method is chosen by particle physics collaborations to present phenomenology studies and experimental analyses. As shown above, this method is based on estimating the signal confidence level through a fully frequentist approach focused on parameter estimation~\\\\cite{lista2016practical,barlow2002systematic}. To include systematic effects, the likelihood function is extended with appropriate distribution functions to describe efficiency effects with a given width \\\\( \\\\sigma \\\\). This methodology requires maximizing the likelihood function and obtaining the background profile as a function of the signal strength \\\\( \\\\mu \\\\). The improvement lies in replacing the normalization of the posterior distribution with a multivariable optimization problem, which is computationally less costly and leads to limits that allow for better model exclusion. The statistical estimator is given by~\\\\cite{conway2005calculation,cms2012observation,atlas2012observation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = - 2 ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu, \\\\hat{\\\\hat{b}}(\\\\mu) )}{ \\\\mathcal{L}(\\\\hat{\\\\mu},\\\\hat{b}) } \\\\bigg).\\n\\\\end{equation}\\nwhere \\\\( \\\\hat{\\\\mu} \\\\) and \\\\( \\\\hat{b} \\\\) are the unconditional maximum likelihood estimators, and \\\\( \\\\hat{\\\\hat{b}}(\\\\mu) \\\\) is the conditional maximum likelihood estimator. Frequentist limits are generally less restrictive and allow for exploring regions of significance while adequately accounting for systematic effects. Similar to the Bayesian approach, maximizing the likelihood as a function of \\\\( \\\\mu \\\\) enables finding the profile likelihood that propagates the effect of the background parameters \\\\( \\\\epsilon \\\\).\\nSystematic effects lead to higher upper limits, which restrict the model exclusion power and experimental sensitivity. For example, in a single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), the \\\\texttt{optimize} package is used to find the best-fit parameters, and Monte Carlo methods are applied to sample the estimator \\\\( q_{\\\\mu} \\\\). The extended likelihood function is given by:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu, \\\\epsilon) = \\\\frac{ e^{ -(\\\\mu s + \\\\epsilon b) } (\\\\mu s + \\\\epsilon b)^{n} }{n!} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^{2}} } e^{ -\\\\frac{(1-\\\\epsilon)^{2}}{2\\\\sigma^{2}} },\\n\\\\end{equation}\\nwhere maximizing the likelihood function for the efficiency \\\\( \\\\epsilon \\\\) yields the conditional nuisance estimator \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\):\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) = \\\\frac{1}{2b} \\\\bigg[ ( b -\\\\mu s - \\\\sigma^{2}b^{2}) + \\\\sqrt{ (b + \\\\mu s - \\\\sigma^{2}b^{2})^{2} + 4 b^{2} \\\\sigma^{2}n} \\\\bigg]. \\n\\\\end{equation}\\nFor the single-channel experiment, Figure~[\\\\ref{fig:22}] shows the profile of the nuisance parameter for various values of \\\\( \\\\mu \\\\). The maximum value of the likelihood function shifts depending on the signal strength \\\\( \\\\mu \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/ProfileLikelihood/ProfileLikelihoodNuissance.ipynb}{Source code}}. The right plot shows the maximum likelihood estimate \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\) using both Equation~(\\\\ref{eq:profile}) and the optimization package. This result indicates the maximal behavior of the uncertainty for each value of the signal strength \\\\( \\\\mu \\\\), ensuring model exclusion as restrictive as allowed by the uncertainty \\\\( \\\\sigma \\\\). More generally, the multi-channel case requires a fully numerical procedure to find the profiles of the nuisance parameters and \\\\( q_{\\\\mu} \\\\)~\\\\cite{lista2016practical,cranmer2015practical}.\\nTo illustrate the behavior of the estimator \\\\( q_{\\\\mu} \\\\) as a function of the systematic uncertainty \\\\( \\\\sigma \\\\), a sweep over the signal strength \\\\( \\\\mu \\\\) is performed while optimizing the estimator for the current values of the observation, background component, and signal events. Figure~[\\\\ref{fig:23}] shows the profile binned likelihood for the single-channel experiment as a function of the width of the systematic uncertainty. A larger uncertainty leads to a higher upper limit, which can be quantified using Wald's approximation \\\\( Z(3\\\\sigma) = \\\\sqrt{q_{\\\\mu}} \\\\)~\\\\cite{cowan2011asymptotic}. \\nThe green dashed line represents the observed upper limit for each profile likelihood. For example, for \\\\( \\\\sigma = 0.20 \\\\), the observed upper limit is \\\\( \\\\mu_{up} \\\\approx 4.5 \\\\), which contrasts with the value obtained from the Bayesian approach (Table~[\\\\ref{tb:3}], \\\\( \\\\mu_{up} = 4.91 \\\\)), a smaller value. The right plot represents the search for the p-value while fixing the value of the systematic uncertainty. Obtaining the pseudo-data requires generating an observation consistent with the background-only hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\), and for the signal + background hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\mu s + \\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\) in each observable channel. For each generated value of \\\\( n \\\\) across all channels, the optimization of \\\\( (\\\\hat{\\\\mu}, \\\\hat{\\\\epsilon}, \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu)) \\\\) is performed.\\nTable~[\\\\ref{tb:4}] shows the upper limit values using the grouped profile likelihood method for several values of \\\\( \\\\sigma \\\\). In comparison with the upper limits obtained by the Bayesian method, consistency is observed within the statistical confidence levels inherent to the sampling.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\n$\\\\sigma$ & Profile likelihood Ratio & MCMC algorithm \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{2}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 \\\\\\\\\\n0.10 & 3.52 & 3.31 \\\\\\\\\\n0.20 & 4.57 & 4.66 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nFor the multi-channel case, results are very close between the upper limits without uncertainty and those found using the profile likelihood method. This behavior is attributed to the combined uncertainty of the background across the 30 channels, which does not significantly affect the confidence in the signal strength, especially in the resonance region~\\\\cite{cms2022portrait,atlas2012observation,cowan2011asymptotic}. Table~[\\\\ref{tb:5}] shows the upper limit values for several mass points \\\\( m(H) \\\\) and uncertainty \\\\( \\\\sigma \\\\).\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\nMass($H$)[GeV] & $\\\\sigma$ & Method: Profile likelihood ratio \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & $\\\\mu_{up}(95\\\\ 110 & 0.1 & 0.43 \\\\\\\\\\n110 & 0.2 & 0.45 \\\\\\\\\\n\\\\hline\\n124 & 0.1 & 1.45 \\\\\\\\\\n124 & 0.2 & 1.46 \\\\\\\\\\n\\\\hline\\n142 & 0.1 & 0.29 \\\\\\\\\\n142 & 0.2 & 0.31 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength for different mass points using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\n\\\\subsection{Experimental sensitivity with systematic effects}\\nIn the case of determining the experimental sensitivity for a specific model $s$, the Asimov data $n = s + b$ are used, and a modification is applied to the statistical estimator $q_{\\\\mu}$~\\\\cite{lista2016practical}.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nIn the Wald approximation, the significance is approximately given by:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\sqrt{q_{0}}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:24}] shows the profile likelihood for various values of the systematic uncertainty $\\\\sigma$. It demonstrates how statistical significance is impacted by uncertainty. In general, greater uncertainty in the estimation of background events leads to a loss of sensitivity in a potential experimental analysis aiming to validate a new hypothetical model.\\nThere are alternative methods to establish statistical significance~\\\\cite{cowan2011asymptotic}, such as:\\n\\\\begin{equation}\\nZ_{0}(\\\\sigma) = \\\\frac{s}{\\\\sqrt{s+(1+\\\\sigma)b}}. \\n\\\\end{equation}\\nHowever, in general, experimental sensitivity is overestimated because the maximal information of $\\\\hat{\\\\hat{b}}$ is not fully captured in the profile likelihood. Table~[\\\\ref{tb:6}] shows the statistical significance for various values of systematic uncertainty $\\\\sigma$. This effect reduces experimental sensitivity and should be calculated using the profile binned likelihood method. Note that the approximation $Z_{0} = s / \\\\sqrt{s + b}$ is no longer valid due to the convolution effect of counting and efficiency distributions.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & $Z$ & $Z_{0}=s/\\\\sqrt{s+b}$ & $Z_{0}(\\\\sigma)$ \\\\\\\\\\n\\\\hline\\n0.05 & 0.878 & 0.932 & 0.931 \\\\\\\\\\n0.1 & 0.695 & 0.932 & 0.912 \\\\\\\\\\n0.2 & 0.443 & 0.932 & 0.877 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Statistical significance as a function of systematic uncertainty for the case of a single-channel experiment. Note how the approximation $Z_{0}$ becomes invalid even for $s \\\\ll b$.}\\n\\\\end{center}\\n\\\\end{table}\\nAs shown in Table~[\\\\ref{tb:6}], when there is a 20\\\\\\n\", \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\"}},\n",
       "       {'entity_name': 'null hypothesis', 'entity_type': 'statistics_concept', 'description': 'A statistical hypothesis that posits no effect or no difference, serving as a baseline for comparison against an alternative hypothesis in hypothesis testing and statistical analyses.', 'relevant_passages': {\"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\"}},\n",
       "       {'entity_name': 'hypothesis testing', 'entity_type': 'statistics_concept', 'description': 'A statistical procedure that involves formulating a null hypothesis and an alternative hypothesis to compare two competing theories or models. It evaluates evidence against the null hypothesis to determine which hypothesis better explains the observed data, representing a statement of effect or difference.', 'relevant_passages': {\"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", \"\\\\section{What is Statistics?}\\nStatistics is used to provide quantitative results that give summaries of available data. In High Energy Physics,\\nthere are several different types of statistical activities that are used:\\n\\\\begin{itemize}\\n\\\\item{Parameter Determination:\\\\\\\\\\nWe analyse the data in order to extract the best value(s) of one or more parameters in a model. This could be,\\nfor example, the gradient and intercept of a straight line fit to the data; or the mass of the Higgs boson, as deduced using its decay products. In all cases, as well as obtaining the best values of the parameter(s), their uncertainties and possible correlations must be specified. }\\n\\\\item{Goodness of Fit: \\\\\\\\\\nWe are comparing a single theory with the data, in order to see if they are compatible. If the theory contains free parameters, their best values need to be used to check the Goodness of Fit. If the quality of the fit is unsatisfactory, the best values of the parameters are probably meaningless. }\\n\\\\item{Hypothesis Testing: \\\\\\\\ \\nHere we are comparing the data with two different theories, to see which provides a better description. For example, we may be very interested in knowing whether a model involving the production of a supersymmetric particle is better than one without it. }\\n\\\\item{Decision Making: \\\\\\\\\\nAs the result of the information we have available, we want to decide what further action to take. For example, we may have some evidence that our data shows hints of an exciting discovery, and need to decide whether we should collect more data. This was the situation faced by the CERN management in 2000, when there were perhaps hints of a Higgs boson in data collected at the LEP Collider.\\nSuch decisions usually require a `cost function' for the various possible outcomes, as well as assessments of their relative probabilities. In the example just quoted, numerical values were needed for the cost of missing an important discovery if the experiment was not continued; and on the other hand of running the LEP Collider for another year and for delaying the \\nstart of building the Large Hadron Collider.\\nDecision Making is not considered further in these lectures. }\\n\\\\end{itemize}\\n\"}},\n",
       "       {'entity_name': 'pvalue', 'entity_type': 'statistics_concept', 'description': 'A statistical measure that indicates the probability of obtaining results at least as extreme as the observed data, assuming the null hypothesis is true. It is used to assess the significance of results in hypothesis testing and particle physics analyses, reflecting the likelihood of observing a result, or one more extreme, under the null hypothesis.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The distribution of the test statistic and $p$-values}\\nThe test statistic should be interpreted as a single real-valued number that represents the outcome of the experiment. More formally, it is a mapping of the data to a single real-valued number: \\\\mbox{$\\\\tilde{q}_\\\\mu: \\\\datasim,\\\\globs \\\\rightarrow \\\\mathbb{R}$}. For the observed data the test statistic has a given value, eg. $\\\\tilde{q}_{\\\\mu,\\\\rm obs}$. If one were to repeat the experiment many times the test statistic would take on different values, thus, conceptually, the test statistic has a distribution. Similarly, we can use our model to generate pseudo-experiments using Monte Carlo techniques or more abstractly consider the distribution. Since the number of expected events $\\\\nu(\\\\mu,\\\\vec\\\\theta)$ and the distributions of the discriminating variables $f_c(x_c|\\\\mu,\\\\vec\\\\theta)$ explicitly depend on $\\\\vec\\\\theta$ the distribution of the test statistic will also depend on $\\\\vec\\\\theta$. Let us denote this distribution \\n\\\\begin{equation}\\nf(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\;,\\n\\\\end{equation}\\nand we have analogous expressions for each of the test statistics described above.\\nThe $p$-value for a given observation under a particular hypothesis ($\\\\mu,\\\\vec\\\\theta$) is the probability for an equally or more `extreme' outcome than observed assuming that hypothesis \\n\\\\begin{equation}\\np_{\\\\mu,\\\\vec\\\\theta} = \\\\int_{\\\\tilde{q}_{\\\\mu,\\\\rm obs}}^\\\\infty f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\, d\\\\tilde{q}_\\\\mu\\\\;.\\n\\\\end{equation}\\nThe logic is that small $p$-values are evidence against the corresponding hypothesis. In Toy Monte Carlo approaches, the integral above is really carried out in the space of the data $\\\\int d\\\\datasim d\\\\globs$.\\nThe immediate difficulty is that we are interested in $\\\\mu$ but the $p$-values depend on both $\\\\mu$ and $\\\\vec\\\\theta$. In the frequentist approach the hypothesis $\\\\mu=\\\\mu_0$ would not be rejected unless the $p$-value is sufficiently small \\\\textit{for all} values of $\\\\vec\\\\theta$. Equivalently, one can use the supremum $p$-value for over all $\\\\vec\\\\theta$ to base the decision to accept or reject the hypothesis at $\\\\mu=\\\\mu_0$.\\n\\\\begin{equation}\\np^{\\\\rm sup}_{\\\\mu} = \\\\sup_{\\\\vec\\\\theta}\\\\; p_{\\\\mu,\\\\vec\\\\theta} \\n\\\\end{equation}\\nThe key conceptual reason for choosing the test statistics based on the profile likelihood ratio is that asymptotically (ie. when there are many events) the distribution of the profile likelihood ratio \\\\mbox{$\\\\lambda(\\\\mu=\\\\mu_{\\\\rm true})$} is independent of the values of the nuisance parameters. This follows from Wilks's theorem. In that limit $p^{\\\\rm sup}_{\\\\mu} = p_{\\\\mu,\\\\vec\\\\theta}$ for all $\\\\vec\\\\theta$. \\nThe asymptotic distributions \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu, \\\\vec\\\\theta)$} and \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu', \\\\vec\\\\theta)$} are known and described in Sec.~\\\\ref{sec:asymptotic}. For results based on generating ensembles of pseudo-experiements using Toy Monte Carlo techniques does not assume the form of the distribution $f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta)$, but knowing that it is approximately independent of $\\\\vec\\\\theta$ means that one does not need to calculate $p$-values for all $\\\\vec\\\\theta$ (which is not computationally feasible). Since there may still be some residual dependence of the $p$-values on the choice of $\\\\vec\\\\theta$ we would like to know the specific value of $\\\\vec\\\\theta^{\\\\rm sup}$ that produces the supremum $p$-value over $\\\\vec\\\\theta$. Since larger $p$-values indicate better agreement of the data with the model, it is not surprising that choosing $\\\\vec\\\\theta^{\\\\rm sup}=\\\\hathatthetamu$ is a good estimate of $\\\\vec\\\\theta^{\\\\rm sup}$. This has been studied in detail by statisticians, and is called the Hybrid Resampling method and is referred to in physics as the `profile construction'~\\\\cite{Feldman,Cranmer,hybridResampling,Bodhi}.\\nBased on the discussion above, the following $p$-value is used to quantify consistency with the hypothesis of a signal strength of $\\\\mu$:\\n\\\\begin{equation}\\np_{\\\\mu}=\\\\int_{\\\\tilde q_{\\\\mu,\\\\rm obs}}^{\\\\infty} f(\\\\tilde q_\\\\mu|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu,\\\\textrm{obs})) \\\\,d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nA standard 95\\\\\\nTo calculate the $CL_s$ upper limit, we define $p'_\\\\mu$ as a ratio of p-values,\\n\\\\begin{equation}\\np'_\\\\mu=\\\\frac{p_\\\\mu}{1-p_b} \\\\; ,\\n\\\\end{equation}\\nwhere $p_b$ is the $p$-value derived from the same test statistic under the background-only hypothesis\\n\\\\begin{equation}\\np_b=1-\\\\int_{\\\\tilde q_{\\\\mu,obs}}^\\\\infty f(\\\\tilde q_\\\\mu|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nThe $CL_s$ upper-limit on $\\\\mu$ is denoted $\\\\mu_{up}$ and obtained by solving for $p'_{\\\\mu_{up}}=5\\\\It is worth noting that while confidence intervals produced with the ``CLs'' method over cover, a value of $\\\\mu$ is regarded as excluded at the 95\\\\ \\nFor the purposes discovery one is interested in compatibility of the data with the background-only hypothesis. Statistically, a discovery corresponds to rejecting the background-only hypothesis. This compatibility is based on the following $p$-value\\n\\\\begin{equation}\\np_0=\\\\int_{\\\\tilde q_{0,obs}}^\\\\infty f(\\\\tilde q_0|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_0 \\\\;.\\n\\\\end{equation}\\nThis $p$-value is also based on the background-only hypothesis, but the test statistic $\\\\tilde q_0$ is suited for testing the background-only while the test statistic $\\\\tilde{q}_\\\\mu$ in Eq.~\\\\ref{eq:pb} is suited for testing a hypothesis with signal.\\nIt is customary to convert the background-only $p$-value into the quantile (or ``sigma'') of a unit Gaussian. This conversion is purely conventional and makes no assumption that the test statistic $q_0$ is Gaussian distributed. The conversion is defined as:\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1-p_0) ;\\\\,\\n\\\\end{equation}\\nwhere $\\\\Phi^{-1}$ is the inverse of the cumulative distribution for a unit Gaussian. One says the significance of the result is $Z\\\\sigma$ and the standard discovery convention is $5\\\\sigma$, corresponding to $p_0=2.87 \\\\cdot 10^{-7}$.\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\", '\\\\section{Least squares for Goodness of Fit}\\n\\\\subsection{The chi-squared distribution}\\nIt turns out that, if we repeated our experiment a large number of times, and certain conditions are satisfied, \\nthen $S_{min}$ will follow a $\\\\chi^2$ distribution with $\\\\nu = n - p$ degrees of freedom, where $n$ is the \\nnumber of data points, $p$ is the number of free parameters in the fit, and $S_{min}$ is the value of $S$ \\nfor the best values of the free parameters. For example, a straight line with free intercept and gradient fitted \\nto 12 data points would have $\\\\nu = 10$.\\nThe conditions for this to be true include:\\n\\\\begin{itemize}\\n\\\\item{the theory is correct:}\\n\\\\item{the data are unbiassed and asymptotic;}\\n\\\\item{the $y_i$ are Gaussian distributed about their true values;}\\n\\\\item{the estimates for $\\\\sigma_i$ are correct;$\\\\ \\\\ \\\\ \\\\ \\\\ $ etc. }\\n\\\\end{itemize}\\nUseful properties to know about the mathematical $\\\\chi^2$ distribution are that their mean is $\\\\nu$ and their variance is\\n$2\\\\nu$. Thus if a global fit to a lot of data has $S_{min}$ = 2200 and there are 2000 degrees of freedom, we can \\nimmediately estimate that this is equivalent to a fluctuation of 3.2$\\\\sigma$.\\nMore useful than plots of $\\\\chi^2$ distributions are those of the fractional tail area beyond a particular value \\nof $\\\\chi^2$ (see figs. \\\\ref{fig:chi_squared} and \\\\ref{fig:chi_sq_tail} respectively).\\nThe $\\\\chi^2$ goodness of fit test consists of\\n\\\\begin{itemize}\\n\\\\item{For the given theoretical form, find the best values of its free parameters, and hence $S_{min}$;}\\n\\\\item{Determine $\\\\nu$ from $n$ and $p$; and}\\n\\\\item{Use $S_{min}$ and $\\\\nu$ to obtain the tail probability $p$ \\\\footnote{If the conditions for $S_{min}$ to follow a \\n$\\\\chi^2$ distribution are satisfied, this simply involves using the tail probability of a $\\\\chi^2$ distribution. \\nIn other cases, it may be necessary to use Monte Carlo simulation to obtain the distribution of $S_{min}$;\\nthis could be tedious. }.} \\n\\\\end{itemize}\\nThen $p$ is the probability that, if the theory is correct, by random fluctuations we would have obtained a value of $S_{min}$ at least as large as the observed one. If this probability is smaller than some pre-defined level $\\\\alpha$, we reject the hypothesis that the model provides a good description of the data.\\n\\\\subsection{When $\\\\nu \\\\ne n - p$}\\nIf we add an extra parameter into our theoretical description, even if it is not really needed, we expect the value of $S_{min}$ to decrease slightly. (This contrasts with including a parameter which is really relevant, which can result in a dramatic reduction in $S_{min}$.) In determining $p$-values, this is allowed for by the reduction of $\\\\nu$. On average,\\na parameter which is not needed reduces $S_{min}$ by 1. But consider the following examples.\\n\\\\subsubsection{Small oscillatory term}\\nImaging we are fitting a histogram of a variable $\\\\phi$ by a distribution of the form\\n\\\\begin{equation}\\n\\\\frac{dy}{d\\\\phi} = N[ 1 + 10^{-6} cos(\\\\phi -\\\\phi_0)],\\n\\\\end{equation}\\nwhere the two parameters are the normaisation $N$ and the phase $\\\\phi_0$. Because of the factor $10^{-6}$ in front\\nof the cosine term, $\\\\phi_0$ will have a miniscule effect on the prediction, and so including this as a parameter has negligible effect on $S_{min}$; $\\\\phi_0$ is effectively not a free parameter.\\n\\\\subsubsection{Neutrino oscillations} \\nFor a scenario of two oscillating neutrino flavours, the probability $P$ of a neutrino of energy $E$ to remain the same flavour after\\na flight length $L$ is\\n\\\\begin{equation}\\nP = 1 - A sin^2(\\\\delta m^2 L/E)\\n\\\\end{equation}\\nwhere the two parameters are $\\\\delta m^2$, the difference in the mass-squareds of the two neutrino flavours, and \\n$A = sin^2 2\\\\theta$ with $\\\\theta$ being the mixing angle. However, since for small angles $\\\\alpha,\\\\ sin\\\\alpha\\\\approx \\\\alpha$, for small $\\\\delta m^2L/E$ the probability $P$ of eqn \\\\ref{neutrino} is approximately $1 - A(\\\\delta m^2 L/E)^2$. Thus the two parameters occur only as the product $A (\\\\delta m^2)^2$, and cannot be determined separately. Thus in that regime we have effectively just a single parameter.\\n\\\\vspace{0.2in}\\nIn both the above examples, an enormous amount of data would enable us to distinguish the small effects produced by the second \\nparameter; hence the requirement for asymptotic conditions. \\n\\\\subsection{Errors of First and Second Kind}\\nIn deciding in a Goodness of Fit test whether or not to reject the null hypothesis $H_0$ (e.g. that the data points lie on a \\nstraight line), there are two sorts of mistake we might make:\\n\\\\begin{itemize}\\n\\\\item{Error of the First Kind. This is when we reject $H_0$ when it is in fact true. The fraction of cases in which this happens \\nshould equal $\\\\alpha$, the cut on the $p$-value. }\\n\\\\item{Error of the Second Kind. This is when we do not reject $H_0$, even though some other hypothesis is true. The rate at which this happens depends on how similar $H_0$ and the alternative hypothesis are, the relative frequencies of the two hypotheses being true, etc.}\\n\\\\end{itemize}\\nAs $\\\\alpha$ increases the rates of Errors of the First and Second kinds go up and down respectively. \\nThese Errors correspond to a loss of efficiency and to an increase of contamination respectively.\\n\\\\subsection{Other Goodness of Fit tests}\\nThe $\\\\chi^2$ method is by no means the only one for testing Goodness of Fit.\\nIndeed whole books have been written on the subject\\\\cite{DAgostino}. Here\\nwe mention just one other, the Kolmogorov-Smirnov method (K-S), which has the \\nadvantage of working with individual observations. It thus can be used with \\nfewer observations than are required for the binned histograms in the $\\\\chi^2$\\napproach. \\nA cumulative plot is produced of the fraction of events as a function of the variable\\nof interest $x$. An example is shown in Fig.~\\\\ref{fig:K_S}. \\nThis shows the fraction of data events with $x$ smaller than any particular \\nvalue. It is thus a stepped plot, with the fraction going from zero at the extreme left, \\nto unity on the right hand side. Also on the plot is a curve showing the expected cumulative\\nfraction for some theory. The K-S method makes use of the largest (as a function of $x$) vertical \\ndiscrepancy $d$ between the data plot and the theoretical curve. Assuming the theory is true\\nand given the number of observations $N$, the probability $p_{KS}$ of obtaining $d$ at least as large \\nas the observed value can be calculated. The beauty of the K-S method is that this probability\\nis independent of the details of the theory. As in the $\\\\chi^2$ approach, the K-S probability \\ngives a numerical way of checking the compatibility of theory and data. If $p_{KS}$ is small, we \\nare likely to reject the theory as being a good description of the data.\\nSome features of the K-S method are:\\n\\\\begin{itemize}\\n\\\\item{The main advantage is that it can use a small number of observations.}\\n\\\\item{The calculation of the K-S probability depends on there being no adjustable parameters in the theory.\\nIf there are, it will be necessary for you to determine the expected distribution for $d$, presumably \\nby Monte Carlo simulation.}\\n\\\\item{It does not extend naturally to data of more than one dimension, because of there being no unique\\nway of producing an ordering in several dimensions.}\\n\\\\item{It is not very sensitive to deviations in the tails of distributions, which is where searches for\\nnew physics are often concentrated e.g. high mass or transverse momentum. Fortunately variants of K-S exist, \\nwhich put more emphasis on discrepancies in the tails.}\\n\\\\item{Instead of comparing a data cumulative distribution with a theoretical curve, it can alternatively be\\ncompared with another distribution. This can be from a simulation of a theory, or with another data set. The\\nlatter could be to check that two data sets are compatible. \\nThe calculation of the K-S probability now requires the maximum discrepancy $d$, and the numbers of events \\n$N_1$ and $N_2$ in each of the two distributions being compared.}\\n\\\\end{itemize}\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Profile likelihood}\\nMost of the recent searches at LHC use the so-called {\\\\it profile likelihood}\\napproach for the treatment of nuisance parameters~\\\\cite{asymptotic}.\\nThe approach is based on the test statistic built as the following likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{L(\\\\vec{x};\\\\mu,\\\\hat{\\\\hat{\\\\theta}}(\\\\mu))}{L(\\\\vec{x};\\\\hat{\\\\mu},\\\\hat{\\\\theta})}\\\\,,\\n\\\\end{equation}\\nwhere in the denominator both $\\\\mu$ and $\\\\theta$ are fit simultaneously\\nas $\\\\hat{\\\\mu}$ and $\\\\hat{\\\\theta}$, respectively, and\\nin the numerator $\\\\mu$ is fixed, and $\\\\hat{\\\\hat{\\\\theta}}(\\\\mu)$ is the best fit of $\\\\theta$\\nfor the fixed value of $\\\\mu$. The motivation for the choice of Eq.~(\\\\ref{eq:profLike})\\nas the test statistic comes from Wilks' theorem that allows to approximate asymptotically\\n$-2\\\\ln\\\\lambda(\\\\mu)$ as a $\\\\chi^2$~\\\\cite{Wilks}.\\nIn general, Wilks' theorem applies if we have two hypotheses $H_0$ and $H_1$ that\\nare {\\\\it nested}, i.e.: they can be expressed as sets of nuisance parameters\\n$\\\\vec{\\\\theta}\\\\in\\\\Theta_0$ and $\\\\vec{\\\\theta}\\\\in\\\\Theta_1$, respectively, such that\\n$\\\\Theta_0\\\\subseteq\\\\Theta_1$. Given the likelihood function:\\n\\\\begin{equation}\\nL = \\\\prod_{i=1}^N L(\\\\vec{x}_i, \\\\vec{\\\\theta})\\\\,,\\n\\\\end{equation}\\nif $H_0$ and $H_1$ are nested, then the following quantity,\\nfor $N\\\\rightarrow\\\\infty$, is distributed as a $\\\\chi^2$ with a number of degrees of freedom\\nequal to the difference of the $\\\\Theta_1$ and $\\\\Theta_0$ dimensionalities: \\n\\\\begin{equation}\\n\\\\chi_r^2 = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_0}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}\\\\in\\\\Theta_1}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nIn case of a search for a new signal where the parameter of interest is $\\\\mu$,\\n$H_0$ corresponds to $\\\\mu = 0$ and $H_1$ to any $\\\\mu\\\\ge0$, Eq.~(\\\\ref{eq:wilks})\\ngives:\\n\\\\begin{equation}\\n\\\\chi_r^2(\\\\mu) = -2\\\\ln\\\\frac{\\\\displaystyle\\n\\\\sup_{\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu,\\\\vec{\\\\theta})\\n}{\\\\displaystyle\\n\\\\sup_{\\\\mu^\\\\prime,\\\\vec{\\\\theta}}\\\\,\\\\prod_{i=1}^NL(\\\\vec{x}_i;\\\\mu^\\\\prime,\\\\vec{\\\\theta})\\n}\\\\,.\\n\\\\end{equation}\\nConsidering that the supremum is equivalent to the\\nbest fit value, the profile likelihood defined in Eq.~(\\\\ref{eq:profLike}) is obtained.\\nAs a concrete example of application of the profile likelihood,\\nconsider a signal with a Gaussian distribution over a background\\ndistributed according to an exponential distribution. A pseudoexperiment that was randomly-extracted\\naccordint to such a model is shown in Fig.~\\\\ref{fig:toyGplusB}, where a signal yield\\n$s=40$ was assumed on top of a background yield $b=100$, exponentially\\ndistributed in the range of the random variable $m$ from 100 to 150~GeV.\\nThe signal was assumed centered at 125~GeV with a standard deviation of 6~GeV,\\nreminding the Higgs boson invariant mass spectrum.\\nThe signal yields $s$ is fit from data.\\nAll parameters in the model are fixed, except the background yield,\\nwhich is assumed to be known with some level of uncertainty modeled\\nwith a log normal distribution whose corresponding nuisance parameter is called $\\\\beta$.\\nThe likelihood function for the model, which only depends on two parameters,\\n$s$ and $\\\\beta$, is, in case of a single measurement $m$:\\n\\\\begin{equation}\\nL(m;s,\\\\beta) = L_0(m;s,b_0 = be^\\\\beta) L_\\\\beta(\\\\beta;\\\\sigma_\\\\beta)\\\\,,\\n\\\\end{equation}\\nwhere:\\n\\\\begin{eqnarray}\\nL_0(m;s,b_0) & = & \\\\frac{e^{-(s+b_0)}}{n!}\\\\left(\\ns \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} e^{-{(m-\\\\mu)^2}/{2\\\\sigma^2}}+b_0\\\\lambda e^{-\\\\lambda m}\\n\\\\right)\\\\,, \\\\\\\\\\nL_\\\\beta(\\\\beta;\\\\sigma_\\\\beta) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma_\\\\beta}e^{-{\\\\beta^2}/{2\\\\sigma^2}}\\\\,.\\n\\\\end{eqnarray}\\nIf we measure a set values $\\\\vec{m}=(m_1,\\\\,\\\\cdots\\\\,m_N)$, the likelihood function is:\\n\\\\begin{equation}\\nL(\\\\vec{m};s,\\\\beta) = \\\\prod_{i=1}^N L(m_i;s,\\\\beta)\\\\,.\\n\\\\end{equation}\\nThe scan of $-\\\\ln\\\\lambda(s)$ is shown in Fig.~\\\\ref{fig:plScan}, where the profile likelihood\\nwas evaluated assuming $\\\\sigma_\\\\beta=0$ (no uncertainty on $b$, blue curve) or $\\\\sigma_\\\\beta=0.3$\\n(red curve). The minimum value of $-\\\\ln\\\\lambda(s)$ is equal to zero, since\\nat the minimum numerator and denominator in Eq.~(\\\\ref{eq:profLike}) are identical.\\nIntroducing the uncertainty on $\\\\beta$ (red curve) makes the curve broader.\\nThis causes an increase of the uncertainty on the estimate of $s$, whose uncertainty\\ninterval is obtained by intersecting the curve of the negative logarithm of the profile likelihood\\nwith an horizontal line at $-\\\\ln\\\\lambda(s) = 0.5$ (green line in Fig.~\\\\ref{fig:plScan}\\\\footnote{\\nThe plot in Fig.~\\\\ref{fig:plScan} was generated with the library {\\\\sc RooStats}\\nin {\\\\sc Root}~\\\\cite{Root}, which by default, uses $-\\\\ln\\\\lambda$ instead of $-2\\\\ln\\\\lambda$.\\n}).\\nIn order to evaluate the significance of the observed signal, Wilks' theorem can be\\nused. If we assume $\\\\mu=0$ (null hypothesis), the quantity $q_0 = -2\\\\ln\\\\lambda(0)$\\ncan be approximated with a $\\\\chi^2$ having one degree of freedom. Hence, the significance\\ncan be approximately evaluated as:\\n\\\\begin{equation}\\nZ\\\\simeq \\\\sqrt{q_0}\\\\,.\\n\\\\end{equation}\\n$q_0$ is twice the intercept of the curve in Fig.~\\\\ref{fig:plScan} with the vertical axis,\\nand gives an approximate significance of $Z\\\\simeq\\\\sqrt{2\\\\times6.66} = 3.66$,\\nin case of no uncertainty on $b$, and $Z\\\\simeq\\\\sqrt{2\\\\times3.93} = 2.81$, in case\\nthe uncertainty on $b$ is considered. \\nIn this example, the effect of background yield uncertainty reduces the\\nsignificance bringing it below the evidence level ($3\\\\sigma$).\\nThose numerical values can be verified\\nby running many pseudo experiments (toy Monte Carlo) assuming $\\\\mu=0$ and\\ncomputing the corresponding $p$-value. In complex cases, the computation\\nof $p$-values using toy Monte Carlo may become unpractical, and Wilks'\\napproximation provides a very convenient, and often rather precise,\\nalternative calculation.\\n\", '\\\\section{Profile frequentist estimator}\\nThe estimator \\\\( \\\\mathcal{Q} \\\\) has been used in calculating upper limits for the mass of the Higgs boson. However, incorporating systematic effects is complex. Including these effects requires randomly varying the central values to obtain \\\\( f(\\\\mathcal{Q},\\\\mu) \\\\), which increases the variance and thus expands the upper limits. This phenomenon will be analyzed in detail in the section dedicated to systematic effects~\\\\cite{barlow2002systematic}. Currently, phenomenology and experimental analyses employ a statistical estimator, \\\\( q_{\\\\mu} \\\\), which combines the unbiased frequentist approach with the incorporation of systematic effects through the profile likelihood. This estimator requires maximizing the likelihood function with respect to signal strength, and for single-channel experiments, it is given by~\\\\cite{lista2016practical,cranmer2015practical}:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(\\\\hat{\\\\mu})}.\\n\\\\end{equation}\\nWhere \\\\( \\\\hat{\\\\mu} \\\\) is the maximum likelihood estimator of the observation \\\\( n \\\\). By appropriately applying the logarithmic function, the estimator can be reformulated as a minimization problem. The hypothesis test without considering systematic effects is expressed as follows:\\n\\\\begin{equation}\\nq_{\\\\mu} = -2ln(\\\\lambda(\\\\mu)).\\n\\\\end{equation}\\nFinally, to quantify the degree of disagreement between the observation and the hypothesis, the p-value of the observation is calculated:\\n\\\\begin{equation}\\np_{\\\\mu} = \\\\int_{q_{\\\\mu,obs}}^{\\\\infty} f(q_{\\\\mu}/\\\\mu) dq_{\\\\mu},\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu, \\\\text{obs}} \\\\) is the observed value of the estimator in the data, and \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) represents the distribution of the estimator for a specific value of \\\\( \\\\mu \\\\). Generally, this is an optimization problem to obtain the best fit of the model, followed by a sampling problem to determine \\\\( f(q_{\\\\mu}/\\\\mu) \\\\). Specifically, in the case of upper limit searches, the statistical test simplifies to~\\\\cite{lista2016practical,read2002presentation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = \\n\\\\begin{cases} \\n-2ln(\\\\lambda(\\\\mu)) & \\\\hat{\\\\mu} \\\\le \\\\mu \\\\\\\\\\n0 & \\\\hat{\\\\mu} > \\\\mu.\\n\\\\end{cases}\\n\\\\end{equation}\\nWhere \\\\( q_{\\\\mu} = 0 \\\\) is adjusted to avoid excluding values smaller than the maximum likelihood estimator (i.e., in the non-physical case). With these definitions, the confidence level (\\\\( CL_{b} \\\\)) associated with the observation under the background-only hypothesis is expressed as:\\n\\\\begin{equation}\\nCL_{b} = 1-p_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{\\\\mu}/0) dq_{\\\\mu},\\n\\\\end{equation}\\nTherefore, the confidence level of the signal, \\\\( CL_{s}(\\\\mu) \\\\), is defined as:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}.\\n\\\\end{equation}\\nAs in the case of the unprofiled estimator, the upper limit is defined by \\\\( CL_{s}(\\\\mu_{up}) = 0.05 \\\\), which corresponds to the model exclusion criterion. For the single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), a scan is performed by sampling the distribution \\\\( f(q_{\\\\mu}/\\\\mu) \\\\) for both hypotheses. Figure~[\\\\ref{fig:13}] shows the scan of the confidence level for the signal strength as a function of \\\\( \\\\mu \\\\), as well as the distributions of \\\\( H_{0} \\\\) and \\\\( H_{1} \\\\) for \\\\( \\\\mu=1 \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LHC/UpperLimit_qm.ipynb}{Source code}}.\\nAt each point, the maximum likelihood estimator is obtained using the specialized \\\\texttt{optimize} package~\\\\cite{virtanen2018scipy}. The expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.13 \\\\), and the observed upper limit is \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.48 \\\\). These values are fully consistent with the upper limits obtained using the estimator \\\\( \\\\mathcal{Q} \\\\).\\n', \"\\\\section{Making a discovery}\\nWe now turn from setting limits, to say what you did not see,\\nto the more exciting prospect of making a discovery.\\nRemembering hypothesis testing, in claiming a discovery you have to show that your data can't be explained without it.\\nThis is \\nquantified by the $p-$value: the probability of getting a result this extreme (or worse) under the null hypothesis/Standard Model. \\n(This is {\\\\it not} `The probability that the Standard Model is correct', but it seems impossible for journalists\\nto understand the difference.)\\nSome journals (particularly in psychology) refuse to publish papers giving $p-$values.\\nIf you do lots of studies, some will have low $p-$values (5\\\\The danger is that these get published, but the unsuccessful ones are binned.\\nIs $p$ like the significance $\\\\alpha$? Yes and no. The formula is the same, but $\\\\alpha$ is a property of the test, computed before you see the data.\\n$p$ is a property of the data. \\n\\\\subsection{Sigma language} \\nThe probability ($p-$value) is often\\ntranslated into Gaussian-like language: the probability of a result more than 3$\\\\sigma$ from the mean is 0.27\\\\ whether one takes the 1-tailed or 2-tailed option. Both are used.)\\nIn reporting a result with a significance of `so many $\\\\sigma$' there is no actual \\n$\\\\sigma$ involved: it is just a translation to give a better feel for the size of the probability.\\nBy convention, 3 sigma, $p= 0.0013$ is reported as `Evidence for' whereas a full \\n5 sigma\\\\\\\\ $p=0.0000003$ is required for `discovery of'.\\n\\\\subsection{The look-elsewhere effect}\\nYou may think that the requirement for 5 $\\\\sigma$ is excessively cautious.\\nIts justification comes from history---too many 3- and 4- sigma `signals' have gone away when more data was taken.\\nThis is partly explained by the `look-elsewhere effect'. How many peaks can you see in the\\ndata in Fig.~\\\\ref{fig:LEE}?\\nThe answer is that there are none. The data is in fact purely random and flat. But the human eye is very good at seeing features.\\nWith 100 bins, a $p-$value below 1\\\\ This can be factored in, to some extent, using pseudo-experiments, but this does\\nnot allow for the sheer number of plots being produced by \\nhard-working physicists looking for something. Hence the need for caution.\\nThis is not just ancient history. ATLAS and CMS recently observed a signal in the $\\\\gamma \\\\gamma$ mass around 750~GeV, with a significance of\\n$3.9 \\\\sigma$ (ATLAS) and $3.4 \\\\sigma$ (CMS), which went away when more data was taken.\\n\\\\subsection{Blind analysis}\\nIt is said\\\\footnote{This story is certainly not historically accurate, but it's still a good story (\\\\textit{quoteinvestigator.com}: \\\\url{https://quoteinvestigator.com/2014/06/22/chip-away/}).} that when Michaelangelo was asked how he created his masterpiece sculpture `David' \\nhe replied\\n`It was easy---all I did was get a block of marble and chip away everything that didn't look like David'.\\nSuch creativity may be good for sculpture, but it's bad for physics. \\nIf you take your data and devise cuts to remove all the events that don't look like the signal you want to see, then whatever is left \\nat the end will look like that signal. \\nMany/most analyses are now done `blind'. \\nCuts are devised using Monte Carlo and/or non-signal data.\\nYou only `open the box' once the cuts are fixed. Most collaborations have a formal procedure for doing this.\\nThis may seem a tedious imposition, but we have learnt the hard way that it avoids embarrassing mistakes.\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Ensemble of pseudo-experiments generated with ``Toy'' Monte Carlo}\\nThe $p$-values in the procedure described above require performing several integrals. In the case of the asymptotic approach, the distributions for $\\\\tilde q_\\\\mu$ and $\\\\tilde q_0$ are known and the integral is performed directly. When the distributions are not assumed to take on their asymptotic form, then they must be constructed using Monte Carlo methods. In the ``toy Monte Carlo'' approach one generates pseudo-experiments in which the number of events in each channel $n_c$, the values of the discriminating variables $\\\\{x_{ec}\\\\}$ for each of those events, and the auxiliary measurements (global observables) $a_p$ are all randomized according to $\\\\F_{\\\\rm tot}$. We denote the resulting data $\\\\data_{\\\\rm toy}$ and global observables $\\\\globs_{\\\\rm toy}$. By doing this several times one can build an ensemble of pseudo-experiments and evaluate the necessary integrals. Recall that Monte Carlo techniques can be viewed as a form of numerical integration.\\nThe fact that the auxiliary measurements $a_p$ are randomized is unfamiliar in particle physics. The more familiar approach for toy Monte Carlo is that the nuisance parameters are randomized. This requires a distribution for the nuisance parameters, and thus corresponds to a Bayesian treatment of the nuisance parameters. The resulting $p$-values are a hybrid Bayesian-Frequentist quantity with no consistent definition of probability. To maintain a strictly frequentist procedure, the corresponding operation is to randomize the auxiliary measurements. \\nWhile formally this procedure is well motivated, as physicists we also know that our models can have deficiencies and we should check that the distribution of the auxiliary measurements does not deviate too far from our expectations. In Section~\\\\ref{Sec:crossChecks} we show the distribution of the auxiliary measurements and the corresponding $\\\\hat{\\\\vec\\\\theta}$ from the toy Monte Carlo technique. \\nTechnically, the pseudo-experiments are generated with the \\\\texttt{RooStats} \\\\texttt{ToyMCSampler}, which is used by the higher-level tool \\\\texttt{FrequentistCalculator}, which is in turn used by \\\\texttt{HypoTestInverter}.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The modified frequentist approach}\\nA {\\\\it modified frequentist approach}~\\\\cite{CLs} was proposed for the first time for the\\ncombination of the results of searches for the Higgs boson by the four LEP experiments, ALEPH, DELPHI, L3 and OPAL~\\\\cite{Higgs_at_LEP}.\\nGiven a test statistic $\\\\lambda(x)$ that depends on some observation $x$, its distribution should be determined\\nunder the two hypotheses\\n$H_1$ (signal plus background) and $H_0$ (background only). The following $p$-values can be used,\\nwhere we assume that the test statistic $\\\\lambda$ tends to have small values for $H_1$ and\\nlarger values for $H_0$:\\n\\\\begin{eqnarray}\\np_{s+b}& = & P(\\\\lambda(x | H_1) \\\\ge \\\\lambda^{\\\\mathrm{obs}} )\\\\,, \\\\\\\\\\np_b & = & P(\\\\lambda(x | H_0) \\\\le \\\\lambda^{\\\\mathrm{obs}} )\\\\,. \\n\\\\end{eqnarray}\\n$p_{s+b}$ and $p_b$ can be interpreted as follows:\\n\\\\begin{itemize}\\n\\\\item $p_{s+b}$ is the probability to obtain a result which is less compatible with the signal than the observed result, assuming the signal hypothesis;\\n\\\\item $p_b$ is the probability to obtain a result less compatible with the background-only hypothesis than the observed one, assuming background only.\\n\\\\end{itemize}\\nInstead of requiring, as for a frequentist upper limit, $p_{s+b} \\\\le \\\\alpha$,\\nthe modified approach introduces a new quantity, $\\\\mathrm{CL}_s$, defined as:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b}\\\\,,\\n}\\n\\\\end{equation}\\nand the upper limit is set by requiring $\\\\mathrm{CL}_s \\\\le \\\\alpha$.\\nFor this reason, the modified frequentist approach is also called {\\\\it ``$\\\\mathrm{CL}_s$ method''}.\\nIn practice, in most of the realistic cases, $p_b$ and $p_{s+b}$ are computed from\\nsimulated pseudoexperiments ({\\\\it toy Monte Carlo}) by approximating the probabilities\\ndefined in Eq.~(\\\\ref{eq:CLSpsb},~\\\\ref{eq:CLSpb}) with the fraction of the total number of pseudoexperiments\\nsatisfying their respective condition:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b} = \\\\frac{N(\\\\lambda_{s+b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}{N(\\\\lambda_{b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}\\\\,.\\n\\\\end{equation}\\nSince $1-p_b \\\\le 1$, then $\\\\mathrm{CL}s \\\\ge p_{s+b}$, hence upper limits computed with the $\\\\mathrm{CL}_s$ method are\\nalways {\\\\it conservative}.\\nIn case the distributions of the test statistic $\\\\lambda$ (or equivalently $-2\\\\ln\\\\lambda$) for the two hypotheses $H_0$ and $H_1$\\nare well separated (Fig.~\\\\ref{fig:CLs12}, left),\\nif $H_1$ is true, than $p_b$ will have a very high chance to be very small, hence $1-p_b \\\\simeq 1$ and $\\\\mathrm{CL}_s \\\\simeq p_{s+b}$. In this case\\n$\\\\mathrm{CL}_s$ and the purely frequentist upper limits coincide.\\nIf the two distributions instead largely overlap (Fig.~\\\\ref{fig:CLs12}, right), indicating that the experiment has poor sensitivity on the\\nsignal, in case $p_b$ is large, because of a statistical fluctuation, then $1 - p_b$ becomes small.\\nThis prevents $\\\\mathrm{CL}_s$ to become too small,\\ni.e.: it prevents to reject cases where the experiment has little sensitivity.\\nIf we apply the $\\\\mathrm{CL}_s$ method to the previous counting experiment, using \\nthe observed number of events $n^{\\\\mathrm{obs}}$ as test statistic,\\nthen $\\\\mathrm{CL}s$ can be written, considering that $n$ tends to be large in case of\\n$H_1$, for this case, as:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{P(n\\\\le n^{\\\\mathrm{obs}} | s+b)}{P(n \\\\le n^{\\\\mathrm{obs}} |b)}\\\\,.\\n\\\\end{equation}\\nExplicitating the Poisson distribution, the computation gives the same result as for the Bayesian case with a uniform prior\\n(Eq.~(\\\\ref{eq:Helene})). In many cases, the $\\\\mathrm{CL}_s$ upper \\nlimits give results that are very close, numerically, to Bayesian\\ncomputations performed assuming a uniform prior.\\nOf course, this does not allow to interpret $\\\\mathrm{CL}_s$ upper limits\\nas Bayesian upper limits.\\nConcerning the interpretation of $\\\\mathrm{CL}_s$, it's worth reporting from Ref~\\\\cite{CLs} the\\nfollowing statements:\\n\\\\begin{displayquote}\\n{\\\\it A specific modification of a purely classical statistical analysis is used to avoid excluding or discovering signals which the search is in fact not sensitive to.}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it The use of\\\\, $\\\\mathrm{CL}_s$ is a conscious decision not to insist on the frequentist concept of full coverage (to guarantee that the confidence interval doesn't include the true value of the parameter in a fixed fraction of experiments).}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it Confidence intervals obtained in this manner do not have the same interpretation as traditional frequentist confidence intervals nor as Bayesian credible intervals.}\\n\\\\end{displayquote}\\n\", \"\\\\section{Inference}\\n\\\\subsection{Likelihood function for binned samples}\\nSometimes data are available in form of a binned histogram. This may be convenient\\nwhen a large number of entries is available, and computing an unbinned likelihood function (Eq.~(\\\\ref{eq:unbinnedLikeFun}))\\nwould be too much computationally expansive.\\nIn most of the cases, each bin content is independent on any other bin and all obey Poissonian distributions,\\nassuming that bins contain event-counting information.\\nThe likelihood function can be written as product of Poissonisn PDFs corresponding to each bin\\nwhose number of entries is given by $n_i$ .\\nThe expected number of entries in each bin depends on some unknown parameters: $\\\\mu_i = \\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nThe function to be minimized, in order to fit $\\\\theta_1, \\\\cdots,\\\\theta_n$, is the following:\\n\\\\begin{eqnarray}\\n-2\\\\ln L(\\\\vec{n};\\\\vec{\\\\theta}) & = &\\n-2\\\\ln \\\\prod_{i=1}^{n_{\\\\mathrm{bins}}}\\\\mathrm{Poiss}(n_i;\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)) \\\\\\\\\\n& = & -2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\ln \\\\frac{\\ne^{-\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)^{n_i}\\n}{n_i!}\\\\\\\\\\n& = & 2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\left(\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)\\n-n_i\\\\ln\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m) +\\\\ln{n_i!}\\\\right)\\\\,.\\n\\\\end{eqnarray}\\nThe expected number of entries in each bin, $\\\\mu_i$, is often approximated by a continuous function $\\\\mu(x)$\\nevaluated at the center of the bin $x=x_i$.\\nAlternatively, $\\\\mu_i$ can be given by the superposition of other histograms ({\\\\it templates}),\\ne.g.: the sum of histograms obtained from different simulated processes.\\nThe overall yields of the considered processes may be left as free parameters in the fit in order to constrain\\nthe normalization of simulated processes from data, rather than relying on simulation prediction,\\nwhich may affected by systematic uncertainties.\\nThe distribution of the number of entries in each bin can be approximated,\\nfor sufficiently large number of entries,\\nby a Gaussian with standard deviation equal to $\\\\sqrt{n_i}$. \\nMaximizing $L$ is equivalent to minimize:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\frac{\\\\left(n_i-\\\\mu(x_i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\right)^2\\n}{n_i }\\n\\\\end{equation}\\nEquation~(\\\\ref{eq:NeymanChi2}) defines the so-called Neyman's $\\\\chi^2$ variable.\\nSometimes, the denominator $n_i$ is replaced by $\\\\mu_i = \\\\mu (x_i; \\\\theta_1, \\\\cdots, \\\\theta_m)$\\n(Pearson's $\\\\chi^2$) in order to avoid cases with $n_i$ equal to zero or very small.\\nAnalytic solutions exist in a limited number of simple cases, e.g.: if $\\\\mu$ is a linear function.\\nIn most of the realistic cases, the $\\\\chi^2$ minimization is performed numerically, as for\\nmost of the unbinned maximum likelihood fits.\\nBinned fits are, in many cases, more convenient with respect to unbinned fits because the number of input\\nvariables decreases from the total number of entries to the number of bins.\\nThis leads usually to simpler and faster numerical implementations,\\nin particular when unbinned fits become unpractical in cases of very large number of entries.\\nAnyway, for limited number of entries, a fraction of the information is lost when\\nmoving from an unbinned to a binned sample and a possible loss of precision may occur.\\nThe maximum value of the likelihood function obtained from an umbinned maximum likelihood fit doesn't in general\\nprovide information about the quality ({\\\\it goodness}) of the fit.\\nInstead, the minimum value of the $\\\\chi^2$ in a fit with a Gaussian underlying model\\nis distributed according to a known PDF given by:\\n\\\\begin{equation}\\nP(\\\\chi^2;n) =\\\\frac{2^{-{n}/{2}}}{\\\\Gamma\\\\left({n}/{2}\\\\right)}\\n\\\\chi^{n-2}e^{-\\\\frac{\\\\chi^2}{2}}\\\\,,\\n\\\\end{equation}\\nwhere $n$ is the {\\\\it number of degrees of freedom}, equal to the number of\\nbins minus the number of fit parameters.\\nThe cumulative distribution (Eq.~(\\\\ref{eq:cumulative})) of $P(\\\\chi^2; n)$ follows a uniform distribution between from 0 to 1,\\nand it is an example of {\\\\it p-value} (See Sec.~\\\\ref{sec:HypTest}).\\nIf the true PDF model deviates from the assumed distribution, the distribution of the $p$-value will be more peaked around zero\\ninstead of being uniformly distributed.\\nIt's important to note that $p$-values are not the ``probability of the fit hypothesis'',\\nbecause that would be a Bayesian probability, with a completely different meaning, and should be evaluated\\nin a different way.\\nIn case of a Poissonian distribution of the number of bin entries that may deviate from the Gaussian approximation,\\nbecause of small number of entries,\\na better alternative to the Gaussian-inspired Neyman's or Pearson's $\\\\chi^2$ has been proposed\\nby Baker and Cousins~\\\\cite{baker_cousins} using the following likelihood ratio\\nas alternative to Eq.~(\\\\ref{eq:PoisBinLik}):\\n\\\\begin{eqnarray}\\n\\\\chi^2_{\\\\lambda} & = & -2\\\\ln\\\\prod_i\\\\frac{L(n_i;\\\\mu_i)}{L(n_i;n_i)} = -2\\\\ln\\\\prod_i\\\\frac{e^{-\\\\mu_i}\\\\mu_i^{n_i}}{n_i!}\\n\\\\frac{n_i!}{e^{-{n_i}}n_i^{n_i}} \\\\\\\\\\n& = & 2\\\\sum_i\\\\left[\\n\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)- n_i + n_i\\\\ln\\\\left(\\n\\\\frac{n_i}{\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\n\\\\right) \\\\right]\\\\,.\\n\\\\end{eqnarray}\\nEquation~(\\\\ref{eq:BakCous}) gives the same minimum value as the Poisson likelihood function,\\nsince a constant term has been added to the log-likelihood function in Eq.~(\\\\ref{eq:PoisBinLik}),\\nbut in addition it provides goodness-of-fit information, since it asymptotically obeys a $\\\\chi^2$\\ndistribution with $n - m$ degrees of freedom. This is due to Wilks' theorem, discussed\\nin Sec.~\\\\ref{sec:profLik}.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The look-elsewhere effect}\\nWhen searching for a signal peak on top of a background that is smoothly distributed over a wide range,\\none can either know the position of the peak or not.\\nOne example in which the peak position is known is the\\nsearch for a rare decay of a known particle, like $\\\\mathrm{B}_{\\\\mathrm{s}}\\\\rightarrow\\\\mu^+\\\\mu^-$.\\nA case when the position was not know was the search for the Higgs boson, whose mass is not\\nprediceted by theory.\\nIn a case like the decay of a particle of known mass, it's easy to compute the peak significance:\\nfrom the distribution of the test statistic $f(q)$ computed assuming $\\\\mu=0$ (background only),\\ngiven the observed value of the test statistic $q^{\\\\mathrm{obs}}$, a $p$-value can be determined and then\\ntranslated into a significance level:\\n\\\\begin{equation}\\np = \\\\int_{q^{\\\\mathrm{obs}}}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q,\\\\quad Z = \\\\Phi^{-1}(1-p)\\\\,.\\n\\\\end{equation}\\nIn case, instead, the search is performed without knowing the position of the peak,\\nEq.~(\\\\ref{eq:localPval}) gives only a {\\\\it local $p$-value}, which means it reflects\\nthe probability that a background fluctuation {\\\\it at a given mass value $m$} gives a value\\nof the test statistic greater than the observed one:\\n\\\\begin{equation}\\np(m) = \\\\int_{q^{\\\\mathrm{obs}}(m)}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q\\\\,.\\n\\\\end{equation}\\nThe {\\\\it global $p$-value}, instead, should quantify the probability that a background\\nfluctuation {\\\\it at any mass value} gives a value of the test statistic greater than the\\nobserved one.\\nThe chance that an overfluctuation occurs for {\\\\it at least} one mass value increases with\\nthe size of the search range, and the magnitude of the effect depends on the resolution.\\nOne possibility to evaluate a global $p$-value is to let also $m$ fluctuate in the test statistic:\\n\\\\begin{equation}\\n\\\\hat{q} = -2\\\\ln \\\\frac{L(\\\\mu=0)}{L(\\\\hat{\\\\mu};\\\\hat{m})}\\\\,.\\n\\\\end{equation}\\nNote that in the numerator $L$ doesn't depend on $m$ for $\\\\mu=0$. This is a case\\nwhere Wilks' theorem doesn't apply, and no simple asymptotic approximations exist.\\nThe global $p$-value can be computed, in principle, as follows:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = \\\\int_{\\\\hat{q}^{\\\\mathrm{obs}}}^{+\\\\infty}f(\\\\hat{q}|\\\\mu=0)\\\\,\\\\mathrm{d}\\\\hat{q}_0\\\\,.\\n\\\\end{equation}\\nThe effect in practice can be evaluated with brute-force toy Monte Carlo:\\n\\\\begin{itemize}\\n\\\\item Produce a large number of pseudoexperiments simulating background-only samples.\\n\\\\item Find the maximum $\\\\hat{q}$ of the test statistic $q$ in the entire search range.\\n\\\\item Determine the distribution of $\\\\hat{q}$.\\n\\\\item Compute the global $p$-value as probability to have a value of $\\\\hat{q}$ greater than the observed one.\\n\\\\end{itemize}\\nThis procedure usually requires very large toy Monte Carlo samples in order to treat a discovery case:\\na $p$-value close to $3\\\\times 10^{−7}$ ($5\\\\sigma$ level) requires\\na sample significantly larger than $\\\\sim 10^7$ entries in order to\\ndetermine the $p$-value with small uncertainty.\\nAn asymptotic approximation for the global $p$-value is given by\\nthe following inequation~\\\\cite{lee_trial}~\\\\footnote{\\nIn case of a test statistic for discovery $q_0$ (Eq.~(\\\\ref{eq:tsfd})), the term $P(\\\\chi^2 > u)$\\nin Eq.~(\\\\ref{eq:leeasin}) achieves an extra factor 1/2, which is usually not be present\\nfor other test statistics.\\n}:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = P(\\\\hat{q}>u ) \\\\le \\\\left<N_u\\\\right> +\\nP(\\\\chi^2>u)\\\\,,\\n\\\\end{equation}\\nwhere $P(\\\\chi^2>u)$ is a standard $\\\\chi^2$ probability and\\n$\\\\left<N_u\\\\right>$ is the average number of {\\\\it upcrossings} of\\nthe test statistic, i.e.: the average number of times that\\nthe curve $q(m)$ crosses a given horizontal line at a level $u$ with a positive derivative,\\nas illustrated in Fig.~\\\\ref{fig:lee}.\\nThe number of upcrossings may be very small for some values of $u$,\\nbut an approximate scaling law exists and allows to perform the computation\\nat a more convenient level $u_0$:\\n\\\\begin{equation}\\n\\\\left<N_u\\\\right> = \\\\left<N_{u_0}\\\\right> e^{-{(u-u_0)}/{2}}\\\\,.\\n\\\\end{equation}\\nSo, $ \\\\left<N_{u_0}\\\\right>$ can be more conveniently evaluated using\\na reasonable number of toy Monte Carlo generations, then it can be extrapolated following\\nthe exponential scaling law.\\nNumerical comparisons of this approach with the full toy Monte Carlo\\nshow that good agreement is achieved for sufficiently\\nlarge number of observations.\\nIn case more parameters are estimated from data, e.g.: when searching for\\na new resonance whose mass and width are both unknown, the look-elsewere\\neffect can be addressed with an extension of the approach described above,\\nas detailed in Ref.~\\\\cite{leeND}.\\n\\\\addcontentsline{toc}{chapter}{Bibliography}\\n\\\\bibliographystyle{ieeetr}\\n\\\\bibliography{lista}\\n\\\\end{documen\", \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Upper Limits for multi-channel experiments}\\nIn the multi-channel case, the likelihood associated with the signal strength \\\\( \\\\mu \\\\) for the complete observation is determined by the joint likelihood of all channels~\\\\cite{wang2023recent}:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\prod_{i}^{Channels} \\\\mathcal{L}_{i}(\\\\mu).\\n\\\\end{equation}\\nWhere it is assumed that the information in each channel is independently and identically distributed. The definitions of the estimators \\\\( \\\\mathcal{Q}(\\\\mu) \\\\) and \\\\( q_{\\\\mu} \\\\) for a single channel naturally extend to the multi-channel case using the properties of logarithms. Thus, the statistical estimators are fully defined as follows, respectively:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = \\\\sum_{i}^{Channels} \\\\mathcal{Q}_{i}(\\\\mu). \\n\\\\end{equation}\\n\\\\begin{equation}\\nq_{\\\\mu} = \\\\sum_{i}^{Channels} q_{\\\\mu,i}. \\n\\\\end{equation}\\nAs an example, synthetic data associated with a resonance near the measured mass of the Higgs boson \\\\( m_{H} = 125 \\\\ \\\\text{GeV} \\\\) were simulated, with an exponential background component characteristic of the invariant mass of a diphoton system. A total of 30 channels were simulated to numerically illustrate the discovery reported by the CMS collaboration in 2012~\\\\cite{cms2012observation}. Figure~[\\\\ref{fig:15}] shows the resonance data with a mass similar to the measured Higgs boson mass under an exponential background component (blue shaded area). Additionally, the alternative signal + background hypothesis is shown, which could be consistent with the observation.\\nAs in the 2012 search, several signal models with masses ranging from 100 to 160 GeV in steps of 6 GeV will be assumed. For each signal point, the expected upper limit (\\\\( n = b \\\\)), representing a measurement compatible with the background-only hypothesis, and the observed upper limit using the synthetic data will be calculated. Given the data observation, the upper limits allow for excluding the Higgs model at a specific mass or rejecting the background-only hypothesis in favor of the model with this new particle. Typically, this discrepancy is indicated by the difference between the expected and observed upper limits; when this difference is large, it must be quantified in terms of the \\\\( 5\\\\sigma \\\\) criterion to report a discovery~\\\\cite{lista2016practical}.\\nFigure~[\\\\ref{fig:16}] shows the upper limit values as a function of the hypothetical particle mass using the estimator \\\\( \\\\mathcal{Q} \\\\). The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are calculated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{\\\\mathcal{Q}(\\\\mu_{up})} \\\\)~\\\\cite{conway2005calculation}. Note that low- and high-mass models are excluded at a \\\\( 95\\\\ \\nOn the other hand, Figure~[\\\\ref{fig:17}] shows the upper limit values as a function of the hypothetical particle mass using the \\\\( q_{\\\\mu} \\\\) estimator. In the generation of each random experiment, \\\\( \\\\hat{\\\\mu} \\\\) is found using the \\\\texttt{Scipy.optimize} package. The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are estimated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{q_{\\\\mu_{up}}} \\\\). Note the consistency of the results using both estimators. The primary difference lies in the approach to incorporating systematic uncertainties in the estimation of upper limits, significance, and \\\\( 5\\\\sigma \\\\) tension, which is why the profile likelihood is currently used by the CMS and ATLAS collaborations for these estimations~\\\\cite{cms2012observation, atlas2012observation}. Finally, to estimate the discrepancy between the observation and the expected number of events, the p-value of the observation is calculated under the assumption that the background-only hypothesis is correct.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0,\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n \\\\) is the number of observed events.\\n\\\\begin{equation}\\np_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:18}] shows the local p-value as a function of the particle mass. The dashed lines indicate the \\\\( 3\\\\sigma \\\\) evidence region and the \\\\( 5\\\\sigma \\\\) discovery region. This graph illustrates the statistical behavior of the p-value in the search for the Higgs boson in 2012~\\\\cite{cms2012observation}.\\n\", '\\\\section{Experimental sensitivity using the $q_{\\\\mu}$ estimator}\\nThe statistical significance is calculated using a strategy similar to that illustrated for the estimator \\\\( \\\\mathcal{Q} \\\\). First, the value \\\\( q_{0, \\\\text{obs}} \\\\) is obtained using the statistical test:\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n = b + s \\\\) for signal sensitivity. The significance is obtained by finding the p-value of the possible observation \\\\( n \\\\) using the background-only distribution \\\\( f(q_{0} / 0) \\\\):\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nIn the asymptotic limit, the significance can be approximated as \\\\( Z_{0} \\\\approx \\\\sqrt{q_{0}} \\\\)~\\\\cite{lista2016practical, cowan2014statistics}. As mentioned previously, the signal region is obtained by finding the event window that maximizes the statistical significance, Equation~(\\\\ref{eq:maxsig}). In the single-channel case, a statistical significance of \\\\( \\\\alpha = 0.174 \\\\) is obtained, which translates to \\\\( Z_{0} = 0.94 \\\\) standard deviations, consistent with the significance calculation using the estimator \\\\( \\\\mathcal{Q} \\\\). Figure~[\\\\ref{fig:14}] shows the background-only distribution and the observed \\\\( q_{0, \\\\text{obs}} \\\\) value in the Asimov data. The area under the distribution for positive values of \\\\( q_{0, \\\\text{obs}} \\\\) represents the statistical significance of the expected new physics events~\\\\cite{cranmer2015practical}.\\n', \"\\\\section{Discoveries and upper limits}\\n\\nThe process towards a discovery, from the point of view of data analysis,\\nproceeds starting with a test of our data sample against two hypotheses concerning the theoretical underlying model:\\n\\\\begin{itemize}\\n\\\\item $H_0$: the data are described by a model that contains background only;\\n\\\\item $H_1$: the data are described by a model that contains a new signal plus background.\\n\\\\end{itemize}\\nThe discrimination between the two hypotheses can be based on a test statistic $\\\\lambda$ whose distribution\\nis known under the two considered hypotheses.\\nWe may assume that $\\\\lambda$ tends to have (conventionally) large values if $H_1$ is true and small values if $H_0$ is true.\\nThis convention is consistent with using as test statistic the likelihood ratio $\\\\lambda =L(x|H_1)/L(x|H_0)$,\\nas in the Neyman--Pearson lemma (Eq.~(\\\\ref{eq:neymanPearsonLemma})).\\nUnder the frequentist approach, it's possible to compute a $p$-value equal to the probability that\\n$\\\\lambda$ is greater or equal to than the value $\\\\lambda^{\\\\mathrm{obs}}$ observed in data.\\nSuch $p$-value is usually converted into an equivalent probability computed as the area\\nunder the rightmost tail of a standard normal distribution:\\n\\\\begin{equation}\\np = \\\\int_Z^{+\\\\infty} \\\\frac{1}{\\\\sqrt{2\\\\pi}}e^{-{x^2}/{2}}\\\\,\\\\mathrm{d}x = 1 - \\\\Phi(Z)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\Phi$ is the cumulative (Eq.~(\\\\ref{eq:cumulative})) of a standard normal distribution.\\n$Z$ in Eq.~(\\\\ref{eq:significance}) is called {\\\\it significance level}.\\nIn literature conventionally a signal with a significance of at least 3 ($3\\\\sigma$ {\\\\it level})\\nis claimed as {\\\\it evidence}. It corresponds to a $p$-value of \\n$1.35\\\\times 10^{-3}$ or less. If the significance exceeds 5 ($5\\\\sigma$ {\\\\it level}), i.e.: the\\n$p$-value is below $2.9\\\\times10^{-7}$, one is allowed to claim the {\\\\it observation} of the new signal.\\nIt's worth noting that the probability that background produces a large test statistic is not equal to\\nthe probability of the null hypothesis (background only), which has only a Bayesian sense.\\nFinding a large significance level, anyway, is only part of the discovery process in the\\ncontext of the scientific method. Below a sentence is reported from a recent statement of the\\nAmerican Statistical Association:\\n\\\\begin{displayquote}\\n{\\\\it The p-value was never intended to be a substitute for scientific reasoning. Well-reasoned statistical arguments contain much more than the value of a single number and whether that number exceeds an arbitrary threshold. The ASA statement is intended to steer research into a `post p < 0.05 era'}~\\\\cite{pvalASA}.\\n\\\\end{displayquote}\\nThis was also remarked by the physicists community, for instance by Cowan {\\\\it et al.}:\\n\\\\begin{displayquote}\\n\\\\textit{It should be emphasized that in an actual scientific context, rejecting the background-only hypothesis in a statistical sense is only part of discovering a new phenomenon. One's \\\\textbf{degree of belief} that a new process is present will depend in general on other factors as well, such as the plausibility of the new signal hypothesis and the degree to which it can describe the data}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Non-profile frequentist estimator}\\nThe modified frequentist method is generalized through the Neyman-Pearson Lemma, a fundamental statistical result stating that the most powerful test for hypothesis comparison is based on minimizing the Type II error. This error occurs when the null hypothesis (\\\\( H_0 \\\\)), which assumes only background, is not rejected despite being false. The Neyman-Pearson Lemma indicates that, given a significance level, the most efficient statistical test to discriminate between two hypotheses is based on the likelihood ratio~\\\\cite{cranmer2015practical,lista2016practical,cowan2011asymptotic}.\\n\\\\begin{equation}\\nR(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)}.\\n\\\\end{equation}\\nThe formula has an asymptotic approximation to a \\\\(\\\\chi^{2}\\\\) distribution when expressed in terms of logarithms~\\\\cite{lista2016practical}. Generally, this formula is expressed as follows:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = - 2 Ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)} \\\\bigg). \\n\\\\end{equation}\\nThis expression is known as the log-likelihood ratio, and it allows for generalization to experiments with multiple channels. Consider, for example, a single-channel experiment measuring the mass of a hypothetical particle \\\\( m(\\\\rho) \\\\) within the range of 100 to 200 GeV. Suppose the observation is \\\\( n = 105 \\\\), the expected number of background events is \\\\( b = 100 \\\\), and the model for this new particle predicts \\\\( s = 10 \\\\). The statistical estimator based on this distribution model is expressed as follows:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & -2Ln \\\\bigg( \\\\frac{ e^{-(\\\\mu s + b)}(\\\\mu s + b)^{n} }{ e^{-b}b^{n} } \\\\bigg) {} \\\\nonumber \\\\\\\\\\n& = & 2 \\\\bigg( \\\\mu s - n Ln \\\\bigg( 1 + \\\\frac{\\\\mu s} {b} \\\\bigg) \\\\bigg). {}\\n\\\\end{eqnarray}\\nFigure~[\\\\ref{fig:9}] shows the single-channel experiment, where the error bars represent the Poisson uncertainty, \\\\( \\\\epsilon = \\\\sqrt{n} \\\\), associated with the observed number of events. Additionally, it describes the behavior of the estimator as a function of the signal strength, \\\\( \\\\mu \\\\). This reveals that the likelihood ratio defines a convex optimization problem with a unique global minimum, corresponding to the best fit of the model under the null hypothesis to describe the observation. Notably, both hypotheses can fit the observed data, making it essential to determine the set of theories that can be excluded based on the measurement of \\\\( n \\\\) or to assess whether there is sufficient evidence to claim the discovery of the particle in question~\\\\cite{cms2012observation,atlas2022detailed}.\\nThe best-fit value is obtained by differentiating the log-likelihood function with respect to the parameter of interest and evaluating the result at zero. In this case, the minimum can be calculated exactly using elementary methods.\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{n-b}{s} = 0.5\\n\\\\end{equation}\\nEstimating the best model is fundamental in current statistical estimators. In general, obtaining the best fit in experiments with multiple channels and systematic uncertainties requires advanced optimization processes and sampling techniques, which will be described later. Moreover, calculating upper limits involves sampling the distributions of the estimator under both the null and alternative hypotheses. The next section will address the sampling of the estimator and the definition of the confidence level for the signal, known as \\\\( CL_s \\\\).\\n\\\\subsubsection{Sampling of the log-likelihood estimator}\\nTo obtain the upper limit using the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), the estimator can be sampled under both the null and alternative hypotheses; these distributions are labeled \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), respectively. To calculate \\\\( f(\\\\mathcal{Q}|0) \\\\), a random number is generated following a Poisson distribution with \\\\( \\\\mu = 0 \\\\), representing the number of observed events under the null hypothesis. Similarly, the distribution \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\) is obtained using a specific value of \\\\( \\\\mu \\\\)~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:10}] shows a schematic of the shapes of the distributions of the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\). The value \\\\( Q_{obs} \\\\) corresponds to the estimator for the observed number of events \\\\( n \\\\). Typically, the background-only distribution is found to the right of \\\\( Q_{obs} \\\\), while the signal + background distribution is found to the left of \\\\( Q_{obs} \\\\). The degree of agreement between the observation and the models is evaluated through the confidence level, represented by the shaded areas in the plot~\\\\cite{cowan2014statistics}.\\nThe green shaded area represents the p-value of the observation under the background-only hypothesis (\\\\( H_{0} \\\\)) and is expressed as:\\n\\\\begin{equation}\\np_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q},\\n\\\\end{equation}\\nThe p-value of the null hypothesis is related to the well-known power of the test, which is the confidence level, denoted as \\\\( \\\\beta \\\\):\\n\\\\begin{equation}\\n\\\\beta = CL_{b} = 1 - p_{0} = 1 - \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nOn the other hand, the p-value for the signal + background hypothesis (\\\\( H_{1} \\\\)) is represented by the yellow shaded area, which directly corresponds to the confidence level of the \\\\( H_{1} \\\\) hypothesis:\\n\\\\begin{equation}\\np_{\\\\mu} = CL_{s+b} = \\\\int_{\\\\mathcal{Q}_{observed}}^{\\\\infty} f(\\\\mathcal{Q}/\\\\mu) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThus, the statistical significance \\\\( \\\\alpha \\\\) is a particular case of the p-value of the observation under the null hypothesis (\\\\( p_{0} \\\\)) and constitutes evidence in favor of \\\\( H_{1} \\\\) against \\\\( H_{0} \\\\). Consequently, maximizing the significance is a tool for optimizing the search window. In various statistical studies, it is suggested that stronger evidence in favor of \\\\( H_{1} \\\\) over \\\\( H_{0} \\\\) is reflected in~\\\\cite{lista2016practical,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{p_{\\\\mu}}{\\\\beta} = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}. \\n\\\\end{equation}\\nThis completely defines the confidence level of the signal. Using these definitions, the upper limit of the signal strength is obtained through the following strategy: 1) vary the signal strength \\\\( \\\\mu \\\\), 2) sample the distributions \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), 3) calculate the p-values corresponding to the observed estimator, and 4) determine the confidence level \\\\( CL_s(\\\\mu) \\\\). In this way, the upper limit \\\\( \\\\mu^{up} \\\\) for exclusion is given by:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = 0.05\\n\\\\end{equation}\\nThis process is typically computationally expensive due to the sampling of distributions for each value of \\\\( \\\\mu \\\\). In the case of multiple channels and nuisance parameter estimation, parallelization is required to obtain results efficiently. This is crucial, as optimizing the search window at the phenomenological level necessitates maximizing statistical significance or other statistical metrics~\\\\cite{florez2016probing,allahverdi2016distinguishing}.\\nIn the experiment related to the invariant mass channel \\\\( m(\\\\rho) \\\\), we have two key estimates. The first is the expected upper limit, which is assumed under the hypothesis that the observation consists of background nuisance only. In this case, the expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.19 \\\\). This implies that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Exp}} \\\\cdot s = 2.19 \\\\times 10 = 21.9 \\\\) events would be excluded, provided that \\\\( n = b \\\\) events are measured. In the second case, the observed upper limit corresponds to the actual data observation. This observed upper limit is estimated as \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.57 \\\\), meaning that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Obs}} \\\\cdot s = 2.57 \\\\times 10 = 25.7 \\\\) events would be excluded based on the observation. Figure~[\\\\ref{fig:11}] shows the search for the confidence level for both the expected and observed limits~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LEP/UpperLimitLnQ.ipynb}{Source code}}. Additionally, the distribution of the statistical estimator for \\\\( \\\\mu = \\\\mu_{up}^{\\\\text{Obs}} \\\\) is presented as an illustrative example. Finally, results obtained using the professional \\\\texttt{RooFit} package, used by the CMS and ATLAS collaborations for upper limit estimation, are included~\\\\cite{verkerke2006roofit,schott2012roostats}.\\nAn important feature in this type of analysis is that when the upper limits are statistically consistent, as in this case, there is insufficient evidence to claim a discovery. In this scenario, we would accept that the null hypothesis \\\\( H_{0} \\\\) (background-only hypothesis) adequately describes the observation. The first sign of tension between the observed data and the background-only hypothesis arises when the expected and observed upper limits differ significantly. From a statistical perspective, this discrepancy must be evaluated, and to consider the observation of a new phenomenon, the difference must exceed the \\\\( 5\\\\sigma \\\\) threshold~\\\\cite{lista2016practical,cranmer2015practical,cms2012observation}.', \"\\\\section{Upper Limits using the profile binned likelihood}\\nThe profile binned likelihood method is chosen by particle physics collaborations to present phenomenology studies and experimental analyses. As shown above, this method is based on estimating the signal confidence level through a fully frequentist approach focused on parameter estimation~\\\\cite{lista2016practical,barlow2002systematic}. To include systematic effects, the likelihood function is extended with appropriate distribution functions to describe efficiency effects with a given width \\\\( \\\\sigma \\\\). This methodology requires maximizing the likelihood function and obtaining the background profile as a function of the signal strength \\\\( \\\\mu \\\\). The improvement lies in replacing the normalization of the posterior distribution with a multivariable optimization problem, which is computationally less costly and leads to limits that allow for better model exclusion. The statistical estimator is given by~\\\\cite{conway2005calculation,cms2012observation,atlas2012observation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = - 2 ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu, \\\\hat{\\\\hat{b}}(\\\\mu) )}{ \\\\mathcal{L}(\\\\hat{\\\\mu},\\\\hat{b}) } \\\\bigg).\\n\\\\end{equation}\\nwhere \\\\( \\\\hat{\\\\mu} \\\\) and \\\\( \\\\hat{b} \\\\) are the unconditional maximum likelihood estimators, and \\\\( \\\\hat{\\\\hat{b}}(\\\\mu) \\\\) is the conditional maximum likelihood estimator. Frequentist limits are generally less restrictive and allow for exploring regions of significance while adequately accounting for systematic effects. Similar to the Bayesian approach, maximizing the likelihood as a function of \\\\( \\\\mu \\\\) enables finding the profile likelihood that propagates the effect of the background parameters \\\\( \\\\epsilon \\\\).\\nSystematic effects lead to higher upper limits, which restrict the model exclusion power and experimental sensitivity. For example, in a single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), the \\\\texttt{optimize} package is used to find the best-fit parameters, and Monte Carlo methods are applied to sample the estimator \\\\( q_{\\\\mu} \\\\). The extended likelihood function is given by:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu, \\\\epsilon) = \\\\frac{ e^{ -(\\\\mu s + \\\\epsilon b) } (\\\\mu s + \\\\epsilon b)^{n} }{n!} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^{2}} } e^{ -\\\\frac{(1-\\\\epsilon)^{2}}{2\\\\sigma^{2}} },\\n\\\\end{equation}\\nwhere maximizing the likelihood function for the efficiency \\\\( \\\\epsilon \\\\) yields the conditional nuisance estimator \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\):\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) = \\\\frac{1}{2b} \\\\bigg[ ( b -\\\\mu s - \\\\sigma^{2}b^{2}) + \\\\sqrt{ (b + \\\\mu s - \\\\sigma^{2}b^{2})^{2} + 4 b^{2} \\\\sigma^{2}n} \\\\bigg]. \\n\\\\end{equation}\\nFor the single-channel experiment, Figure~[\\\\ref{fig:22}] shows the profile of the nuisance parameter for various values of \\\\( \\\\mu \\\\). The maximum value of the likelihood function shifts depending on the signal strength \\\\( \\\\mu \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/ProfileLikelihood/ProfileLikelihoodNuissance.ipynb}{Source code}}. The right plot shows the maximum likelihood estimate \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\) using both Equation~(\\\\ref{eq:profile}) and the optimization package. This result indicates the maximal behavior of the uncertainty for each value of the signal strength \\\\( \\\\mu \\\\), ensuring model exclusion as restrictive as allowed by the uncertainty \\\\( \\\\sigma \\\\). More generally, the multi-channel case requires a fully numerical procedure to find the profiles of the nuisance parameters and \\\\( q_{\\\\mu} \\\\)~\\\\cite{lista2016practical,cranmer2015practical}.\\nTo illustrate the behavior of the estimator \\\\( q_{\\\\mu} \\\\) as a function of the systematic uncertainty \\\\( \\\\sigma \\\\), a sweep over the signal strength \\\\( \\\\mu \\\\) is performed while optimizing the estimator for the current values of the observation, background component, and signal events. Figure~[\\\\ref{fig:23}] shows the profile binned likelihood for the single-channel experiment as a function of the width of the systematic uncertainty. A larger uncertainty leads to a higher upper limit, which can be quantified using Wald's approximation \\\\( Z(3\\\\sigma) = \\\\sqrt{q_{\\\\mu}} \\\\)~\\\\cite{cowan2011asymptotic}. \\nThe green dashed line represents the observed upper limit for each profile likelihood. For example, for \\\\( \\\\sigma = 0.20 \\\\), the observed upper limit is \\\\( \\\\mu_{up} \\\\approx 4.5 \\\\), which contrasts with the value obtained from the Bayesian approach (Table~[\\\\ref{tb:3}], \\\\( \\\\mu_{up} = 4.91 \\\\)), a smaller value. The right plot represents the search for the p-value while fixing the value of the systematic uncertainty. Obtaining the pseudo-data requires generating an observation consistent with the background-only hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\), and for the signal + background hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\mu s + \\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\) in each observable channel. For each generated value of \\\\( n \\\\) across all channels, the optimization of \\\\( (\\\\hat{\\\\mu}, \\\\hat{\\\\epsilon}, \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu)) \\\\) is performed.\\nTable~[\\\\ref{tb:4}] shows the upper limit values using the grouped profile likelihood method for several values of \\\\( \\\\sigma \\\\). In comparison with the upper limits obtained by the Bayesian method, consistency is observed within the statistical confidence levels inherent to the sampling.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\n$\\\\sigma$ & Profile likelihood Ratio & MCMC algorithm \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{2}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 \\\\\\\\\\n0.10 & 3.52 & 3.31 \\\\\\\\\\n0.20 & 4.57 & 4.66 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nFor the multi-channel case, results are very close between the upper limits without uncertainty and those found using the profile likelihood method. This behavior is attributed to the combined uncertainty of the background across the 30 channels, which does not significantly affect the confidence in the signal strength, especially in the resonance region~\\\\cite{cms2022portrait,atlas2012observation,cowan2011asymptotic}. Table~[\\\\ref{tb:5}] shows the upper limit values for several mass points \\\\( m(H) \\\\) and uncertainty \\\\( \\\\sigma \\\\).\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\nMass($H$)[GeV] & $\\\\sigma$ & Method: Profile likelihood ratio \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & $\\\\mu_{up}(95\\\\ 110 & 0.1 & 0.43 \\\\\\\\\\n110 & 0.2 & 0.45 \\\\\\\\\\n\\\\hline\\n124 & 0.1 & 1.45 \\\\\\\\\\n124 & 0.2 & 1.46 \\\\\\\\\\n\\\\hline\\n142 & 0.1 & 0.29 \\\\\\\\\\n142 & 0.2 & 0.31 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength for different mass points using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\n\\\\subsection{Experimental sensitivity with systematic effects}\\nIn the case of determining the experimental sensitivity for a specific model $s$, the Asimov data $n = s + b$ are used, and a modification is applied to the statistical estimator $q_{\\\\mu}$~\\\\cite{lista2016practical}.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nIn the Wald approximation, the significance is approximately given by:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\sqrt{q_{0}}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:24}] shows the profile likelihood for various values of the systematic uncertainty $\\\\sigma$. It demonstrates how statistical significance is impacted by uncertainty. In general, greater uncertainty in the estimation of background events leads to a loss of sensitivity in a potential experimental analysis aiming to validate a new hypothetical model.\\nThere are alternative methods to establish statistical significance~\\\\cite{cowan2011asymptotic}, such as:\\n\\\\begin{equation}\\nZ_{0}(\\\\sigma) = \\\\frac{s}{\\\\sqrt{s+(1+\\\\sigma)b}}. \\n\\\\end{equation}\\nHowever, in general, experimental sensitivity is overestimated because the maximal information of $\\\\hat{\\\\hat{b}}$ is not fully captured in the profile likelihood. Table~[\\\\ref{tb:6}] shows the statistical significance for various values of systematic uncertainty $\\\\sigma$. This effect reduces experimental sensitivity and should be calculated using the profile binned likelihood method. Note that the approximation $Z_{0} = s / \\\\sqrt{s + b}$ is no longer valid due to the convolution effect of counting and efficiency distributions.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & $Z$ & $Z_{0}=s/\\\\sqrt{s+b}$ & $Z_{0}(\\\\sigma)$ \\\\\\\\\\n\\\\hline\\n0.05 & 0.878 & 0.932 & 0.931 \\\\\\\\\\n0.1 & 0.695 & 0.932 & 0.912 \\\\\\\\\\n0.2 & 0.443 & 0.932 & 0.877 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Statistical significance as a function of systematic uncertainty for the case of a single-channel experiment. Note how the approximation $Z_{0}$ becomes invalid even for $s \\\\ll b$.}\\n\\\\end{center}\\n\\\\end{table}\\nAs shown in Table~[\\\\ref{tb:6}], when there is a 20\\\\\\n\", \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\"}},\n",
       "       {'entity_name': 'acceptance region', 'entity_type': 'statistics_concept', 'description': 'A defined range of values for a test statistic in hypothesis testing, where if the test statistic falls within this region, the null hypothesis is accepted.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\"}},\n",
       "       {'entity_name': 'test size and power', 'entity_type': 'statistics_concept', 'description': 'Key concepts in hypothesis testing, where the size of the test refers to the probability of rejecting the null hypothesis when it is true (Type I error rate, denoted by alpha, α), and the power of the test indicates the probability of correctly rejecting a false null hypothesis when the alternative hypothesis is true (denoted as 1 - beta, β). These metrics are essential for evaluating the effectiveness of statistical tests.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote'}},\n",
       "       {'entity_name': 'type i and type ii errors', 'entity_type': 'statistics_concept', 'description': 'Type I error refers to the mistake of rejecting a true null hypothesis, leading to a false positive result, while Type II error involves failing to reject a false null hypothesis, resulting in a false negative outcome. The Type I error rate is the probability of incorrectly rejecting a true null hypothesis, denoted by alpha (α), and is a critical concept in hypothesis testing. Conversely, the Type II error rate represents the probability of not rejecting a false null hypothesis, which is influenced by the proximity of the null and alternative hypotheses.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote', \"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\", '\\\\section{Least squares for Goodness of Fit}\\n\\\\subsection{The chi-squared distribution}\\nIt turns out that, if we repeated our experiment a large number of times, and certain conditions are satisfied, \\nthen $S_{min}$ will follow a $\\\\chi^2$ distribution with $\\\\nu = n - p$ degrees of freedom, where $n$ is the \\nnumber of data points, $p$ is the number of free parameters in the fit, and $S_{min}$ is the value of $S$ \\nfor the best values of the free parameters. For example, a straight line with free intercept and gradient fitted \\nto 12 data points would have $\\\\nu = 10$.\\nThe conditions for this to be true include:\\n\\\\begin{itemize}\\n\\\\item{the theory is correct:}\\n\\\\item{the data are unbiassed and asymptotic;}\\n\\\\item{the $y_i$ are Gaussian distributed about their true values;}\\n\\\\item{the estimates for $\\\\sigma_i$ are correct;$\\\\ \\\\ \\\\ \\\\ \\\\ $ etc. }\\n\\\\end{itemize}\\nUseful properties to know about the mathematical $\\\\chi^2$ distribution are that their mean is $\\\\nu$ and their variance is\\n$2\\\\nu$. Thus if a global fit to a lot of data has $S_{min}$ = 2200 and there are 2000 degrees of freedom, we can \\nimmediately estimate that this is equivalent to a fluctuation of 3.2$\\\\sigma$.\\nMore useful than plots of $\\\\chi^2$ distributions are those of the fractional tail area beyond a particular value \\nof $\\\\chi^2$ (see figs. \\\\ref{fig:chi_squared} and \\\\ref{fig:chi_sq_tail} respectively).\\nThe $\\\\chi^2$ goodness of fit test consists of\\n\\\\begin{itemize}\\n\\\\item{For the given theoretical form, find the best values of its free parameters, and hence $S_{min}$;}\\n\\\\item{Determine $\\\\nu$ from $n$ and $p$; and}\\n\\\\item{Use $S_{min}$ and $\\\\nu$ to obtain the tail probability $p$ \\\\footnote{If the conditions for $S_{min}$ to follow a \\n$\\\\chi^2$ distribution are satisfied, this simply involves using the tail probability of a $\\\\chi^2$ distribution. \\nIn other cases, it may be necessary to use Monte Carlo simulation to obtain the distribution of $S_{min}$;\\nthis could be tedious. }.} \\n\\\\end{itemize}\\nThen $p$ is the probability that, if the theory is correct, by random fluctuations we would have obtained a value of $S_{min}$ at least as large as the observed one. If this probability is smaller than some pre-defined level $\\\\alpha$, we reject the hypothesis that the model provides a good description of the data.\\n\\\\subsection{When $\\\\nu \\\\ne n - p$}\\nIf we add an extra parameter into our theoretical description, even if it is not really needed, we expect the value of $S_{min}$ to decrease slightly. (This contrasts with including a parameter which is really relevant, which can result in a dramatic reduction in $S_{min}$.) In determining $p$-values, this is allowed for by the reduction of $\\\\nu$. On average,\\na parameter which is not needed reduces $S_{min}$ by 1. But consider the following examples.\\n\\\\subsubsection{Small oscillatory term}\\nImaging we are fitting a histogram of a variable $\\\\phi$ by a distribution of the form\\n\\\\begin{equation}\\n\\\\frac{dy}{d\\\\phi} = N[ 1 + 10^{-6} cos(\\\\phi -\\\\phi_0)],\\n\\\\end{equation}\\nwhere the two parameters are the normaisation $N$ and the phase $\\\\phi_0$. Because of the factor $10^{-6}$ in front\\nof the cosine term, $\\\\phi_0$ will have a miniscule effect on the prediction, and so including this as a parameter has negligible effect on $S_{min}$; $\\\\phi_0$ is effectively not a free parameter.\\n\\\\subsubsection{Neutrino oscillations} \\nFor a scenario of two oscillating neutrino flavours, the probability $P$ of a neutrino of energy $E$ to remain the same flavour after\\na flight length $L$ is\\n\\\\begin{equation}\\nP = 1 - A sin^2(\\\\delta m^2 L/E)\\n\\\\end{equation}\\nwhere the two parameters are $\\\\delta m^2$, the difference in the mass-squareds of the two neutrino flavours, and \\n$A = sin^2 2\\\\theta$ with $\\\\theta$ being the mixing angle. However, since for small angles $\\\\alpha,\\\\ sin\\\\alpha\\\\approx \\\\alpha$, for small $\\\\delta m^2L/E$ the probability $P$ of eqn \\\\ref{neutrino} is approximately $1 - A(\\\\delta m^2 L/E)^2$. Thus the two parameters occur only as the product $A (\\\\delta m^2)^2$, and cannot be determined separately. Thus in that regime we have effectively just a single parameter.\\n\\\\vspace{0.2in}\\nIn both the above examples, an enormous amount of data would enable us to distinguish the small effects produced by the second \\nparameter; hence the requirement for asymptotic conditions. \\n\\\\subsection{Errors of First and Second Kind}\\nIn deciding in a Goodness of Fit test whether or not to reject the null hypothesis $H_0$ (e.g. that the data points lie on a \\nstraight line), there are two sorts of mistake we might make:\\n\\\\begin{itemize}\\n\\\\item{Error of the First Kind. This is when we reject $H_0$ when it is in fact true. The fraction of cases in which this happens \\nshould equal $\\\\alpha$, the cut on the $p$-value. }\\n\\\\item{Error of the Second Kind. This is when we do not reject $H_0$, even though some other hypothesis is true. The rate at which this happens depends on how similar $H_0$ and the alternative hypothesis are, the relative frequencies of the two hypotheses being true, etc.}\\n\\\\end{itemize}\\nAs $\\\\alpha$ increases the rates of Errors of the First and Second kinds go up and down respectively. \\nThese Errors correspond to a loss of efficiency and to an increase of contamination respectively.\\n\\\\subsection{Other Goodness of Fit tests}\\nThe $\\\\chi^2$ method is by no means the only one for testing Goodness of Fit.\\nIndeed whole books have been written on the subject\\\\cite{DAgostino}. Here\\nwe mention just one other, the Kolmogorov-Smirnov method (K-S), which has the \\nadvantage of working with individual observations. It thus can be used with \\nfewer observations than are required for the binned histograms in the $\\\\chi^2$\\napproach. \\nA cumulative plot is produced of the fraction of events as a function of the variable\\nof interest $x$. An example is shown in Fig.~\\\\ref{fig:K_S}. \\nThis shows the fraction of data events with $x$ smaller than any particular \\nvalue. It is thus a stepped plot, with the fraction going from zero at the extreme left, \\nto unity on the right hand side. Also on the plot is a curve showing the expected cumulative\\nfraction for some theory. The K-S method makes use of the largest (as a function of $x$) vertical \\ndiscrepancy $d$ between the data plot and the theoretical curve. Assuming the theory is true\\nand given the number of observations $N$, the probability $p_{KS}$ of obtaining $d$ at least as large \\nas the observed value can be calculated. The beauty of the K-S method is that this probability\\nis independent of the details of the theory. As in the $\\\\chi^2$ approach, the K-S probability \\ngives a numerical way of checking the compatibility of theory and data. If $p_{KS}$ is small, we \\nare likely to reject the theory as being a good description of the data.\\nSome features of the K-S method are:\\n\\\\begin{itemize}\\n\\\\item{The main advantage is that it can use a small number of observations.}\\n\\\\item{The calculation of the K-S probability depends on there being no adjustable parameters in the theory.\\nIf there are, it will be necessary for you to determine the expected distribution for $d$, presumably \\nby Monte Carlo simulation.}\\n\\\\item{It does not extend naturally to data of more than one dimension, because of there being no unique\\nway of producing an ordering in several dimensions.}\\n\\\\item{It is not very sensitive to deviations in the tails of distributions, which is where searches for\\nnew physics are often concentrated e.g. high mass or transverse momentum. Fortunately variants of K-S exist, \\nwhich put more emphasis on discrepancies in the tails.}\\n\\\\item{Instead of comparing a data cumulative distribution with a theoretical curve, it can alternatively be\\ncompared with another distribution. This can be from a simulation of a theory, or with another data set. The\\nlatter could be to check that two data sets are compatible. \\nThe calculation of the K-S probability now requires the maximum discrepancy $d$, and the numbers of events \\n$N_1$ and $N_2$ in each of the two distributions being compared.}\\n\\\\end{itemize}\\n', '\\\\section{Upper Limits for one channel experiment}\\n\\nIn scientific research, experiments are designed to collect data, and theories or models are developed to explain those observations. In general, the falsification of theories is based on hypothesis testing. Hypothesis tests determine, with a given confidence level ($CL$), whether the observed data provide sufficient evidence to reject an initial hypothesis, called the null hypothesis ($H_{0}$), in favor of an alternative hypothesis ($H_{1}$). The null hypothesis ($H_{0}$) is considered true until observations indicate otherwise; in such a case, the initial explanation is rejected, and the new theory ($H_{1}$) is accepted~\\\\cite{sinervo2002signal}. Both the frequentist and Bayesian approaches applied here yield robust upper limit estimations, adaptable to various experimental setups, making them vital tools for model testing and exclusion.\\nIn high-energy physics (HEP), the null hypothesis ($H_{0}$) refers to all known physical processes, which are summarized in what is known as the Standard Model. The alternative hypothesis ($H_{1}$) represents potential models that could explain new observations that the accepted model cannot account for, such as supersymmetry, extra dimensions, among others~\\\\cite{cowan2011asymptotic,florez2016probing}.\\nAdditionally, hypothesis testing requires the selection of a confidence level in terms of statistical significance ($\\\\alpha$).\\n\\\\begin{equation}\\nCL = 1 - \\\\alpha.\\n\\\\end{equation}\\nWhere $\\\\alpha$ (type I error) is the probability of rejecting the null hypothesis when it is true. By convention, model exclusion in particle physics is done for a value of $\\\\alpha = 0.05$, which corresponds to a confidence level ($CL$) of $95\\\\\\n\\\\begin{equation}\\n\\\\alpha = \\\\int_{P}^{\\\\infty} \\\\frac{1}{2\\\\pi} e^{-x^{2}/2} dx.\\n\\\\end{equation}\\nWhere $P$ is the percentile of the distribution, for which the type I error is $\\\\alpha = 0.05$. Figure~[\\\\ref{fig:2}] shows the standard normal distribution; the shaded area represents the type I error for the percentile $P_{95} \\\\approx 1.645$, which corresponds to the model exclusion condition at the $3\\\\sigma$ level.\\nThe confidence level for an observation is significantly higher. In general, the discovery threshold is set at $5\\\\sigma$, where the type I error is $\\\\alpha = 2.86 \\\\times 10^{-7}$. Table~[\\\\ref{tb:1}] summarizes different confidence levels and their interpretation in high-energy physics (HEP)~\\\\cite{lista2016practical,cranmer2015practical,cowan2011asymptotic}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccl}\\n\\\\hline\\n$\\\\alpha$ & $CL [\\\\ \\\\hline\\n$0.1586$ & $84.13$ & $1\\\\sigma$ & $H_{1}$ no excluded \\\\\\\\\\n$0.05$ & $95.00$ & $1.645\\\\sigma$ & $H_{1}$ excluded (Model exclusion) \\\\\\\\\\n$0.0227$ & $97.72$ & $2\\\\sigma$ & $H_{1}$ excluded \\\\\\\\ \\n$1.349 \\\\times 10^{-3}$ & $99.86$ & $3\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$3.167 \\\\times 10^{-5}$ & $99.99$ & $4\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$2.8665 \\\\times 10^{-7}$ & $99.99997$ & $5\\\\sigma$ & $H_{0}$ excluded (Observation) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Significance at different observation points. The exclusion of the alternative hypothesis requires a result statistical consistent with the background-only hypothesis ($H_{0}$), while confirmation of the observation requires compatibility with the signal + background hypothesis ($H_{1}$).}\\n\\\\end{center}\\n\\\\end{table}\\n'}},\n",
       "       {'entity_name': 'neymanpearson lemma', 'entity_type': 'statistics_concept', 'description': 'A fundamental theorem in statistical hypothesis testing that provides a method for determining and constructing the most powerful tests for simple hypotheses at a given significance level. It is based on maximizing the likelihood ratio and minimizing Type II error, thereby optimizing the detection of signals in various applications, including particle physics.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Overdensity estimation}\\nIn order to train the most powerful ML-based classifier to discriminate signal from background, one would ideally train a network in a supervised manner with labeled data. This relies on a signal hypothesis that is chosen a-priori. An early attempt at discriminating background from \"everything else\" in order to obtain some degree of model-independence, was demonstrated in Ref.~\\\\citen{antiqcd}. Targeting searches for new physics in hadronic final states, a classifier was trained to discriminate QCD jets from various potential signal jets using Monte Carlo simulation. The disadvantage of such an approach is the dependence on signal simulation and which signals are to be included in the training.\\nAlthough simulated particle physics data is highly accurate over several orders of magnitude in length scale, simulation is known to not fully accurately reproduce collider data and this disagreement affects the tagging performance. Using weakly- or self-supervised (see Section~\\\\ref{sec:selfsupervised}) methods, algorithms can be trained directly on the data itself which has the added benefit of not having to derive transfer factors when training on synthetic data and testing on real data.\\n\\\\subsubsection{Weakly supervised methods}\\nIn weakly supervised learning, impure or noisy data sources can be used to label signal and background data in such a way that models can be trained in a supervised manner. Such methods can be utilized for anomaly detection when the signal is unknown, but there exist datasets where both signal and background are expected to be present in some relative fraction. This can be achieved by placing weak assumptions on the signal and background processes using domain knowledge.\\nThe goal of the weakly supervised methods we will discuss here, is to learn an approximation of the likelihood ratio $R(x)$ between the underlying probability densities of background $p_\\\\text{bg}(x)$ and data (possibly including signal) $p_\\\\text{data}(x)$, as a function of some input variables $x$:\\n\\\\begin{equation} \\nR(x)=\\\\frac{p_\\\\text{data}(x)}{p_\\\\text{bg}(x)}.\\n\\\\end{equation}\\nThis likelihood ratio, if it could be learned exactly, would be the most powerful model-agnostic anomaly detector, as given \\n\\\\begin{equation}\\np_\\\\text{data}(x)=(1-\\\\epsilon)p_\\\\text{bg}(x)+\\\\epsilon p_\\\\text{sig}(x),\\n\\\\end{equation}\\nwhere $p_\\\\text{sig}(x)$ is the probability density of signal, it would be monotonically related to the signal-to-background LR for any signal present in the data. A strategy for learning a good approximation of the likelihood ratio $R(x)$, is to train a classifier between data from a signal enriched region and samples drawn from a (fully data-driven) background model. If the background model is accurate and the classifier is well-trained, this approaches the likelihood ratio $R(x)$ by the Neyman-Pearson Lemma~\\\\cite{neyman1933ix}.\\nHence, the aim is to test whether the signal region data contains a combination of signal and background data. In the event that there are signal events present in the signal region, the classifier can differentiate between the signal region data and the background template. The true signal events are expected to have higher classification scores than the true background data. A cut on this classifier score can then be used to enhance the significance of signal events, making it a useful anomaly detection metric.\\nIn Ref.~\\\\citen{Dery2017WeaklySC} a method referred to as Learning from Label Proportions~\\\\cite{LLP} was utilized to discriminate between quarks and gluons using impure data samples. Despite not having access to the per-instance labels, the class proportions could be derived using domain knowledge. A supervised task was then defined using the class proportions themselves as the target, although operating the algorithm at a per-instance level. This concept has been extended in in the Classification WithOut Labels (CWola)~\\\\cite{cwola} framework. In this setup, the class proportions themselves do not need to be known, and it is enough to have two datasets at hand with an unequal fraction of signal instances in each set. A standard classifier can then be trained to discriminate between the two mixed datasets, and this can be shown to be the optimal classifier to discriminate between signal and background instances. The larger the difference in signal fraction between is dataset, the better the classifier becomes. The challenge is being able to design such mixed datasets, especially for a model-independent setup.\\nThe CWola strategy has been demonstrated and deployed for various model-independent search setups. In Ref.~\\\\citen{cwolabumphunt}, the authors introduce the \\\\textit{CWola bumphunt}. In this setup, one attempts to look for new, heavy generic particles that resonate around the particle mass in the dijet invariant mass spectrum. Starting from the weak assumption that this is a localized, narrow resonance, two mixed samples are created in the following way: The region in the dijet invariant mass close to the particle mass is defined as the signal-enriched mixed sample, and the regions next to it are defined as background-enriched regions. This is illustrated in Fig.~\\\\ref{fig:fig1}. In this way, the dijet invariant mass sideband regions serve as the background samples; these can serve as a good model for the background if the input features are statistically independent from the dijet invariant mass. If there is a signal present in the signal-like region around the particle mass, the classifier learns to identify it, while in the absence of a signal the classifier will likely learn random noise as there would be no difference between the two groups of events. It is crucial that the features being used for classification are not correlated with the dijet invariant mass. Otherwise, the classifier will be able to differentiate background events in the signal region from those in the adjacent dijet invariant mass regions used as the background-enriched mixed sample. Background events within the signal region will then be classified as signal-like, which can introduce artificial sculpting of the dijet invariant mass distribution. Note that the above strategy only works for narrow resonances, if there is a significant amount of signal in both datasets, as would be the case for a broad resonance, the classification performance is reduced. This method was used to analyze data collected by the ATLAS experiment in the search for generic new heavy resonances decaying into jets in Ref.~\\\\citen{ATLAS:2020iwa}, a first of its kind using weak supervision for model-agnostic searches. The power of this analysis can be seen in Figure~\\\\ref{fig:atlascwola}. This plot shows 95\\\\\\nThis methodology can also be applied in other setups than for a dijet bumphunt. In Ref.~\\\\citen{cwola_monojet}, model-agnostic learning using the CWola method is harnessed in order to improve the sensitivity of searches for new physics models with anomalous jet dynamics and a mono-jet signature. Focusing on cases where a heavy new particle decays into two jets which hadronize partially in the dark sector (making them \\\\textit{semi-visible jets}), and where one of the jets become completely invisible and the other partially visible, anomaly detection is utilized to detect the semi-visible anomalous jet. The degree of visibility of this jet can vary, making it difficult to train a supervised algorithm for each visibility fraction. Rather, CWola is deployed to train a generic anomalous jet identification classifier. The dominant background for a mono jet search is the electroweak production of vector bosons and jets, where the vector boson further decays to neutrinos, $Z(\\\\nu\\\\nu)$+jets. The experimental signature is missing transverse energy and a jet, mimicking the signal signature. Taking advantage of the fact that the vector boson also can decay visibly into two leptons and in these cases the jet remains the same, a background enriched control CWola sample can be defined using $Z(\\\\ell\\\\ell)$+jets events. None of the signal should be present in events with a di-lepton and jet signature. A model-independent anomalous jet tagger is then trained supervised to discriminate between jets coming from a $(\\\\ell\\\\ell)+jet$ and a $(\\\\nu\\\\nu)$+jet sample. If the monojet signature is present, CWola guarantees that the best algorithm trained to distinguish between these two regions, is also the best algorithm to discriminate between a normal SM jet from the V+jet background, and a semi-visible anomalous jet.\\nThis illustrates how generally CWola can be used. The only requirement is that one is able to define regions of the data depleted and enriched in signal, and that the signal and background events are statistically equal in the two regions. In terms of model independence, some degree of signal assumption is needed in order to define appropriate mixed samples.\\nMethods can also be used to bootstrap CWola and further improve the classification performance. In \\\\citen{Amram:2020ykb}, a powerful and model-independent anomalous jet tagger is defined starting from the CWola hunting methodology, but defining the mixed samples for training differently. Targeting signals where both jets in the event are anomalous, the key idea is that for a resonance decaying to a pair of anomalous jets, one can use an initial self- or supervised classifier (like an autoencoder, see Section~\\\\ref{sec:selfsupervised}) \\nto tag an event as signal-like or background-like using one jet and then use that information to construct samples for training a classifier using the other jet with weak supervision. By using an autoencoder as an initial classifier, one can group events into a signal-like and background-like sample based on the anomaly score on one of the jets (assuming that if the one jet is anomalous, the other must be too). A classifier can then be trained for the other jet that has not been tagged, where the mixed samples are defined based on the anomaly score of the tagged jet.\\nAnother weakly-supervised method for over-density detection is ANODE~\\\\cite{nachman2020anode}. In ANODE, conditional neural density estimation is used in order to interpolate probability densities from a data sideband into the data signal region. This interpolation is used and compared to the probability density of the actual data observed in the signal region, and used to construct a likelihood ratio as in Eq.~\\\\ref{eq:weak-supervision-likelihood-ratio}. This implies having to learn both the interpolated likelihood of the background in the signal region, as well as the likelihood for data in the signal region (see Fig.~\\\\ref{fig:fig1}). An improvement on this method is CATHODE (Classifying Anomalies Through Outer Density Estimation)~\\\\cite{Hallin:2021wme}. In CATHODE, rather than directly constructing the likelihood ratio, one rather samples events from the trained background estimator after it is interpolated into the signal region. This avoids having to learn the likelihood of data in the signal region. Then, a classifier is trained to discriminate data in the signal region from the data samples from the interpolated density estimator. This algorithm was first demonstrated for searches for heavy particles decaying into two jets. CATHODE proceeds by first training a conditional normalizing flow~\\\\cite{rezende2016variational} on the dijet invariant mass sidebands and then interpolating this into the signal region; samples from this flow are used as the background model and should correctly take into account any correlations between the input features and the dijet invariant mass.\\nNormalizing flows are a type of generative model that learn to transform a simple probability distribution (usually a standard Gaussian distribution) into a more complex distribution that resembles the target distribution of the data. This is achieved by defining a sequence of invertible transformations that map samples from the simple distribution to samples from the target distribution. The resulting model can be used to generate new data samples, perform density estimation, and compute likelihoods. Invertibility is important, as it ensures that the transformation has a well-defined inverse, which is needed for density estimation and likelihood computation. The key challenge in designing normalizing flows is to ensure that the resulting distribution is both complex enough to capture the target distribution and easy to work with, in the sense that likelihood computation and sampling are efficient. Recent work has focused on designing more expressive and flexible transformations, such as coupling layers, which allow for the transformation to depend on only a subset of the input variables~\\\\cite{dinh2017realnvp}. CATHODE utilizes such a normalizing flow to estimate the background density, conditioned on the dijet invariant mass. The density can then be interpolated into the dijet invariant mass signal region, while accounting for all correlations between the input features. Finally, a classifier is trained to distinguish between the artificially generated background samples from the normalizing flow (trained in data sidebands) and actual samples from the data signal region, yielding an estimate of the likelihood ratio as an anomaly metric (following the CWola paradigm).\\nA similar method is CURTAINS~\\\\cite{Raine:2022hht}. This method also takes advantage of a conditioned invertible neural network to learn the distribution of background events in a sideband and then use that to transform datapoints to those of the target distribution in the signal region. CURTAINs use an optimal transport loss to train the network to minimize the distance between the model output and the target data. The goal is to approximate the optimal transport function between two points in feature space when moving along the resonant spectrum. As a result, instead of generating new samples to create a background template, CURTAINs transforms the data in the side-bands to equivalent data points with a mass in the signal region. This approach eliminates the need to match data encodings to an intermediate prior distribution, which is the case of CATHODE, as it can lead to mismodelling of underlying correlations between the observables in the data if the trained posterior is not in perfect agreement with the prior distribution. Additionally, CURTAINs can also be employed to transform side-band data into validation regions, rather than simply constructing the background template in the signal region, making the algorithm easier to validate and test. Once the CURTAINs density estimation algorithm has been trained, a similar approach as in CATHODE is taken. Specifically, the transformed data (from sideband to signal region) is assumed to represent a sample of pure background events, while the signal region data represents a mixture of signal and background. A CWola classifier is trained to discriminate between the two datasets based on this assumption. \\nIn Ref.~\\\\citen{klein2022flows,curtains2}, an improvement of the CURTAINs technique is introduced, where a maximum likelihood estimation is used instead of an optimal transport loss. This improves the fidelity of the transformed data and is significantly faster and easier to train.\\nMore recently, diffusion models~\\\\cite{10.5555/3045118.3045358}, emerging as potent tools for high-dimensional density estimation, have been explored both for overdensity estimation~\\\\cite{sengupta2023improving} and for outlier detection~\\\\cite{mikuni2023highdimensional}.\\nThere are also weakly supervised methods that take advantage of simulation in the training of density estimators. In Simulation Assisted Likelihood-free Anomaly Detection (SALAD), a reweighting function for reweighting simulation to match data in the data sidebands is trained. This (parametrized) reweighting function is then interpolated into the signal region. Finally, a classifier to discriminate between the two is trained to get the likelihood ratio. Another simulation-assisted technique is Flow-enhanced transportation for anomaly detection (FETA)~\\\\cite{feta}, a mixture of SALAD and CURTAINS. A normalizing flow is trained in the sideband to map MC simulation to data. This learned flow is then applied to simulation in the signal region to obtain an approximation of the background.\\nThere are caveats when deploying weakly supervised methods.\\nAsymptotically, a weakly supervised classifier will converge to the performance of a fully supervised one. But in practice, performance typically degrades with smaller samples sizes available for training and lower fractions of signal events in the data sample. However, one can still obtain signal versus background classifiers with reasonable performance even with signal fractions well below 1\\\\\\n', \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Discovery as hypothesis tests} \\nLet us examine the statistical statement associated to the claim of discovery for new physics. Typically, new physics searches are looking for a signal that is additive on top of the background, though in some cases there are interference effects that need to be taken into account and one cannot really talk about 'signal' and 'background' in any meaningful way. Discovery is formulated in terms of a hypothesis test where the background-only hypothesis plays the role of the null hypothesis and the signal-plus-background hypothesis plays the roll of the alternative. Roughly speaking, the claim of discovery is a statement that the data are incompatible with the background-only hypothesis. Consider the simplest scenario where one is counting events in the signal region, $n_{\\\\rm SR}$ and expects $\\\\nu_B$ events from background and $\\\\nu_S$ events from the putative signal. Then we have the following hypotheses:\\n\\\\begin{center}\\n\\\\begin{tabular}{llll}\\nsymbol & statistical name & physics name & probability model \\\\\\\\ \\\\hline\\n$H_0$ & null hypothesis & background-only & $\\\\Pois(n_{SR} | \\\\nu_B)$ \\\\\\\\\\n$H_1$ & alternate hypothesis & signal-plus-background & $\\\\Pois(n_{SR} | \\\\nu_S+\\\\nu_B)$ \\n\\\\end{tabular}\\n\\\\end{center}\\nIn this simple example it's fairly obvious that evidence for a signal shows up as an excess of events and a reasonable way to quantify the compatibility of the observed data $n_{CR}^0$ and the null hypothesis is to calculate the probability that the background-only would produce at least this many events; the $p$-value\\n\\\\begin{equation}\\np = \\\\sum_{n=n_{SR}^0}^\\\\infty \\\\Pois(n | \\\\nu_B) \\\\; .\\n\\\\end{equation}\\nIf this $p$-value is very small, then one might choose to reject the null hypothesis.\\nNote, the $p$-value is \\\\textit{not} a to be interpreted as the probability of the null hypothesis given the data -- that is a manifestly Bayesian statement. Instead, the $p$-value is a statement about the probability to have obtained data with a certain property assuming the null hypothesis.\\nHow do we generalize this to more complicated situations? There were really two ingredients in our simple example. The first was the proposal that we would reject the null hypothesis based on the probability for it to produce data at least as extreme as the observed data. The second ingredient was the prescription for what is meant by more discrepant; in this case the possible observations are ordered according to increasing $n_{SR}$. One could imagine using difference between observed and expected, $n_{SR}-\\\\nu_B$, as the measure of discrepancy. In general, a function that maps the data to a single real number is called a \\\\textit{test statistic}: $T(\\\\data)\\\\to\\\\mathbb{R}$. How does one choose from the infinite number of test statistics?\\nNeyman and Pearson provided a framework for hypothesis testing that addresses the choice of the test statistic. This setup treats the null and the alternate hypotheses in an asymmetric way. First, one defines an \\\\textit{acceptance region} in terms of a test statistic, such that if $T(\\\\data)< k_\\\\alpha$ one accepts the null hypothesis. One can think of the $T(\\\\data) = k_\\\\alpha$ as defining a contour in the space of the data, which is the boundary of this acceptance region. Next, one defines the \\\\textit{size of the test}, $\\\\alpha$,\\\\footnote{Note, $\\\\alpha$ is the conventional notation for the size of the test, and has nothing to do with a model parameter in Eq.~\\\\ref{Eq:simultaneous}.} as the probability the null hypothesis will be rejected when it is true (a so-called Type-I error). This is equivalent to the probability under the null hypothesis that the data will not be found in this acceptance region, ie. $\\\\alpha = P(T(\\\\data) \\\\ge k_\\\\alpha | H_0)$. Note, it is now clear why there is a subscript on $k_\\\\alpha$, since the contour level is related to the size of the test. In contrast, if one accepts the null hypothesis when the alternate is true, it is called a Type-II error. The probability to commit a Type-II error is denoted as $\\\\beta$ and it is given by $\\\\beta=P(T(\\\\data) < k_\\\\alpha|H_1)$. One calls $1-\\\\beta$ the \\\\textit{power} of the test. With these definitions in place, one looks for a test statistic that maximizes the power of the test for a fixed test size. This is a problem for the calculus of variations, and sounds like it might be very difficult for complicated probability models. \\nIt turns out that in the case of two simple hypotheses (probability models without any parameters), there is a simple solution! In particular, the test statistic leading to the most powerful test is given by the likelihood ratio $T_{NP}(\\\\data) = \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$. This result is referred to as the Neyman-Pearson lemma, and I will give an informal proof. We will prove this by considering a small variation to the acceptance region defined by the likelihood ratio. The solid red contour in Fig.~\\\\ref{fig:neymanpearson} represents the rejection region (the complement to the acceptance region) based on the likelihood ratio and the dashed blue contour represents a small perturbation. If we can say that any variation to the likelihood ratio has less power, then we will have proved the Neyman-Pearson lemma. The variation adds (the left, blue wedge) and removes (the right, red wedge) rejection regions. Because the Neyman-Pearson setup requires that both tests have the same size, we know that the probability for the data to be found in the two wedges must be the same under the null hypothesis. Because the two regions are on opposite sides of the contour defined by $ \\\\f(\\\\data|H_1)/\\\\f(\\\\data|H_0)$, then we know that the data is less likely to be found in the small region that we added than the small region we subtracted assuming the alternate hypothesis. In other words, there is less probability to reject the null when the alternate is true; thus the test based on the new contour is less powerful.\\nHow does this generalize for our most general model in Eq.~\\\\ref{Eq:ftot} with many free parameters? First one must still define the null and the alternate hypotheses. Typically is done by saying some parameters -- the parameters of interest $\\\\vec\\\\alpha_{\\\\rm poi}$ -- take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis. For instance, the signal production cross-section might be singled out as the \\\\textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background. The remainder of the parameters are called the \\\\textit{nuisance parameters} $\\\\vec\\\\alpha_{\\\\rm nuis}$. Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models. Nevertheless, there is a natural generalization based on the profile likelihood ratio.\\nRemembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\\\\f_{\\\\rm tot}(\\\\data|\\\\vec\\\\alpha)$ implies a distribution for the test statistic $f(T|\\\\vec\\\\alpha)$. Note, the distribution for the test statistic depends on the value of $\\\\vec\\\\alpha$. Below we will discuss how one constructs this distribution, but lets take it as given for the time being. Once one has the distribution, then one can calculate the $p$-value is given by\\n\\\\begin{equation}\\np(\\\\vec\\\\alpha) = \\\\int_{T_0}^\\\\infty f(T | \\\\vec\\\\alpha) dT = \\\\int \\\\f(\\\\data | \\\\vec\\\\alpha )\\\\, \\\\theta(T(\\\\data) - T_0) \\\\,d\\\\data = P(T\\\\ge T_0 | \\\\vec\\\\alpha) \\\\;,\\n\\\\end{equation}\\nwhere $T_0$ is the value of the test statistic based on the observed data and $\\\\theta( \\\\cdot )$ is the Heaviside function.\\\\footnote{The integral $\\\\int d\\\\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\\\\vec\\\\alpha)$ to make its $\\\\vec\\\\alpha$-dependence explicit. \\nGiven that the $p$-value depends on $\\\\vec\\\\alpha$, how does one decide to accept or reject the null hypothesis? Remembering that $\\\\vec\\\\alpha_{\\\\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters. It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \\\\textit{for any value of the nuisance parameters}. Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\\\\vec{\\\\alpha}_{\\\\rm nuis}$ or take its maximal (or supremum) value \\n\\\\begin{equation} \\np_{\\\\rm sup}(\\\\vec\\\\alpha_{\\\\rm poi}) = \\\\sup_{ \\\\vec{\\\\alpha}_{\\\\rm nuis}} p(\\\\vec{\\\\alpha}_{\\\\rm nuis}) \\\\; .\\n\\\\end{equation}\\nAs a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional. In most sciences conventional choices of the size are 10\\\\\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Non-profile frequentist estimator}\\nThe modified frequentist method is generalized through the Neyman-Pearson Lemma, a fundamental statistical result stating that the most powerful test for hypothesis comparison is based on minimizing the Type II error. This error occurs when the null hypothesis (\\\\( H_0 \\\\)), which assumes only background, is not rejected despite being false. The Neyman-Pearson Lemma indicates that, given a significance level, the most efficient statistical test to discriminate between two hypotheses is based on the likelihood ratio~\\\\cite{cranmer2015practical,lista2016practical,cowan2011asymptotic}.\\n\\\\begin{equation}\\nR(\\\\mu) = \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)}.\\n\\\\end{equation}\\nThe formula has an asymptotic approximation to a \\\\(\\\\chi^{2}\\\\) distribution when expressed in terms of logarithms~\\\\cite{lista2016practical}. Generally, this formula is expressed as follows:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = - 2 Ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu)}{\\\\mathcal{L}(0)} \\\\bigg). \\n\\\\end{equation}\\nThis expression is known as the log-likelihood ratio, and it allows for generalization to experiments with multiple channels. Consider, for example, a single-channel experiment measuring the mass of a hypothetical particle \\\\( m(\\\\rho) \\\\) within the range of 100 to 200 GeV. Suppose the observation is \\\\( n = 105 \\\\), the expected number of background events is \\\\( b = 100 \\\\), and the model for this new particle predicts \\\\( s = 10 \\\\). The statistical estimator based on this distribution model is expressed as follows:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & -2Ln \\\\bigg( \\\\frac{ e^{-(\\\\mu s + b)}(\\\\mu s + b)^{n} }{ e^{-b}b^{n} } \\\\bigg) {} \\\\nonumber \\\\\\\\\\n& = & 2 \\\\bigg( \\\\mu s - n Ln \\\\bigg( 1 + \\\\frac{\\\\mu s} {b} \\\\bigg) \\\\bigg). {}\\n\\\\end{eqnarray}\\nFigure~[\\\\ref{fig:9}] shows the single-channel experiment, where the error bars represent the Poisson uncertainty, \\\\( \\\\epsilon = \\\\sqrt{n} \\\\), associated with the observed number of events. Additionally, it describes the behavior of the estimator as a function of the signal strength, \\\\( \\\\mu \\\\). This reveals that the likelihood ratio defines a convex optimization problem with a unique global minimum, corresponding to the best fit of the model under the null hypothesis to describe the observation. Notably, both hypotheses can fit the observed data, making it essential to determine the set of theories that can be excluded based on the measurement of \\\\( n \\\\) or to assess whether there is sufficient evidence to claim the discovery of the particle in question~\\\\cite{cms2012observation,atlas2022detailed}.\\nThe best-fit value is obtained by differentiating the log-likelihood function with respect to the parameter of interest and evaluating the result at zero. In this case, the minimum can be calculated exactly using elementary methods.\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{n-b}{s} = 0.5\\n\\\\end{equation}\\nEstimating the best model is fundamental in current statistical estimators. In general, obtaining the best fit in experiments with multiple channels and systematic uncertainties requires advanced optimization processes and sampling techniques, which will be described later. Moreover, calculating upper limits involves sampling the distributions of the estimator under both the null and alternative hypotheses. The next section will address the sampling of the estimator and the definition of the confidence level for the signal, known as \\\\( CL_s \\\\).\\n\\\\subsubsection{Sampling of the log-likelihood estimator}\\nTo obtain the upper limit using the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), the estimator can be sampled under both the null and alternative hypotheses; these distributions are labeled \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), respectively. To calculate \\\\( f(\\\\mathcal{Q}|0) \\\\), a random number is generated following a Poisson distribution with \\\\( \\\\mu = 0 \\\\), representing the number of observed events under the null hypothesis. Similarly, the distribution \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\) is obtained using a specific value of \\\\( \\\\mu \\\\)~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:10}] shows a schematic of the shapes of the distributions of the estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\). The value \\\\( Q_{obs} \\\\) corresponds to the estimator for the observed number of events \\\\( n \\\\). Typically, the background-only distribution is found to the right of \\\\( Q_{obs} \\\\), while the signal + background distribution is found to the left of \\\\( Q_{obs} \\\\). The degree of agreement between the observation and the models is evaluated through the confidence level, represented by the shaded areas in the plot~\\\\cite{cowan2014statistics}.\\nThe green shaded area represents the p-value of the observation under the background-only hypothesis (\\\\( H_{0} \\\\)) and is expressed as:\\n\\\\begin{equation}\\np_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q},\\n\\\\end{equation}\\nThe p-value of the null hypothesis is related to the well-known power of the test, which is the confidence level, denoted as \\\\( \\\\beta \\\\):\\n\\\\begin{equation}\\n\\\\beta = CL_{b} = 1 - p_{0} = 1 - \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{observed}} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nOn the other hand, the p-value for the signal + background hypothesis (\\\\( H_{1} \\\\)) is represented by the yellow shaded area, which directly corresponds to the confidence level of the \\\\( H_{1} \\\\) hypothesis:\\n\\\\begin{equation}\\np_{\\\\mu} = CL_{s+b} = \\\\int_{\\\\mathcal{Q}_{observed}}^{\\\\infty} f(\\\\mathcal{Q}/\\\\mu) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThus, the statistical significance \\\\( \\\\alpha \\\\) is a particular case of the p-value of the observation under the null hypothesis (\\\\( p_{0} \\\\)) and constitutes evidence in favor of \\\\( H_{1} \\\\) against \\\\( H_{0} \\\\). Consequently, maximizing the significance is a tool for optimizing the search window. In various statistical studies, it is suggested that stronger evidence in favor of \\\\( H_{1} \\\\) over \\\\( H_{0} \\\\) is reflected in~\\\\cite{lista2016practical,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\frac{p_{\\\\mu}}{\\\\beta} = \\\\frac{CL_{s+b}}{CL_{b}} = \\\\frac{p_{\\\\mu}}{1-p_{0}}. \\n\\\\end{equation}\\nThis completely defines the confidence level of the signal. Using these definitions, the upper limit of the signal strength is obtained through the following strategy: 1) vary the signal strength \\\\( \\\\mu \\\\), 2) sample the distributions \\\\( f(\\\\mathcal{Q}|0) \\\\) and \\\\( f(\\\\mathcal{Q}|\\\\mu) \\\\), 3) calculate the p-values corresponding to the observed estimator, and 4) determine the confidence level \\\\( CL_s(\\\\mu) \\\\). In this way, the upper limit \\\\( \\\\mu^{up} \\\\) for exclusion is given by:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = 0.05\\n\\\\end{equation}\\nThis process is typically computationally expensive due to the sampling of distributions for each value of \\\\( \\\\mu \\\\). In the case of multiple channels and nuisance parameter estimation, parallelization is required to obtain results efficiently. This is crucial, as optimizing the search window at the phenomenological level necessitates maximizing statistical significance or other statistical metrics~\\\\cite{florez2016probing,allahverdi2016distinguishing}.\\nIn the experiment related to the invariant mass channel \\\\( m(\\\\rho) \\\\), we have two key estimates. The first is the expected upper limit, which is assumed under the hypothesis that the observation consists of background nuisance only. In this case, the expected upper limit is \\\\( \\\\mu_{up}^{\\\\text{Exp}} = 2.19 \\\\). This implies that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Exp}} \\\\cdot s = 2.19 \\\\times 10 = 21.9 \\\\) events would be excluded, provided that \\\\( n = b \\\\) events are measured. In the second case, the observed upper limit corresponds to the actual data observation. This observed upper limit is estimated as \\\\( \\\\mu_{up}^{\\\\text{Obs}} = 2.57 \\\\), meaning that any theory predicting more than \\\\( s_{up} = \\\\mu_{up}^{\\\\text{Obs}} \\\\cdot s = 2.57 \\\\times 10 = 25.7 \\\\) events would be excluded based on the observation. Figure~[\\\\ref{fig:11}] shows the search for the confidence level for both the expected and observed limits~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/LEP/UpperLimitLnQ.ipynb}{Source code}}. Additionally, the distribution of the statistical estimator for \\\\( \\\\mu = \\\\mu_{up}^{\\\\text{Obs}} \\\\) is presented as an illustrative example. Finally, results obtained using the professional \\\\texttt{RooFit} package, used by the CMS and ATLAS collaborations for upper limit estimation, are included~\\\\cite{verkerke2006roofit,schott2012roostats}.\\nAn important feature in this type of analysis is that when the upper limits are statistically consistent, as in this case, there is insufficient evidence to claim a discovery. In this scenario, we would accept that the null hypothesis \\\\( H_{0} \\\\) (background-only hypothesis) adequately describes the observation. The first sign of tension between the observed data and the background-only hypothesis arises when the expected and observed upper limits differ significantly. From a statistical perspective, this discrepancy must be evaluated, and to consider the observation of a new phenomenon, the difference must exceed the \\\\( 5\\\\sigma \\\\) threshold~\\\\cite{lista2016practical,cranmer2015practical,cms2012observation}.', \"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Variations on test statistics}\\nA number of test statistics is proposed in Ref.~\\\\cite{asymptotic} that better\\nserve various purposes. Below the main ones are reported:\\n\\\\begin{itemize}\\n\\\\item {\\\\bf Test statistic for discovery:}\\n\\\\begin{equation}\\nq_0 = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(0), &\\\\hat{\\\\mu}\\\\ge 0\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} < 0\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIn case of a negative estimate of $\\\\mu$ ($\\\\hat{\\\\mu}<0$), the test statistic is set to zero in order to\\nconsider only positive $\\\\hat{\\\\mu}$ as evidence against the background-only hypothesis.\\nWithin an asymptotic approximation, the significance is given by: $Z\\\\simeq\\\\sqrt{q_0}$.\\n\\\\item {\\\\bf Test statistic for upper limit:}\\n\\\\begin{equation}\\nq_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(\\\\mu), &\\\\hat{\\\\mu}\\\\le \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} > \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIf the $\\\\hat{\\\\mu}$ estimate is larger than the assumed value for $\\\\mu$, an upward fluctuation occurred.\\nIn those cases, $\\\\mu$ is not excluded by setting the test statistic to zero.\\n\\\\item {\\\\bf Test statistic for Higgs boson search:}\\n\\\\begin{equation}\\n\\\\tilde q_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0))}, & \\\\hat{\\\\mu} < 0\\\\,,\\\\\\\\\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\vec{\\\\theta}}(\\\\mu))}, & 0\\\\le \\\\hat{\\\\mu} < \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} \\\\ge \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThis test statistics both protects against unphysical cases with $\\\\mu <0$\\nand, as the test statistic for upper limits, protects upper limits\\nin cases of an upward $\\\\hat{\\\\mu}$ fluctuation.\\n\\\\end{itemize}\\nA number of measurements performed at LEP and Tevatron used a\\ntest statistic based on the ratio of the likelihood function evaluated under\\nthe signal plus background hypothesis and under the background only hypothesis,\\ninspired by the Neyman--Pearson lemma:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|s+b)}{L(\\\\vec{x}|b)}\\\\,.\\n\\\\end{equation}\\nIn many LEP and Tevatron analyses, nuisance parameters were treated using the hybrid Cousins--Hyghland approach.\\nAlternatively, one could use a formalism similar to the profile likelihood, \\nsetting $\\\\mu=0$ in the denominator and $\\\\mu=1$ in the numerator, and minimizing\\nthe likelihood functions with respect to the nuisance parameters:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu=1, \\\\hat{\\\\hat{\\\\theta}}(1))}{L(\\\\vec{x}|\\\\mu=0,\\\\hat{\\\\hat{\\\\theta}}(0))}\\\\,.\\n\\\\end{equation}\\nFor all the mentioned test statistics, asymptotic approximations exist and\\nare reported in Ref.~\\\\cite{asymptotic}. Those are based either on Wilks' theorem\\nor on Wald's approximations~\\\\cite{Wald}. If a value $\\\\mu$ is tested, and \\nthe data are supposed to be distributed according to another value of the signal strength $\\\\mu^\\\\prime$,\\nthe following approximation holds, asymptotically:\\n\\\\begin{equation}\\n-2\\\\ln\\\\lambda(\\\\mu) = \\\\frac{(\\\\mu-\\\\hat{\\\\mu})^2}{\\\\sigma^2} + {\\\\cal O}\\\\left(\\\\frac{1}{\\\\sqrt{N}}\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\mu}$ is distributed according to a Gaussian with average $\\\\mu^\\\\prime$ and\\nstandard deviation $\\\\sigma$. The covariance matrix for the nuisance parameters is\\ngiven, in the asymptotic approximation, by:\\n\\\\begin{equation}\\nV_{ij}^{-1} = \\\\left.\\\\left<\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right>\\\\right|_{\\\\mu=\\\\mu^\\\\prime}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu^\\\\prime$ is assumed as value for the signal strength.\\nIn some cases, asymptotic approximations (Eq.~(\\\\ref{eq:waldTestStat})) can be written in terms of an\\n{\\\\it Asimov dataset}~\\\\cite{Asimov}:\\n\\\\begin{displayquote}\\n{\\\\it We define the Asimov data set such that when one uses it to evaluate the estimators\\nfor all parameters, one obtains the true parameter values}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\nIn practice, an Asimov dataset is a single ``representative'' dataset obtained by replacing all\\nobservable (random) varibles with their expecteted value. In particular,\\nall yields in the data sample (e.g.: in a binned case) are replaced with their expected values, that may be non integer values.\\nThe median significance for different cases of test statistics can be computed in this\\nway without need of producing extensive sets of toy Monte Carlo. The implementation\\nof those asymptotic formulate is available in the {\\\\sc RooStats} library, released\\nas part an optional component {\\\\sc Root}~\\\\cite{Root}.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Modified frequentist method}\\nIn general, the construction of frequentist estimators is based on determining the confidence level associated with a statistical estimator. For a statistic \\\\( Q \\\\), the confidence level of the hypothesis considering only the background component is given by the probability that \\\\( Q \\\\) takes a value less than or equal to the observed value \\\\( Q_{obs} \\\\)~\\\\cite{lista2016practical, barlow2019practical, read2002presentation}.\\n\\\\begin{equation}\\nCL_{b} = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{b}}{dQ} dQ,\\n\\\\end{equation}\\nwhere \\\\( Q_{obs} \\\\) depends on the observed values: \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Similarly, the confidence level for the signal + background hypothesis is defined by the probability that \\\\( Q \\\\) is less than or equal to \\\\( Q_{obs} \\\\), thus:\\n\\\\begin{equation}\\nCL_{s+b}(\\\\mu) = \\\\int_{-\\\\infty}^{Q_{Obs}} \\\\frac{dP_{\\\\mu s+b}}{dQ} dQ,\\n\\\\end{equation}\\nThe modification of the frequentist method involves the renormalization of the confidence level for the alternative hypothesis:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = CL_{s+b}(\\\\mu)/C_{b}.\\n\\\\end{equation}\\nThe purpose of this definition is to maintain the coverage of the estimator to protect the null hypothesis \\\\( H_{0} \\\\). In other words, the exclusion values of \\\\( \\\\mu \\\\) are positive. In particular, \\\\( CL_{s} \\\\) for event counting is given by:\\n\\\\begin{equation}\\nCL_{s}(\\\\mu) = \\\\sum_{i=0}^{n} \\\\frac{e^{-(\\\\mu s + b)} (\\\\mu s + b)^{i}}{i!} \\\\bigg/ \\n\\\\sum_{i=0}^{n} \\\\frac{e^{-(b)} (b)^{i}}{i!}.\\n\\\\end{equation}\\nUsing the expression for the cumulative Poisson distribution (Appendix~\\\\ref{sec:AppendixA}), it is possible to find the value of \\\\( CL_{s} \\\\) for different values of the signal strength \\\\( \\\\mu \\\\). Figure~[\\\\ref{fig:8}] shows the exploration of the p-value as a function of the signal strength for \\\\( b \\\\approx n = 0 \\\\) and \\\\( s = 1 \\\\). The upper limit \\\\( \\\\mu_{up} = 2.99 \\\\) is consistent with values obtained using previous methods.\\nIn the previous sections, estimations were made for a specific point defined by \\\\( n \\\\), \\\\( b \\\\), and \\\\( s \\\\). Table~[\\\\ref{tb:2}] summarizes the calculation of upper limits for the three methods discussed above, evaluated for different values of \\\\( n \\\\) and \\\\( b \\\\) while keeping \\\\( s = 1 \\\\) constant~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/ModifiedFrequentist/ModifiedUpperLimit.ipynb}{Source code}}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{lcccc}\\n\\\\hline\\nObservation ($n$) & Expected background ($b$) & Frequentist & Bayesian & Modified frequentist \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0 & 0 & 2.99 & 3.00 & 2.99 \\\\\\\\\\n& 1 & 1.99 & 3.00 & 2.99 \\\\\\\\\\n& 2 & 0.99 & 3.00 & 2.99 \\\\\\\\\\n& 3 & 0.00 & 3.00 & 2.99 \\\\\\\\\\n\\\\hline\\n1 & 0 & 4.74 & 4.76 & 4.75 \\\\\\\\\\n& 1 & 3.74 & 4.11 & 4.12 \\\\\\\\\\n& 2 & 2.74 & 3.82 & 3.82 \\\\\\\\\\n& 3 & 1.74 & 3.65 & 3.65 \\\\\\\\\\n\\\\hline\\n2 & 0 & 6.29 & 6.30 & 6.29 \\\\\\\\\\n& 1 & 5.29 & 5.41 & 5.42 \\\\\\\\\\n& 2 & 4.29 & 4.83 & 4.83 \\\\\\\\\\n& 3 & 3.29 & 4.45 & 4.45 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nThe development of the concept of confidence level and the application of the Neyman-Pearson Lemma for the signal hypothesis have facilitated the creation of frequentist statistical estimators that are unbiased by prior distributions. At the LEP collider, a parameter-independent estimator \\\\( Q(\\\\mu) \\\\) was developed~\\\\cite{cms2022portrait, cranmer2015practical}. More recently, at the LHC, the estimator \\\\( q_{\\\\mu} \\\\) has been employed, which is based on the profile likelihood~\\\\cite{lista2016practical, jme2010cms}. This estimator maximizes the parameters for statistical and systematic uncertainty within the likelihood function to incorporate these effects in the calculation of upper limits, experimental sensitivity, or the potential observation of new physics.\\n'}},\n",
       "       {'entity_name': 'neyman construction and confidence intervals', 'entity_type': 'analysis_technique', 'description': \"A set of procedures in frequentist statistics for constructing confidence intervals, including Neyman's construction and Neyman's belt construction. These methods involve inverting hypothesis tests to determine parameter values consistent with observed data, ensuring that a specified fraction of intervals will contain the true parameter value, and providing coverage guarantees for both continuous and discrete variables.\", 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Excluded and allowed regions as confidence intervals}\\nOften we consider a new physics model that is parametrized by theoretical parameters. For instance, the mass or coupling of a new particle. In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data. Figure~\\\\ref{fig:confidenceIntervals} shows two examples. Figure~\\\\ref{fig:confidenceIntervals}(a) shows an example with $\\\\vec\\\\alpha_{\\\\rm poi} = (\\\\sigma/\\\\sigma_{SM}, M_H)$, where $\\\\sigma/\\\\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model. All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\\\\$\\\\vec\\\\alpha_{\\\\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses. The blue ellipse `is the 68\\\\\\nIn a frequentist setting, these allowed regions are called \\\\textit{confidence intervals} or \\\\textit{confidence regions}, and the parameter points outside them are considered excluded. Associated with a confidence interval is a confidence level, i.e. the 95\\\\\\nHow can one possibly construct a confidence interval has the desired property, that it \\\\textit{covers} the true value with a specified probability, given that we don't know the true value? The procedure for building confidence intervals is called the Neyman Construction~\\\\cite{Neyman}, and it is based on `inverting' a series of hypothesis tests (as described in Sec.~\\\\ref{S:hypothesis test}). In particular, for each value of $\\\\vec\\\\alpha$ in the parameter space one performs a hypothesis test based on some test statistic where the null hypothesis is $\\\\vec\\\\alpha$. Note, that in this context, the null hypothesis is changing for each test and generally is not the background-only. If one wants a 95\\\\\\\\begin{equation}\\nI(\\\\data) = \\\\left\\\\{ \\\\vec\\\\alpha \\\\middle |\\\\, P(T(\\\\data)>k_\\\\alpha \\\\,|\\\\, \\\\vec\\\\alpha) < \\\\alpha \\\\right\\\\} \\\\;,\\n\\\\end{equation}\\nwhere the final $\\\\alpha$ and the subscript $k_\\\\alpha$ refer to the size of the test.\\nSince a hypothesis test with a size of 5\\\\\\\\begin{equation}\\n\\\\textrm{coverage}(\\\\vec\\\\alpha) = P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha) \\\\; .\\n\\\\end{equation}\\nThe equation above can easily be mis-interpreted as the probability the parameter is in a fixed interval $I$; but one must remember that in evaluating the probability above the data $\\\\data$, and, thus, the corresponding intervals produced by the procedure $I(\\\\data)$, are the random quantities. \\nNote, that coverage is a property that can be quantified for any procedure that produces the confidence intervals $I$. Intervals produced using the Neyman Construction procedure are said to ``cover by construction''; however, one can consider alternative procedures that may either under-cover or over-cover. Undercoverage means that \\\\mbox{$P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$} is smaller than desired and over-coverage means that $P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$ is larger than desired. Note that in general coverage depends on the assumed true value $\\\\vec\\\\alpha$.\\nSince one typically is only interested in forming confidence intervals on the parameters of interest, then one could use the supremum $p$-value of Eq.~\\\\ref{Eq:psup}. This procedure ensures that the coverage is at least the desired level, though for some values of $\\\\vec\\\\alpha$ it may over-cover (perhaps significantly). This procedure, which I call the `full construction', is also computationally very intensive when $\\\\vec\\\\alpha$ has many parameters as it require performing many hypothesis tests. In the naive approach where each $\\\\alpha_p$ is scanned in a regular grid, the number of parameter points tested grows exponentially in the number of parameters. There is an alternative approach, which I call the `profile construction'~\\\\cite{Feldman,Cranmer:2005hi}\\nand which statisticians call an `hybrid resampling technique'~ \\\\cite{Hybrid,Bodhi} that is approximate to the full construction, but typically has good coverage properties. We return to the procedures and properties for the different types of Neyman Constructions later.\\nFigure~\\\\ref{fig:NC_schematic} provides an overview of the classic Neyman construction corresponding to the left panel of Fig.~\\\\ref{fig:neyman}. The left panel of Fig.~\\\\ref{fig:neyman} is taken from the Feldman and Cousins's paper~\\\\cite{Feldman:1997qc} where the parameter of the model is denoted $\\\\mu$ instead of $\\\\theta$. For each value of the parameter $\\\\mu$, the acceptance region in $x$ is illustrated as a horizontal bar. Those regions are the ones that satisfy $T(\\\\data)<k_\\\\alpha$, and in the case of Feldman-Cousins the test statistic is the one of Eq.~\\\\ref{eq:tmu}. This presentation of the confidence belt works well for a simple model in which the data consists of a single measurement $\\\\data=\\\\{x\\\\}$. Once one has the confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$ simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the confidence belt.\\nUnfortunately, this convenient visualization doesn't generalize to complicated models with many channels or even a single channel marked Poisson model where $\\\\data=\\\\{x_1,\\\\dots,x_n\\\\}$. In those more complicated cases, the confidence belt can still be visualized where the observable $x$ is replaced with $T$, the test statistic itself. Thus, the boundary of the belt is given by $k_\\\\alpha$ vs. $\\\\mu$ as in the right panel of Figure~\\\\ref{fig:neyman}. The analog to the vertical line in the left panel is now a curve showing how the observed value of the test statistic depends on $\\\\mu$. The confidence interval still corresponds to the intersection of the observed test statistic curve and the confidence belt, which clearly satisfies $T(\\\\data)<k_\\\\alpha$. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\\nNote, a 95\\\\\\\\begin{equation}\\nP(\\\\alpha\\\\in I | \\\\data) = \\\\frac{ \\\\int_{I} \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha }{\\\\int \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha} \\\\;.\\n\\\\end{equation}\", \"\\\\section{Inference}\\n\\\\subsection{Binomial intervals}\\nThe Neyman's belt construction may only guarantee approximate coverage in case of a discrete\\nvariable $n$. This because the interval for a discrete variable is a set of integer values,\\n$\\\\{ n_{\\\\mathrm{min}}, \\\\cdots, n_{\\\\mathrm{max}}\\\\}$, and cannot be ``tuned'' like in\\na continuous case. The choice of the discrete interval should be such to provide\\n{\\\\it at least} the desired coverage (i.e.: it may {\\\\it overcover}).\\nFor a binomial distribution, the problem consists of finding the interval such that:\\n\\\\begin{equation}\\n\\\\sum_{n=n_{\\\\mathrm{min}}}^{n_{\\\\mathrm{max}}}\\n\\\\frac{N!}{n!(N-n)!} p^n (1-p)^{N-n} \\\\ge 1-\\\\alpha\\\\,.\\n\\\\end{equation}\\nClopper and Pearson~\\\\cite{clopper_pearson} solved the belt inversion problem for\\ncentral intervals.\\nFor an observed $n = k$, one has to find the lowest $p^{\\\\mathrm{lo}}$ and highest\\n$p^{\\\\mathrm{up}}$ such that:\\n\\\\begin{eqnarray}\\nP(n \\\\ge k | N, p^{\\\\mathrm{lo}}) & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\nP(n \\\\le k | N, p^{\\\\mathrm{up}}) & = & \\\\frac{\\\\alpha}{2}\\\\,.\\n\\\\end{eqnarray}\\nAn example of Neyman belt constructed using the Clopper--Pearson\\nmethod is shown in Fig.~\\\\ref{fig:CloPear}.\\nFor instance for $n = N$, Eq.~(\\\\ref{eq:CP1}) becomes:\\n\\\\begin{equation}\\nP(n\\\\ge N|N,p^{\\\\mathrm{lo}}) = P(n=N|N,p^{\\\\mathrm{lo}}) = (p^{\\\\mathrm{lo}})^N = \\\\frac{\\\\alpha}{2}\\\\,,\\n\\\\end{equation}\\nhence, for the specific case $N=10$:\\n\\\\begin{equation}\\np^{\\\\mathrm{lo}} = \\\\sqrt[10]{\\\\frac{\\\\alpha}{2}} = 0.83\\\\,\\\\text(1-\\\\alpha = 0.683), \\\\,0.74\\\\,(1-\\\\alpha = 0.90)\\\\,.\\n\\\\end{equation}\\nIn fact, in Fig.~\\\\ref{fig:CloPear}, the bottom line of the belt reaches\\nthe value $p=0.83$ for $n=10$.\\nA frequently used approximation, inspired by Eq.~(\\\\ref{eq:binomVar}) is:\\n\\\\begin{equation}\\n\\\\hat{p} = \\\\frac{n}{N},\\\\,\\\\,\\\\,\\\\sigma_{\\\\hat{p}} \\\\simeq \\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{N}}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:varEff}) gives $\\\\sigma_{\\\\hat{p}}=0$ for $n=0$ or $N=n$, which is\\nclearly an underestimate of the uncertainty on $\\\\hat{p}$. For this reason,\\nClopper--Pearson intervals should be preferred to the approximate\\nformula in Eq.~(\\\\ref{eq:varEff}).\\nClopper--Pearson intervals are often defined as ``exact'' in literature,\\nthough exact coverage is often impossible to achieve for discrete variables.\\nFigure~\\\\ref{fig:CloPearCov} shows the coverage of Clopper--Pearson intervals as a\\nfunction of $p$ for $N=10$ and $N=100$ for $1-\\\\alpha = 0.683$. A ``ripple'' structure\\nis present which, for large $N$, tends to gets closer to the nominal 68.3\\\\\\n\", \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\", \"\\\\section{Inference}\\n\\\\subsection{Neyman's confidence intervals}\\nA procedure to determine frequentist {\\\\it confidence intervals} is due to\\nNeyman~\\\\cite{neyman_belt}. It proceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Scan the allowed range of the unknown parameter of interest $\\\\theta$.\\n\\\\item Given a value $\\\\theta_0$ of $\\\\theta$, compute the interval $[x_1(\\\\theta_0), x_2(\\\\theta_0)]$ that contains $x$ with a probability $1 - \\\\alpha$\\n({\\\\it confidence level}, or CL) equal to 68.3\\\\ For this procedure, a choice of interval ({\\\\it ordering rule}) is needed, as discussed in Sec.~\\\\ref{sec:BayesianInference}.\\n\\\\item For the observed value of $x$, invert the confidence belt: find the corresponding interval $[\\\\theta_1(x), \\\\theta_2(x)]$.\\n\\\\end{itemize}\\nBy construction, a fraction of the experiments equal to $1 -\\\\alpha$ will measure $x$ such that the corresponding\\n{\\\\it confidence interval} $[\\\\theta_1(x), \\\\theta_2(x)]$ contains ({\\\\it covers}) the true value of $\\\\theta$.\\nIt should be noted that the random variables are $\\\\theta_1(x)$ and $\\\\theta_2(x)$, not $\\\\theta$.\\nAn example of application of the Neyman's belt construction and inversion is shown in Fig.~\\\\ref{fig:NeymanBelt}.\\nThe simplest application of Neyman's belt construction can be done with a Gaussian\\ndistribution with known parameter $\\\\sigma=1$, as shown in Fig.~\\\\ref{fig:NeymanGaussianBelt}.\\nThe belt inversion is trivial and gives the expected result: a central value $\\\\hat{\\\\mu} = x$\\nand a confidence interval $[\\\\mu_1, \\\\mu_2] = [x - \\\\sigma, x + \\\\sigma]$.\\nThe result can be quoted as $\\\\mu = x\\\\pm\\\\sigma$, similarly to what was determined\\nwith Eq.~(\\\\ref{eq:trivialML}).\\n\"}},\n",
       "       {'entity_name': 'coverage and coverage probability', 'entity_type': 'statistics_concept', 'description': 'A statistical concept that quantifies the reliability of confidence intervals in estimating true parameter values. Coverage refers to the proportion of times that confidence intervals, constructed from repeated experiments or samples, contain the true parameter value. Coverage probability is the likelihood that a confidence interval will include the true parameter value, ideally meeting or exceeding a specified confidence level. The coverage problem arises when confidence intervals fail to contain the true parameter value as frequently as expected, leading to potential misinterpretations in hypothesis testing.', 'relevant_passages': {'\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\nThe most important principle in this approach is that enunciated by \\nthe Polish statistician Jerzy Neyman in the 1930s, namely, \\n\\\\begin{quote}\\n\\\\textbf{The Frequentist Principle}\\nThe goal of a frequentist analysis is to construct statements so that a fraction $f \\\\geq p$ of\\nthem are guaranteed to be true over an infinite ensemble of statements.\\n\\\\end{quote}\\nThe fraction $f$ is called the \\\\textbf{coverage probability}, or coverage for short, and $p$ is called the \\\\textbf{confidence level} (C.L.). A procedure which satisfies the frequentist principle is said to \\\\emph{cover}.\\nThe confidence level as well as the coverage is a property of the\\nensemble of statements. Consequently, the confidence level may change if the ensemble changes. Here is an example of the frequentist principle in action.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nOver the course of a long career, a doctor sees thousands of patients. For each \\npatient he issues one of two conclusions: ``you are sick\" or ``you are well\" depending on the\\nresults of diagnostic measurements. Because he is a frequentist, he has devised\\nan approach to medicine in which although he does not know which of his conclusions\\nwere correct, he can at least retire happy in the knowledge that he was correct at least 75\\\\\\\\end{quote}\\n\\\\bigskip \\nIn a seminal paper published in 1937, Neyman~\\\\cite{Neyman37} \\ninvented the concept of the confidence interval, a way to quantify\\nuncertainty, that respects the frequentist principle. The confidence interval is such an important idea, and its\\nmeaning so different from the superficially similar Bayesian concept of a credible\\ninterval, that it is worth working through the concept in detail.\\n', \"\\\\section{Inference}\\n\\\\subsection{Binomial intervals}\\nThe Neyman's belt construction may only guarantee approximate coverage in case of a discrete\\nvariable $n$. This because the interval for a discrete variable is a set of integer values,\\n$\\\\{ n_{\\\\mathrm{min}}, \\\\cdots, n_{\\\\mathrm{max}}\\\\}$, and cannot be ``tuned'' like in\\na continuous case. The choice of the discrete interval should be such to provide\\n{\\\\it at least} the desired coverage (i.e.: it may {\\\\it overcover}).\\nFor a binomial distribution, the problem consists of finding the interval such that:\\n\\\\begin{equation}\\n\\\\sum_{n=n_{\\\\mathrm{min}}}^{n_{\\\\mathrm{max}}}\\n\\\\frac{N!}{n!(N-n)!} p^n (1-p)^{N-n} \\\\ge 1-\\\\alpha\\\\,.\\n\\\\end{equation}\\nClopper and Pearson~\\\\cite{clopper_pearson} solved the belt inversion problem for\\ncentral intervals.\\nFor an observed $n = k$, one has to find the lowest $p^{\\\\mathrm{lo}}$ and highest\\n$p^{\\\\mathrm{up}}$ such that:\\n\\\\begin{eqnarray}\\nP(n \\\\ge k | N, p^{\\\\mathrm{lo}}) & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\nP(n \\\\le k | N, p^{\\\\mathrm{up}}) & = & \\\\frac{\\\\alpha}{2}\\\\,.\\n\\\\end{eqnarray}\\nAn example of Neyman belt constructed using the Clopper--Pearson\\nmethod is shown in Fig.~\\\\ref{fig:CloPear}.\\nFor instance for $n = N$, Eq.~(\\\\ref{eq:CP1}) becomes:\\n\\\\begin{equation}\\nP(n\\\\ge N|N,p^{\\\\mathrm{lo}}) = P(n=N|N,p^{\\\\mathrm{lo}}) = (p^{\\\\mathrm{lo}})^N = \\\\frac{\\\\alpha}{2}\\\\,,\\n\\\\end{equation}\\nhence, for the specific case $N=10$:\\n\\\\begin{equation}\\np^{\\\\mathrm{lo}} = \\\\sqrt[10]{\\\\frac{\\\\alpha}{2}} = 0.83\\\\,\\\\text(1-\\\\alpha = 0.683), \\\\,0.74\\\\,(1-\\\\alpha = 0.90)\\\\,.\\n\\\\end{equation}\\nIn fact, in Fig.~\\\\ref{fig:CloPear}, the bottom line of the belt reaches\\nthe value $p=0.83$ for $n=10$.\\nA frequently used approximation, inspired by Eq.~(\\\\ref{eq:binomVar}) is:\\n\\\\begin{equation}\\n\\\\hat{p} = \\\\frac{n}{N},\\\\,\\\\,\\\\,\\\\sigma_{\\\\hat{p}} \\\\simeq \\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{N}}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:varEff}) gives $\\\\sigma_{\\\\hat{p}}=0$ for $n=0$ or $N=n$, which is\\nclearly an underestimate of the uncertainty on $\\\\hat{p}$. For this reason,\\nClopper--Pearson intervals should be preferred to the approximate\\nformula in Eq.~(\\\\ref{eq:varEff}).\\nClopper--Pearson intervals are often defined as ``exact'' in literature,\\nthough exact coverage is often impossible to achieve for discrete variables.\\nFigure~\\\\ref{fig:CloPearCov} shows the coverage of Clopper--Pearson intervals as a\\nfunction of $p$ for $N=10$ and $N=100$ for $1-\\\\alpha = 0.683$. A ``ripple'' structure\\nis present which, for large $N$, tends to gets closer to the nominal 68.3\\\\\\n\", \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\", \"\\\\section{Likelihood}\\nThe likelihood function is very widely used in many statistics applications. In this \\nSection, we consider it just for Parameter Determination. An important feature of the \\nlikelihood approach is that it can be used with {\\\\bf unbinned} data, and\\nhence can be applied in situations where there are not enough individual observations\\nto construct a histogram for the $\\\\chi^2$ approach. \\nWe start by assuming that we wish to fit our data $x$, using a model $f(x;\\\\mu)$ \\nwhich has one or more free parameters $\\\\mu$, whose value(s) we need to determine. \\nThe function $f$ is known as the `probability distribution' ($pdf$) and \\nspecifies the probability (or probability density, for the data having continuous as\\nopposed to discrete values) for obtaining different values of the data, when the parameter(s)\\nare specified. Without this \\nit is impossible to apply the likelihood (or many other) approaches. \\nFor example $x$ could be observations of a variable of interest within some \\nrange, and $f$ could be\\nany function such as a straight line, with gradient and intercept as parameters.\\nBut we will start with an angular distribution\\n\\\\begin{equation}\\ny(\\\\cos\\\\theta;\\\\beta) = \\\\frac{d\\\\ p}{d\\\\cos\\\\theta} = N(1+\\\\beta \\\\cos^2\\\\theta)\\n\\\\end{equation}\\nHere $\\\\theta$ is the angle at which a particle is observed, $dp/d\\\\cos\\\\theta$ is the $pdf$\\nspecifying the probability density for observing a decay at any $\\\\cos\\\\theta$, $\\\\beta$ is\\nthe parameter we want to determine, and $N$ is the crucial nomalisation factor \\nwhich ensures that the probability of observing a given decay at any $\\\\cos\\\\theta$\\nin the whole range from $-1$ to $+1$ is unity. In this case $N = 1/(2(1+\\\\beta/3))$. \\nThe data consists of $N$ decays, with their individual observations $\\\\cos\\\\theta_i$.\\nAssuming temporarily that the value of the parameter $\\\\beta$ is specified,\\nthe probability density $y_1$ of observing the first decay at $\\\\cos\\\\theta_1$ is\\n\\\\begin{equation}\\ny_1 = N (1+\\\\beta \\\\cos^2\\\\theta_1) = 0.5 (1+\\\\beta \\\\cos^2\\\\theta_1)/(1 + \\\\beta/3),\\n\\\\end{equation}\\nand similarly for the rest of the $N$ observations. Since the individual observations\\nare independent, the overall probability $P(\\\\beta)$ of observing the complete data set\\nof $N$ events is given by the product of the individual probabilities\\n\\\\begin{equation}\\nP(\\\\beta) = \\\\Pi y_i = \\\\Pi \\\\ 0.5 (1+\\\\beta \\\\cos^2\\\\theta_i)/(1 + \\\\beta/3) \\n\\\\end{equation}\\nWe imagine that this is computed for all values of the parameter $\\\\beta$; \\nthen this is known as the likelihood function ${\\\\it L}(\\\\beta)$.\\nThe likelihood method then takes as the estimate of $\\\\beta$ that value which \\nmaximises the likelihood. That is, it is the value which maximises (with respect to \\n$\\\\beta$) the probability density of observing the given data set. Conversely \\nwe rule out values of $\\\\beta$ for which ${\\\\it L}(\\\\beta)$ is very small. The\\nuncertainty on $\\\\beta$ is related to the width of the ${\\\\it L}(\\\\beta)$ \\ndistribution (see later). \\nIt is often convenient to consider the logarithm of the likelihood\\n\\\\begin{equation} \\n{\\\\it l} = \\\\ln{\\\\it L} = \\\\Sigma \\\\ln y_i\\n\\\\end{equation}\\nOne reason for this is that, for a large number of observations \\nsome fraction could have small $y_i$. Then the likelihood, involving the product of the\\n$y_i$, could be very small and may underflow the computer's range for real numbers.\\nIn contrast, {\\\\it l} involves a sum rather than a product, and $\\\\ln y_i$ rather than \\n$y_i$, and so produces a gentler number.\\n\\\\subsection{Likelihood and $pdf$}\\nThe procedure for constructing the likelihood is first to write down the $pdf$, and then to insert into that \\nexpression the observed data values in order to evaluate their product, which is the likelihood. Thus both \\nthe $pdf$ and the likelihood involve the data $x$ and the parameter(s) $\\\\mu$. The difference is that the $pdf$ is a function of $x$ for fixed values of $\\\\mu$, while the likelihood is a function of $\\\\mu$ given the fixed observed \\ndata $x_{obs}$. \\nThus for a Poisson distribution, the probability of observing $n$ events when the rate $\\\\mu$ is specified is \\n\\\\begin{equation}\\nP(n;\\\\mu) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $n$, while the likelihood is\\n\\\\begin{equation}\\nL(\\\\mu;n) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $\\\\mu$ for the fixed observed number $n$.\\n\\\\subsection{Intuitive example: Location and width of peak}\\nWe consider a \\nsituation where we are studying a resonant state which would result in a bump in the mass distribution of its decay particles.\\nWe assume that the bump can be parametrised as a simple Breit-Wigner\\n\\\\begin{equation}\\ny(m;M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nwhere $y$ is the probability density of obtaining a mass $m$ if the location and width the state are $M_0$ and $\\\\Gamma$,\\nthe parameters we want to determine. It is essential that $y$ is normalised, i.e. its integral over all physical values of \\n$m$ is unity; hence the normalisation factor of $\\\\Gamma/(2\\\\pi)$. The data consists of $n$ observations of $m$, as shown in fig. \\\\ref{fig:L_for_Resonance}.\\nAssume for the moment that we know $M_0$ and $\\\\Gamma$. Then the probability density for observing the $i^{th}$\\nevent with mass $m_i$ is\\n\\\\begin{equation}\\ny_i(M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nSince the events are independent, the probability density for observing the whole data sample is\\n\\\\begin{equation}\\ny_{all}(M_0,\\\\Gamma) =\\\\Pi \\\\ \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nand this is known as the likelihood $L(M_0,\\\\Gamma)$. Then the best values for the parameters are taken as\\nthe combination that maximises the probability density for the whole data sample i.e. $L(M_0,\\\\Gamma)$. \\nParameter values for which $L$ is very small compared to its maximum value are rejected, and the uncertainties \\non the parameters are related to the width of the distribution of $L$; we will be more specific later.\\nThe curve in\\nfig. \\\\ref{fig:L_for_Resonance}(left) shows the expected probability distribution for fixed parameter values. The way $L$ is calculated involves\\nmultiplying the heights of the curve at all the observed $m_i$ values. If we now consider varying $M_0$, this moves the curve bodily to the left or right without changing its shape or normalisation. So to determine the best value of $M_0$, we need to find where to locate the curve so that the product of the heights is a maximum; it is plausibe that the peak will be located where the majority of events are to be found.\\nNow we will consider how the optimum value of $\\\\Gamma$ is obtained. A small $\\\\Gamma$ results in a narrow curve, so the masses in the tail will make an even smaller contribution to the product in eqn. \\\\ref{product}, and hence reduce the likelihood. But a large $\\\\Gamma$ is not good, because not only is the width larger, but because of the normalisation condition, the peak height is reduced, and so the observations in the peak region make a smaller contribution to the likelihood. The optimal \\n$\\\\Gamma$ involves a trade-off between these two effects.\\nOf course, in finding the optimal of values of the two parameters, in general it is necessary to find the maximum of the \\nlikelihood as a function of the two parameters, rather than maximising with respect to just one, and then with respect to the other and then stopping (see section \\\\ref{More_variables}).\\n\\\\subsection{Uncertainty on parameter}\\nWith a large amount of data, the likelihood as a function of a parameter $\\\\mu$ is \\noften approximately Gaussian. In that case, ${\\\\it l}$ is an upturned parabola. Then\\nthe following definitions of $\\\\sigma_\\\\mu$, the uncertainty on $\\\\mu_{best}$, \\nyield identical answers:\\n\\\\begin{itemize}\\n\\\\item{The RMS of the likelihood distribution.}\\n\\\\item{[$-\\\\frac{d^2 {\\\\it l}}{d \\\\mu^2}]^{-1/2}$. If you remember that \\nthe second derivative of the log likelihood function is involved because it \\ncontrols the width of the ${\\\\it l}$ distribution, a mneumonic helps \\nyou remember the formula for $\\\\sigma_\\\\mu$: Since $\\\\sigma_\\\\mu$ must have the \\nsame units as $\\\\mu$, the second derivative must appear to the power $-1/2$. But because the\\nlog of the likelihood has a maximum, the second derivative is negative, so the minus \\nsign is necessary before we take the square root.}\\n\\\\item{It is the distance in $\\\\mu$ from the maximum in order to decrease ${\\\\it l}$ by half a unit\\nfrom its maximum value. i.e.\\n\\\\begin{equation}\\n{\\\\it l} (\\\\mu_{best} + \\\\sigma_{\\\\mu}) = {\\\\it l}_{max} - 0.5 \\n\\\\end{equation}\\n}\\n\\\\end{itemize} \\nIn situations where the likelihood is not Gaussian in shape, these three definitions no longer agree.\\nThe third one is most commonly used in that case. Now the upper and lower ends of the intervals can \\nbe asymmetric with respect to the central value. It is a mistake to believe that this method \\nprovides intervals which have a $68\\\\parameter\\\\footnote{Unfortunately, this incorrect statement occurs in my book\\\\cite{LL_book}. It is \\ncorrected in a separate update\\\\cite{LL_book_update}.}.\\nSymmetric uncertainties are easier to work with than asymmetric ones. It is thus sometimes better to quote the \\nuncertainty on a function of the first variable you think of. For example, for a charged particle in a magnetic field,\\nthe reciprocal of the momentum has a nearly symmetric uncertainty. Especially for high\\nmomentum tracks, the upper uncertainty on the momentum can be much larger than the lower one \\ne.g. $1.0\\\\ ^{+1.5}_{-0.4}$ TeV.\\n\\\\subsection{Coverage}\\nAn important feature of any statistical method for estimating a range for some parameter $\\\\mu$ at a \\nspecified confidence level $\\\\alpha$ is its coverage $C$. If the procedure is applied many times, \\nthese ranges will vary because of statistical fluctuations in the observed data. Then $C$ is defined as\\nthe fraction of ranges which contain the true value $\\\\mu_{true}$; it can vary with $\\\\mu_{true}$. \\nIt is very\\nimportant to realise that coverage is a property of the {\\\\bf statistical procedure} and does not apply\\nto your particular measurement. An ideal plot of coverage as a function of $\\\\mu$ would have $C$ constant \\nat its nominal value $\\\\alpha$. For a Poisson counting experiment, figure \\\\ref{fig:PoissonCoverage} shows $C$ as a \\nfunction of the Poisson parameter $\\\\mu$, when the observed number of counts $n$ is used to determine a range \\nfor $\\\\mu$ via the change in log-likelihood being 0.5. The coverage is far from constant at small $\\\\mu$.\\nIf $C$ is smaller than $\\\\alpha$, this is known as undercoverage. Certainly frequentists would regard this \\nas unfortunate; it means that people reading an article containing parameters determined this way are \\nlikely to place more than justified reliance on the quoted range. Methods using the Neyman construction \\nto determine parameter ranges by construction do not have undercoverage. \\nCoverage involves a statement about $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$. This is to be interpreted as a\\nprobability statement about how often the ranges $\\\\mu_l$ to $\\\\mu_u$ contain the (unknown but constant) true\\nvalue $\\\\mu_{true}$. This is a frequentist statement; Bayesians do not want to consider the ensemble of possible\\nresults if the measurement procedure were to be repeated. Thus Bayesians would regard the statement\\nabout $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$ as describing what fraction of their estimated \\nposterior probability density for $\\\\mu_{true}$ would be \\nbetween the fixed values $\\\\mu_l$ and $\\\\mu_u$, derived from their actual measurement.\\n\\\\subsection{More than one parameter}\\nFor the case of just one parameter $\\\\mu$, the likelihood best estimate $\\\\hat{\\\\mu}$ is given \\nby the value of $\\\\mu$ which maximises $L$. Its uncertainty $\\\\sigma_\\\\mu$ is determined either from \\n\\\\begin{equation}\\n1/\\\\sigma_\\\\mu^2 = -d^2\\\\ln L/d\\\\mu^2 ;\\n\\\\end{equation} \\nof by finding how far $\\\\hat{\\\\mu}$ would have to be changed in order to reduce $\\\\ln L$ by 0.5.\\nWhen we have two or more parameters $\\\\beta_i$ the rule for finding the best estimates $\\\\hat{\\\\beta_i}$\\nis still to maximise $L$.\\nFor the uncertainties and their correlations, the generalisation of equation \\\\ref{error} is to construct\\nthe inverse covariance matrix ${\\\\bf M}$, whose elements are given by\\n\\\\begin{equation}\\nM_{ij} = -\\\\frac{\\\\partial^2 \\\\ln L} {\\\\partial \\\\beta_i\\\\ \\\\partial \\\\beta_j} \\n\\\\end{equation} \\nThen the inverse of $\\\\bf{M}$ is the covariance matrix, whose diagonal elements are the variances of $\\\\beta_i$,\\nand whose off-diagonal ones are the covariances.\\nAlternatively (and more common in practice), the uncertainty on a specific $\\\\beta_j$ can be obtained \\nby using the profile likelihood $L_{prof}(\\\\beta_j)$.\\nThis is the likelihood as a function of the specific $\\\\beta_j$, where for each value of $\\\\beta_j,\\\\ L$ has been remaximised\\nwith respect to all the other $\\\\beta$. Then $L_{prof}(\\\\beta_j)$ is used with the `reduce $\\\\ln L_{prof}$ = 0.5' rule\\nto obtain the uncertainty on $\\\\beta_j$. This is equivalent to determining the contour in $\\\\beta$-space where \\n$\\\\ln L = \\\\ln L_{max} - 0.5$, and finding the values $\\\\beta_{j,1}$ and $\\\\beta_{j,2}$ on the contour which are \\nfurthest from $\\\\hat{\\\\beta_j.}$ Then the (probably asymmetric) upper and lower uncertainties on $\\\\beta_j$ are \\ngiven by $\\\\beta_{j,2}- \\\\hat{\\\\beta_j}$ and $\\\\hat{\\\\beta_j} - \\\\beta_{j,1}$ respectively.\\nBecause these are likelihood methods of obtaining the intervals, these estimates of uncertainities provide only\\n{\\\\bf nominal} regions of 68\\\\the region within \\nthe contour described in the previous paragraph for the multidimensional $\\\\beta$ space will have less than 68\\\\overage. To achieve that, the $`0.5'$ in the rule for how much $\\\\ln L$ has to be reduced from its maximum \\nmust be replaced by a larger number, whose value depends on the dimensionality of $\\\\beta$.\\n\", '\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Frequentist upper limit}\\nIn practical terms, the simplest approach for making estimates is to consider single-channel experiments, where we have an observed value ($n$), an expected number of background events ($b$), and an expected number of signal events ($s$). Since the measurement process involves counting events in the channel, we will model it using a Poisson distribution.\\nFor $H_{0}$, the mean of the distribution will be $\\\\lambda = b$, meaning that the observation is explained solely by background events. For $H_{1}$, the mean is given by $\\\\lambda(\\\\mu) = \\\\mu s + b$, where $\\\\mu$ is known as the signal strength and measures the agreement between $H_{1}$ and the observation. In this way, to find the confidence level of both hypotheses with respect to the observation, we use the cumulative Poisson distribution, which is given by:\\n\\\\begin{equation}\\nF_{P}(\\\\lambda) = \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda(\\\\mu))^{i}e^{-\\\\lambda(\\\\mu)}}{(i)!}.\\n\\\\end{equation}\\nWhere $\\\\mu = 0$ for $H_{0}$ and $\\\\mu = 1$ for $H_{1}$. This cumulative distribution has a direct relation with the cumulative $\\\\chi^2(x; k)$ distribution, with $k = 2(n + 1)$ degrees of freedom and $x = 2\\\\lambda$ (see Appendix \\\\ref{sec:AppendixA}). Therefore, the confidence level $CL = 0.95$ is given by~\\\\cite{lista2016practical}:\\n\\\\begin{eqnarray}\\n1-\\\\alpha & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) {} \\\\nonumber \\\\\\\\ \\n0.95 & = & 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)). {}\\n\\\\end{eqnarray}\\nThe signal strength $\\\\mu$ allows us to statistically assess the degree of agreement between the alternative hypothesis and the observation. In general, we can find the upper limit of the signal strength, i.e., the point at which the alternative hypothesis can no longer explain the observation. This means excluding all models with $\\\\mu > \\\\mu_{up}$ at a $3\\\\sigma$ confidence level. By inverting the relation~(\\\\ref{eq:4}), it is possible to calculate the upper limit for all new theories, given the observation $n$, an expected number of background events $b$, and new physics events $s$. Thus, we obtain:\\n\\\\begin{equation}\\n\\\\mu_{up}= \\\\frac{1}{s} (\\\\frac{1}{2}F^{-1}_{\\\\chi^2}(1-\\\\alpha;k=2(n+1)) - b).\\n\\\\end{equation}\\nWhere $\\\\mu_{up}$ is the upper limit at $95\\\\\\nIn the general case, when the number of observations is different from 0, the upper bounds of the model are determined by varying both the observations and the expected background~\\\\cite{lista2016practical,cranmer2015practical}. Figure~[\\\\ref{fig:4}] illustrates the behavior of the upper bound $\\\\mu_{up}$ for a given $n$, as a function of the expected nuisance level $b$~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Frequentist/FrequentistUpperLimitScan.ipynb}{Source code}}.\\nIt is important to note that as the number of observations decreases, the upper bound value as a function of the background component tends to negative values. A value of $\\\\mu_{up} < 0$ implies the exclusion of the null hypothesis in the absence of signal events $s$. This scenario presents a contradiction, as it represents a non-physical condition known as the coverage problem of the statistical estimator. To address this issue, variants of the frequentist method have been proposed that correct the coverage problem while adhering to solid probabilistic principles. These approaches include Bayesian methods and a modified version of the frequentist method~\\\\cite{lista2016practical,cranmer2015practical,conway2005calculation}.\\n', \"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Excluded and allowed regions as confidence intervals}\\nOften we consider a new physics model that is parametrized by theoretical parameters. For instance, the mass or coupling of a new particle. In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data. Figure~\\\\ref{fig:confidenceIntervals} shows two examples. Figure~\\\\ref{fig:confidenceIntervals}(a) shows an example with $\\\\vec\\\\alpha_{\\\\rm poi} = (\\\\sigma/\\\\sigma_{SM}, M_H)$, where $\\\\sigma/\\\\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model. All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\\\\$\\\\vec\\\\alpha_{\\\\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses. The blue ellipse `is the 68\\\\\\nIn a frequentist setting, these allowed regions are called \\\\textit{confidence intervals} or \\\\textit{confidence regions}, and the parameter points outside them are considered excluded. Associated with a confidence interval is a confidence level, i.e. the 95\\\\\\nHow can one possibly construct a confidence interval has the desired property, that it \\\\textit{covers} the true value with a specified probability, given that we don't know the true value? The procedure for building confidence intervals is called the Neyman Construction~\\\\cite{Neyman}, and it is based on `inverting' a series of hypothesis tests (as described in Sec.~\\\\ref{S:hypothesis test}). In particular, for each value of $\\\\vec\\\\alpha$ in the parameter space one performs a hypothesis test based on some test statistic where the null hypothesis is $\\\\vec\\\\alpha$. Note, that in this context, the null hypothesis is changing for each test and generally is not the background-only. If one wants a 95\\\\\\\\begin{equation}\\nI(\\\\data) = \\\\left\\\\{ \\\\vec\\\\alpha \\\\middle |\\\\, P(T(\\\\data)>k_\\\\alpha \\\\,|\\\\, \\\\vec\\\\alpha) < \\\\alpha \\\\right\\\\} \\\\;,\\n\\\\end{equation}\\nwhere the final $\\\\alpha$ and the subscript $k_\\\\alpha$ refer to the size of the test.\\nSince a hypothesis test with a size of 5\\\\\\\\begin{equation}\\n\\\\textrm{coverage}(\\\\vec\\\\alpha) = P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha) \\\\; .\\n\\\\end{equation}\\nThe equation above can easily be mis-interpreted as the probability the parameter is in a fixed interval $I$; but one must remember that in evaluating the probability above the data $\\\\data$, and, thus, the corresponding intervals produced by the procedure $I(\\\\data)$, are the random quantities. \\nNote, that coverage is a property that can be quantified for any procedure that produces the confidence intervals $I$. Intervals produced using the Neyman Construction procedure are said to ``cover by construction''; however, one can consider alternative procedures that may either under-cover or over-cover. Undercoverage means that \\\\mbox{$P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$} is smaller than desired and over-coverage means that $P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$ is larger than desired. Note that in general coverage depends on the assumed true value $\\\\vec\\\\alpha$.\\nSince one typically is only interested in forming confidence intervals on the parameters of interest, then one could use the supremum $p$-value of Eq.~\\\\ref{Eq:psup}. This procedure ensures that the coverage is at least the desired level, though for some values of $\\\\vec\\\\alpha$ it may over-cover (perhaps significantly). This procedure, which I call the `full construction', is also computationally very intensive when $\\\\vec\\\\alpha$ has many parameters as it require performing many hypothesis tests. In the naive approach where each $\\\\alpha_p$ is scanned in a regular grid, the number of parameter points tested grows exponentially in the number of parameters. There is an alternative approach, which I call the `profile construction'~\\\\cite{Feldman,Cranmer:2005hi}\\nand which statisticians call an `hybrid resampling technique'~ \\\\cite{Hybrid,Bodhi} that is approximate to the full construction, but typically has good coverage properties. We return to the procedures and properties for the different types of Neyman Constructions later.\\nFigure~\\\\ref{fig:NC_schematic} provides an overview of the classic Neyman construction corresponding to the left panel of Fig.~\\\\ref{fig:neyman}. The left panel of Fig.~\\\\ref{fig:neyman} is taken from the Feldman and Cousins's paper~\\\\cite{Feldman:1997qc} where the parameter of the model is denoted $\\\\mu$ instead of $\\\\theta$. For each value of the parameter $\\\\mu$, the acceptance region in $x$ is illustrated as a horizontal bar. Those regions are the ones that satisfy $T(\\\\data)<k_\\\\alpha$, and in the case of Feldman-Cousins the test statistic is the one of Eq.~\\\\ref{eq:tmu}. This presentation of the confidence belt works well for a simple model in which the data consists of a single measurement $\\\\data=\\\\{x\\\\}$. Once one has the confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$ simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the confidence belt.\\nUnfortunately, this convenient visualization doesn't generalize to complicated models with many channels or even a single channel marked Poisson model where $\\\\data=\\\\{x_1,\\\\dots,x_n\\\\}$. In those more complicated cases, the confidence belt can still be visualized where the observable $x$ is replaced with $T$, the test statistic itself. Thus, the boundary of the belt is given by $k_\\\\alpha$ vs. $\\\\mu$ as in the right panel of Figure~\\\\ref{fig:neyman}. The analog to the vertical line in the left panel is now a curve showing how the observed value of the test statistic depends on $\\\\mu$. The confidence interval still corresponds to the intersection of the observed test statistic curve and the confidence belt, which clearly satisfies $T(\\\\data)<k_\\\\alpha$. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\\nNote, a 95\\\\\\\\begin{equation}\\nP(\\\\alpha\\\\in I | \\\\data) = \\\\frac{ \\\\int_{I} \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha }{\\\\int \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha} \\\\;.\\n\\\\end{equation}\", '\\\\section{Inference}\\n\\\\subsection{Frequentist inference}\\nAssigning a probability level to an unknown parameter makes no sense in the frequentist approach\\nsince unknown parameters are not random variables.\\nA frequentist inference procedure should determine a central value and an uncertainty interval that depend\\non the observed measurements without introducing any subjective element.\\nSuch central value and interval extremes are random variables themselves.\\nThe function that returns the central value given an observed measurement is called {\\\\it estimator}.\\nThe parameter value provided by an estimator is also called {\\\\it best fit} value.\\nDifferent estimator choices are possible, the most frequently adopted is the {\\\\it maximum likelihood\\nestimator} because of its statistical properties discussed in Sec.~\\\\ref{sec:estimatorProperties}.\\nRepeating the experiment will result each time in a different data sample\\nand, for each data sample, the estimator returns a different central value $\\\\hat{\\\\theta}$.\\nAn uncertainty interval $[\\\\hat{\\\\theta} -\\\\delta, \\\\hat{\\\\theta} +\\\\delta]$ can be associated to\\nthe estimator value $\\\\hat{\\\\theta}$. In some cases, as for the Bayesian inference, an\\nasymmetric interval choice is also possible with frequentist inference:\\n$[\\\\hat{\\\\theta} -\\\\delta^-, \\\\hat{\\\\theta} +\\\\delta^+]$.\\nSome of the intervals obtained with this method contain the fixed and unknown true\\nvalue of $\\\\theta$, corresponding to a fraction equal to 68.3\\\\large number of experiments. This property is called {\\\\it coverage}.\\nThe simplest example of frequentist inference \\nassumes a Gaussian PDF (Eq.~(\\\\ref{eq:GaussianPDF})) with a known $\\\\sigma$ and an unknown $\\\\mu$.\\nA single experiment provides a measurement $x$, and we can estimate $\\\\mu$ as $\\\\hat{\\\\mu} = x$.\\nThe distribution of $\\\\hat{\\\\mu}$ is the original Gaussian because $\\\\hat{\\\\mu}$ is just equal to $x$.\\nA fraction of 68.3\\\\estimate $\\\\hat{\\\\mu}$ within: $\\\\mu - \\\\sigma < \\\\hat{\\\\mu} < \\\\mu + \\\\sigma$. This means that we can quote:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mu = x \\\\pm \\\\sigma\\\\,.\\n}\\n\\\\end{equation}\\n'}},\n",
       "       {'entity_name': 'supremum pvalue', 'entity_type': 'analysis_technique', 'description': 'A method used to ensure that the coverage of confidence intervals meets a desired level by considering the maximum p-value across multiple tests.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Excluded and allowed regions as confidence intervals}\\nOften we consider a new physics model that is parametrized by theoretical parameters. For instance, the mass or coupling of a new particle. In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data. Figure~\\\\ref{fig:confidenceIntervals} shows two examples. Figure~\\\\ref{fig:confidenceIntervals}(a) shows an example with $\\\\vec\\\\alpha_{\\\\rm poi} = (\\\\sigma/\\\\sigma_{SM}, M_H)$, where $\\\\sigma/\\\\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model. All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\\\\$\\\\vec\\\\alpha_{\\\\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses. The blue ellipse `is the 68\\\\\\nIn a frequentist setting, these allowed regions are called \\\\textit{confidence intervals} or \\\\textit{confidence regions}, and the parameter points outside them are considered excluded. Associated with a confidence interval is a confidence level, i.e. the 95\\\\\\nHow can one possibly construct a confidence interval has the desired property, that it \\\\textit{covers} the true value with a specified probability, given that we don't know the true value? The procedure for building confidence intervals is called the Neyman Construction~\\\\cite{Neyman}, and it is based on `inverting' a series of hypothesis tests (as described in Sec.~\\\\ref{S:hypothesis test}). In particular, for each value of $\\\\vec\\\\alpha$ in the parameter space one performs a hypothesis test based on some test statistic where the null hypothesis is $\\\\vec\\\\alpha$. Note, that in this context, the null hypothesis is changing for each test and generally is not the background-only. If one wants a 95\\\\\\\\begin{equation}\\nI(\\\\data) = \\\\left\\\\{ \\\\vec\\\\alpha \\\\middle |\\\\, P(T(\\\\data)>k_\\\\alpha \\\\,|\\\\, \\\\vec\\\\alpha) < \\\\alpha \\\\right\\\\} \\\\;,\\n\\\\end{equation}\\nwhere the final $\\\\alpha$ and the subscript $k_\\\\alpha$ refer to the size of the test.\\nSince a hypothesis test with a size of 5\\\\\\\\begin{equation}\\n\\\\textrm{coverage}(\\\\vec\\\\alpha) = P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha) \\\\; .\\n\\\\end{equation}\\nThe equation above can easily be mis-interpreted as the probability the parameter is in a fixed interval $I$; but one must remember that in evaluating the probability above the data $\\\\data$, and, thus, the corresponding intervals produced by the procedure $I(\\\\data)$, are the random quantities. \\nNote, that coverage is a property that can be quantified for any procedure that produces the confidence intervals $I$. Intervals produced using the Neyman Construction procedure are said to ``cover by construction''; however, one can consider alternative procedures that may either under-cover or over-cover. Undercoverage means that \\\\mbox{$P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$} is smaller than desired and over-coverage means that $P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$ is larger than desired. Note that in general coverage depends on the assumed true value $\\\\vec\\\\alpha$.\\nSince one typically is only interested in forming confidence intervals on the parameters of interest, then one could use the supremum $p$-value of Eq.~\\\\ref{Eq:psup}. This procedure ensures that the coverage is at least the desired level, though for some values of $\\\\vec\\\\alpha$ it may over-cover (perhaps significantly). This procedure, which I call the `full construction', is also computationally very intensive when $\\\\vec\\\\alpha$ has many parameters as it require performing many hypothesis tests. In the naive approach where each $\\\\alpha_p$ is scanned in a regular grid, the number of parameter points tested grows exponentially in the number of parameters. There is an alternative approach, which I call the `profile construction'~\\\\cite{Feldman,Cranmer:2005hi}\\nand which statisticians call an `hybrid resampling technique'~ \\\\cite{Hybrid,Bodhi} that is approximate to the full construction, but typically has good coverage properties. We return to the procedures and properties for the different types of Neyman Constructions later.\\nFigure~\\\\ref{fig:NC_schematic} provides an overview of the classic Neyman construction corresponding to the left panel of Fig.~\\\\ref{fig:neyman}. The left panel of Fig.~\\\\ref{fig:neyman} is taken from the Feldman and Cousins's paper~\\\\cite{Feldman:1997qc} where the parameter of the model is denoted $\\\\mu$ instead of $\\\\theta$. For each value of the parameter $\\\\mu$, the acceptance region in $x$ is illustrated as a horizontal bar. Those regions are the ones that satisfy $T(\\\\data)<k_\\\\alpha$, and in the case of Feldman-Cousins the test statistic is the one of Eq.~\\\\ref{eq:tmu}. This presentation of the confidence belt works well for a simple model in which the data consists of a single measurement $\\\\data=\\\\{x\\\\}$. Once one has the confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$ simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the confidence belt.\\nUnfortunately, this convenient visualization doesn't generalize to complicated models with many channels or even a single channel marked Poisson model where $\\\\data=\\\\{x_1,\\\\dots,x_n\\\\}$. In those more complicated cases, the confidence belt can still be visualized where the observable $x$ is replaced with $T$, the test statistic itself. Thus, the boundary of the belt is given by $k_\\\\alpha$ vs. $\\\\mu$ as in the right panel of Figure~\\\\ref{fig:neyman}. The analog to the vertical line in the left panel is now a curve showing how the observed value of the test statistic depends on $\\\\mu$. The confidence interval still corresponds to the intersection of the observed test statistic curve and the confidence belt, which clearly satisfies $T(\\\\data)<k_\\\\alpha$. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\\nNote, a 95\\\\\\\\begin{equation}\\nP(\\\\alpha\\\\in I | \\\\data) = \\\\frac{ \\\\int_{I} \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha }{\\\\int \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha} \\\\;.\\n\\\\end{equation}\"}},\n",
       "       {'entity_name': 'profile construction', 'entity_type': 'analysis_technique', 'description': 'An alternative approach to the Neyman Construction that approximates the full construction while typically maintaining good coverage properties, often involving hybrid resampling techniques.', 'relevant_passages': {\"\\\\section{Physics questions formulated in statistical language}\\n\\\\subsection{Excluded and allowed regions as confidence intervals}\\nOften we consider a new physics model that is parametrized by theoretical parameters. For instance, the mass or coupling of a new particle. In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data. Figure~\\\\ref{fig:confidenceIntervals} shows two examples. Figure~\\\\ref{fig:confidenceIntervals}(a) shows an example with $\\\\vec\\\\alpha_{\\\\rm poi} = (\\\\sigma/\\\\sigma_{SM}, M_H)$, where $\\\\sigma/\\\\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model. All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\\\\$\\\\vec\\\\alpha_{\\\\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses. The blue ellipse `is the 68\\\\\\nIn a frequentist setting, these allowed regions are called \\\\textit{confidence intervals} or \\\\textit{confidence regions}, and the parameter points outside them are considered excluded. Associated with a confidence interval is a confidence level, i.e. the 95\\\\\\nHow can one possibly construct a confidence interval has the desired property, that it \\\\textit{covers} the true value with a specified probability, given that we don't know the true value? The procedure for building confidence intervals is called the Neyman Construction~\\\\cite{Neyman}, and it is based on `inverting' a series of hypothesis tests (as described in Sec.~\\\\ref{S:hypothesis test}). In particular, for each value of $\\\\vec\\\\alpha$ in the parameter space one performs a hypothesis test based on some test statistic where the null hypothesis is $\\\\vec\\\\alpha$. Note, that in this context, the null hypothesis is changing for each test and generally is not the background-only. If one wants a 95\\\\\\\\begin{equation}\\nI(\\\\data) = \\\\left\\\\{ \\\\vec\\\\alpha \\\\middle |\\\\, P(T(\\\\data)>k_\\\\alpha \\\\,|\\\\, \\\\vec\\\\alpha) < \\\\alpha \\\\right\\\\} \\\\;,\\n\\\\end{equation}\\nwhere the final $\\\\alpha$ and the subscript $k_\\\\alpha$ refer to the size of the test.\\nSince a hypothesis test with a size of 5\\\\\\\\begin{equation}\\n\\\\textrm{coverage}(\\\\vec\\\\alpha) = P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha) \\\\; .\\n\\\\end{equation}\\nThe equation above can easily be mis-interpreted as the probability the parameter is in a fixed interval $I$; but one must remember that in evaluating the probability above the data $\\\\data$, and, thus, the corresponding intervals produced by the procedure $I(\\\\data)$, are the random quantities. \\nNote, that coverage is a property that can be quantified for any procedure that produces the confidence intervals $I$. Intervals produced using the Neyman Construction procedure are said to ``cover by construction''; however, one can consider alternative procedures that may either under-cover or over-cover. Undercoverage means that \\\\mbox{$P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$} is smaller than desired and over-coverage means that $P(\\\\vec\\\\alpha \\\\in I\\\\, |\\\\, \\\\vec\\\\alpha)$ is larger than desired. Note that in general coverage depends on the assumed true value $\\\\vec\\\\alpha$.\\nSince one typically is only interested in forming confidence intervals on the parameters of interest, then one could use the supremum $p$-value of Eq.~\\\\ref{Eq:psup}. This procedure ensures that the coverage is at least the desired level, though for some values of $\\\\vec\\\\alpha$ it may over-cover (perhaps significantly). This procedure, which I call the `full construction', is also computationally very intensive when $\\\\vec\\\\alpha$ has many parameters as it require performing many hypothesis tests. In the naive approach where each $\\\\alpha_p$ is scanned in a regular grid, the number of parameter points tested grows exponentially in the number of parameters. There is an alternative approach, which I call the `profile construction'~\\\\cite{Feldman,Cranmer:2005hi}\\nand which statisticians call an `hybrid resampling technique'~ \\\\cite{Hybrid,Bodhi} that is approximate to the full construction, but typically has good coverage properties. We return to the procedures and properties for the different types of Neyman Constructions later.\\nFigure~\\\\ref{fig:NC_schematic} provides an overview of the classic Neyman construction corresponding to the left panel of Fig.~\\\\ref{fig:neyman}. The left panel of Fig.~\\\\ref{fig:neyman} is taken from the Feldman and Cousins's paper~\\\\cite{Feldman:1997qc} where the parameter of the model is denoted $\\\\mu$ instead of $\\\\theta$. For each value of the parameter $\\\\mu$, the acceptance region in $x$ is illustrated as a horizontal bar. Those regions are the ones that satisfy $T(\\\\data)<k_\\\\alpha$, and in the case of Feldman-Cousins the test statistic is the one of Eq.~\\\\ref{eq:tmu}. This presentation of the confidence belt works well for a simple model in which the data consists of a single measurement $\\\\data=\\\\{x\\\\}$. Once one has the confidence belt, then one can immediately find the confidence interval for a particular measurement of $x$ simply by taking drawing a vertical line for the measured value of $x$ and finding the intersection with the confidence belt.\\nUnfortunately, this convenient visualization doesn't generalize to complicated models with many channels or even a single channel marked Poisson model where $\\\\data=\\\\{x_1,\\\\dots,x_n\\\\}$. In those more complicated cases, the confidence belt can still be visualized where the observable $x$ is replaced with $T$, the test statistic itself. Thus, the boundary of the belt is given by $k_\\\\alpha$ vs. $\\\\mu$ as in the right panel of Figure~\\\\ref{fig:neyman}. The analog to the vertical line in the left panel is now a curve showing how the observed value of the test statistic depends on $\\\\mu$. The confidence interval still corresponds to the intersection of the observed test statistic curve and the confidence belt, which clearly satisfies $T(\\\\data)<k_\\\\alpha$. For more complicated models with many parameters the confidence belt will have one axis for the test statistic and one axis for each model parameter.\\nNote, a 95\\\\\\\\begin{equation}\\nP(\\\\alpha\\\\in I | \\\\data) = \\\\frac{ \\\\int_{I} \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha }{\\\\int \\\\f(\\\\data | \\\\vec\\\\alpha) \\\\pi(\\\\vec\\\\alpha) d\\\\vec\\\\alpha} \\\\;.\\n\\\\end{equation}\"}},\n",
       "       {'entity_name': 'monte carlo methods', 'entity_type': 'analysis_technique', 'description': 'A set of statistical techniques used in particle physics to simulate the behavior of particles and interactions through random sampling. These methods allow for the modeling of complex systems, estimation of statistical uncertainties, evaluation of integrals, and assessment of statistical significance by generating pseudo-experiments and comparing observed data distributions with theoretical models.', 'relevant_passages': {\"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The look-elsewhere effect}\\nWhen searching for a signal peak on top of a background that is smoothly distributed over a wide range,\\none can either know the position of the peak or not.\\nOne example in which the peak position is known is the\\nsearch for a rare decay of a known particle, like $\\\\mathrm{B}_{\\\\mathrm{s}}\\\\rightarrow\\\\mu^+\\\\mu^-$.\\nA case when the position was not know was the search for the Higgs boson, whose mass is not\\nprediceted by theory.\\nIn a case like the decay of a particle of known mass, it's easy to compute the peak significance:\\nfrom the distribution of the test statistic $f(q)$ computed assuming $\\\\mu=0$ (background only),\\ngiven the observed value of the test statistic $q^{\\\\mathrm{obs}}$, a $p$-value can be determined and then\\ntranslated into a significance level:\\n\\\\begin{equation}\\np = \\\\int_{q^{\\\\mathrm{obs}}}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q,\\\\quad Z = \\\\Phi^{-1}(1-p)\\\\,.\\n\\\\end{equation}\\nIn case, instead, the search is performed without knowing the position of the peak,\\nEq.~(\\\\ref{eq:localPval}) gives only a {\\\\it local $p$-value}, which means it reflects\\nthe probability that a background fluctuation {\\\\it at a given mass value $m$} gives a value\\nof the test statistic greater than the observed one:\\n\\\\begin{equation}\\np(m) = \\\\int_{q^{\\\\mathrm{obs}}(m)}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q\\\\,.\\n\\\\end{equation}\\nThe {\\\\it global $p$-value}, instead, should quantify the probability that a background\\nfluctuation {\\\\it at any mass value} gives a value of the test statistic greater than the\\nobserved one.\\nThe chance that an overfluctuation occurs for {\\\\it at least} one mass value increases with\\nthe size of the search range, and the magnitude of the effect depends on the resolution.\\nOne possibility to evaluate a global $p$-value is to let also $m$ fluctuate in the test statistic:\\n\\\\begin{equation}\\n\\\\hat{q} = -2\\\\ln \\\\frac{L(\\\\mu=0)}{L(\\\\hat{\\\\mu};\\\\hat{m})}\\\\,.\\n\\\\end{equation}\\nNote that in the numerator $L$ doesn't depend on $m$ for $\\\\mu=0$. This is a case\\nwhere Wilks' theorem doesn't apply, and no simple asymptotic approximations exist.\\nThe global $p$-value can be computed, in principle, as follows:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = \\\\int_{\\\\hat{q}^{\\\\mathrm{obs}}}^{+\\\\infty}f(\\\\hat{q}|\\\\mu=0)\\\\,\\\\mathrm{d}\\\\hat{q}_0\\\\,.\\n\\\\end{equation}\\nThe effect in practice can be evaluated with brute-force toy Monte Carlo:\\n\\\\begin{itemize}\\n\\\\item Produce a large number of pseudoexperiments simulating background-only samples.\\n\\\\item Find the maximum $\\\\hat{q}$ of the test statistic $q$ in the entire search range.\\n\\\\item Determine the distribution of $\\\\hat{q}$.\\n\\\\item Compute the global $p$-value as probability to have a value of $\\\\hat{q}$ greater than the observed one.\\n\\\\end{itemize}\\nThis procedure usually requires very large toy Monte Carlo samples in order to treat a discovery case:\\na $p$-value close to $3\\\\times 10^{−7}$ ($5\\\\sigma$ level) requires\\na sample significantly larger than $\\\\sim 10^7$ entries in order to\\ndetermine the $p$-value with small uncertainty.\\nAn asymptotic approximation for the global $p$-value is given by\\nthe following inequation~\\\\cite{lee_trial}~\\\\footnote{\\nIn case of a test statistic for discovery $q_0$ (Eq.~(\\\\ref{eq:tsfd})), the term $P(\\\\chi^2 > u)$\\nin Eq.~(\\\\ref{eq:leeasin}) achieves an extra factor 1/2, which is usually not be present\\nfor other test statistics.\\n}:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = P(\\\\hat{q}>u ) \\\\le \\\\left<N_u\\\\right> +\\nP(\\\\chi^2>u)\\\\,,\\n\\\\end{equation}\\nwhere $P(\\\\chi^2>u)$ is a standard $\\\\chi^2$ probability and\\n$\\\\left<N_u\\\\right>$ is the average number of {\\\\it upcrossings} of\\nthe test statistic, i.e.: the average number of times that\\nthe curve $q(m)$ crosses a given horizontal line at a level $u$ with a positive derivative,\\nas illustrated in Fig.~\\\\ref{fig:lee}.\\nThe number of upcrossings may be very small for some values of $u$,\\nbut an approximate scaling law exists and allows to perform the computation\\nat a more convenient level $u_0$:\\n\\\\begin{equation}\\n\\\\left<N_u\\\\right> = \\\\left<N_{u_0}\\\\right> e^{-{(u-u_0)}/{2}}\\\\,.\\n\\\\end{equation}\\nSo, $ \\\\left<N_{u_0}\\\\right>$ can be more conveniently evaluated using\\na reasonable number of toy Monte Carlo generations, then it can be extrapolated following\\nthe exponential scaling law.\\nNumerical comparisons of this approach with the full toy Monte Carlo\\nshow that good agreement is achieved for sufficiently\\nlarge number of observations.\\nIn case more parameters are estimated from data, e.g.: when searching for\\na new resonance whose mass and width are both unknown, the look-elsewere\\neffect can be addressed with an extension of the approach described above,\\nas detailed in Ref.~\\\\cite{leeND}.\\n\\\\addcontentsline{toc}{chapter}{Bibliography}\\n\\\\bibliographystyle{ieeetr}\\n\\\\bibliography{lista}\\n\\\\end{documen\", '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\nThe simulation narrative is probably the easiest to explain and produces statistical models with the strongest logical connection to physical theory being tested. We begin with an relation that every particle physicists should know for the rate of events expected from a specific physical process\\n\\\\begin{equation}\\n\\\\textrm{rate} = \\\\textrm{(flux)} \\\\times \\\\textrm{(cross section)} \\\\times\\\\textrm{(efficiency)} \\\\times \\\\textrm{(acceptance)} \\\\;,\\n\\\\end{equation}\\nwhere the cross section is predicted from the theory, the flux is controlled by the accelerator\\\\footnote{In some cases, like cosmic rays, the flux must be estimated since the accelerator is quite far away.}, and the efficiency and acceptance are properties of the detector and event selection criteria. It is worth noting that the equation above is actually a repackaging of a more fundamental relationship. In fact the fundamental quantity that is predicted from first principles in quantum theory is the \\\\textit{scattering probability} \\\\mbox{$P(i\\\\to f)=|\\\\langle i|f\\\\rangle|^2/ (\\\\langle i|i\\\\rangle\\\\langle f | f \\\\rangle)$} inside a box of size $V$ over some time interval $T$, which is then repackaged into the Lorentz invariant form above~\\\\cite{Sredniki}.\\nIn the simulation narrative the efficiency and acceptance are estimated with computer simulations of the detector. Typically, a large sample of events is generated using Monte Carlo techniques~\\\\cite{MonteCarlo}. The Monte Carlo sampling is performed separately for the hard (perturbative) interaction (e.g. \\\\texttt{MadGraph}), the parton shower and hadronization process (e.g. \\\\texttt{Pythia} and \\\\texttt{Herwig}), and the interaction of particles with the detector (e.g. \\\\texttt{Geant}). Note, the efficiency and acceptance depend on the physical process considered, and I will refer to each such process as a \\\\textit{sample} (in reference to the corresponding sample of events generated with Monte Carlo techniques).\\nTo simplify the notation, I will define the effective cross section, $\\\\sigma_{\\\\rm eff.}$ to be the product of the total cross section, efficiency, and acceptance. Thus, the total number of events expected to be selected for a given scattering process, $\\\\nu$, is the product of the time-integrated flux or time-integrated luminosity, $\\\\lambda$, and the effective cross section\\n\\\\begin{equation}\\n\\\\nu = \\\\lambda \\\\sigma_{\\\\rm eff.}\\\\;.\\n\\\\end{equation}\\nI use $\\\\lambda$ here instead of the more common $L$ to avoid confusion with the likelihood function and because when we incorporate uncertainty on the time-integrated luminosity it will be a parameter of the model for which I have chosen to use greek letters. \\nIf we did not need to worry about detector effects and we could measure the final state perfectly, then the distribution for any observable $x$ would be given by\\n\\\\begin{equation}\\n\\\\textrm{(idealized)}\\\\hspace{2em}f(x) = \\\\frac{1}{\\\\sigma_{\\\\rm eff.}} \\\\frac{d\\\\sigma_{\\\\rm eff.}}{dx}\\\\;.\\\\hspace{5em}\\n\\\\end{equation}\\nOf course, we do need to worry about detector effects and we incorporate them with the detector simulation discussed above. From the Monte Carlo sample of events\\\\footnote{Here I only consider unweighted Monte Carlo samples, but the discussion below can be generalized for weighted Monte Carlo samples.} $\\\\{x_1, \\\\dots, x_N\\\\}$ we can estimate the underlying distribution $f(x)$ simply by creating a histogram. If we want we can write the histogram based on $B$ bins centered at $x_b$ with bin width $w_b$ explicitly as\\n\\\\begin{equation}\\n\\\\textrm{(histogram)} \\\\hspace{2em}f(x) \\\\approx h(x) = \\\\sum_{i=1}^N \\\\sum_{b=1}^B \\\\frac{ \\\\theta(|x_i-x_b|/w_b) }{N} \\\\frac{\\\\theta(|x -x_b|/w_b)}{w_b}\\\\;, \\\\end{equation}\\nwhere the first Heaviside function accumulates simulated events in the bin and the second selects the bin containing the value of $x$ in question. Histograms are the most common way to estimate a probability density function based on a finite sample, but there are other possibilities. The downsides of histograms as an estimate for the distribution $f(x)$ is that they are discontinuous and have dependence on the location of the bin boundaries. A particularly nice alternative is called kernel estimation~\\\\cite{Cranmer:2000du}. In this approach, one places a kernel of probability $K(x)$ centered around each event in the sample:\\n\\\\begin{equation} \\n\\\\textrm{(kernel estimate)}\\\\hspace{2em}f(x) \\\\approx \\\\hat{f}_0(x) = \\\\frac{1}{N} \\\\sum_{i=1}^N K\\\\left( \\\\frac{x-x_i}{h} \\\\right)\\\\;.\\\\hspace{1em}\\n\\\\end{equation}\\nThe most common choice of the kernel is a Gaussian distribution, and there are results for the optimal width of the kernel $h$. Equation~\\\\ref{eq:keys} is referred to as the fixed kernel estimate since $h$ is common for all the events in the sample. A second order estimate or adaptive kernel estimation provides better performance when the distribution is multimodal or has both narrow and wide features~\\\\cite{Cranmer:2000du}.\\n', '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Incorporating Monte Carlo statistical uncertainty on the histogram templates}\\nThe histogram based approach described above are based Monte Carlo simulations of full detector simulation. These simulations are very computationally intensive and often the histograms are sparsely populated. In this case the histograms are not good descriptions of the underlying distribution, but are estimates of that distribution with some statistical uncertainty. Barlow and Beeston outlined a treatment of this situation in which each bin of each sample is given a nuisance parameter for the true rate, which is then fit using both the data measurement and the Monte Carlo estimate~\\\\cite{Barlow:1993dm}. This approach would lead to several hundred nuisance parameters in the current analysis. Instead, the \\\\texttt{HistFactory} employs a lighter weight version in which there is only one nuisance parameter per bin associated with the total Monte Carlo estimate and the total statistical uncertainty in that bin. If we focus on an individual bin with index $b$ the contribution to the full statistical model is the factor\\n\\\\begin{equation}\\n\\\\Pois(n_b | \\\\nu_b(\\\\vec\\\\alpha) + \\\\gamma_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)) \\\\, \\\\Pois(m_b | \\\\gamma_b \\\\tau_b) \\\\;,\\n\\\\end{equation}\\nwhere $n_b$ is the number of events observed in the bin, $\\\\nu_b(\\\\vec\\\\alpha)$ is the number of events expected in the bin where Monte Carlo statistical uncertainties need not be included (either because the estimate is data driven or because the Monte Carlo sample is sufficiently large), $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)$ is the number of events estimated using Monte Carlo techniques where the statistical uncertainty needs to be taken into account. Both expectations include the dependence on the parameters $\\\\vec\\\\alpha$. The factor $\\\\gamma_b$ is the nuisance parameter reflecting that the true rate may differ from the Monte Carlo estimate $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) $ by some amount. If the total statistical uncertainty is $\\\\delta_b$, then the relative statistical uncertainty is given by $\\\\nu_b^{\\\\rm MC}/\\\\delta_b$. This corresponds to a total Monte Carlo sample in that bin of size $m_b = (\\\\delta_b/\\\\nu_b^{\\\\rm MC})^2$. Treating the Monte Carlo estimate as an auxiliary measurement, we arrive at a Poisson constraint term $ \\\\Pois(m_b | \\\\gamma_b \\\\tau_b)$, where $m_b$ would fluctuate about $\\\\gamma_b \\\\tau_b$ if we generated a new Monte Carlo sample. Since we have scaled $\\\\gamma$ to be a factor about 1, then we also have $\\\\tau_b=(\\\\nu_b^{\\\\rm MC}/\\\\delta_b)^2$; however, $\\\\tau_b$ is treated as a fixed constant and does not fluctuate when generating ensembles of pseudo-experiments.\\nIt is worth noting that the conditional maximum likelihood estimate $\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha)$ can be solved analytically with a simple quadratic expression.\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha) = \\\\frac{-B + \\\\sqrt{B^2 - 4 AC}}{2A} \\\\;,\\n\\\\end{equation}\\nwith\\n\\\\begin{equation}\\nA = \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)^2 + \\\\tau_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nB= \\\\nu_b(\\\\vec\\\\alpha) \\\\tau + \\\\nu_b(\\\\vec\\\\alpha) \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - n_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - m_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nC= m_b \\\\nu_b(\\\\vec\\\\alpha) \\\\;.\\n\\\\end{equation}\\nIn a Bayesian technique with a flat prior on $\\\\gamma_b$, the posterior distribution is a gamma distribution. Similarly, the distribution of $\\\\hat\\\\gamma_b$ will take on a skew distribution with an envelope similar to the gamma distribution, but with features reflecting the discrete values of $m_b$. Because the maximum likelihood estimate of $\\\\gamma_b$ will also depend on $n_b$ and $\\\\hat{\\\\vec\\\\alpha}$, the features from the discrete values of $m_b$ will be smeared. This effect will be more noticeable for large statistical uncertainties where $\\\\tau_b$ is small and the distribution of $\\\\hat\\\\gamma_b$ will have several small peaks. For smaller statistical uncertainties where $\\\\tau_b$ is large the distribution of $\\\\hat\\\\gamma_b$ will be approximately Gaussian.', '\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\nSearching for physics beyond the Standard Model is one of the most important aspects of the physics program at the Large Hadron Collider (LHC). Since the start of proton-proton collisions at the LHC in 2011, the ATLAS~\\\\cite{ATLAS} and CMS~\\\\cite{CMS} Collaborations have derived stringent bounds on a range of new physics signatures, pushing the allowed mass range for many postulated new particles far into the TeV scale. While it is possible that these particles have yet to be observed because they are too heavy to be produced at the LHC, or have to small cross section to be detected with the current data size, it could also be that new particles are kinematically accessible and produced at observable rates, but our current methods of detection prevent their discovery.\\nSearches for new physics processes at particle colliders are usually performed as \\\\textit{blind searches}. Such searches proceed by defining a region of interest in the parameter space, using simulated data of the signal and the Standard Model background processes in order to enhance the data purity. The data is only looked at in the very end where it is tested for the presence of signal through a simultaneous fit of the signal and background probability distributions, hoping to extract a non-zero signal component.\\nHundreds of such searches have been performed for hundreds of different potential new particles, but thus far none have been discovered. Despite this, there are still regions of the data that have not yet been probed for the presence of a signal. This has led to an increased interest in more \\\\textit{model-agnostic} search strategies. Model-independent searches is nothing new in high energy particle physics, and strategies relying less on a signal hypothesis have been devised and utilized~\\\\cite{D0:2000vuh,H1:2008aak,H1:2004rlm,Cranmer:823591,CDF:2007iou,CDF:2007ykt,CDF:2008voc,CMS-PAS-EXO-14-016,CMS-PAS-EXO-10-021,CMS-PAS-EXO-19-008,CMS:2020zjg,ATLAS:2018zdn,ATLAS-CONF-2014-006,ATLAS-CONF-2012-107,ATLAS:2020iwa}.\\nThese mainly take advantage of Monte Carlo simulation, and use this to compare distributions in the observed data to simulation across several observables and many histogram bins. The drawback of this methodology is that one needs to rely on accurate simulation, and also that, due to the vast size of the parameter space being searched, an observation that appears statistically significant could potentially be the result of a statistical fluctuation.\\nIn the following, we discuss machine learning techniques which mitigate some of these challenges and have the potential to improve and extend model-independent searches.\\n', \"\\\\section{Upper Limits using the profile binned likelihood}\\nThe profile binned likelihood method is chosen by particle physics collaborations to present phenomenology studies and experimental analyses. As shown above, this method is based on estimating the signal confidence level through a fully frequentist approach focused on parameter estimation~\\\\cite{lista2016practical,barlow2002systematic}. To include systematic effects, the likelihood function is extended with appropriate distribution functions to describe efficiency effects with a given width \\\\( \\\\sigma \\\\). This methodology requires maximizing the likelihood function and obtaining the background profile as a function of the signal strength \\\\( \\\\mu \\\\). The improvement lies in replacing the normalization of the posterior distribution with a multivariable optimization problem, which is computationally less costly and leads to limits that allow for better model exclusion. The statistical estimator is given by~\\\\cite{conway2005calculation,cms2012observation,atlas2012observation}:\\n\\\\begin{equation}\\nq_{\\\\mu} = - 2 ln \\\\bigg( \\\\frac{\\\\mathcal{L}(\\\\mu, \\\\hat{\\\\hat{b}}(\\\\mu) )}{ \\\\mathcal{L}(\\\\hat{\\\\mu},\\\\hat{b}) } \\\\bigg).\\n\\\\end{equation}\\nwhere \\\\( \\\\hat{\\\\mu} \\\\) and \\\\( \\\\hat{b} \\\\) are the unconditional maximum likelihood estimators, and \\\\( \\\\hat{\\\\hat{b}}(\\\\mu) \\\\) is the conditional maximum likelihood estimator. Frequentist limits are generally less restrictive and allow for exploring regions of significance while adequately accounting for systematic effects. Similar to the Bayesian approach, maximizing the likelihood as a function of \\\\( \\\\mu \\\\) enables finding the profile likelihood that propagates the effect of the background parameters \\\\( \\\\epsilon \\\\).\\nSystematic effects lead to higher upper limits, which restrict the model exclusion power and experimental sensitivity. For example, in a single-channel experiment with \\\\( n=105 \\\\), \\\\( b=100 \\\\), and \\\\( s=10 \\\\), the \\\\texttt{optimize} package is used to find the best-fit parameters, and Monte Carlo methods are applied to sample the estimator \\\\( q_{\\\\mu} \\\\). The extended likelihood function is given by:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu, \\\\epsilon) = \\\\frac{ e^{ -(\\\\mu s + \\\\epsilon b) } (\\\\mu s + \\\\epsilon b)^{n} }{n!} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^{2}} } e^{ -\\\\frac{(1-\\\\epsilon)^{2}}{2\\\\sigma^{2}} },\\n\\\\end{equation}\\nwhere maximizing the likelihood function for the efficiency \\\\( \\\\epsilon \\\\) yields the conditional nuisance estimator \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\):\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) = \\\\frac{1}{2b} \\\\bigg[ ( b -\\\\mu s - \\\\sigma^{2}b^{2}) + \\\\sqrt{ (b + \\\\mu s - \\\\sigma^{2}b^{2})^{2} + 4 b^{2} \\\\sigma^{2}n} \\\\bigg]. \\n\\\\end{equation}\\nFor the single-channel experiment, Figure~[\\\\ref{fig:22}] shows the profile of the nuisance parameter for various values of \\\\( \\\\mu \\\\). The maximum value of the likelihood function shifts depending on the signal strength \\\\( \\\\mu \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/ProfileLikelihood/ProfileLikelihoodNuissance.ipynb}{Source code}}. The right plot shows the maximum likelihood estimate \\\\( \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu) \\\\) using both Equation~(\\\\ref{eq:profile}) and the optimization package. This result indicates the maximal behavior of the uncertainty for each value of the signal strength \\\\( \\\\mu \\\\), ensuring model exclusion as restrictive as allowed by the uncertainty \\\\( \\\\sigma \\\\). More generally, the multi-channel case requires a fully numerical procedure to find the profiles of the nuisance parameters and \\\\( q_{\\\\mu} \\\\)~\\\\cite{lista2016practical,cranmer2015practical}.\\nTo illustrate the behavior of the estimator \\\\( q_{\\\\mu} \\\\) as a function of the systematic uncertainty \\\\( \\\\sigma \\\\), a sweep over the signal strength \\\\( \\\\mu \\\\) is performed while optimizing the estimator for the current values of the observation, background component, and signal events. Figure~[\\\\ref{fig:23}] shows the profile binned likelihood for the single-channel experiment as a function of the width of the systematic uncertainty. A larger uncertainty leads to a higher upper limit, which can be quantified using Wald's approximation \\\\( Z(3\\\\sigma) = \\\\sqrt{q_{\\\\mu}} \\\\)~\\\\cite{cowan2011asymptotic}. \\nThe green dashed line represents the observed upper limit for each profile likelihood. For example, for \\\\( \\\\sigma = 0.20 \\\\), the observed upper limit is \\\\( \\\\mu_{up} \\\\approx 4.5 \\\\), which contrasts with the value obtained from the Bayesian approach (Table~[\\\\ref{tb:3}], \\\\( \\\\mu_{up} = 4.91 \\\\)), a smaller value. The right plot represents the search for the p-value while fixing the value of the systematic uncertainty. Obtaining the pseudo-data requires generating an observation consistent with the background-only hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\), and for the signal + background hypothesis \\\\( n \\\\sim \\\\text{Pois}(\\\\mu s + \\\\epsilon b) \\\\) with \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\) in each observable channel. For each generated value of \\\\( n \\\\) across all channels, the optimization of \\\\( (\\\\hat{\\\\mu}, \\\\hat{\\\\epsilon}, \\\\hat{\\\\hat{\\\\epsilon}}(\\\\mu)) \\\\) is performed.\\nTable~[\\\\ref{tb:4}] shows the upper limit values using the grouped profile likelihood method for several values of \\\\( \\\\sigma \\\\). In comparison with the upper limits obtained by the Bayesian method, consistency is observed within the statistical confidence levels inherent to the sampling.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\n$\\\\sigma$ & Profile likelihood Ratio & MCMC algorithm \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{2}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 \\\\\\\\\\n0.10 & 3.52 & 3.31 \\\\\\\\\\n0.20 & 4.57 & 4.66 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nFor the multi-channel case, results are very close between the upper limits without uncertainty and those found using the profile likelihood method. This behavior is attributed to the combined uncertainty of the background across the 30 channels, which does not significantly affect the confidence in the signal strength, especially in the resonance region~\\\\cite{cms2022portrait,atlas2012observation,cowan2011asymptotic}. Table~[\\\\ref{tb:5}] shows the upper limit values for several mass points \\\\( m(H) \\\\) and uncertainty \\\\( \\\\sigma \\\\).\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{ccc}\\n\\\\hline\\nMass($H$)[GeV] & $\\\\sigma$ & Method: Profile likelihood ratio \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c}{} & $\\\\mu_{up}(95\\\\ 110 & 0.1 & 0.43 \\\\\\\\\\n110 & 0.2 & 0.45 \\\\\\\\\\n\\\\hline\\n124 & 0.1 & 1.45 \\\\\\\\\\n124 & 0.2 & 1.46 \\\\\\\\\\n\\\\hline\\n142 & 0.1 & 0.29 \\\\\\\\\\n142 & 0.2 & 0.31 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength for different mass points using the profile likelihood ratio at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\n\\\\subsection{Experimental sensitivity with systematic effects}\\nIn the case of determining the experimental sensitivity for a specific model $s$, the Asimov data $n = s + b$ are used, and a modification is applied to the statistical estimator $q_{\\\\mu}$~\\\\cite{lista2016practical}.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0.\\n\\\\end{cases}\\n\\\\end{equation}\\nIn the Wald approximation, the significance is approximately given by:\\n\\\\begin{equation}\\nZ_{0} \\\\approx \\\\sqrt{q_{0}}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:24}] shows the profile likelihood for various values of the systematic uncertainty $\\\\sigma$. It demonstrates how statistical significance is impacted by uncertainty. In general, greater uncertainty in the estimation of background events leads to a loss of sensitivity in a potential experimental analysis aiming to validate a new hypothetical model.\\nThere are alternative methods to establish statistical significance~\\\\cite{cowan2011asymptotic}, such as:\\n\\\\begin{equation}\\nZ_{0}(\\\\sigma) = \\\\frac{s}{\\\\sqrt{s+(1+\\\\sigma)b}}. \\n\\\\end{equation}\\nHowever, in general, experimental sensitivity is overestimated because the maximal information of $\\\\hat{\\\\hat{b}}$ is not fully captured in the profile likelihood. Table~[\\\\ref{tb:6}] shows the statistical significance for various values of systematic uncertainty $\\\\sigma$. This effect reduces experimental sensitivity and should be calculated using the profile binned likelihood method. Note that the approximation $Z_{0} = s / \\\\sqrt{s + b}$ is no longer valid due to the convolution effect of counting and efficiency distributions.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & $Z$ & $Z_{0}=s/\\\\sqrt{s+b}$ & $Z_{0}(\\\\sigma)$ \\\\\\\\\\n\\\\hline\\n0.05 & 0.878 & 0.932 & 0.931 \\\\\\\\\\n0.1 & 0.695 & 0.932 & 0.912 \\\\\\\\\\n0.2 & 0.443 & 0.932 & 0.877 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Statistical significance as a function of systematic uncertainty for the case of a single-channel experiment. Note how the approximation $Z_{0}$ becomes invalid even for $s \\\\ll b$.}\\n\\\\end{center}\\n\\\\end{table}\\nAs shown in Table~[\\\\ref{tb:6}], when there is a 20\\\\\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Ensemble of pseudo-experiments generated with ``Toy'' Monte Carlo}\\nThe $p$-values in the procedure described above require performing several integrals. In the case of the asymptotic approach, the distributions for $\\\\tilde q_\\\\mu$ and $\\\\tilde q_0$ are known and the integral is performed directly. When the distributions are not assumed to take on their asymptotic form, then they must be constructed using Monte Carlo methods. In the ``toy Monte Carlo'' approach one generates pseudo-experiments in which the number of events in each channel $n_c$, the values of the discriminating variables $\\\\{x_{ec}\\\\}$ for each of those events, and the auxiliary measurements (global observables) $a_p$ are all randomized according to $\\\\F_{\\\\rm tot}$. We denote the resulting data $\\\\data_{\\\\rm toy}$ and global observables $\\\\globs_{\\\\rm toy}$. By doing this several times one can build an ensemble of pseudo-experiments and evaluate the necessary integrals. Recall that Monte Carlo techniques can be viewed as a form of numerical integration.\\nThe fact that the auxiliary measurements $a_p$ are randomized is unfamiliar in particle physics. The more familiar approach for toy Monte Carlo is that the nuisance parameters are randomized. This requires a distribution for the nuisance parameters, and thus corresponds to a Bayesian treatment of the nuisance parameters. The resulting $p$-values are a hybrid Bayesian-Frequentist quantity with no consistent definition of probability. To maintain a strictly frequentist procedure, the corresponding operation is to randomize the auxiliary measurements. \\nWhile formally this procedure is well motivated, as physicists we also know that our models can have deficiencies and we should check that the distribution of the auxiliary measurements does not deviate too far from our expectations. In Section~\\\\ref{Sec:crossChecks} we show the distribution of the auxiliary measurements and the corresponding $\\\\hat{\\\\vec\\\\theta}$ from the toy Monte Carlo technique. \\nTechnically, the pseudo-experiments are generated with the \\\\texttt{RooStats} \\\\texttt{ToyMCSampler}, which is used by the higher-level tool \\\\texttt{FrequentistCalculator}, which is in turn used by \\\\texttt{HypoTestInverter}.\\n\", \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\"}},\n",
       "       {'entity_name': 'effective cross section', 'entity_type': 'statistics_concept', 'description': 'A quantity that represents the product of the total cross section, efficiency, and acceptance in a scattering process, used to predict the number of events expected to be selected.', 'relevant_passages': {'\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\nThe simulation narrative is probably the easiest to explain and produces statistical models with the strongest logical connection to physical theory being tested. We begin with an relation that every particle physicists should know for the rate of events expected from a specific physical process\\n\\\\begin{equation}\\n\\\\textrm{rate} = \\\\textrm{(flux)} \\\\times \\\\textrm{(cross section)} \\\\times\\\\textrm{(efficiency)} \\\\times \\\\textrm{(acceptance)} \\\\;,\\n\\\\end{equation}\\nwhere the cross section is predicted from the theory, the flux is controlled by the accelerator\\\\footnote{In some cases, like cosmic rays, the flux must be estimated since the accelerator is quite far away.}, and the efficiency and acceptance are properties of the detector and event selection criteria. It is worth noting that the equation above is actually a repackaging of a more fundamental relationship. In fact the fundamental quantity that is predicted from first principles in quantum theory is the \\\\textit{scattering probability} \\\\mbox{$P(i\\\\to f)=|\\\\langle i|f\\\\rangle|^2/ (\\\\langle i|i\\\\rangle\\\\langle f | f \\\\rangle)$} inside a box of size $V$ over some time interval $T$, which is then repackaged into the Lorentz invariant form above~\\\\cite{Sredniki}.\\nIn the simulation narrative the efficiency and acceptance are estimated with computer simulations of the detector. Typically, a large sample of events is generated using Monte Carlo techniques~\\\\cite{MonteCarlo}. The Monte Carlo sampling is performed separately for the hard (perturbative) interaction (e.g. \\\\texttt{MadGraph}), the parton shower and hadronization process (e.g. \\\\texttt{Pythia} and \\\\texttt{Herwig}), and the interaction of particles with the detector (e.g. \\\\texttt{Geant}). Note, the efficiency and acceptance depend on the physical process considered, and I will refer to each such process as a \\\\textit{sample} (in reference to the corresponding sample of events generated with Monte Carlo techniques).\\nTo simplify the notation, I will define the effective cross section, $\\\\sigma_{\\\\rm eff.}$ to be the product of the total cross section, efficiency, and acceptance. Thus, the total number of events expected to be selected for a given scattering process, $\\\\nu$, is the product of the time-integrated flux or time-integrated luminosity, $\\\\lambda$, and the effective cross section\\n\\\\begin{equation}\\n\\\\nu = \\\\lambda \\\\sigma_{\\\\rm eff.}\\\\;.\\n\\\\end{equation}\\nI use $\\\\lambda$ here instead of the more common $L$ to avoid confusion with the likelihood function and because when we incorporate uncertainty on the time-integrated luminosity it will be a parameter of the model for which I have chosen to use greek letters. \\nIf we did not need to worry about detector effects and we could measure the final state perfectly, then the distribution for any observable $x$ would be given by\\n\\\\begin{equation}\\n\\\\textrm{(idealized)}\\\\hspace{2em}f(x) = \\\\frac{1}{\\\\sigma_{\\\\rm eff.}} \\\\frac{d\\\\sigma_{\\\\rm eff.}}{dx}\\\\;.\\\\hspace{5em}\\n\\\\end{equation}\\nOf course, we do need to worry about detector effects and we incorporate them with the detector simulation discussed above. From the Monte Carlo sample of events\\\\footnote{Here I only consider unweighted Monte Carlo samples, but the discussion below can be generalized for weighted Monte Carlo samples.} $\\\\{x_1, \\\\dots, x_N\\\\}$ we can estimate the underlying distribution $f(x)$ simply by creating a histogram. If we want we can write the histogram based on $B$ bins centered at $x_b$ with bin width $w_b$ explicitly as\\n\\\\begin{equation}\\n\\\\textrm{(histogram)} \\\\hspace{2em}f(x) \\\\approx h(x) = \\\\sum_{i=1}^N \\\\sum_{b=1}^B \\\\frac{ \\\\theta(|x_i-x_b|/w_b) }{N} \\\\frac{\\\\theta(|x -x_b|/w_b)}{w_b}\\\\;, \\\\end{equation}\\nwhere the first Heaviside function accumulates simulated events in the bin and the second selects the bin containing the value of $x$ in question. Histograms are the most common way to estimate a probability density function based on a finite sample, but there are other possibilities. The downsides of histograms as an estimate for the distribution $f(x)$ is that they are discontinuous and have dependence on the location of the bin boundaries. A particularly nice alternative is called kernel estimation~\\\\cite{Cranmer:2000du}. In this approach, one places a kernel of probability $K(x)$ centered around each event in the sample:\\n\\\\begin{equation} \\n\\\\textrm{(kernel estimate)}\\\\hspace{2em}f(x) \\\\approx \\\\hat{f}_0(x) = \\\\frac{1}{N} \\\\sum_{i=1}^N K\\\\left( \\\\frac{x-x_i}{h} \\\\right)\\\\;.\\\\hspace{1em}\\n\\\\end{equation}\\nThe most common choice of the kernel is a Gaussian distribution, and there are results for the optimal width of the kernel $h$. Equation~\\\\ref{eq:keys} is referred to as the fixed kernel estimate since $h$ is common for all the events in the sample. A second order estimate or adaptive kernel estimation provides better performance when the distribution is multimodal or has both narrow and wide features~\\\\cite{Cranmer:2000du}.\\n'}},\n",
       "       {'entity_name': 'histogram', 'entity_type': 'analysis_technique', 'description': 'A graphical representation of the distribution of a dataset, used to estimate a probability density function based on a finite sample by dividing the data into bins.', 'relevant_passages': {'\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\nThe simulation narrative is probably the easiest to explain and produces statistical models with the strongest logical connection to physical theory being tested. We begin with an relation that every particle physicists should know for the rate of events expected from a specific physical process\\n\\\\begin{equation}\\n\\\\textrm{rate} = \\\\textrm{(flux)} \\\\times \\\\textrm{(cross section)} \\\\times\\\\textrm{(efficiency)} \\\\times \\\\textrm{(acceptance)} \\\\;,\\n\\\\end{equation}\\nwhere the cross section is predicted from the theory, the flux is controlled by the accelerator\\\\footnote{In some cases, like cosmic rays, the flux must be estimated since the accelerator is quite far away.}, and the efficiency and acceptance are properties of the detector and event selection criteria. It is worth noting that the equation above is actually a repackaging of a more fundamental relationship. In fact the fundamental quantity that is predicted from first principles in quantum theory is the \\\\textit{scattering probability} \\\\mbox{$P(i\\\\to f)=|\\\\langle i|f\\\\rangle|^2/ (\\\\langle i|i\\\\rangle\\\\langle f | f \\\\rangle)$} inside a box of size $V$ over some time interval $T$, which is then repackaged into the Lorentz invariant form above~\\\\cite{Sredniki}.\\nIn the simulation narrative the efficiency and acceptance are estimated with computer simulations of the detector. Typically, a large sample of events is generated using Monte Carlo techniques~\\\\cite{MonteCarlo}. The Monte Carlo sampling is performed separately for the hard (perturbative) interaction (e.g. \\\\texttt{MadGraph}), the parton shower and hadronization process (e.g. \\\\texttt{Pythia} and \\\\texttt{Herwig}), and the interaction of particles with the detector (e.g. \\\\texttt{Geant}). Note, the efficiency and acceptance depend on the physical process considered, and I will refer to each such process as a \\\\textit{sample} (in reference to the corresponding sample of events generated with Monte Carlo techniques).\\nTo simplify the notation, I will define the effective cross section, $\\\\sigma_{\\\\rm eff.}$ to be the product of the total cross section, efficiency, and acceptance. Thus, the total number of events expected to be selected for a given scattering process, $\\\\nu$, is the product of the time-integrated flux or time-integrated luminosity, $\\\\lambda$, and the effective cross section\\n\\\\begin{equation}\\n\\\\nu = \\\\lambda \\\\sigma_{\\\\rm eff.}\\\\;.\\n\\\\end{equation}\\nI use $\\\\lambda$ here instead of the more common $L$ to avoid confusion with the likelihood function and because when we incorporate uncertainty on the time-integrated luminosity it will be a parameter of the model for which I have chosen to use greek letters. \\nIf we did not need to worry about detector effects and we could measure the final state perfectly, then the distribution for any observable $x$ would be given by\\n\\\\begin{equation}\\n\\\\textrm{(idealized)}\\\\hspace{2em}f(x) = \\\\frac{1}{\\\\sigma_{\\\\rm eff.}} \\\\frac{d\\\\sigma_{\\\\rm eff.}}{dx}\\\\;.\\\\hspace{5em}\\n\\\\end{equation}\\nOf course, we do need to worry about detector effects and we incorporate them with the detector simulation discussed above. From the Monte Carlo sample of events\\\\footnote{Here I only consider unweighted Monte Carlo samples, but the discussion below can be generalized for weighted Monte Carlo samples.} $\\\\{x_1, \\\\dots, x_N\\\\}$ we can estimate the underlying distribution $f(x)$ simply by creating a histogram. If we want we can write the histogram based on $B$ bins centered at $x_b$ with bin width $w_b$ explicitly as\\n\\\\begin{equation}\\n\\\\textrm{(histogram)} \\\\hspace{2em}f(x) \\\\approx h(x) = \\\\sum_{i=1}^N \\\\sum_{b=1}^B \\\\frac{ \\\\theta(|x_i-x_b|/w_b) }{N} \\\\frac{\\\\theta(|x -x_b|/w_b)}{w_b}\\\\;, \\\\end{equation}\\nwhere the first Heaviside function accumulates simulated events in the bin and the second selects the bin containing the value of $x$ in question. Histograms are the most common way to estimate a probability density function based on a finite sample, but there are other possibilities. The downsides of histograms as an estimate for the distribution $f(x)$ is that they are discontinuous and have dependence on the location of the bin boundaries. A particularly nice alternative is called kernel estimation~\\\\cite{Cranmer:2000du}. In this approach, one places a kernel of probability $K(x)$ centered around each event in the sample:\\n\\\\begin{equation} \\n\\\\textrm{(kernel estimate)}\\\\hspace{2em}f(x) \\\\approx \\\\hat{f}_0(x) = \\\\frac{1}{N} \\\\sum_{i=1}^N K\\\\left( \\\\frac{x-x_i}{h} \\\\right)\\\\;.\\\\hspace{1em}\\n\\\\end{equation}\\nThe most common choice of the kernel is a Gaussian distribution, and there are results for the optimal width of the kernel $h$. Equation~\\\\ref{eq:keys} is referred to as the fixed kernel estimate since $h$ is common for all the events in the sample. A second order estimate or adaptive kernel estimation provides better performance when the distribution is multimodal or has both narrow and wide features~\\\\cite{Cranmer:2000du}.\\n'}},\n",
       "       {'entity_name': 'nuisance parameters', 'entity_type': 'statistics_concept', 'description': 'Parameters in statistical models that account for uncertainties or variations in the data that are not of primary interest but can significantly affect the results of the analysis. These parameters are included to address unknown or unmeasured effects, systematic uncertainties, and to avoid bias in the estimation of the parameters of interest.', 'relevant_passages': {\"\\\\section{Errors}\\n\\\\subsection{Asymmetric errors}\\nSo what happens if you plot the likelihood function and it is not symmetric like \\nFig.~\\\\ref{fig:ML} but looks more like Fig.~\\\\ref{fig:asym}?\\nThis \\narises in many cases when numbers are small. For instance, in a simple Poisson count suppose you observe one event. $P(1;\\\\lambda)=\\\\lambda e^{-\\\\lambda}$ is not symmetric: $\\\\lambda=1.5$ is more likely to fluctuate down to 1 than $\\\\lambda=0.5$ is to\\nfluctuate up to 1.\\nYou can read off $\\\\sigma_+$ and $\\\\sigma_-$ from the two $\\\\Delta \\\\ln L=-{1 \\\\over 2}$ crossings, but they are different.\\nThe result can then be given as $a^{+\\\\sigma_+}_{-\\\\sigma_-}$. What happens after that?\\nThe first advice is to avoid this if possible. \\nIf you get $\\\\hat a=4.56$ with $\\\\sigma_+=1.61, \\\\sigma_-=1.59$ \\nthen quote this as $4.6 \\\\pm 1.6$ rather than $4.56^{+1.61}_{-1.59}$.\\nThose extra significant digits have no real meaning. If you can convince yourself that the difference between\\n$\\\\sigma_+$ and $\\\\sigma_-$ is small enough to be ignored then you should do so, as the alternative brings in a whole \\nlot of trouble and it's not worth it.\\nBut there will be some cases where the difference is too great to be swept away, so\\nlet's consider that case.\\nThere are two problems that arise: combination of measurements and combination of errors.\\n\\\\subsubsection{Combination of measurements with asymmetric errors}\\nSuppose you have two measurements of the same parameter $a$: $\\\\hat {a_1}^{+\\\\sigma^+_1}_{-\\\\sigma^-_1}$\\nand $\\\\hat {a_2}^{+\\\\sigma^+_2}_{-\\\\sigma^-_2}$ and you want to combine them to give the best estimate and, of course, its error. For symmetric errors the answer is well established to be\\n$\\\\hat a = {\\\\hat a_1/\\\\sigma_1^2 + \\\\hat a_2/\\\\sigma_2^2 \\\\over 1/\\\\sigma_1^2 + 1/\\\\sigma_2^2}$.\\nIf you know the likelihood functions, you can do it. The joint likelihood is just the sum.\\nThis is shown in Fig.~\\\\ref{fig:combine1} where \\nthe\\nred and green curves are measurements of $a$. \\nThe log likelihood functions just add (blue), from which the peak is found and the $\\\\Delta \\\\ln L=-\\\\half$ errors read off.\\nBut you don't know the full likelihood function: just 3 points (and that it had a maximum at the second).\\nThere are, of course, an infinite number of curves that could be drawn, and several models have been tried (cubics, constrained quartic...) on likely instances---see Ref.~\\\\cite{asym} for details. Some do better than others.\\nThe two most plausible are\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2}\\\\left( {a-\\\\hat a \\\\over \\\\sigma+\\\\sigma'(a-\\\\hat a)}\\\\right)^2\\n\\\\quad \\\\text{and}\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\ln L = -{1 \\\\over 2} {\\\\left( a-\\\\hat a \\\\right)^2 \\\\over V+V'(a-\\\\hat a)}\\n\\\\quad.\\n\\\\end{equation}\\nThese are similar to the Gaussian parabola, but the denominator is not constant. It varies with the value of $a$, being linear either in the standard deviation or in the variance.\\nBoth are pretty good. The first does better with errors on $\\\\log a$ (which are asymmetric if $a$ is symmetric: such asymmetric error bars are often seen on plots where the $y$ axis is logarithmic), the\\nsecond does better with Poisson measurements.\\nFrom the 3 numbers given one readily obtains\\n\\\\begin{equation}\\n\\\\sigma={2 \\\\sigma^+\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-} \\\\qquad \\n\\\\sigma'={\\\\sigma^+-\\\\sigma^- \\\\over \\\\sigma^+ + \\\\sigma^-}\\n\\\\end{equation}\\nor, if preferred\\n\\\\begin{equation}\\nV= \\\\sigma^+\\\\sigma^- \\\\qquad\\nV'=\\\\sigma^+-\\\\sigma^- \\n\\\\quad.\\n\\\\end{equation}\\nFrom the total likelihood you then find the maximum of sum, numerically, and the $\\\\Delta \\\\ln L=-{1\\\\over 2}$ points.\\nCode for doing this is available on GitHub\\\\footnote{\\\\url{ https://github.com/RogerJBarlow/Asymmetric-Errors}} in\\nboth R and Root.\\nAn example is shown in Fig.~\\\\ref{fig:asymex}. Combining $1.9^{+0.7}_{-0.5}$, $2.4^{+0.6}_{-0.8}$ and $3.1^{+0.5}_{-0.4}$ gives $2.76 ^{+0.29}_{-0.27} \\\\ .$\\n\\\\subsubsection{Combination of errors for asymmetric errors}\\nFor symmetric errors, given $x \\\\pm \\\\sigma_x, y \\\\pm \\\\sigma_y$, (and $\\\\rho_{xy}=0$) the error on \\n$f(x,y)$\\nis the sum in quadrature: $\\\\sigma_f^2 =\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2 + \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2\\n\\\\sigma_y^2$.\\nWhat is the equivalent for the error on $f(x,y)$ when the errors are asymmetric,\\n$x ^{+\\\\sigma^+_x}_{-\\\\sigma^-_x}, y^{+\\\\sigma^+_y}_{-\\\\sigma^-_y}$? Such a problem arises frequently at the end of an analysis when the systematic errors from various sources are all combined.\\nThe standard procedure---which you will see done, though it has not, to my knowledge, been written down anywhere---is to add the positive and negative errors\\nin quadrature separately: ${\\\\sigma^+_f}^2={\\\\sigma^+_x}^2+{\\\\sigma^+_y}^2$,\\\\quad \\n${\\\\sigma^-_f}^2={\\\\sigma^-_x}^2+{\\\\sigma^-_y}^2$.\\nThis looks plausible, but it is \\n{\\\\em manifestly wrong} as it breaks the central limit theorem.\\nTo see this, suppose you have to average \\n$N$ i.i.d. variables each with the same errors which are asymmetric: $\\\\sigma^+ = 2 \\\\sigma^-$ . \\nThe \\nstandard procedure reduces both $\\\\sigma^+$ and $\\\\sigma^-$ by a factor $1/\\\\sqrt N$, but the skewness remains.\\nThe positive error is twice the negative error. This is therefore not Gaussian, and never will be, even as $N \\\\to \\\\infty$.\\nYou can see what's happening by considering the combination of two of these measurements. They both may fluctuate upwards, or they both may fluctuate downwards, and yes, the upward fluctuation will be, on average, twice as big. But there is a 50\\\\ chance of one upward and one downward fluctuation, which is not considered in the standard procedure.\\nFor simplicity we write $z_i={\\\\partial f \\\\over \\\\partial x_i} (x_i-x^0_i)$, the deviation of the parameter \\nfrom its nominal value, scaled by the differential. The individual likelihoods are again parametrized\\nas Gaussian with a linear dependence of the standard deviation or of the variance, giving\\n\\\\begin{equation}\\n\\\\ln L(\\\\vec z)=- \\\\half \\\\sum_i \\\\left( { z_i \\\\over \\\\sigma_i + \\\\sigma'_i z_i}\\\\right)^2 \\\\qquad {\\\\rm or}\\\\qquad\\n- \\\\half \\\\sum_i { z_i ^2\\\\over V_i + V'_i z_i}\\n\\\\quad,\\n\\\\end{equation}\\nwhere $\\\\sigma, \\\\sigma', V,V'$ are obtained from Eqs.~\\\\ref{eq:asyms} or \\\\ref{eq:asymV}.\\nThe $z_i$ are nuisance parameters (as described later) and can be removed by profiling. \\nLet $u=\\\\sum z_i$ be the total deviation in the quoted $f$ arising from the individual deviations.\\nWe form $\\\\hat L(u)$ as the maximum of $L(\\\\vec z)$ subject to the constraint $\\\\sum_i z_i=u$.\\nThe method of undetermined multipliers readily gives the solution\\n\\\\begin{equation}\\nz_i=u {w_i \\\\over \\\\sum_j w_j}\\n\\\\quad,\\n\\\\end{equation}\\nwhere\\n\\\\begin{equation}\\nw_i = {(\\\\sigma_i + \\\\sigma'_i z_i)^3 \\\\over 2 \\\\sigma_i}\\n\\\\qquad {\\\\rm or } \\\\qquad\\n{(V_i+V'_i z_i)^2 \\\\over 2V_i + V'_i z_i}\\n\\\\quad.\\n\\\\end{equation}\\nThe equations are nonlinear, but can be solved iteratively. At $u=0$ all the $z_i$ are zero. Increasing (or decreasing) $u$ in small steps, Eqs.~\\\\ref{eq:comsol1} and \\\\ref{eq:comsol2} are applied successively to give the $z_i$ and the $w_i$: convergence is rapid. The value of $u$ which maximises the likelihood should in principle be applied as a correction to the quoted result.\\nPrograms to do this are also available on the GitHub site.\\nAs an example, consider a counting experiment with a number of backgrounds, each determined by an ancillary Poisson experiment, and that for simplicity each background was determined by running the apparatus for the same time as the actual experiment. (In practice this is unlikely, but scale factors\\ncan easily be added.)\\nSuppose two backgrounds are measured, one giving four events and the other five. These would be reported, using $\\\\Delta ln L=-\\\\half$ errors, as $4^{+2.346}_{-1.682}$ and $5^{+2.581}_{-1.916}$.\\nThe method, using linear $V$, gives the combined error on the background count as ${}^{+3.333}_{-2.668}$.\\nIn this simple case we can check the result against the total background count of nine events, which has errors ${}^{+3.342}_{-2.676}$. The agreement is impressive. Further examples of the same total, partitioned differently, are shown in table~\\\\ref{tab:asymcom}.\\n\\\\begin{table}[h]\\n\\\\begin{centering}\\n\\\\begin{tabular}{ l c c c c }\\nInputs & \\\\multicolumn {2} {c} {Linear $\\\\sigma$}& \\\\multicolumn {2} {c} {Linear $V$}\\\\\\\\\\n& $\\\\sigma^-$ & $\\\\sigma^+$ & $\\\\sigma^-$ & $\\\\sigma^+$ \\\\\\\\\\n\\\\hline\\n4+5 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+6 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n2+7 & 2.653 & 3.310 & 2.668 & 3.333 \\\\\\\\\\n3+3+3 & 2.630 & 3.278 & 2.659 & 3.323 \\\\\\\\\\n1+1+1+1+1+1+1+1+1 & 2.500 & 3.098 & 2.610 & 3.270 \\\\\\\\\\n\\\\end{tabular}\\n\\\\caption{ Various combinations of Poisson errors. The target value is $\\\\sigma^-=2.676$, $\\\\sigma^+=3.342$}\\n\\\\end{centering}\\n\\\\end{table}\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Treatment of nuisance parameters}\\nNuisance parameters have been introduced in Sec.~\\\\ref{sec:extLikFun}.\\nUsually, signal extraction procedures (either parameter fits or upper limits determinations) determine,\\ntogether with parameters of interest, also nuisance parameters that model effects\\nnot strictly related to our final measurement, like\\nbackground yield and shape, detector resolution, etc.\\nNuisance parameters are also used to model sources of systematic\\nuncertainties.\\nOften, the true value of a nuisance parameter is not known, but we may have some\\nestimate from sources that are external to our problem.\\nIn those cases, we can refer to {\\\\it nominal values} of the nuisance parameter and\\ntheir uncertainty. Nominal values of nuisance parameters are random variables\\ndistributed according to some PDF that depend on their true value.\\nA Gaussian distribution is the simplest assumption for nominal values of nuisance parameters.\\nAnyway, this may give negative values corresponding to the\\nleftmost tail, which are not suitable for\\nnon-negative quantities like cross sections.\\nFor instance, we may have an estimate of some background yield $b$ given by:\\n\\\\begin{equation}\\nb = \\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $L_{\\\\mathrm{int}}$ is the estimate of the integrated luminosity (assumed to be known\\nwith negligible uncertainty), $\\\\sigma_b$ is the background cross section evaluated\\nfrom theory, and $\\\\beta$ is a nuisance parameter, whose nominal value is equal to one, representing the\\nuncertainty on the cross-section evaluation. If the uncertainty on $\\\\beta$ is large, one may\\nhave a negative value of $\\\\beta$ with non-negligible probability, hence an unphysical negative value of\\nthe background yield $b$.\\nA safer assumption in such cases is to take a log normal distribution for the uncertain non-negative\\nquantities:\\n\\\\begin{equation}\\nb = e^\\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\beta$ is again distributed according to a normal distribution with nominal value equal to zero,\\nin this case.\\nUnder the Bayesian approach, nuisance parameters don't require a special treatment.\\nIf we have a parameter of interest $\\\\mu$ and a nuisance parameter $\\\\theta$,\\na Bayesian posterior will be obtained as (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(\\\\mu,\\\\theta|\\\\vec{x}) = \\\\frac{\\nL(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\pi(\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta^\\\\prime\\n}\\\\,.\\n\\\\end{equation}\\n$P(\\\\mu|\\\\vec{x})$ can be obtained as marginal PDF of $\\\\mu$ by integrating $P(\\\\mu,\\\\theta|\\\\vec{x})$ over $\\\\theta$:\\n\\\\begin{equation}\\nP(\\\\mu|\\\\vec{x}) = \\\\int P(\\\\mu,\\\\theta|\\\\vec{x}),\\\\mathrm{d}\\\\theta =\\n\\\\frac{\\n\\\\int L(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\\\,\\\\mathrm{d}\\\\theta\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta)\\\\pi(\\\\mu^\\\\prime,\\\\theta)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta\\n}\\\\,.\\n\\\\end{equation}\\nIn the frequentist approach, one possibility is to introduce in the likelihood\\nfunction a model for a data sample that can constrain the nuisance parameter.\\nIdeally, we may have a control sample $\\\\vec{y}$, complementary to the main\\ndata sample $\\\\vec{x}$, that only depends on the nuisance parameter, and\\nwe can write a global likelihood function as:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_y(\\\\vec{y};\\\\theta)\\\\,.\\n\\\\end{equation}\\nUsing control regions to constrain nuisance parameters is usually a good method\\nto reduce systematic uncertainties. Anyway, it may not always be\\nfeasible and in many cases we may just have information abut the nominal\\nvalue $\\\\theta^{\\\\mathrm{nom}}$ of $\\\\theta$ and its distribution obtained from a complementary measurement:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_\\\\theta(\\\\theta^{\\\\mathrm{nom}};\\\\theta)\\\\,.\\n\\\\end{equation}\\n$L_\\\\theta$ may be a Gaussian or log normal distribution in the easiest cases.\\nIn order to achieve a likelihood function that does not depend on\\nnuisance parameters, for many measurements at LEP or Tevatron a method\\nproposed by Cousins and Highland was adopted~\\\\cite{Cousins_Highlands}\\nwhich consists of integrating the likelihood function over the nuisance\\nparameters, similarly to what is done in the Bayesian approach (Eq.~(\\\\ref{eq:BayesianNuisance})).\\nFor this reason, this method was also called hybrid.\\nAnyway the Cousins--Highland does not guarantee to provide exact coverage,\\nand was often used as pragmatic solution in the frequentist context.\\n\", '\\\\section{Inference}\\n\\\\subsection{Maximum likelihood estimates}\\nThe maximum likelihood method takes as best-fit values of the unknown parameter\\nthe values that maximize the likelihood function (defined Sec.~\\\\ref{sec:likeFun}).\\nThe maximization of the likelihood function can be performed analytically only in the simplest cases,\\nwhile a numerical treatment is needed in most of the realistic cases.\\n{\\\\sc Minuit}~\\\\cite{minuit} is historically the most widely used minimization software engine in High Energy Physics.\\n\\\\subsubsection{Extended likelihood function}\\nGiven a sample of $N$ measurements of the variables $\\\\vec{x}=(x_1, \\\\cdots, x_n)$, the likelihood function expresses the probability\\ndensity evaluated for our sample as a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\n\\\\prod_{i=1}^Nf(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nThe size $N$ of the sample is in many cases also a random variable. In those cases,\\nthe {\\\\it extended likelihood function} can be defined as:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) \\\\prod_{i=1}^N f(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,,\\n\\\\end{equation}\\nwhere $P(N;\\\\theta_1,\\\\cdots,\\\\theta_m)$ is the distribution of $N$, and in practice is always a Poissonian \\nwhose expected rate parameter is a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) = \\\\frac{\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)^N e^{-\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)}}{N!}\\\\,.\\n\\\\end{equation}\\nIn many cases, either with a standard or an extended likelihood function,\\nit may be convenient to use $-\\\\ln L$ or $-2\\\\ln L$ in the numerical treatment\\nrather than $L$, because\\nthe product of the various terms is transformed into the sum of the logarithms of\\nthose terms, which may have advantages in the computation.\\nFor a Poissonian process that is given by the sum of a signal plus a background process,\\nthe extended likelihood function may be written as:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\n\\\\frac{(s+b)^N e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nf_sP_s(x_i;\\\\vec{\\\\theta}) + f_b P_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $s$ and $b$ are the signal and background expected yields, respectively,\\n$f_s$ and $f_b$ are the fraction of signal and background events, namely:\\n\\\\begin{eqnarray}\\nf_s & = & \\\\frac{s}{s+b} \\\\,,\\\\\\\\\\nf_b & = & \\\\frac{b}{s+b} \\\\,,\\n\\\\end{eqnarray}\\nand $P_s$ and $P_b$ are the PDF of the variable $x$ for signal and background,\\nrespectively.\\nReplacing $f_s$ and $f_b$ into Eq.~(\\\\ref{eq:extLikSB}) gives:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) = \\\\frac{e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIt may be more convenient to use the negative logarithm of Eq.~(\\\\ref{eq:extLikInt}),\\nthat should be minimize in order to determine the best-fit values of $s$, $b$ and $\\\\vec{\\\\theta}$:\\n\\\\begin{equation}\\n-\\\\ln L(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\ns + b -\\\\sum_{i=1}^N\\\\ln\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right) +\\\\ln N!\\\\,.\\n\\\\end{equation}\\nThe last term $\\\\ln N!$ is a constant with respect to the fit parameters,\\nand can be omitted in the minimization.\\nIn many cases, instead of using $s$ as parameter of interest,\\nthe {\\\\it signal strength} $\\\\mu$ is introduced, defined by the following equation:\\n\\\\begin{equation}\\ns = \\\\mu s_0\\\\,,\\n\\\\end{equation}\\nwhere $s_0$ is the theory prediction for the signal yield $s$.\\n$\\\\mu=1$ corresponds to the nominal value of the theory prediction for the signal yield.\\nAn example of unbinned maximum likelihood fit is given in Fig.~\\\\ref{fig:sbFit},\\nwhere the data are fit with a model inspired to Eq.~(\\\\ref{eq:extLikInt}), with\\n$P_s$ and $P_b$ taken as a Gaussian and an exponential distribution, respectively.\\nThe observed variable has been called $m$ in that case because the spectrum resembles an invariant mass peak,\\nand the position of the peak at 3.1~GeV reminds a $\\\\mathrm{J}/\\\\psi$ particle.\\nThe two PDFs can be written as:\\n\\\\begin{eqnarray}\\nP_s(m) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}e^{-\\\\frac{(m-\\\\mu)^2}{2\\\\sigma^2}}\\\\,,\\\\\\\\\\nP_b(m) & = & \\\\lambda e^{-\\\\lambda m}\\\\,.\\n\\\\end{eqnarray}\\nThe parameters $\\\\mu$, $\\\\sigma$ and $\\\\lambda$ are fit together with the\\nsignal and background yields $s$ and $b$. While $s$ is our {\\\\it parameter of interest},\\nbecause we will eventually determine a production cross section or branching fraction from\\nits measurement, the other additional parameters, that are not directly\\nrelated to our final measurement, are said {\\\\it nuisance parameters}.\\nIn general, nuisance parameters are needed to model background yield,\\ndetector resolution and efficiency, various parameters modeling the\\nsignal and background shapes, etc. Nuisance parameters are also important\\nto model {\\\\it systematic uncertainties}, as will be discussed more in\\ndetails in the following sections.\\n', '\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Expected sensitivity and bands}\\nThe expected sensitivity for limits and discovery are useful quantities, though subject to some degree of ambiguity. Intuitively, the expected upper limit is the upper limit one would expect to obtain if the background-only hypothesis is true. Similarly, the expected significance is the significance of the observation assuming the standard model signal rate (at some $\\\\mh$). To find the expected limit one needs a distribution $f(\\\\mu_up | \\\\mu=0,\\\\vec\\\\theta)$. To find the expected significance one needs the distribution $f(Z | \\\\mu=1,\\\\vec\\\\theta)$ or, equivalently, $f(p_0 | \\\\mu=1,\\\\vec\\\\theta)$. We use the median instead of the mean, as it is invariant to the choice of $Z$ or $p_0$. More importantly, is that the expected limit and significance depend on the value of the nuisance parameters $\\\\vec\\\\theta$, for which we do not know the true values. Thus, the expected limit and significance will depend on some convention for choosing $\\\\vec\\\\theta$. While many nuisance parameters have a nominal estimate (i.e. the global observables in the constraint terms), others do not (eg. the exponent in the $H\\\\to\\\\gamma\\\\gamma$ background model). Thus, we choose a convention that treats all of the nuisance parameters consistently, which is the profiled value based on the observed data. Thus for the expected limit we use $ f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))$ and for the expected significance we use $f(p_0 | \\\\mu=1,\\\\hat{\\\\hat{\\\\vec\\\\theta}}(\\\\mu=1, \\\\rm obs))$. An unintuitive and possibly undesirable feature of this choice is that the expected limit and significance depend on the observed data through the conventional choice for $\\\\vec\\\\theta$.\\nWith these distributions we can also define bands around the median upper limit. Our standard limit plot shows a dark green band corresponding to $\\\\mu_{\\\\pm 1}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 1}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 1) \\n\\\\end{equation}\\nand a light yellow band corresponding to $\\\\mu_{\\\\pm 2}$ defined by \\n\\\\begin{equation}\\n\\\\int_{0}^{\\\\mu_{\\\\pm 2}} f(\\\\mu_{\\\\rm up}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs})) d\\\\mu_{\\\\rm up} = \\\\Phi^{-1}(\\\\pm 2) \\n\\\\end{equation}\\n', '\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Simulation Narrative}\\n\\\\subsubsection{Incorporating Monte Carlo statistical uncertainty on the histogram templates}\\nThe histogram based approach described above are based Monte Carlo simulations of full detector simulation. These simulations are very computationally intensive and often the histograms are sparsely populated. In this case the histograms are not good descriptions of the underlying distribution, but are estimates of that distribution with some statistical uncertainty. Barlow and Beeston outlined a treatment of this situation in which each bin of each sample is given a nuisance parameter for the true rate, which is then fit using both the data measurement and the Monte Carlo estimate~\\\\cite{Barlow:1993dm}. This approach would lead to several hundred nuisance parameters in the current analysis. Instead, the \\\\texttt{HistFactory} employs a lighter weight version in which there is only one nuisance parameter per bin associated with the total Monte Carlo estimate and the total statistical uncertainty in that bin. If we focus on an individual bin with index $b$ the contribution to the full statistical model is the factor\\n\\\\begin{equation}\\n\\\\Pois(n_b | \\\\nu_b(\\\\vec\\\\alpha) + \\\\gamma_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)) \\\\, \\\\Pois(m_b | \\\\gamma_b \\\\tau_b) \\\\;,\\n\\\\end{equation}\\nwhere $n_b$ is the number of events observed in the bin, $\\\\nu_b(\\\\vec\\\\alpha)$ is the number of events expected in the bin where Monte Carlo statistical uncertainties need not be included (either because the estimate is data driven or because the Monte Carlo sample is sufficiently large), $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)$ is the number of events estimated using Monte Carlo techniques where the statistical uncertainty needs to be taken into account. Both expectations include the dependence on the parameters $\\\\vec\\\\alpha$. The factor $\\\\gamma_b$ is the nuisance parameter reflecting that the true rate may differ from the Monte Carlo estimate $\\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) $ by some amount. If the total statistical uncertainty is $\\\\delta_b$, then the relative statistical uncertainty is given by $\\\\nu_b^{\\\\rm MC}/\\\\delta_b$. This corresponds to a total Monte Carlo sample in that bin of size $m_b = (\\\\delta_b/\\\\nu_b^{\\\\rm MC})^2$. Treating the Monte Carlo estimate as an auxiliary measurement, we arrive at a Poisson constraint term $ \\\\Pois(m_b | \\\\gamma_b \\\\tau_b)$, where $m_b$ would fluctuate about $\\\\gamma_b \\\\tau_b$ if we generated a new Monte Carlo sample. Since we have scaled $\\\\gamma$ to be a factor about 1, then we also have $\\\\tau_b=(\\\\nu_b^{\\\\rm MC}/\\\\delta_b)^2$; however, $\\\\tau_b$ is treated as a fixed constant and does not fluctuate when generating ensembles of pseudo-experiments.\\nIt is worth noting that the conditional maximum likelihood estimate $\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha)$ can be solved analytically with a simple quadratic expression.\\n\\\\begin{equation}\\n\\\\hat{\\\\hat{\\\\gamma_b}}(\\\\vec\\\\alpha) = \\\\frac{-B + \\\\sqrt{B^2 - 4 AC}}{2A} \\\\;,\\n\\\\end{equation}\\nwith\\n\\\\begin{equation}\\nA = \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)^2 + \\\\tau_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nB= \\\\nu_b(\\\\vec\\\\alpha) \\\\tau + \\\\nu_b(\\\\vec\\\\alpha) \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - n_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha) - m_b \\\\nu_b^{\\\\rm MC}(\\\\vec\\\\alpha)\\n\\\\end{equation}\\n\\\\begin{equation}\\nC= m_b \\\\nu_b(\\\\vec\\\\alpha) \\\\;.\\n\\\\end{equation}\\nIn a Bayesian technique with a flat prior on $\\\\gamma_b$, the posterior distribution is a gamma distribution. Similarly, the distribution of $\\\\hat\\\\gamma_b$ will take on a skew distribution with an envelope similar to the gamma distribution, but with features reflecting the discrete values of $m_b$. Because the maximum likelihood estimate of $\\\\gamma_b$ will also depend on $n_b$ and $\\\\hat{\\\\vec\\\\alpha}$, the features from the discrete values of $m_b$ will be smeared. This effect will be more noticeable for large statistical uncertainties where $\\\\tau_b$ is small and the distribution of $\\\\hat\\\\gamma_b$ will have several small peaks. For smaller statistical uncertainties where $\\\\tau_b$ is large the distribution of $\\\\hat\\\\gamma_b$ will be approximately Gaussian.'}},\n",
       "       {'entity_name': 'onoff model', 'entity_type': 'analysis_technique', 'description': 'A statistical method used in particle physics to estimate the background in a signal region by comparing it to a control region, often involving Poisson distributions.', 'relevant_passages': {\"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Data-Driven Narrative}\\nThe strength of the simulation narrative lies in its direct logical link from the underlying theory to the modeling of the experimental observations. The weakness of the simulation narrative derives from the weaknesses in the simulation itself. Data-driven approaches are more motivated when they address specific deficiencies in the simulation. Before moving to a more abstract or general discussion of the data-driven narrative, let us first consider a few examples.\\nThe first example we have already considered in Sec.~\\\\ref{S:AuxMeas} in the context of the ``on/off'' problem. There we introduced an auxiliary measurement that counted $n_{CR}$ events in a control region to estimate the background $\\\\nu_B$ in the signal region. In order to do this we needed to understand the ratio of the number of events from the background process in the control and signal regions, $\\\\tau$. This ratio $\\\\tau$ either comes from some reasonable assumption or simulation. For example, if one wanted to estimate the background due to jets faking muons $j\\\\to\\\\mu$ for a search selecting $\\\\mu^+\\\\mu^-$ , then one might use a sample of $\\\\mu^\\\\pm\\\\mu^\\\\pm$ events as a control region. Here the motivation for using a data-driven approach is that modeling the processes that lead to $j\\\\to\\\\mu$ rely heavily on the tails of fragmentation functions and detector response, which one might reasonably have some skepticism. If one assumes that control region is expected to have negligible signal in it, that backgrounds that produce $\\\\mu^+\\\\mu^-$ other than the jets faking muons, and that the rate for $j\\\\to\\\\mu^-$ is the same\\\\footnote{Given that the LHC collides $pp$ and not $p\\\\bar{p}$, there is clearly a reason to worry if this assumption is valid.} as the rate for $j\\\\to\\\\mu^+$, then one can assume $\\\\tau=1$. Thus, this background estimate is as trustworthy as the assumptions that went into it. In practice, several of these assumptions may be violated. Another approach is to use simulation of these background processes to estimate the ratio $\\\\tau$; a hybrid of the data-driven and simulation narratives.\\nLet us now consider the search for $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad}. The right plot of Fig~\\\\ref{fig:H2photons} shows the composition of the backgrounds in this search, including the continuum production of $pp\\\\to\\\\gamma\\\\gamma$, the $\\\\gamma$+jets process with a jet faking a photon $j\\\\to\\\\gamma$, and the multi jet process with two jets faking photons. The continuum production of $\\\\gamma\\\\gamma$ has a theoretical uncertainty that is much larger than the statistical fluctuations one would expect in the data. Similarly, the rate of jets faking photons is sensitive to fragmentation and the detector simulation. These uncertainties are large compared to the statistical fluctuations in the data itself. Thus we can use the distribution in Fig~\\\\ref{fig:H2photons} to measure the total background rate. Of course, the signal would also be in this distribution, so one either needs to apply a mass window around the signal and consider the region outside of the window as a sideband control sample or model the signal and background contributions to the distribution. In the case of the $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad} the modeling of the distribution signal and background distributions is not based on histograms from simulation, but instead a continuous function is used as an effective model. I will discuss this effective modeling narrative below, but point out that here this is another example of a hybrid narrative.\\nThe final example to consider is an extension of the `on/off' model, often referred to as the `ABCD' method. Let us start with the `on/off' model:\\n\\\\mbox{ $\\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)\\\\cdot \\\\Pois(n_{\\\\rm CR}|\\\\tau\\\\nu_B)$}. As mentioned above, this requires that one estimate $\\\\tau$ either from simulation or through some assumptions. The ABCD method aims to estimate introduce two new control regions that can be used to measure $\\\\tau$. To see this, let us imagine that the signal and control regions correspond to requiring some continuous variable $x$ being greater than or less than some threshold value $x_c$. If we could introduce a second discriminating variable $y$ such that the distribution for background factorizes $f_B(x,y)=f_B(x)f_B(y)$, then we have a handle to measure the factor $\\\\tau$. Typically, one introduces a threshold $y_c$ so that the signal contribution is small below this threshold\\\\footnote{The relative sign of the cut is not important, but has been chosen for consistency with Fig~\\\\ref{fig:ABCD}.}. Figure~\\\\ref{fig:ABCD} shows an example where $x_c=y_c=5$. With this we these two thresholds we have four regions that we can schematically refer to as A, B, C, and D. In the case of simply counting events in these regions we can write the total expectation as\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + 1\\\\cdot\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\tau_B\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\tau_C\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + \\\\tau_B\\\\tau_C\\\\nu_A \\n\\\\end{eqnarray}\\nwhere $\\\\mu$ is the signal rate in region A, $\\\\epsilon_i$ is the ratio of the signal in the regions B, C, D with respect to the signal in region A, $\\\\nu_i^{MC}$ is the rate of background in each of the regions being estimated from simulation, $\\\\nu_i$ is the rate of the background being estimated with the data driven technique in the signal region, and $\\\\tau_i$ are the ratios of the background rates in the regions B, C, and D with respect to the background in region A. The key is that we have used the factorization $f_B(x,y)=f_B(x)f_B(y)$ to write $\\\\tau_D=\\\\tau_B\\\\tau_C$. The right panel of Fig.~\\\\ref{fig:ABCD} shows a more complicated extension of the ABCD method from a recent ATLAS SUSY analysis~\\\\cite{ATLAS:2011ad}.\\nd\\nAn alternative parametrization, which can be more numerically stable is\\\\\\\\\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + \\\\eta_C\\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\eta_C\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + 1\\\\cdot\\\\nu_D \\n\\\\end{eqnarray}\\n\"}},\n",
       "       {'entity_name': 'abcd method', 'entity_type': 'analysis_technique', 'description': 'A data-driven technique in particle physics that utilizes multiple control regions to estimate background rates. It involves designing four distinct data regions based on orthogonal selections of two independent variables, enabling the transfer of background predictions into the signal region.', 'relevant_passages': {\"\\\\section{Real-time anomaly detection}\\nThe ultimate limiting factor for many searches for new physics is the event selection system of particle detectors. Tens of terabytes of data per second are produced from proton-proton collisions occurring every 25 ns, an extreme data rate that can not be read out and stored. The rate is reduced by a real-time, two-stage event filter processing system -- the {\\\\em trigger} -- which decides whether each collision event should be kept for further analysis or be discarded. The first stage, the Level-1 trigger, is completely hardware-based running around one thousand large field programmable (FPGA) gate arrays on custom boards. The data is buffered close to the detector while the processing occurs, with a maximum latency of ${\\\\mathcal O}(1)~\\\\mu$s to make the trigger decision. High selection accuracy in the trigger is crucial in order to keep only the most interesting events while keeping the output bandwidth low, reducing the event rate from $40$~MHz to $100$~kHz. The data accepted by the Level-1 trigger, are read out from the detector and sent to the second software-based event filtering system, the High Level Trigger (HLT). Here the data rate is further reduced from $100$~kHz to $1$~kHz. This processing is done on commercially available CPUs and, recently, also on GPU accelerators~\\\\citen{Bocci_2023}. The latency requirement at the HLT is ${\\\\mathcal O}(100)$ms.\\nRecently, it has been proposed to look for Beyond Standard Model physics signatures in a model-agnostic way at the trigger level, both at the HLT~\\\\citen{vaemining} and at the Level-1~\\\\citen{ad_nmi} stage. The existing selection algorithms within the trigger system currently prioritize collisions that generate high-energy outgoing particles. However, these algorithms have reduced sensitivity to e.g. signatures involving a high multiplicity of low-momentum particles. To address this, outlier detection techniques have garnered attention for their potential to enhance acceptance rates for events that are challenging to capture using conventional algorithms. The challenge when designing such models is to adhere to the strict latency, resource and throughput constraints of the trigger.\\nAlgorithm targeting the completely hardware-based Level-1 system are especially difficult to design. Deploying ML algorithms on FPGAs presents a significant challenge due to the specialized engineering expertise required. Unlike traditional software implementations, which can be executed on general-purpose processors, FPGAs demand a deep understanding of hardware design and optimization. The process of mapping complex mathematical operations, like those found in neural networks, onto FPGA circuits is intricate and requires careful consideration of factors such as data flow, parallelism, and memory access patterns. This becomes especially important in the Level-1 trigger, where the maximum latency per algorithm can be as low as 50 ns and only a few percent of the FPGA resources can be allocated to one specific algorithm. Recently, this process has been made easier through the introduction of software libraries that perform an automatic translation of ML models into highly parallel FPGA firmware, hls4ml~\\\\cite{hlsfml} and Finn~\\\\cite{finn}. These libraries are interfaced to libraries that perform \\\\textit{quantization-aware training}, a method for reducing the numerical precision of weights and activations in a neural network during training, hence reducing their memory footprint.\\nUtilizing these tools, Ref.~\\\\citen{ad_nmi} demonstrates that real-time anomaly detection using a variational auto-encoder architecture is feasible within 100 ns and using only a fraction of the FPGA resources. This is made possible through quantization-aware training, and clever architecture choices. For instance, rather than using the mean-squared error between the input and the output of the autoencoder as anomaly score, only the KL divergence term entering the VAE loss is used. The benefits of using the encoder and the KL term only is that one can avoid performing Gaussian sampling on the hardware, saving resources and latency by not having to evaluate the decoder and in addition there is no need to buffer the input data for computation of the MSE.\\nThis demonstration of the capability to perform real-time anomaly detection on FPGAs has generated attention within the community and initiated a challenge similar to those found on Kaggle, the ADC 2021 challenge~\\\\cite{adcchallenge}, as well as a dataset~\\\\cite{ADC} for benchmarking such algorithms.\\nRecently, an outlier detection algorithm similar to the one described in Ref.~\\\\citen{ad_nmi} has been deployed into a copy of the CMS Experiment Global Trigger (GT) board~\\\\cite{axo}. This copy of the GT system receives exactly the same input as the CMS GT, but cannot trigger a full read-out of the CMS detector, making it an excellent test-bench for algorithms targeting the main system. The anomaly detection algorithm, referred to as AXOL1TL, has been trained on unbiased data collected by the CMS experiment and shown to improve the signal efficiency for a range of different BSM signals by up to 46\\\\Figure~\\\\ref{fig:axo} shows an event display of the highest anomaly score event selected by AXOL1TL after analyzing CMS data collected in 2023. The event does not pass and other L1 trigger algorithm and is characterized by a very high number of low to medium momentum jets.\\nWhen using outlier detection as a way of discovering new physics phenomena, the detection of outliers is not sufficient. A full statistical framework for hypothesis testing is also necessary in order to claim discovery. This requires a background estimate with which the observed data in the signal region can be compared to, and this can be either based on simulated data or it can be fully data-driven. This is also true when it comes to analyzing data collected by an \\\\textit{anomaly trigger}. From the discussion above on density estimation, the background estimate was an important part of the anomaly detection method itself, for instance in the CWoLa bumphunt the estimate was taken from the regions adjacent to the signal region and in Cathode it was sampled from the ML-generated density estimate itself. For autoencoder-based anomaly, the background estimate is not provided by the model itself. It only offers a method for achieving high signal sensitivity. In Ref.~\\\\citen{PhysRevD.105.055006}, a statistical method for detecting non-resonant anomalies using auto-encoders is introduced. In this approach, multiple autoencoders are trained with the aim of maximizing their independence from one another. This is achieved by utilizing the distance de-correlation (DisCo) method~\\\\cite{PhysRevLett.125.122001,PhysRevD.103.035021}, where a regularizer term based on the DisCo measure of statistical dependence is included in the training. Events are classified as anomalous if their reconstruction quality is poor across all autoencoders. Instances classified as anomalous by one autoencoder, but not by all, provide the necessary context for estimating the Standard Model background in a manner that is not model dependent. This estimation is carried out using the ABCD method. The ABCD method is commonly used for data-driven background estimation in particle physics. It consists of designing four data regions, A, B C, and D, based on orthogonal selections on two independent variables, $\\\\tau_1$ and $\\\\tau_2$, and then transferring the background prediction from the three signal-free regions into the signal region. In Ref.~\\\\citen{PhysRevD.105.055006}, the two variables are the anomaly scores of the two autoencoders, which are now statistically independent due to the inclusion of the DisCo term at training time. In order to use this to design an anomaly detection method that can run in the trigger system, the authors propose to preserve all events falling within the signal-sensitive region as defined by the two autoencoders. Additionally, a random subset of events in the other three regions would be conserved for subsequent offline background estimation.\\nTrigger algorithms typically focus on achieving a specific signal efficiency. However, in anomaly detection, which aims to be sensitive to a broad spectrum of new signals, there isn't a definite signal efficiency to target. Instead, these algorithms are calibrated for a certain false positive rate (FPR), which is directly related to a predetermined trigger rate. This rate generally falls within the range of around $\\\\mathcal{O}(10-100)$~Hz, necessitating an FPR near $\\\\sim10^{-6}$. During the tuning process, the signal efficiency across various potential new physics scenarios is tracked for guidance, along with the loss on the self-supervisory label (measured as the Mean Squared Error between input and output). Yet, developing improved metrics for refining outlier detection methods, particularly those that include sensitivity to novel, unidentified signals, remains a challenging and unresolved task.\\n\", \"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{Data-Driven Narrative}\\nThe strength of the simulation narrative lies in its direct logical link from the underlying theory to the modeling of the experimental observations. The weakness of the simulation narrative derives from the weaknesses in the simulation itself. Data-driven approaches are more motivated when they address specific deficiencies in the simulation. Before moving to a more abstract or general discussion of the data-driven narrative, let us first consider a few examples.\\nThe first example we have already considered in Sec.~\\\\ref{S:AuxMeas} in the context of the ``on/off'' problem. There we introduced an auxiliary measurement that counted $n_{CR}$ events in a control region to estimate the background $\\\\nu_B$ in the signal region. In order to do this we needed to understand the ratio of the number of events from the background process in the control and signal regions, $\\\\tau$. This ratio $\\\\tau$ either comes from some reasonable assumption or simulation. For example, if one wanted to estimate the background due to jets faking muons $j\\\\to\\\\mu$ for a search selecting $\\\\mu^+\\\\mu^-$ , then one might use a sample of $\\\\mu^\\\\pm\\\\mu^\\\\pm$ events as a control region. Here the motivation for using a data-driven approach is that modeling the processes that lead to $j\\\\to\\\\mu$ rely heavily on the tails of fragmentation functions and detector response, which one might reasonably have some skepticism. If one assumes that control region is expected to have negligible signal in it, that backgrounds that produce $\\\\mu^+\\\\mu^-$ other than the jets faking muons, and that the rate for $j\\\\to\\\\mu^-$ is the same\\\\footnote{Given that the LHC collides $pp$ and not $p\\\\bar{p}$, there is clearly a reason to worry if this assumption is valid.} as the rate for $j\\\\to\\\\mu^+$, then one can assume $\\\\tau=1$. Thus, this background estimate is as trustworthy as the assumptions that went into it. In practice, several of these assumptions may be violated. Another approach is to use simulation of these background processes to estimate the ratio $\\\\tau$; a hybrid of the data-driven and simulation narratives.\\nLet us now consider the search for $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad}. The right plot of Fig~\\\\ref{fig:H2photons} shows the composition of the backgrounds in this search, including the continuum production of $pp\\\\to\\\\gamma\\\\gamma$, the $\\\\gamma$+jets process with a jet faking a photon $j\\\\to\\\\gamma$, and the multi jet process with two jets faking photons. The continuum production of $\\\\gamma\\\\gamma$ has a theoretical uncertainty that is much larger than the statistical fluctuations one would expect in the data. Similarly, the rate of jets faking photons is sensitive to fragmentation and the detector simulation. These uncertainties are large compared to the statistical fluctuations in the data itself. Thus we can use the distribution in Fig~\\\\ref{fig:H2photons} to measure the total background rate. Of course, the signal would also be in this distribution, so one either needs to apply a mass window around the signal and consider the region outside of the window as a sideband control sample or model the signal and background contributions to the distribution. In the case of the $H\\\\to\\\\gamma\\\\gamma$ shown in Fig~\\\\ref{fig:H2photons}~\\\\cite{ATLAS-CONF-2011-161,ATLAS:2012ad} the modeling of the distribution signal and background distributions is not based on histograms from simulation, but instead a continuous function is used as an effective model. I will discuss this effective modeling narrative below, but point out that here this is another example of a hybrid narrative.\\nThe final example to consider is an extension of the `on/off' model, often referred to as the `ABCD' method. Let us start with the `on/off' model:\\n\\\\mbox{ $\\\\Pois(n_{\\\\rm SR} | \\\\nu_S + \\\\nu_B)\\\\cdot \\\\Pois(n_{\\\\rm CR}|\\\\tau\\\\nu_B)$}. As mentioned above, this requires that one estimate $\\\\tau$ either from simulation or through some assumptions. The ABCD method aims to estimate introduce two new control regions that can be used to measure $\\\\tau$. To see this, let us imagine that the signal and control regions correspond to requiring some continuous variable $x$ being greater than or less than some threshold value $x_c$. If we could introduce a second discriminating variable $y$ such that the distribution for background factorizes $f_B(x,y)=f_B(x)f_B(y)$, then we have a handle to measure the factor $\\\\tau$. Typically, one introduces a threshold $y_c$ so that the signal contribution is small below this threshold\\\\footnote{The relative sign of the cut is not important, but has been chosen for consistency with Fig~\\\\ref{fig:ABCD}.}. Figure~\\\\ref{fig:ABCD} shows an example where $x_c=y_c=5$. With this we these two thresholds we have four regions that we can schematically refer to as A, B, C, and D. In the case of simply counting events in these regions we can write the total expectation as\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + 1\\\\cdot\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\tau_B\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\tau_C\\\\nu_A \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + \\\\tau_B\\\\tau_C\\\\nu_A \\n\\\\end{eqnarray}\\nwhere $\\\\mu$ is the signal rate in region A, $\\\\epsilon_i$ is the ratio of the signal in the regions B, C, D with respect to the signal in region A, $\\\\nu_i^{MC}$ is the rate of background in each of the regions being estimated from simulation, $\\\\nu_i$ is the rate of the background being estimated with the data driven technique in the signal region, and $\\\\tau_i$ are the ratios of the background rates in the regions B, C, and D with respect to the background in region A. The key is that we have used the factorization $f_B(x,y)=f_B(x)f_B(y)$ to write $\\\\tau_D=\\\\tau_B\\\\tau_C$. The right panel of Fig.~\\\\ref{fig:ABCD} shows a more complicated extension of the ABCD method from a recent ATLAS SUSY analysis~\\\\cite{ATLAS:2011ad}.\\nd\\nAn alternative parametrization, which can be more numerically stable is\\\\\\\\\\n\\\\begin{eqnarray}\\n\\\\nu_A &=& 1\\\\cdot\\\\mu + \\\\nu_A^{MC} + \\\\eta_C\\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_B &=& \\\\epsilon_B\\\\mu \\\\,+ \\\\nu_B^{MC} + \\\\eta_B\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_C &=& \\\\epsilon_C\\\\mu \\\\,+ \\\\nu_C^{MC} + \\\\eta_C\\\\nu_D \\\\\\\\\\\\nonumber\\n\\\\nu_D &=& \\\\epsilon_D\\\\mu \\\\,+ \\\\nu_D^{MC} + 1\\\\cdot\\\\nu_D \\n\\\\end{eqnarray}\\n\"}},\n",
       "       {'entity_name': 'matrix element method', 'entity_type': 'analysis_technique', 'description': 'A technique in particle physics that employs a probability model over multi-dimensional discriminating variables to distinguish the process of interest from background processes. It enhances sensitivity in analyses by utilizing fully differential cross sections at the parton level, allowing for improved exploration of parameter spaces and often incorporates multivariate algorithms.', 'relevant_passages': {\"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{The Matrix Element Method}\\nIdeally, one would not use a single discriminating variable to distinguish the process of interest from the other background processes, but instead would use as much discriminating power as possible. This implies forming a probability model over a multi-dimensional discriminating variable (ie. a multivariate analysis technique). In principle, both the histogram-based and kernel-based approach generalize to distributions of multi-dimensional discriminating variables; however, in practice, they are limited to only a few dimensions. In the case of histograms this is particularly severe unless one employs clever binning choices, while in the kernel-based approach one can model up to about 5-dimensional distributions with reasonable Monte Carlo sample sizes. In practice, one often uses multivariate algorithms like Neural Networks or boosted decision trees\\\\footnote{A useful toolkit for high-energy physics is TMVA, which is packaged with ROOT~\\\\cite{tmva}.} to map the multiple variables into a single discriminating variable. Often these multivariate techniques are seen as somewhat of a black-box. If we restrict ourselves to discriminating variables associated with the kinematics of final state particles (as opposed to the more detailed signature of particles in the detector), then we can often approximate he detailed simulation of the detector with a parametrized detector response. If we denote the kinematic configuration of all the final state particles in the Lorentz invariant phase space as $\\\\Phi$, the initial state as $i$, the matrix element (potentially averaged over unmeasured spin configurations) as $\\\\mathcal{M}(i,\\\\Phi)$, and the probability due to parton density functions for the initial state $i$ going into the hard scattering as $f(i)$, then we can write that the distribution of the, possibly multi-dimensional, discriminating variable $x$ as\\n\\\\begin{equation}\\nf(x) \\\\propto \\\\int d\\\\Phi \\\\, f(i) |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) \\\\;,\\n\\\\end{equation}\\nwhere $W(x|\\\\Phi)$ is referred to as the transfer function of $x$ given the final state configuration $\\\\Phi$. It is natural to think of $W(x|\\\\Phi)$ as a conditional distribution, but here I let $W$ encode the efficiency and acceptance so that we have\\n\\\\begin{equation}\\n\\\\frac{\\\\sigma_{\\\\rm eff.}}{\\\\sigma} = \\\\frac{\\\\int dx \\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) }{\\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 }\\\\;.\\n\\\\end{equation}\\nOtherwise, the equation above looks like another application one Bayes's theorem where $W(x|\\\\Phi)$ plays the role of the pdf/likelihood function and $\\\\mathcal{M}(i,\\\\Phi)$ plays the role of the prior over the $\\\\Phi$. It is worth pointing out that this is a frequentist use of Bayes's theorem since $d\\\\Phi$ is the Lorentz invariant phase space which explicitly has a measure associated with it.\\n\", \"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\"}},\n",
       "       {'entity_name': 'transfer function', 'entity_type': 'statistics_concept', 'description': 'A function that describes the relationship between input and output variables in a probabilistic model, often used to encode efficiency and acceptance in particle physics analyses.', 'relevant_passages': {\"\\\\section{Modeling and the Scientific Narrative}\\n\\\\subsection{The Matrix Element Method}\\nIdeally, one would not use a single discriminating variable to distinguish the process of interest from the other background processes, but instead would use as much discriminating power as possible. This implies forming a probability model over a multi-dimensional discriminating variable (ie. a multivariate analysis technique). In principle, both the histogram-based and kernel-based approach generalize to distributions of multi-dimensional discriminating variables; however, in practice, they are limited to only a few dimensions. In the case of histograms this is particularly severe unless one employs clever binning choices, while in the kernel-based approach one can model up to about 5-dimensional distributions with reasonable Monte Carlo sample sizes. In practice, one often uses multivariate algorithms like Neural Networks or boosted decision trees\\\\footnote{A useful toolkit for high-energy physics is TMVA, which is packaged with ROOT~\\\\cite{tmva}.} to map the multiple variables into a single discriminating variable. Often these multivariate techniques are seen as somewhat of a black-box. If we restrict ourselves to discriminating variables associated with the kinematics of final state particles (as opposed to the more detailed signature of particles in the detector), then we can often approximate he detailed simulation of the detector with a parametrized detector response. If we denote the kinematic configuration of all the final state particles in the Lorentz invariant phase space as $\\\\Phi$, the initial state as $i$, the matrix element (potentially averaged over unmeasured spin configurations) as $\\\\mathcal{M}(i,\\\\Phi)$, and the probability due to parton density functions for the initial state $i$ going into the hard scattering as $f(i)$, then we can write that the distribution of the, possibly multi-dimensional, discriminating variable $x$ as\\n\\\\begin{equation}\\nf(x) \\\\propto \\\\int d\\\\Phi \\\\, f(i) |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) \\\\;,\\n\\\\end{equation}\\nwhere $W(x|\\\\Phi)$ is referred to as the transfer function of $x$ given the final state configuration $\\\\Phi$. It is natural to think of $W(x|\\\\Phi)$ as a conditional distribution, but here I let $W$ encode the efficiency and acceptance so that we have\\n\\\\begin{equation}\\n\\\\frac{\\\\sigma_{\\\\rm eff.}}{\\\\sigma} = \\\\frac{\\\\int dx \\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 \\\\, W(x | \\\\Phi) }{\\\\int d\\\\Phi \\\\, |\\\\mathcal{M}(i,\\\\Phi)|^2 }\\\\;.\\n\\\\end{equation}\\nOtherwise, the equation above looks like another application one Bayes's theorem where $W(x|\\\\Phi)$ plays the role of the pdf/likelihood function and $\\\\mathcal{M}(i,\\\\Phi)$ plays the role of the prior over the $\\\\Phi$. It is worth pointing out that this is a frequentist use of Bayes's theorem since $d\\\\Phi$ is the Lorentz invariant phase space which explicitly has a measure associated with it.\\n\"}},\n",
       "       {'entity_name': 'feldmancousins method for confidence intervals', 'entity_type': 'analysis_technique', 'description': 'A statistical technique for constructing confidence intervals that effectively incorporates uncertainties and nuisance parameters. This method addresses issues such as the flip-flopping problem by utilizing likelihood ratios to select intervals based on the parameter of interest, ensuring robust parameter estimation in the presence of background noise and discrete data.', 'relevant_passages': {\"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The test statistics and estimators of $\\\\mu$ and $\\\\vec\\\\theta$}\\nThis definitions in this section are all relative to a given dataset $\\\\datasim$ and value of the global observables $\\\\globs$, thus we will suppress their appearance. The nomenclature follows from Ref.~\\\\cite{asimov}.\\nThe maximum likelihood estimates (MLEs) $\\\\hat\\\\mu$ and $\\\\hat{\\\\vec\\\\theta}$ and the values of the parameters that maximize the likelihood function $L(\\\\mu,\\\\vec\\\\theta)$ or, equivalently, minimize $-\\\\ln L(\\\\mu,\\\\vec\\\\theta)$. The dependence of the likelihood function on the data propagates to the values of the MLEs, so when needed the MLEs will be given subscripts to indicate the data set used. For instance, $\\\\hat{\\\\vec\\\\theta}_{\\\\rm obs}$ is the MLE of $\\\\vec\\\\theta$ derived from the observed data and global observables. \\nThe conditional maximum likelihood estimate (CMLEs) $\\\\hathatthetamu$ is the value of $\\\\vec\\\\theta$ that maximizes the likelihood function with $\\\\mu$ fixed; it can be seen as a multidimensional function of the single variable $\\\\mu$. Again, the dependence on $\\\\datasim$ and $\\\\globs$ is implicit. This procedure for choosing specific values of the nuisance parameters for a given value of $\\\\mu$, $\\\\datasim$, and $\\\\globs$ is often referred to as ``profiling''. Similarly, $\\\\hathatthetamu$ is often called ``the profiled value of $\\\\vec\\\\theta$''.\\nGiven these definitions, we can construct the profile likelihood ratio\\n\\\\begin{equation}\\n{\\\\lambda}({\\\\mu}) = \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } \\\\;,\\n\\\\end{equation}\\nwhich depends explicitly on the parameter of interest $\\\\mu$, implicitly on the data $\\\\datasim$ and global observables $\\\\globs$, and is independent of the nuisance parameters $\\\\vec\\\\theta$ (which have been eliminated via ``profiling'').\\nIn any physical theory the rate of signal events is non-negative, thus $\\\\mu\\\\ge 0$. However, it is often convenient to allow $\\\\mu<0$ (as long as the pdf $f_c(x_c | \\\\mu,\\\\vec\\\\theta)\\\\ge 0$ everywhere). In particular, $\\\\hat\\\\mu<0$ indicates a deficit of events signal-like with respect to the background only and the boundary at $\\\\mu=0$ complicates the asymptotic distributions. Ref.~\\\\cite{asimov} uses a trick that is equivalent to requiring $\\\\mu\\\\ge 0$ while avoiding the formal complications of a boundary, which is to allow $\\\\mu< 0$ and impose the constraint in the test statistic itself. In particular, one defines $\\\\tilde \\\\lambda(\\\\mu)$\\n\\\\begin{equation}\\n\\\\tilde{\\\\lambda}({\\\\mu}) = \\\\left\\\\{ \\n\\\\begin{array}{ll} \\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) }\\n{L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}}) } & \\\\hat{\\\\mu} \\\\ge 0 , \\\\\\\\*[0.3 cm]\\n\\\\frac{ L(\\\\mu, \\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu)) } {L(0,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0)) } & \\\\hat{\\\\mu} < 0 \\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nThis is not necessary when ensembles of pseudo-experiments are generated with ``Toy'' Monte Carlo techniques, but since they are equivalent we will write $\\\\tilde\\\\lambda$ to emphasize the boundary at $\\\\mu=0$.\\nFor discovery the test statistic $\\\\tilde{q}_0$ is used to differentiate the background-only hypothesis $\\\\mu=0$ from the alternative hypothesis $\\\\mu>0$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{0} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} > 0\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} \\\\le 0 \\n\\\\end{array} \\\\right. \\n\\\\end{equation}\\nNote that $\\\\tilde{q}_0$ is test statistic for a one-sided alternative. Note also that if we consider the parameter of interest $\\\\mu\\\\ge 0$, then it is equivalent to the two-sided test (because there are no values of $\\\\mu$ less than $\\\\mu=0$. \\nFor limit setting the test statistic $\\\\tilde{q}_{\\\\mu}$ is used to differentiate the hypothesis of signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser rate $\\\\mu'<\\\\mu$:\\n\\\\begin{equation}\\n\\\\tilde{q}_{\\\\mu} = \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{ll} - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) & \\\\hat{\\\\mu} \\\\le \\\\mu\\n\\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\n\\\\end{array} \\\\right. \\\\quad = \\\\quad \\\\: \\\\left\\\\{ \\\\!\\n\\\\! \\\\begin{array}{lll} - 2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(0, \\\\hat{\\\\hat{\\\\theta}}(0))} &\\n\\\\hat{\\\\mu} < 0 \\\\;, \\\\\\\\*[0.2 cm] -2 \\\\ln \\\\frac{L(\\\\mu,\\n\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))} {L(\\\\hat{\\\\mu}, \\\\hat{\\\\vec{\\\\theta}})} &\\n0 \\\\le \\\\hat{\\\\mu} \\\\le \\\\mu \\\\;, \\\\\\\\*[0.2 cm] 0 & \\\\hat{\\\\mu} > \\\\mu \\\\;.\\n\\\\end{array} \\\\right.\\n\\\\end{equation}\\nNote that $\\\\tilde{q}_{\\\\mu}$ is a test statistic for a one-sided alternative; it is a test statistic for a one-sided upper limit. \\nThe test statistic $\\\\tilde{t}_\\\\mu$ is used to differentiate signal being produced at a rate $\\\\mu$ from the alternative hypothesis of signal events being produced at a lesser or greater rate $\\\\mu' \\\\ne\\\\mu$.\\n\\\\begin{equation}\\n\\\\tilde{t}_{\\\\mu} = - 2 \\\\ln \\\\tilde{\\\\lambda}(\\\\mu) \\\\; . \\n\\\\end{equation}\\nNote that $\\\\tilde{t}_\\\\mu$ is a test statistic for a two-sided alternative (as in the case of the Feldman-Cousins technique, though this is more general as it incorporates nuisance parameters). Note that if we consider the parameter of interest $\\\\mu\\\\ge 0$ and we the test at $\\\\mu=0$ then there is no ``other side'' and we have $\\\\tilde{t}_{\\\\mu=0} = \\\\tilde{q}_0$. Finally, if one relaxes the constraint $\\\\mu\\\\ge0$ then the two-sided test statistic is written $t_\\\\mu$ or, simply, $-2\\\\ln\\\\lambda(\\\\mu)$.\\n\", \"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\", '\\\\section{Discoveries and upper limits}\\n\\\\subsection{Feldman--Cousins intervals}\\nA solution to the flip-flopping problem was developed by Feldman and Cousins~\\\\cite{feldman_cousins}.\\nThey proposed to select confidence interval based on a likelihood-ratio criterion.\\nGiven a value $\\\\theta=\\\\theta_0$ of the parameter of interest, the\\nFeldman--Cousins confidence interval is defined as:\\n\\\\begin{equation}\\nR_\\\\mu = \\\\left\\\\{x : \\\\frac{L(x;\\\\theta_0)}{L(x;\\\\hat{\\\\theta})} >k_\\\\alpha\\\\right\\\\}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\theta}$ is the best-fit value for $\\\\theta$ and the constant $k_\\\\alpha$\\nshould be set in order to ensure the desired confidence level $1-\\\\alpha$.\\nAn example of the confidence belt computed with the Feldman--Cousins approach\\nis shown in Fig.~\\\\ref{fig:fcBelt}\\nfor the Gaussian case illustrated in Fig.~\\\\ref{fig:flipFlop}.\\nWith the Feldman--Cousins approach, the confidence interval smoothly changes\\nfrom a fully asymmetric one, which leads to an upper limit, for low values of $x$, to\\nan asymmetric interval for higher values of $x$ interval, then finally a symmetric interval\\n(to a very good approximation) is obtained for large values of $x$, recovering\\nthe usual result as in Fig.~\\\\ref{fig:flipFlop}.\\nEven for the simplest Gaussian case, the computation of Feldman--Cousins intervals\\nrequires numerical treatment and for complex cases their computation may be very CPU intensive.\\n', \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Upper limits for event counting experiments}\\nThe simplest search for a new signal consists of counting the number of events\\npassing a specified selection.\\nThe number of selected events $n$ is distributed according to a Poissonian distribution\\nwhere the expected value, in case of presence of signal plus background ($H_1$)\\nis $s + b$, and for background only ($H_0$) is $b$.\\nAssume we count $n$ events, we then want to compare the two hypotheses $H_1$ and $H_0$.\\nAs simplest case, we can assume that $b$ is known with negligible uncertainty.\\nIf not, uncertainty on its estimate must be taken into account.\\nThe likelihood function for this case can be written as:\\n\\\\begin{equation}\\nL(n;s) = \\\\frac{(s+b)^n}{n!}e^{-(s+b)}\\\\,.\\n\\\\end{equation}\\n$H_0$ corresponds to the case $s=0$.\\nUsing the Bayesian approach, an upper limit\\n$s^{\\\\mathrm{up}}$ on $s$ can be determined by requiring that the posterior probability\\ncorresponding to the interval $[0,s^{\\\\mathrm{up}}]$ is equal to the confidence\\nlevel $1-\\\\alpha$:\\n\\\\begin{equation}\\n1-\\\\alpha = \\\\int_0^{s^{\\\\mathrm{up}}} P(s|n)\\\\,\\\\mathrm{d}s =\\n\\\\frac{\\n\\\\displaystyle\\\\int_0^{s\\\\mathrm{up}} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}{\\n\\\\displaystyle\\\\int_0^{+\\\\infty} L(n;a)\\\\pi(s)\\\\,\\\\mathrm{d}s\\n}\\\\,.\\n\\\\end{equation}\\nThe choice of a uniform prior, $\\\\pi(s)=1$, simplifies the computation and\\nEq.~(\\\\ref{eq:poisBayesUL}) reduces to~\\\\cite{Helene}:\\n\\\\begin{equation}\\n\\\\alpha = e^{-s^{\\\\mathrm{up}}}\\\\frac{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{(s^{\\\\mathrm{up}}+b)^m}{m!}\\n}{\\n\\\\displaystyle\\\\sum_{m=0}^n\\\\frac{b^m}{m!}\\n}\\\\,.\\n\\\\end{equation}\\nUpper limits obtained with Eq.~(\\\\ref{eq:Helene}) are shown in Fig.~\\\\ref{fig:Helene}.\\nIn the case $b=0$, the results obtained in Eq.~(\\\\ref{eq:BayesianPoissonUL90}) and~(\\\\ref{eq:BayesianPoissonUL95}) are\\nagain recovered.\\nFrequentist upper limits for a counting experiment can be easily computed\\nin case of negligible background ($b=0$). If zero events are observed ($n=0$),\\nthe likelihood function simplifies to:\\n\\\\begin{equation}\\nL(n=0;s) = \\\\mathrm{Poiss}(0;s) = e^{-s}\\\\,.\\n\\\\end{equation}\\nThe inversion of the fully asymmetric Neyman belt reduces to:\\n\\\\begin{equation}\\nP(n\\\\le0;s^{\\\\mathrm{up}}) = P(n=0;s^{\\\\mathrm{up}}) = \\\\alpha \\\\implies s^{\\\\mathrm{up}} = -\\\\ln\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhich lead to results that are numerically identical to the Bayesian computation:\\n\\\\begin{eqnarray}\\ns & < & s^{\\\\mathrm{up}} = 2.303\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.1\\\\,\\\\text{(90\\\\ s & < & s^{\\\\mathrm{up}} = 2.996\\\\quad\\\\text{for}\\\\quad\\\\alpha=0.05\\\\,\\\\text{(95\\\\\\\\end{eqnarray}\\nIn spite of the numerical coincidence, the interpretation of frequentist and Bayesian upper limits remain very different.\\nUpper limits from Eq.~(\\\\ref{eq:FreqPoissonUL90}) and~(\\\\ref{eq:FreqPoissonUL95})\\nanyway suffer from the flip-flopping problem and the coverage is spoiled when deciding to switch\\nfrom an upper limit to a central value depending on the observed significance level.\\nFeldman--Cousins intervals cure the flip-flopping issue and ensure the correct coverage\\n(or may overcover for discrete variables).\\nUpper limits at 90\\\\are reported in Fig.~\\\\ref{fig:fcUL}.\\nThe ``ripple'' structure is due to the discrete nature of Poissonian counting.\\nIt's evident from the figure that, even for $n = 0$, the upper limit decrease as $b$ increases (apart from ripple effects).\\nThis means that if two experiment are designed for an expected background of --say-- 0.1 and 0.01, the\\n``worse'' experiment (i.e.: the one which expects 0.1 events) achieves the best upper limit in case\\nno event is observed ($n=0$), which is the most likely outcome if no signal is present.\\nThis feature was noted in the 2001 edition of the PDG~\\\\cite{PDG2001}\\n\\\\begin{displayquote}\\n{\\\\it The intervals constructed according to the unified procedure [Feldman--Cousins] for a Poisson variable $n$ consisting\\nof signal and background have the property that for $n = 0$ observed events, the upper limit decreases for increasing\\nexpected background. This is counter-intuitive, since it is known that if $n = 0$ for the experiment in question,\\nthen no background was observed, and therefore one may argue that the expected background should not be relevant.\\nThe extent to which one should regard this feature as a drawback is a subject of some controversy.}\\n\\\\end{displayquote}\\nThis counter-intuitive feature of frequentist upper limits is one of the reasons that led to the use in High-Energy Physics of\\na modified approach, whose main feature is that is also prevents rejecting cases where the experiment has little sensitivity\\ndue to statistical fluctuation, as will be described in next Section.\\n\", \"\\\\section{Upper limits}\\n\\\\subsection{Limits in the presence of background}\\nThis is where it gets tricky.\\nTypically an experiment may observe $N_D$ events, with an expected background $N_B$ and efficiency $\\\\eta$, and wants to present results for $N_S={N_D-N_B \\\\over \\\\eta}$.\\nUncertainties in $\\\\eta$ and $N_B$ are handled by profiling or marginalising.\\nThe problem is that the \\n{\\\\it actual number} of background events is not $N_B$ but Poisson in $N_B$.\\nSo in a straightforward case, if you observe twelve events, with expected background 3.4 and $\\\\eta=1$\\nit is obviously sensible to say $N_S=8.6$\\n(though the error is $\\\\sqrt{12}$ not $\\\\sqrt{8.6}$)\\nBut suppose, with the same background, you see four events, three events or zero events.\\nCan you say $N_S=0.6$? or $-0.4$? Or $-3.4$???\\nWe will look at four methods of handling this, considering as an example the observation of three events with expected background 3.40 and wanting to present the 95\\\\ \\n\\\\subsubsection{Method 1: Pure frequentist}\\n$N_D-N_B$ is an unbiased estimator of $N_S$ and its properties are known.\\nQuote the result. Even if it is non-physical.\\nThe argument for doing so\\nis that\\nthis is needed for balance: if there is really no signal, approximately half of the experiments will give positive values and half negative. \\nIf the negative results are not published, but the positive ones are, the world average will be spuriously high.\\nFor a 95\\\\clearly one of them. So what?\\nA counter-argument is that if\\n$N_D<N_B$, we {\\\\it know} that the background has fluctuated downwards. But this cannot be incorporated \\ninto the formalism.\\nAnyway, the upper limit from 3 is 7.75, as $\\\\sum_0^3 e^{-7.75}7.75^r/r! = 0.05$, and the \\n95\\\\\\n\\\\subsubsection{Method 2: Go Bayesian}\\nAssign a uniform prior to $N_S$, for $N_S>0$, zero for $N_S<0$.\\nThe posterior is then just the likelihood, $P(N_S | N_D,N_B)=e^{-(N_S+N_B)}{(N_S+N_B)^{N_D} \\\\over N_D!}$.\\nThe required limit is obtained from integrating $\\\\int_0^{N_{hi}} P(N_S)\\\\, dN_S = 0.95$\\nwhere\\n$P(N_S)\\\\propto e^{-(N_s+3.40)}{(N_s+3.4)^3 \\\\over 3!}$; this is illustrated in Fig.~\\\\ref{fig:Bayeslimit}\\nand the value of the limit is \\n5.21.\\n\\\\subsubsection{Method 3: Feldman-Cousins}\\nThis---called `the unified approach' by Feldman and Cousins~\\\\cite{FC}---takes a step backwards\\nand considers the ambiguity in the use of confidence belts. \\nIn principle, if you decide to work at, say, 90\\\\This is shown in Fig.~\\\\ref{fig:FC1}.\\nIn practice, if you happen to get a low result you would quote an upper limit, but if you get a high result you would quote a central limit.\\nThis, which they call `flip-flopping', is illustrated in the plot by a break shown here for $r=10$. \\nNow the confidence belt is the green one for $r< 10$ and the red one for $r\\\\geq 10$. The\\nprobability of lying in the band is no longer 90\\\\Flip-flopping invalidates the Frequentist construction, leading to undercoverage. \\nThey show how to avoid this. You draw the plot slightly differently:\\n$r \\\\equiv N_D$ is still the horizontal variable, but as the vertical variable you use $N_S$. \\n(This means a different plot for any different $N_B$, whereas the previous Poisson plot is universal, but this is not a problem.)\\nThis is to be filled using $P(r;N_s)=e^{-(N_s+N_B)}{(N_S+N_B)^r \\\\over r!}$\\\\ .\\nFor each $N_S$ you define a region $R$ such that $\\\\sum_{r\\\\epsilon R}P(r;N_s) \\\\geq 90\\\\You have a choice of strategy that goes beyond `central' or `upper limit': one \\nplausible suggestion would be to\\nrank $r$ by probability and take them in order until the desired total probability content is achieved (which would, incidentally, give the shortest interval).\\nHowever this has the drawback that outcomes with $r < N_B$ will have small probabilities and be excluded for all $N_S$, so that, if such a result does occur, one cannot say anything constructive, just `This was unlikely'. \\nAn improved form of this suggestion is that for each $N_S$, considering each $r$ you compare $P(r;N_S)$ with the largest possible value obtained by varying $N_S$. This is easier than it sounds because this highest value is either at $N_S=r-N_B$ (if $r\\\\geq N_B$) or $N_S=0$ (if $r\\\\leq N_B$ ).\\nRank on the ratio $P(r;N_S)/P(r;N^{best}_S)$ and again take them in order till their sum gives the desired probability.\\nThis gives a band as shown in Fig.~\\\\ref{fig:FC2}, which has $N_B=3.4$. You can see that \\n`flip-flopping' occurs naturally: for small values of $r$ one just has an upper limit, whereas for larger values, above $r=7$, one obtains a lower limit as well. Yet there is a single band, and the coverage is\\ncorrect (i.e. it does not undercover).\\nIn the case we are considering, $r=3$, just an upper limit is given, at $4.86$. \\nLike other good ideas, this has not found universal favour. Two arguments are raised against the method.\\nFirst, that it deprives the physicist of the choice of whether to publish an upper limit or a range. \\nIt could be embarrassing if you\\nlook for something weird and are `forced' to publish a non-zero result. \\nBut this is actually the point, and in such cases one can always explain\\nthat the limits should not be taken as implying that the quantity actually is nonzero.\\nSecondly, if two experiments with different $N_B$ get the same small $N_D$, the one with the higher $N_B$ will quote a smaller limit on $N_S$. The worse experiment gets the better result, which is clearly unfair!\\nBut this is not comparing like with like: for a `bad' experiment with large background to get a small number of events is much less likely than it is for a `good' low background experiment.\\n\\\\subsubsection {Method 4: $CL_s$}\\nThis is a modification of the standard frequentist approach to include the \\nfact, as mentioned above, that a small observed signal implies a downward \\nfluctuation in background~\\\\cite{Read}. Although presented here using just numbers of events, the method is usually extended to use the full likelihood of the result, as will be discussed in Section~\\\\ref{subsection:Extension}.\\nDenote the (strict frequentist) \\nprobability of getting a result this small (or less) from $s+b$ events as \\n$CL_{s+b}$, and the equivalent probability from pure background as \\n$CL_b$ (so \\n$CL_b=CL_{s+b}$ for $s=0$).\\nThen introduce\\n\\\\begin{equation}\\nCL_s={CL_{s+b} \\\\over CL_b}\\n\\\\quad.\\n\\\\end{equation}\\nLooking at Fig.~\\\\ref{fig:CLS}, the $CL_{s+b}$ curve shows that if $s+b$ is small then the probability of getting three events or less is high, near 100\\\\the probability of only getting three events or less is only 5\\\\\\nThe point $s+b=3.4$ corresponds to $s=0$, at which the probability $CL_b$ is 56\\\\incorporate this by renormalizing the (blue) $CL_{s+b}$ curve to have a maximum of 100\\\\physically sensible region, dividing it by 0.56 to get the (green) $CL_s$ curve.\\nThis is treated in the same way as the $CL_{s+b}$ curve, reading off the point at $s+b=8.61$ where it falls to 5\\\\This is larger than the strict frequentist limit: the method over-covers (which, as we have seen, is allowed if not encouraged)\\nand is, in this respect `conservative'\\\\footnote{`Conservative' is a misleading word. It is used by people \\ndescribing their analyses to \\nimply safety and caution, whereas it usually entails cowardice and sloppy thinking.}. This is the same value as the Bayesian Method 2, as it makes the same assumptions. \\n$CL_s$ is not frequentist, just `frequentist inspired'. In terms of statistics there is perhaps little in its favour. But it has an intuitive appeal, and is widely used.\\n\\\\subsubsection{Summary so far}\\nGiven three observed events, and an expected background of 3.4 events, what is\\nthe 95\\\\Possible answers are shown in table~\\\\ref{tab:summary}.\\n\\\\begin{table}[h]\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nStrict Frequentist & 4.35 \\\\\\\\\\nBayesian (uniform prior) & 5.21 \\\\\\\\\\nFeldman-Cousins & 4.86 \\\\\\\\\\n$CL_s$ & 5.21 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{ Upper limits from different methods}\\n\\\\end{table}\\nWhich is `right'? Take your pick!\\nAll are correct. (Well, not wrong.). The golden rule is to say what you are doing, and if possible give the raw numbers. \\n\\\\subsubsection{Extension: not just counting numbers}\\nThese examples have used \\nsimple counting experiments. But a simple number does not (usually) exploit the full information.\\nConsider the illustration in Fig.~\\\\ref{fig:beyondsimple}. One is searching for (or putting an upper limit on) some broad resonance around 7~GeV. One could count the number of events inside some window\\n(perhaps 6 to 8~GeV?) and subtract the estimated background. This might work with high statistics, as in the left, but would be pretty useless with small numbers, as in the right. It is clearly not optimal \\njust to count an event as `in', whether it is at 7.0 or 7.9, and to treat an event as `out', if it is at 8.1 or \\n10.1.\\nIt is better to calculate the \\nLikelihood $\\\\ln L_{s+b}=\\\\sum_i \\\\ln{N_s S(x_i)+N_b B(x_i)} \\\\quad;\\\\quad \\\\ln{ L_b}=\\\\sum_i \\\\ln{N_b B(x_i)}$.\\nThen, for example using $CL_s$, you can work with $L_{s+b}/L_b$, or $-2 \\\\ln{(L_{s+b}/L_b)}$.\\nThe confidence/probability quantities can be found from simulations, or sometimes from data.\\n\\\\subsubsection{Extension: From numbers to masses}\\nLimits on numbers of events can readily be translated into limits on branching ratios,\\n$BR={N_s \\\\over N_{total}}$,\\nor limits on cross sections,\\n$\\\\sigma={N_s \\\\over \\\\int {\\\\cal L} dt}$\\\\ .\\nThese may translate to limits on other, theory, parameters.\\nIn the Higgs search (to take an example) the cross section depends on the mass, $M_H$---and so does the detection efficiency---which may require changing strategy (hence different backgrounds). This leads to the need\\nto basically repeat the analysis for all (of many) $M_H$ values. This can be presented in two ways. \\nThe first is shown in Fig.~\\\\ref{fig:significanceplot}, taken from Ref.~\\\\cite{ATLAS1}. For each $M_H$ (or whatever is being studied) you search for a signal and plot the $CL_s$ (or whatever limit method you prefer) significance \\nin a {\\\\it Significance Plot}. \\nSmall values indicate that it is unlikely to get a signal this large just from background.\\nOne often also plots the expected (from MC) significance, assuming the signal hypothesis is true. This is a measure of a `good experiment'. In this case there is a discovery level \\ndrop at $M_H \\\\approx 125$~GeV, which exceeds the expected significance, though not by much: ATLAS were lucky but not incredibly lucky.\\nThe second method is---for some reason---known as the green-and-yellow plot.\\nThis is basically the same data, but fixing $CL$ at a chosen value: in Fig.~\\\\ref{fig:greenandyellow} it is 95\\\\ You find the limit on signal strength, at this confidence level, and interpret it as \\na limit on the cross section $\\\\sigma / \\\\sigma_{SM}$.\\nAgain, as well as plotting the actual data one also plots the expected (from MC) limit, with variations.\\nIf there is no signal, 68\\\\ \\nSo this figure shows the experimental result as a black line. Around 125~GeV the 95\\\\is more than the Standard Model prediction indicating a discovery. There are peaks between 200 and 300~GeV, but they do not approach the SM value, indicating that they are just fluctuations. The value rises at 600~GeV, but the green (and yellow) bands rise also, showing that the experiment is not\\nsensitive for such high masses: basically it sees nothing but would expect to see nothing.\"}},\n",
       "       {'entity_name': 'hybrid resampling method', 'entity_type': 'analysis_technique', 'description': 'A statistical technique used to estimate the distribution of a test statistic by resampling data, which helps in determining the significance of results in particle physics analyses.', 'relevant_passages': {\"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{The distribution of the test statistic and $p$-values}\\nThe test statistic should be interpreted as a single real-valued number that represents the outcome of the experiment. More formally, it is a mapping of the data to a single real-valued number: \\\\mbox{$\\\\tilde{q}_\\\\mu: \\\\datasim,\\\\globs \\\\rightarrow \\\\mathbb{R}$}. For the observed data the test statistic has a given value, eg. $\\\\tilde{q}_{\\\\mu,\\\\rm obs}$. If one were to repeat the experiment many times the test statistic would take on different values, thus, conceptually, the test statistic has a distribution. Similarly, we can use our model to generate pseudo-experiments using Monte Carlo techniques or more abstractly consider the distribution. Since the number of expected events $\\\\nu(\\\\mu,\\\\vec\\\\theta)$ and the distributions of the discriminating variables $f_c(x_c|\\\\mu,\\\\vec\\\\theta)$ explicitly depend on $\\\\vec\\\\theta$ the distribution of the test statistic will also depend on $\\\\vec\\\\theta$. Let us denote this distribution \\n\\\\begin{equation}\\nf(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\;,\\n\\\\end{equation}\\nand we have analogous expressions for each of the test statistics described above.\\nThe $p$-value for a given observation under a particular hypothesis ($\\\\mu,\\\\vec\\\\theta$) is the probability for an equally or more `extreme' outcome than observed assuming that hypothesis \\n\\\\begin{equation}\\np_{\\\\mu,\\\\vec\\\\theta} = \\\\int_{\\\\tilde{q}_{\\\\mu,\\\\rm obs}}^\\\\infty f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta) \\\\, d\\\\tilde{q}_\\\\mu\\\\;.\\n\\\\end{equation}\\nThe logic is that small $p$-values are evidence against the corresponding hypothesis. In Toy Monte Carlo approaches, the integral above is really carried out in the space of the data $\\\\int d\\\\datasim d\\\\globs$.\\nThe immediate difficulty is that we are interested in $\\\\mu$ but the $p$-values depend on both $\\\\mu$ and $\\\\vec\\\\theta$. In the frequentist approach the hypothesis $\\\\mu=\\\\mu_0$ would not be rejected unless the $p$-value is sufficiently small \\\\textit{for all} values of $\\\\vec\\\\theta$. Equivalently, one can use the supremum $p$-value for over all $\\\\vec\\\\theta$ to base the decision to accept or reject the hypothesis at $\\\\mu=\\\\mu_0$.\\n\\\\begin{equation}\\np^{\\\\rm sup}_{\\\\mu} = \\\\sup_{\\\\vec\\\\theta}\\\\; p_{\\\\mu,\\\\vec\\\\theta} \\n\\\\end{equation}\\nThe key conceptual reason for choosing the test statistics based on the profile likelihood ratio is that asymptotically (ie. when there are many events) the distribution of the profile likelihood ratio \\\\mbox{$\\\\lambda(\\\\mu=\\\\mu_{\\\\rm true})$} is independent of the values of the nuisance parameters. This follows from Wilks's theorem. In that limit $p^{\\\\rm sup}_{\\\\mu} = p_{\\\\mu,\\\\vec\\\\theta}$ for all $\\\\vec\\\\theta$. \\nThe asymptotic distributions \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu, \\\\vec\\\\theta)$} and \\\\mbox{$f(\\\\lambda(\\\\mu) | \\\\mu', \\\\vec\\\\theta)$} are known and described in Sec.~\\\\ref{sec:asymptotic}. For results based on generating ensembles of pseudo-experiements using Toy Monte Carlo techniques does not assume the form of the distribution $f(\\\\tilde{q}_\\\\mu | \\\\mu, \\\\vec\\\\theta)$, but knowing that it is approximately independent of $\\\\vec\\\\theta$ means that one does not need to calculate $p$-values for all $\\\\vec\\\\theta$ (which is not computationally feasible). Since there may still be some residual dependence of the $p$-values on the choice of $\\\\vec\\\\theta$ we would like to know the specific value of $\\\\vec\\\\theta^{\\\\rm sup}$ that produces the supremum $p$-value over $\\\\vec\\\\theta$. Since larger $p$-values indicate better agreement of the data with the model, it is not surprising that choosing $\\\\vec\\\\theta^{\\\\rm sup}=\\\\hathatthetamu$ is a good estimate of $\\\\vec\\\\theta^{\\\\rm sup}$. This has been studied in detail by statisticians, and is called the Hybrid Resampling method and is referred to in physics as the `profile construction'~\\\\cite{Feldman,Cranmer,hybridResampling,Bodhi}.\\nBased on the discussion above, the following $p$-value is used to quantify consistency with the hypothesis of a signal strength of $\\\\mu$:\\n\\\\begin{equation}\\np_{\\\\mu}=\\\\int_{\\\\tilde q_{\\\\mu,\\\\rm obs}}^{\\\\infty} f(\\\\tilde q_\\\\mu|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu,\\\\textrm{obs})) \\\\,d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nA standard 95\\\\\\nTo calculate the $CL_s$ upper limit, we define $p'_\\\\mu$ as a ratio of p-values,\\n\\\\begin{equation}\\np'_\\\\mu=\\\\frac{p_\\\\mu}{1-p_b} \\\\; ,\\n\\\\end{equation}\\nwhere $p_b$ is the $p$-value derived from the same test statistic under the background-only hypothesis\\n\\\\begin{equation}\\np_b=1-\\\\int_{\\\\tilde q_{\\\\mu,obs}}^\\\\infty f(\\\\tilde q_\\\\mu|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_\\\\mu \\\\;.\\n\\\\end{equation}\\nThe $CL_s$ upper-limit on $\\\\mu$ is denoted $\\\\mu_{up}$ and obtained by solving for $p'_{\\\\mu_{up}}=5\\\\It is worth noting that while confidence intervals produced with the ``CLs'' method over cover, a value of $\\\\mu$ is regarded as excluded at the 95\\\\ \\nFor the purposes discovery one is interested in compatibility of the data with the background-only hypothesis. Statistically, a discovery corresponds to rejecting the background-only hypothesis. This compatibility is based on the following $p$-value\\n\\\\begin{equation}\\np_0=\\\\int_{\\\\tilde q_{0,obs}}^\\\\infty f(\\\\tilde q_0|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu=0,\\\\textrm{obs}))d\\\\tilde q_0 \\\\;.\\n\\\\end{equation}\\nThis $p$-value is also based on the background-only hypothesis, but the test statistic $\\\\tilde q_0$ is suited for testing the background-only while the test statistic $\\\\tilde{q}_\\\\mu$ in Eq.~\\\\ref{eq:pb} is suited for testing a hypothesis with signal.\\nIt is customary to convert the background-only $p$-value into the quantile (or ``sigma'') of a unit Gaussian. This conversion is purely conventional and makes no assumption that the test statistic $q_0$ is Gaussian distributed. The conversion is defined as:\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1-p_0) ;\\\\,\\n\\\\end{equation}\\nwhere $\\\\Phi^{-1}$ is the inverse of the cumulative distribution for a unit Gaussian. One says the significance of the result is $Z\\\\sigma$ and the standard discovery convention is $5\\\\sigma$, corresponding to $p_0=2.87 \\\\cdot 10^{-7}$.\\n\"}},\n",
       "       {'entity_name': 'cl_s methodology', 'entity_type': 'analysis_technique', 'description': 'A statistical framework in particle physics used to set upper limits on signal strength in the presence of background noise. This methodology incorporates likelihood ratio test statistics and modifies the frequentist approach to produce confidence limits that account for potential downward fluctuations in background, ensuring that the intervals over-cover the true value more than the desired level.', 'relevant_passages': {\"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The modified frequentist approach}\\nA {\\\\it modified frequentist approach}~\\\\cite{CLs} was proposed for the first time for the\\ncombination of the results of searches for the Higgs boson by the four LEP experiments, ALEPH, DELPHI, L3 and OPAL~\\\\cite{Higgs_at_LEP}.\\nGiven a test statistic $\\\\lambda(x)$ that depends on some observation $x$, its distribution should be determined\\nunder the two hypotheses\\n$H_1$ (signal plus background) and $H_0$ (background only). The following $p$-values can be used,\\nwhere we assume that the test statistic $\\\\lambda$ tends to have small values for $H_1$ and\\nlarger values for $H_0$:\\n\\\\begin{eqnarray}\\np_{s+b}& = & P(\\\\lambda(x | H_1) \\\\ge \\\\lambda^{\\\\mathrm{obs}} )\\\\,, \\\\\\\\\\np_b & = & P(\\\\lambda(x | H_0) \\\\le \\\\lambda^{\\\\mathrm{obs}} )\\\\,. \\n\\\\end{eqnarray}\\n$p_{s+b}$ and $p_b$ can be interpreted as follows:\\n\\\\begin{itemize}\\n\\\\item $p_{s+b}$ is the probability to obtain a result which is less compatible with the signal than the observed result, assuming the signal hypothesis;\\n\\\\item $p_b$ is the probability to obtain a result less compatible with the background-only hypothesis than the observed one, assuming background only.\\n\\\\end{itemize}\\nInstead of requiring, as for a frequentist upper limit, $p_{s+b} \\\\le \\\\alpha$,\\nthe modified approach introduces a new quantity, $\\\\mathrm{CL}_s$, defined as:\\n\\\\begin{equation}\\n\\\\boxed{\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b}\\\\,,\\n}\\n\\\\end{equation}\\nand the upper limit is set by requiring $\\\\mathrm{CL}_s \\\\le \\\\alpha$.\\nFor this reason, the modified frequentist approach is also called {\\\\it ``$\\\\mathrm{CL}_s$ method''}.\\nIn practice, in most of the realistic cases, $p_b$ and $p_{s+b}$ are computed from\\nsimulated pseudoexperiments ({\\\\it toy Monte Carlo}) by approximating the probabilities\\ndefined in Eq.~(\\\\ref{eq:CLSpsb},~\\\\ref{eq:CLSpb}) with the fraction of the total number of pseudoexperiments\\nsatisfying their respective condition:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{p_{s+b}}{1-p_b} = \\\\frac{N(\\\\lambda_{s+b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}{N(\\\\lambda_{b}\\\\ge\\\\lambda^{\\\\mathrm{obs}})}\\\\,.\\n\\\\end{equation}\\nSince $1-p_b \\\\le 1$, then $\\\\mathrm{CL}s \\\\ge p_{s+b}$, hence upper limits computed with the $\\\\mathrm{CL}_s$ method are\\nalways {\\\\it conservative}.\\nIn case the distributions of the test statistic $\\\\lambda$ (or equivalently $-2\\\\ln\\\\lambda$) for the two hypotheses $H_0$ and $H_1$\\nare well separated (Fig.~\\\\ref{fig:CLs12}, left),\\nif $H_1$ is true, than $p_b$ will have a very high chance to be very small, hence $1-p_b \\\\simeq 1$ and $\\\\mathrm{CL}_s \\\\simeq p_{s+b}$. In this case\\n$\\\\mathrm{CL}_s$ and the purely frequentist upper limits coincide.\\nIf the two distributions instead largely overlap (Fig.~\\\\ref{fig:CLs12}, right), indicating that the experiment has poor sensitivity on the\\nsignal, in case $p_b$ is large, because of a statistical fluctuation, then $1 - p_b$ becomes small.\\nThis prevents $\\\\mathrm{CL}_s$ to become too small,\\ni.e.: it prevents to reject cases where the experiment has little sensitivity.\\nIf we apply the $\\\\mathrm{CL}_s$ method to the previous counting experiment, using \\nthe observed number of events $n^{\\\\mathrm{obs}}$ as test statistic,\\nthen $\\\\mathrm{CL}s$ can be written, considering that $n$ tends to be large in case of\\n$H_1$, for this case, as:\\n\\\\begin{equation}\\n\\\\mathrm{CL}_s = \\\\frac{P(n\\\\le n^{\\\\mathrm{obs}} | s+b)}{P(n \\\\le n^{\\\\mathrm{obs}} |b)}\\\\,.\\n\\\\end{equation}\\nExplicitating the Poisson distribution, the computation gives the same result as for the Bayesian case with a uniform prior\\n(Eq.~(\\\\ref{eq:Helene})). In many cases, the $\\\\mathrm{CL}_s$ upper \\nlimits give results that are very close, numerically, to Bayesian\\ncomputations performed assuming a uniform prior.\\nOf course, this does not allow to interpret $\\\\mathrm{CL}_s$ upper limits\\nas Bayesian upper limits.\\nConcerning the interpretation of $\\\\mathrm{CL}_s$, it's worth reporting from Ref~\\\\cite{CLs} the\\nfollowing statements:\\n\\\\begin{displayquote}\\n{\\\\it A specific modification of a purely classical statistical analysis is used to avoid excluding or discovering signals which the search is in fact not sensitive to.}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it The use of\\\\, $\\\\mathrm{CL}_s$ is a conscious decision not to insist on the frequentist concept of full coverage (to guarantee that the confidence interval doesn't include the true value of the parameter in a fixed fraction of experiments).}\\n\\\\end{displayquote}\\n\\\\begin{displayquote}\\n{\\\\it Confidence intervals obtained in this manner do not have the same interpretation as traditional frequentist confidence intervals nor as Bayesian credible intervals.}\\n\\\\end{displayquote}\\n\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{One-sided intervals, CLs, power-constraints, and Negatively Biased Relevant Subsets}\\nParticle physicists regularly set upper-limits on cross sections and other parameters that are bounded to be non-negative. Standard frequentist confidence intervals should nominally cover at the stated value. The implication that a 95\\\\\\nTwo main approaches have been proposed to protect from excluding signals to which we do not consider ourselves sensitive. The first is the CLs procedure introduced by Read and described above~\\\\cite{Read2,Read1,CLsWikipedia}. The CLs procedure produce intervals that over-cover -- meaning that the intervals cover the true value more than the desired level. The coverage for small values of the cross-section approaches 100\\\\\\nAn alternate approach called power-constrained limits (PCL) is to leave the standard frequentist procedure unchanged while adding an additional requirement for a parameter point to be considered `excluded'. The additional requirement is directly a measure of the sensitivity of to that parameter point based on the notion of power (or Type II error). This approach makes the coverage of the procedure manifest~\\\\cite{2011arXiv1105.3166C}.\\nSurprisingly, one-sided upper limits on a bounded parameter are a subtle topic that has led to debates among the experts of statistics in the collaborations and a string of interesting articles from statisticians. The discussion is beyond the scope of the current version of these notes, but the interested reader is invited and encouraged to read~\\\\cite{Mandelkern2002} and the responses from notable statisticians on the topic. More recently Cousins tried to formalize the sensitivity problem in terms of a concept called Negatively Biased Relevant Subsets (NBRS)~\\\\cite{2011arXiv1109.2023C}. While the power-constrained limits do not formally emit NBRS, it is an interesting insight. Even more recently, Vitells has found interesting connections with CLs and the work of Birnbaum~\\\\cite{Birnbaum:1962,CLsWikipedia}. This connection is significant since statisticians have primarily seen CLs as an ad hoc procedure mixing the notion of size and power with no satisfying properties.\", \"\\\\section{Upper limits}\\n\\\\subsection{Limits in the presence of background}\\nThis is where it gets tricky.\\nTypically an experiment may observe $N_D$ events, with an expected background $N_B$ and efficiency $\\\\eta$, and wants to present results for $N_S={N_D-N_B \\\\over \\\\eta}$.\\nUncertainties in $\\\\eta$ and $N_B$ are handled by profiling or marginalising.\\nThe problem is that the \\n{\\\\it actual number} of background events is not $N_B$ but Poisson in $N_B$.\\nSo in a straightforward case, if you observe twelve events, with expected background 3.4 and $\\\\eta=1$\\nit is obviously sensible to say $N_S=8.6$\\n(though the error is $\\\\sqrt{12}$ not $\\\\sqrt{8.6}$)\\nBut suppose, with the same background, you see four events, three events or zero events.\\nCan you say $N_S=0.6$? or $-0.4$? Or $-3.4$???\\nWe will look at four methods of handling this, considering as an example the observation of three events with expected background 3.40 and wanting to present the 95\\\\ \\n\\\\subsubsection{Method 1: Pure frequentist}\\n$N_D-N_B$ is an unbiased estimator of $N_S$ and its properties are known.\\nQuote the result. Even if it is non-physical.\\nThe argument for doing so\\nis that\\nthis is needed for balance: if there is really no signal, approximately half of the experiments will give positive values and half negative. \\nIf the negative results are not published, but the positive ones are, the world average will be spuriously high.\\nFor a 95\\\\clearly one of them. So what?\\nA counter-argument is that if\\n$N_D<N_B$, we {\\\\it know} that the background has fluctuated downwards. But this cannot be incorporated \\ninto the formalism.\\nAnyway, the upper limit from 3 is 7.75, as $\\\\sum_0^3 e^{-7.75}7.75^r/r! = 0.05$, and the \\n95\\\\\\n\\\\subsubsection{Method 2: Go Bayesian}\\nAssign a uniform prior to $N_S$, for $N_S>0$, zero for $N_S<0$.\\nThe posterior is then just the likelihood, $P(N_S | N_D,N_B)=e^{-(N_S+N_B)}{(N_S+N_B)^{N_D} \\\\over N_D!}$.\\nThe required limit is obtained from integrating $\\\\int_0^{N_{hi}} P(N_S)\\\\, dN_S = 0.95$\\nwhere\\n$P(N_S)\\\\propto e^{-(N_s+3.40)}{(N_s+3.4)^3 \\\\over 3!}$; this is illustrated in Fig.~\\\\ref{fig:Bayeslimit}\\nand the value of the limit is \\n5.21.\\n\\\\subsubsection{Method 3: Feldman-Cousins}\\nThis---called `the unified approach' by Feldman and Cousins~\\\\cite{FC}---takes a step backwards\\nand considers the ambiguity in the use of confidence belts. \\nIn principle, if you decide to work at, say, 90\\\\This is shown in Fig.~\\\\ref{fig:FC1}.\\nIn practice, if you happen to get a low result you would quote an upper limit, but if you get a high result you would quote a central limit.\\nThis, which they call `flip-flopping', is illustrated in the plot by a break shown here for $r=10$. \\nNow the confidence belt is the green one for $r< 10$ and the red one for $r\\\\geq 10$. The\\nprobability of lying in the band is no longer 90\\\\Flip-flopping invalidates the Frequentist construction, leading to undercoverage. \\nThey show how to avoid this. You draw the plot slightly differently:\\n$r \\\\equiv N_D$ is still the horizontal variable, but as the vertical variable you use $N_S$. \\n(This means a different plot for any different $N_B$, whereas the previous Poisson plot is universal, but this is not a problem.)\\nThis is to be filled using $P(r;N_s)=e^{-(N_s+N_B)}{(N_S+N_B)^r \\\\over r!}$\\\\ .\\nFor each $N_S$ you define a region $R$ such that $\\\\sum_{r\\\\epsilon R}P(r;N_s) \\\\geq 90\\\\You have a choice of strategy that goes beyond `central' or `upper limit': one \\nplausible suggestion would be to\\nrank $r$ by probability and take them in order until the desired total probability content is achieved (which would, incidentally, give the shortest interval).\\nHowever this has the drawback that outcomes with $r < N_B$ will have small probabilities and be excluded for all $N_S$, so that, if such a result does occur, one cannot say anything constructive, just `This was unlikely'. \\nAn improved form of this suggestion is that for each $N_S$, considering each $r$ you compare $P(r;N_S)$ with the largest possible value obtained by varying $N_S$. This is easier than it sounds because this highest value is either at $N_S=r-N_B$ (if $r\\\\geq N_B$) or $N_S=0$ (if $r\\\\leq N_B$ ).\\nRank on the ratio $P(r;N_S)/P(r;N^{best}_S)$ and again take them in order till their sum gives the desired probability.\\nThis gives a band as shown in Fig.~\\\\ref{fig:FC2}, which has $N_B=3.4$. You can see that \\n`flip-flopping' occurs naturally: for small values of $r$ one just has an upper limit, whereas for larger values, above $r=7$, one obtains a lower limit as well. Yet there is a single band, and the coverage is\\ncorrect (i.e. it does not undercover).\\nIn the case we are considering, $r=3$, just an upper limit is given, at $4.86$. \\nLike other good ideas, this has not found universal favour. Two arguments are raised against the method.\\nFirst, that it deprives the physicist of the choice of whether to publish an upper limit or a range. \\nIt could be embarrassing if you\\nlook for something weird and are `forced' to publish a non-zero result. \\nBut this is actually the point, and in such cases one can always explain\\nthat the limits should not be taken as implying that the quantity actually is nonzero.\\nSecondly, if two experiments with different $N_B$ get the same small $N_D$, the one with the higher $N_B$ will quote a smaller limit on $N_S$. The worse experiment gets the better result, which is clearly unfair!\\nBut this is not comparing like with like: for a `bad' experiment with large background to get a small number of events is much less likely than it is for a `good' low background experiment.\\n\\\\subsubsection {Method 4: $CL_s$}\\nThis is a modification of the standard frequentist approach to include the \\nfact, as mentioned above, that a small observed signal implies a downward \\nfluctuation in background~\\\\cite{Read}. Although presented here using just numbers of events, the method is usually extended to use the full likelihood of the result, as will be discussed in Section~\\\\ref{subsection:Extension}.\\nDenote the (strict frequentist) \\nprobability of getting a result this small (or less) from $s+b$ events as \\n$CL_{s+b}$, and the equivalent probability from pure background as \\n$CL_b$ (so \\n$CL_b=CL_{s+b}$ for $s=0$).\\nThen introduce\\n\\\\begin{equation}\\nCL_s={CL_{s+b} \\\\over CL_b}\\n\\\\quad.\\n\\\\end{equation}\\nLooking at Fig.~\\\\ref{fig:CLS}, the $CL_{s+b}$ curve shows that if $s+b$ is small then the probability of getting three events or less is high, near 100\\\\the probability of only getting three events or less is only 5\\\\\\nThe point $s+b=3.4$ corresponds to $s=0$, at which the probability $CL_b$ is 56\\\\incorporate this by renormalizing the (blue) $CL_{s+b}$ curve to have a maximum of 100\\\\physically sensible region, dividing it by 0.56 to get the (green) $CL_s$ curve.\\nThis is treated in the same way as the $CL_{s+b}$ curve, reading off the point at $s+b=8.61$ where it falls to 5\\\\This is larger than the strict frequentist limit: the method over-covers (which, as we have seen, is allowed if not encouraged)\\nand is, in this respect `conservative'\\\\footnote{`Conservative' is a misleading word. It is used by people \\ndescribing their analyses to \\nimply safety and caution, whereas it usually entails cowardice and sloppy thinking.}. This is the same value as the Bayesian Method 2, as it makes the same assumptions. \\n$CL_s$ is not frequentist, just `frequentist inspired'. In terms of statistics there is perhaps little in its favour. But it has an intuitive appeal, and is widely used.\\n\\\\subsubsection{Summary so far}\\nGiven three observed events, and an expected background of 3.4 events, what is\\nthe 95\\\\Possible answers are shown in table~\\\\ref{tab:summary}.\\n\\\\begin{table}[h]\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nStrict Frequentist & 4.35 \\\\\\\\\\nBayesian (uniform prior) & 5.21 \\\\\\\\\\nFeldman-Cousins & 4.86 \\\\\\\\\\n$CL_s$ & 5.21 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{ Upper limits from different methods}\\n\\\\end{table}\\nWhich is `right'? Take your pick!\\nAll are correct. (Well, not wrong.). The golden rule is to say what you are doing, and if possible give the raw numbers. \\n\\\\subsubsection{Extension: not just counting numbers}\\nThese examples have used \\nsimple counting experiments. But a simple number does not (usually) exploit the full information.\\nConsider the illustration in Fig.~\\\\ref{fig:beyondsimple}. One is searching for (or putting an upper limit on) some broad resonance around 7~GeV. One could count the number of events inside some window\\n(perhaps 6 to 8~GeV?) and subtract the estimated background. This might work with high statistics, as in the left, but would be pretty useless with small numbers, as in the right. It is clearly not optimal \\njust to count an event as `in', whether it is at 7.0 or 7.9, and to treat an event as `out', if it is at 8.1 or \\n10.1.\\nIt is better to calculate the \\nLikelihood $\\\\ln L_{s+b}=\\\\sum_i \\\\ln{N_s S(x_i)+N_b B(x_i)} \\\\quad;\\\\quad \\\\ln{ L_b}=\\\\sum_i \\\\ln{N_b B(x_i)}$.\\nThen, for example using $CL_s$, you can work with $L_{s+b}/L_b$, or $-2 \\\\ln{(L_{s+b}/L_b)}$.\\nThe confidence/probability quantities can be found from simulations, or sometimes from data.\\n\\\\subsubsection{Extension: From numbers to masses}\\nLimits on numbers of events can readily be translated into limits on branching ratios,\\n$BR={N_s \\\\over N_{total}}$,\\nor limits on cross sections,\\n$\\\\sigma={N_s \\\\over \\\\int {\\\\cal L} dt}$\\\\ .\\nThese may translate to limits on other, theory, parameters.\\nIn the Higgs search (to take an example) the cross section depends on the mass, $M_H$---and so does the detection efficiency---which may require changing strategy (hence different backgrounds). This leads to the need\\nto basically repeat the analysis for all (of many) $M_H$ values. This can be presented in two ways. \\nThe first is shown in Fig.~\\\\ref{fig:significanceplot}, taken from Ref.~\\\\cite{ATLAS1}. For each $M_H$ (or whatever is being studied) you search for a signal and plot the $CL_s$ (or whatever limit method you prefer) significance \\nin a {\\\\it Significance Plot}. \\nSmall values indicate that it is unlikely to get a signal this large just from background.\\nOne often also plots the expected (from MC) significance, assuming the signal hypothesis is true. This is a measure of a `good experiment'. In this case there is a discovery level \\ndrop at $M_H \\\\approx 125$~GeV, which exceeds the expected significance, though not by much: ATLAS were lucky but not incredibly lucky.\\nThe second method is---for some reason---known as the green-and-yellow plot.\\nThis is basically the same data, but fixing $CL$ at a chosen value: in Fig.~\\\\ref{fig:greenandyellow} it is 95\\\\ You find the limit on signal strength, at this confidence level, and interpret it as \\na limit on the cross section $\\\\sigma / \\\\sigma_{SM}$.\\nAgain, as well as plotting the actual data one also plots the expected (from MC) limit, with variations.\\nIf there is no signal, 68\\\\ \\nSo this figure shows the experimental result as a black line. Around 125~GeV the 95\\\\is more than the Standard Model prediction indicating a discovery. There are peaks between 200 and 300~GeV, but they do not approach the SM value, indicating that they are just fluctuations. The value rises at 600~GeV, but the green (and yellow) bands rise also, showing that the experiment is not\\nsensitive for such high masses: basically it sees nothing but would expect to see nothing.\", \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\"}},\n",
       "       {'entity_name': 'chisquared distribution and statistics', 'entity_type': 'statistics_concept', 'description': \"A family of statistical distributions and measures used in hypothesis testing and model fitting, particularly in assessing the goodness of fit between observed data and expected values. The chi-squared distribution arises from the sum of the squares of independent standard normal random variables and is commonly utilized in various statistical tests, including Neyman's and Pearson's chi-squared tests, which evaluate how well observed data conform to expected distributions.\", 'relevant_passages': {\"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{The Profile Likelihood}\\nAs noted in Sec.~\\\\ref{sec:likelihood}, likelihood functions can be used to estimate the parameters\\non which they depend. The method of choice to do so, in a frequentist analysis, is called \\\\textbf{maximum\\nlikelihood}, a method first used by Karl Frederick Gauss, \\\\emph{The Prince of Mathematics}, but developed into a formidable statistical tool in the 1930s by \\nSir Ronald A. Fisher~\\\\cite{Fisher}, perhaps the most influential statistician of the twentieth century.\\nFisher showed that a good way to estimate the parameters of a likelihood function is to pick\\nthe value that maximizes it. Such estimates are called \\\\textrm{maximum likelihood estimates} (MLE). In general, a function into which data can be inserted to yield an MLE of\\na parameter is called a maximum likelihood estimator. For simplicity, we shall use the \\nsame abbreviation MLE\\nto mean both the estimate and the estimator and we shall not be too\\npicky about distinguishing the two. The D\\\\O\\\\ top quark\\ndiscovery example illustrates the\\nmethod. \\n\\\\begin{quote}\\n\\\\paragraph*{Example: Top Quark Discovery Revisited}\\nWe start by listing\\n\\\\begin{align*}\\n& \\\\textbf{the knowns} \\\\\\\\\\n& D = N, B \\\\text{ where} \\\\\\\\\\n& N = 17 \\\\textrm{ observed events} \\\\\\\\\\n& B = 3.8 \\\\textrm{ estimated background events with uncertainty } \\\\delta B = 0.6 \\\\\\\\\\n&\\\\textbf{and the unknowns} \\\\\\\\\\n& b \\\\quad\\\\textrm{mean background count}\\\\\\\\\\n& s \\\\quad\\\\textrm{mean signal count}.\\n\\\\end{align*} \\nNext, we construct a probability model for the data $D = N, B$ assuming that \\n$N$ and $B$ are statistically independent. Since this is a counting\\nexperiment, we shall assume that $p(x| s, b)$ is a Poisson distribution with mean\\ncount $s + b$. In the absence of details about how the background $B$ was arrived\\nat, the standard assumption is that data of the form $y \\\\pm \\\\delta y$ can be modeled\\nwith a Gaussian (or normal) density. However, we can do a bit better. Background estimates are usually based on auxiliary experiments, either real or simulated, that define control regions. \\nSuppose that the observed count in the control region is $Q$ and the mean count is $b k$, where $k$ (ideally) is the known scale factor between the control and signal regions. We can model these data with a\\nPoisson distribution with count $Q$ and mean $b k$. But, we are given $B$ and $\\\\delta B$ rather than $Q$ and $k$, so we need a model to relate the two pairs of numbers. \\nThe simplest model is $B = Q / k$ and $\\\\delta B = \\\\sqrt{Q} / k$ from which we can infer an effective count $Q$ using $Q = (B / \\\\delta B)^2$. What of the scale factor $k$? Well, since it\\nis not given, it must be estimated. The obvious estimate is $Q / B = B / \\\\delta B^2$.\\nWith these assumptions, our likelihood function is\\n\\\\begin{eqnarray}\\np(D | s, b) & = & \\\\textrm{Poisson}(N, s + b) \\\\, \\\\textrm{Poisson}(Q, bk), \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nQ & = & (B / \\\\delta B)^2 = 41.11,\\\\nonumber\\\\\\\\\\nk & = & B / \\\\delta B^2 = 10.56. \\\\nonumber\\n\\\\end{eqnarray}\\nThe first term in Eq.~(\\\\ref{eq:toplh}) is the likelihood for the count $N = 17$, while\\nthe second term is the likelihood for $B = 3.8$, or equivalently the count $Q$. The\\nfact that $Q$ is not an integer causes no difficulty: we merely write\\nthe Poisson distribution as\\n$(bk)^Q \\\\exp(-bk) / \\\\Gamma(Q+1)$, which permits continuation to non-integer counts $Q$.\\nThe maximum likelihood estimators \\nfor $s$ and $b$ are found by maximizing Eq.~(\\\\ref{eq:toplh}), that is, by solving the equations \\n\\\\begin{align}\\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial s} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{s} = N - B, \\\\nonumber\\\\\\\\ \\n\\\\frac{\\\\partial \\\\ln p(D|s, b)}{\\\\partial b} & = 0\\\\quad\\\\textrm{leading to } \\\\hat{b} = B, \\\\nonumber\\n\\\\end{align}\\nas expected.\\nA more complete analysis would account for the uncertainty in\\n$k$. One way is to introduce two more control regions with observed counts $V$ and $W$ and mean counts $v$ and $w k$, respectively, and extend \\nEq.~(\\\\ref{eq:toplh}) with two more Poisson distributions.\\n\\\\end{quote}\\n\\\\bigskip\\nThe maximum likelihood method is the most widely used method for\\nestimating parameters because it generally leads to reasonable estimates. But the\\nmethod has features, or encourages practices, which, somewhat uncharitably, we label the\\ngood, the bad, and the ugly!\\n\\\\begin{itemize}\\n\\\\item \\\\emph{The Good}\\n\\\\begin{itemize}\\n\\\\item Maximum likelihood estimators are consistent: the RMS goes to zero as more\\nand more data are included in the likelihood. This is an extremely important property,\\nwhich basically says it makes sense to take more data because we shall get more accurate results. One would not knowingly use an inconsistent estimator!\\n\\\\item If an unbiased estimator for a parameter exists the maximum\\nlikelihood method will find it.\\n\\\\item Given the MLE for $s$, the MLE for any function $y = g(s)$ of $s$ is,\\nvery conveniently, just $\\\\hat{y} = g(\\\\hat{s})$. This is a very nice practical feature which\\nmakes it possible to maximize the likelihood using the most convenient parameterization of it and then transform back to the parameter of interest at the end. \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Bad (according to some!)}\\n\\\\begin{itemize}\\n\\\\item In general, MLEs are biased.\\\\\\\\ \\\\\\\\\\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 7:} Show this\\\\\\\\\\nHint: Taylor expand $y = g(\\\\hat{s} + h)$ about the MLE $\\\\hat{s}$,\\\\\\\\\\nthen consider its ensemble average. }} \\n\\\\end{itemize}\\n\\\\item \\\\emph{The Ugly (according to some!)}\\n\\\\begin{itemize}\\n\\\\item The fact that most MLEs are biased encourages the routine application of bias correction, which can waste data and, sometimes, yield absurdities.\\n\\\\end{itemize}\\n\\\\end{itemize}\\n\\\\noindent\\nHere is an example of the seriously ugly.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nFor a discrete probability distribution $p(k)$, the \\\\textbf{moment generating function} is the ensemble average\\n\\\\begin{align*}\\nG(x) & = < e^{xk} > \\\\\\\\\\n& = \\\\sum_{k} e^{xk} \\\\, p(k).\\n\\\\end{align*}\\nFor the binomial, with parameters $p$ and $n$, this is\\n\\\\begin{align*}\\nG(x) & = (e^x p + 1 - p)^n, \\\\quad \\\\framebox{\\\\textbf{Exercise 8a:} Show this}\\n\\\\end{align*}\\nwhich is useful for calculating \\\\textbf{moments}\\n\\\\begin{align*}\\n\\\\mu_r & = \\\\left. \\\\frac{d^rG}{dx^r}\\\\right |_{x=0} = \\\\sum_k k^r \\\\, p(k),\\n\\\\end{align*}\\ne.g., $\\\\mu_2 = (np)^2 + np - np^2$ for the binomial distribution.\\nGiven that $k$ events out $n$ pass a set of cuts, the MLE of the event selection efficiency is\\nthe obvious estimate $\\\\hat{p} = k / n$. The equally obvious estimate of $p^2$ is $( k / n)^2$.\\nBut,\\n\\\\begin{align*}\\n< ( k / n)^2 > & = p^2 + V / n , \\\\quad \\\\framebox{\\\\textbf{Exercise 8b:} Show this}\\n\\\\end{align*}\\nso $(k / n)^2$ is a biased estimate of $p^2$ with positive bias $V / n$. The unbiased estimate of $p^2$ is\\n\\\\begin{align*}\\nk(k-1) / [ n (n - 1)] , \\\\quad \\\\framebox{\\\\textbf{Exercise 8c:} Show this}\\n\\\\end{align*}\\nwhich, for a single success, i.e., $k = 1$, yields the sensible estimate $\\\\hat{p} = 1 / n$, but\\nthe less than helpful one $\\\\hat{p^2} = 0!$\\n\\\\end{quote}\\n\\\\bigskip\\nIn order to infer a value for the parameter of interest, for example, \\nthe signal $s$ in\\nour 2-parameter likelihood function in Eq.~(\\\\ref{eq:toplh}), the likelihood\\nmust be reduced to one involving the parameter of interest only, here $s$, \\nby somehow getting rid of all the \\\\textbf{nuisance} parameters, here the background\\nparameter $b$. A nuisance parameter is simply a parameter that is not of current interest.\\nIn a strict frequentist calculation, this reduction to the parameter of interest must be done\\nin such a way as to respect the frequentist principle: \\\\emph{coverage probability $\\\\geq$ confidence level}. In general, this is very difficult to do exactly.\\nIn practice, we replace all nuisance parameters by their \\\\textbf{conditional maximum likelihood\\nestimates} (CMLE). The CMLE is the maximum likelihood estimate conditional on\\na \\\\emph{given} value of the current parameter (or parameters) of interest. In the top discovery example, we construct an estimator of $b$ as a function of $s$, $\\\\hat{b}(s)$, and\\nreplace $b$ in the likelihood $p(D | s, b)$ by $\\\\hat{b}(s)$ to yield a function\\n$p_{PL}(D | s)$ called the \\\\textbf{profile likelihood}.\\n\\\\begin{quote}\\n\\\\emph{Since the profile likelihood entails an approximation, namely, replacing unknown parameters by their conditional estimates, it is not the likelihood but rather an approximation to it. Consequently, \\nthe frequentist principle is not guaranteed to be satisfied exactly.}\\n\\\\end{quote}\\nThis does not seem to be much progress. However, things are much better than they may appear because of\\nan important theorem proved by Wilks in 1938. If certain conditions are met, roughly that the\\nMLEs do not occur on the boundary of the parameter space and the likelihood becomes\\never more Gaussian as the data become more numerous --- that is, in the so-called\\n\\\\textbf{asymptotic limit}, then if the true density of $x$ is $p(x| s, b)$ the random number\\n\\\\begin{align}\\nt(x, s) & = -2 \\\\ln \\\\lambda(x, s), \\\\\\\\\\n\\\\textrm{where } \\\\lambda(x, s) & = \\\\frac{p_{PL}(x | s)}{ p_{PL}(x | \\\\hat{s})}, \\n\\\\end{align}\\nhas a probability density that converges to a $\\\\chi^2$ density with one degree of\\nfreedom. More generally, if the numerator of $\\\\lambda$ contains $m$ free parameters the\\nasymptotic density of $t$ is a $\\\\chi^2$ density with $m$ degrees of freedom. Therefore, we may take $t(D, s)$ to be a $\\\\chi^2$ variate, at least\\napproximately, and solve $t(D, s) = n^2$ for $s$ to get \\napproximate $n$-standard deviation confidence intervals. In particular, if we solve $t(D, s) = 1$, we\\nobtain approximate 68\\\\Wilks' theorem provides the main justification for using the profile likelihood. \\nWe again use the top discovery example to illustrate the procedure.\\n\\\\begin{quote} \\n\\\\paragraph*{Example: Top Quark Discovery Revisited Again}\\nThe conditional MLE of $b$ is found to be\\n\\\\begin{align}\\n\\\\hat{b}(s) & = \\\\frac{g + \\\\sqrt{g^2 + 4 (1 + k) Q s}}{2(1+k)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\ng & = N + Q - (1+k) s.\\\\nonumber\\n\\\\end{align}\\nThe likelihood $p(D | s, b)$ is shown in Fig.~\\\\ref{fig:toppl}(a) together with\\nthe graph of $\\\\hat{b}(s)$. The mode (i.e. the peak) occurs at $s = \\\\hat{s} = N - B$.\\nBy solving $$-2 \\\\ln \\\\frac{p_{PL}(17 | s)}{ p_{PL}(17 | 17 - 3.8)} = 1$$ for $s$ we get two solutions\\n$s = 9.4$ and $s = 17.7$. Therefore, we can make the statement\\n$s \\\\in [9.4, 17.7]$ at approximately 68\\\\$-\\\\ln \\\\lambda(17, s)$ created using \\nthe {\\\\tt RooFit}~\\\\cite{RooFit} and {\\\\tt RooStats}~\\\\cite{RooStats} packages.\\n\\\\smallskip\\n\\\\framebox{\\\\textbf{Exercise 9:} Verify this interval using the {\\\\tt RooFit/RooStats} package}\\n\\\\medskip\\nIntervals constructed this way are not guaranteed to\\nsatisfy the frequentist principle. In practice, however, \\ntheir coverage is very good for the typical probability models\\nused in particle physics, even for modest amounts of\\ndata. This is illustrated in Fig.~\\\\ref{fig:wilks}, which shows how rapidly the density of $t(x, s)$ \\nconverges to a $\\\\chi^2$ density for the probability distribution $p(x, y| s, b) = \\\\textrm{Poisson}(x|s+b) \\\\textrm{Poisson}(y | b)$\\\\footnote{It was the difficulty of extracting information\\nfrom this distribution that compelled the \\nauthor (against his will) to repair his parlous knowledge of statistics~\\\\cite{Fidecaro:1985cm}!}.\\nThe figure also shows what happens if we impose the restriction $\\\\hat{s} \\\\geq 0$, that is,\\nwe forbid negative signal estimates.\\n\\\\end{quote}\\n\", \"\\\\section{Inference}\\n\\\\subsection{Likelihood function for binned samples}\\nSometimes data are available in form of a binned histogram. This may be convenient\\nwhen a large number of entries is available, and computing an unbinned likelihood function (Eq.~(\\\\ref{eq:unbinnedLikeFun}))\\nwould be too much computationally expansive.\\nIn most of the cases, each bin content is independent on any other bin and all obey Poissonian distributions,\\nassuming that bins contain event-counting information.\\nThe likelihood function can be written as product of Poissonisn PDFs corresponding to each bin\\nwhose number of entries is given by $n_i$ .\\nThe expected number of entries in each bin depends on some unknown parameters: $\\\\mu_i = \\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nThe function to be minimized, in order to fit $\\\\theta_1, \\\\cdots,\\\\theta_n$, is the following:\\n\\\\begin{eqnarray}\\n-2\\\\ln L(\\\\vec{n};\\\\vec{\\\\theta}) & = &\\n-2\\\\ln \\\\prod_{i=1}^{n_{\\\\mathrm{bins}}}\\\\mathrm{Poiss}(n_i;\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)) \\\\\\\\\\n& = & -2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\ln \\\\frac{\\ne^{-\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)^{n_i}\\n}{n_i!}\\\\\\\\\\n& = & 2\\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\left(\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)\\n-n_i\\\\ln\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m) +\\\\ln{n_i!}\\\\right)\\\\,.\\n\\\\end{eqnarray}\\nThe expected number of entries in each bin, $\\\\mu_i$, is often approximated by a continuous function $\\\\mu(x)$\\nevaluated at the center of the bin $x=x_i$.\\nAlternatively, $\\\\mu_i$ can be given by the superposition of other histograms ({\\\\it templates}),\\ne.g.: the sum of histograms obtained from different simulated processes.\\nThe overall yields of the considered processes may be left as free parameters in the fit in order to constrain\\nthe normalization of simulated processes from data, rather than relying on simulation prediction,\\nwhich may affected by systematic uncertainties.\\nThe distribution of the number of entries in each bin can be approximated,\\nfor sufficiently large number of entries,\\nby a Gaussian with standard deviation equal to $\\\\sqrt{n_i}$. \\nMaximizing $L$ is equivalent to minimize:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_{i=1}^{n_{\\\\mathrm{bins}}}\\\\frac{\\\\left(n_i-\\\\mu(x_i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\right)^2\\n}{n_i }\\n\\\\end{equation}\\nEquation~(\\\\ref{eq:NeymanChi2}) defines the so-called Neyman's $\\\\chi^2$ variable.\\nSometimes, the denominator $n_i$ is replaced by $\\\\mu_i = \\\\mu (x_i; \\\\theta_1, \\\\cdots, \\\\theta_m)$\\n(Pearson's $\\\\chi^2$) in order to avoid cases with $n_i$ equal to zero or very small.\\nAnalytic solutions exist in a limited number of simple cases, e.g.: if $\\\\mu$ is a linear function.\\nIn most of the realistic cases, the $\\\\chi^2$ minimization is performed numerically, as for\\nmost of the unbinned maximum likelihood fits.\\nBinned fits are, in many cases, more convenient with respect to unbinned fits because the number of input\\nvariables decreases from the total number of entries to the number of bins.\\nThis leads usually to simpler and faster numerical implementations,\\nin particular when unbinned fits become unpractical in cases of very large number of entries.\\nAnyway, for limited number of entries, a fraction of the information is lost when\\nmoving from an unbinned to a binned sample and a possible loss of precision may occur.\\nThe maximum value of the likelihood function obtained from an umbinned maximum likelihood fit doesn't in general\\nprovide information about the quality ({\\\\it goodness}) of the fit.\\nInstead, the minimum value of the $\\\\chi^2$ in a fit with a Gaussian underlying model\\nis distributed according to a known PDF given by:\\n\\\\begin{equation}\\nP(\\\\chi^2;n) =\\\\frac{2^{-{n}/{2}}}{\\\\Gamma\\\\left({n}/{2}\\\\right)}\\n\\\\chi^{n-2}e^{-\\\\frac{\\\\chi^2}{2}}\\\\,,\\n\\\\end{equation}\\nwhere $n$ is the {\\\\it number of degrees of freedom}, equal to the number of\\nbins minus the number of fit parameters.\\nThe cumulative distribution (Eq.~(\\\\ref{eq:cumulative})) of $P(\\\\chi^2; n)$ follows a uniform distribution between from 0 to 1,\\nand it is an example of {\\\\it p-value} (See Sec.~\\\\ref{sec:HypTest}).\\nIf the true PDF model deviates from the assumed distribution, the distribution of the $p$-value will be more peaked around zero\\ninstead of being uniformly distributed.\\nIt's important to note that $p$-values are not the ``probability of the fit hypothesis'',\\nbecause that would be a Bayesian probability, with a completely different meaning, and should be evaluated\\nin a different way.\\nIn case of a Poissonian distribution of the number of bin entries that may deviate from the Gaussian approximation,\\nbecause of small number of entries,\\na better alternative to the Gaussian-inspired Neyman's or Pearson's $\\\\chi^2$ has been proposed\\nby Baker and Cousins~\\\\cite{baker_cousins} using the following likelihood ratio\\nas alternative to Eq.~(\\\\ref{eq:PoisBinLik}):\\n\\\\begin{eqnarray}\\n\\\\chi^2_{\\\\lambda} & = & -2\\\\ln\\\\prod_i\\\\frac{L(n_i;\\\\mu_i)}{L(n_i;n_i)} = -2\\\\ln\\\\prod_i\\\\frac{e^{-\\\\mu_i}\\\\mu_i^{n_i}}{n_i!}\\n\\\\frac{n_i!}{e^{-{n_i}}n_i^{n_i}} \\\\\\\\\\n& = & 2\\\\sum_i\\\\left[\\n\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)- n_i + n_i\\\\ln\\\\left(\\n\\\\frac{n_i}{\\\\mu_i(\\\\theta_1,\\\\cdots,\\\\theta_m)}\\n\\\\right) \\\\right]\\\\,.\\n\\\\end{eqnarray}\\nEquation~(\\\\ref{eq:BakCous}) gives the same minimum value as the Poisson likelihood function,\\nsince a constant term has been added to the log-likelihood function in Eq.~(\\\\ref{eq:PoisBinLik}),\\nbut in addition it provides goodness-of-fit information, since it asymptotically obeys a $\\\\chi^2$\\ndistribution with $n - m$ degrees of freedom. This is due to Wilks' theorem, discussed\\nin Sec.~\\\\ref{sec:profLik}.\\n\", '\\\\section{Inference}\\n\\\\subsection{Combination of measurements}\\nThe simplest combination of two measurements can be performed when no correlation is present between them:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1 \\\\,,\\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2 \\\\,.\\n\\\\end{eqnarray}\\nThe following $\\\\chi^2$ can be built, assuming a Gaussian PDF model for the two measurements,\\nsimilarly to Eq.~(\\\\ref{eq:GausChi2}):\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\frac{(m-m_1)^2}{\\\\sigma_1^2} + \\\\frac{(m-m_2)^2}{\\\\sigma_2^2}\\\\,.\\n\\\\end{equation}\\nThe minimization of the $\\\\chi^2$ in Eq.~(\\\\ref{eq:Chi2Comb}) leads to the following equation:\\n\\\\begin{equation}\\\\\\n0 = \\\\frac{\\\\partial\\\\chi^2}{\\\\partial m} =\\n2 \\\\frac{(m-m_1)}{\\\\sigma_1^2} + 2 \\\\frac{(m-m_2)}{\\\\sigma_2^2}\\\\,,\\n\\\\end{equation}\\nwhich is solved by:\\n\\\\begin{equation}\\nm = \\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^2} + \\\\frac{m_2}{\\\\sigma_2^2}\\n}{\\n\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}\\n}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:wAvPart}) can also be written in form of {\\\\it weighted average}:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{w_1 m_1 + w_2 m_2}{w_1 + w_2}\\\\,,\\n\\\\end{equation}\\nwhere the weights $w_i$ are equal to $\\\\sigma_i^{-2}$.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}}\\\\,.\\n\\\\end{equation}\\nIn case $m_1$ and $m_2$ are correlated measurements, the $\\\\chi^2$ changes from Eq.~(\\\\ref{eq:Chi2Comb}) to the following, including a non-null correlation coefficient $\\\\rho$:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\left(\\n\\\\begin{array}{cc} m - m_1 & m - m_2 \\\\end{array}\\n\\\\right)\\\\left(\\\\begin{array}{cc}\\n\\\\sigma_1^2 & \\\\rho\\\\sigma_1\\\\sigma_2 \\\\\\\\\\n\\\\rho\\\\sigma_1\\\\sigma_2 & \\\\sigma_2^2\\n\\\\end{array}\\n\\\\right)^{-1}\\\\left(\\n\\\\begin{array}{c}\\nm - m_1 \\\\\\\\ m - m_2\\n\\\\end{array}\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIn this case, the minimization of the $\\\\chi^2$ defined by Eq.~(\\\\ref{eq:chi2BLUE}) gives:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\nm_1(\\\\sigma_2^2 -\\\\rho\\\\sigma_1\\\\sigma_2) + m_2(\\\\sigma_1^2 -\\\\rho\\\\sigma_1\\\\sigma_2)\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,,\\n\\\\end{equation}\\nwith uncertainty given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{\\n\\\\sigma_1^2\\\\sigma_2^2(1-\\\\rho)^2\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,.\\n\\\\end{equation}\\nThis solution is also called best linear unbiased estimator (BLUE)~\\\\cite{lyons_gibaut}\\nand can be generalized to more measurements.\\nAn example of application of the BLUE method is the world combination\\nof the top-quark mass measurements at LHC and Tevatron~\\\\cite{topMassComb}.\\nIt can be shown that, in case the uncertainties $\\\\sigma_1$ and $\\\\sigma_2$\\nare estimates that may depend on the assumed central value,\\na bias may arise, which can be mitigated by evaluating the uncertainties\\n$\\\\sigma_1$ and $\\\\sigma_2$ at the central value obtained with the combination,\\nthen applying the BLUE combination, iteratively, until the procedure converges~\\\\cite{LyonsMartinSaxon, ListaBLUE}.\\nImagine we can write the two measurements as:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1^\\\\prime \\\\pm \\\\sigma_C\\\\,, \\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2^\\\\prime \\\\pm \\\\sigma_C\\\\,,\\n\\\\end{eqnarray}\\nwhere $\\\\sigma_C^2 = \\\\rho \\\\sigma_1\\\\sigma_2$. This is the case where the two measurements\\nare affected by a statistical uncertainty, which is uncorrelated between the two measurements,\\nand a fully correlated systematic uncertainty.\\nIn those case, Eq.~(\\\\ref{eq:BLUE}) becomes:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{m_2}{\\\\sigma_2^{\\\\prime 2}}\\n}{\\n\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}\\n}\\\\,,\\n\\\\end{equation}\\ni.e.: it assumes again the form of a weighted average with weights $w_i = \\\\sigma^{\\\\prime -2}$\\ncomputed on the uncorrelated uncertainty contributions.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}} + \\\\sigma_C^2\\\\,,\\n\\\\end{equation}\\nwhich is the sum in quadrature of the uncertainty of the weighted average (Eq.~(\\\\ref{eq:wAvgErr})) and the\\ncommon uncertainty $\\\\sigma_C$~\\\\cite{Valassi:2013bga}.\\nIn a more general case, we may have $n$ measurements $m_1,\\\\cdots,m_n$\\nwith a $n\\\\times n$ covariance matrix $C_{ij}$.\\nThe expected values for $m_1, \\\\cdots, m_n$ are $M_1, \\\\cdots, M_n$ and\\nmay depend on some unknown parameters $\\\\vec{\\\\theta}=(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nFor this case, the $\\\\chi^2$ to be minimized is:\\n\\\\begin{eqnarray}\\n\\\\chi^2 & = & \\\\sum_{i,j=1}^n (m_i-M_i(\\\\vec{\\\\theta}))\\\\,C_{ij}^{-1}\\\\,(m_j-M_j(\\\\vec{\\\\theta})) \\\\\\\\\\n& = & \\\\left(\\\\begin{array}{ccc} m_1 - M_1(\\\\vec{\\\\theta}) & \\\\cdots & m_n - M_n(\\\\vec{\\\\theta})\\\\end{array} \\\\right)\\\\!\\\\!\\n\\\\left(\\n\\\\begin{array}{ccc} C_{11} & \\\\cdots & C_{1n} \\\\\\\\\\n\\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\nC_{n1} & \\\\cdots & C_{nn}\\n\\\\end{array}\\n\\\\right)^{\\\\!\\\\!\\\\!-1}\\\\!\\\\!\\\\!\\\\!\\\\!\\\\left(\\n\\\\begin{array}{c}\\nm_1 - M_1(\\\\vec{\\\\theta}) \\\\\\\\ \\\\cdots \\\\\\\\ m_n - M_n(\\\\vec{\\\\theta})\\n\\\\end{array}\\n\\\\right).\\n\\\\end{eqnarray}\\nAn example of application of such a combination of measurement\\nis given by fit of the Standard Model parameters using the\\nelectroweak precision measurements\\nat colliders~\\\\cite{LEP-2, Baak:2014ora}.', \"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Asymptotic Formulas }\\nThe following has been extracted from Ref.~\\\\cite{asimov} and has been reproduced here for convenience. The primary message of Ref.~\\\\cite{asimov} is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form. In particular, Wilks's theorem~\\\\cite{Wilks} can be used to obtain the distribution $f(\\\\lambda(\\\\mu)|\\\\mu)$, that is the distribution of the test statistic $\\\\lambda(\\\\mu)$ when $\\\\mu$ is true. Note that the asymptotic distribution is independent of the value of the nuisance parameters. Wald's theorem~\\\\cite{Wald} provides the generalization to $f(\\\\lambda(\\\\mu)|\\\\mu',\\\\vec\\\\theta)$, that is when the true value is not the same as the tested value. The various formulae listed below are corollaries of Wilks's and Wald's theorems for the likelihood ratio test statistics described above. The Asimov data described immediately below was a novel result of Ref.~\\\\cite{asimov}.\\n\\\\subsubsection{The Asimov data and $\\\\sigma=\\\\textrm{var}$($\\\\hat\\\\mu$)}\\nThe asymptotic formulae below require knowing the variance of the maximum likelihood estimate of $\\\\mu$\\n\\\\begin{equation}\\n\\\\sigma=\\\\textrm{var}[\\\\hat\\\\mu]\\\\;.\\n\\\\end{equation}\\nOne result of Ref.~\\\\cite{asimov} is that $\\\\sigma$ can be\\nestimated with an artificial dataset referred to as the \\\\textit{ Asimov} dataset. The Asimov dataset is defined as a binned dataset, where the number of events in bin $b$ is exactly the number of events expected in bin $b$. Note, this means that the dataset generally has non-integer number of events in each bin. For our general model one can write\\n\\\\begin{equation}\\nn_{b,A} = \\\\int_{x \\\\in \\\\textrm{bin}~b} \\\\nu(\\\\vec\\\\alpha) f(x|\\\\vec\\\\alpha) dx \\\\;\\n\\\\end{equation}\\nwhere the subscript $A$ denotes that this is the Asimov data. Note, that the dataset depends on the value of $\\\\vec\\\\alpha$ implicitly. For an model of unbinned data, one can simply take the limit of narrow bin widths for the Asimov data. We denote the likelihood evaluated with the Asimov data as $L_{\\\\rm A}(\\\\mu)$. \\nThe important result is that one can calculate the expected Fisher information of Eq.~\\\\ref{Eq:expfisher} by computing the observed Fisher information on the likelihood function based on this special Asimov dataset. \\nA related and convenient way to calculate the variance of $\\\\hat\\\\mu$ is \\n\\\\begin{equation}\\n\\\\sigma \\\\sim \\\\frac{\\\\mu}{\\\\sqrt {\\\\tilde q_{\\\\mu,A}}} \\\\;.\\n\\\\end{equation}\\nwhere $\\\\tilde q_{\\\\mu,A}$ is the to use the $\\\\tilde q_\\\\mu$ test statistic based on a background-only Asimov data (ie. the one with$\\\\mu=0$ in Eq.~\\\\ref{eq:asimovData}). It is worth noting that higher-order corrections to the formulae below are being developed to address the case when the variance of $\\\\hat\\\\mu$ depends strongly on $\\\\mu$.\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{0}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{0} | \\\\mu')$ is found to approach\\n\\\\begin{equation}\\nf(q_0 | \\\\mu^{\\\\prime}) = \\\\left( 1 - \\n\\\\Phi \\\\left( \\\\frac{ \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right) \\\\right) \\\\delta(q_0) + \\n\\\\frac{1}{2}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} \\\\exp \\n\\\\left[ - \\\\frac{1}{2} \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right)^2 \\\\right] \\n\\\\;.\\n\\\\end{equation}\\nFor the special case of $\\\\mu^{\\\\prime} = 0$, this reduces to\\n\\\\begin{equation}\\nf(q_0 | 0) = \\\\frac{1}{2} \\\\delta(q_0) + \\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{q_0}} e^{-q_0/2} \\\\;.\\n\\\\end{equation}\\nThat is, one finds a mixture of a delta function at zero and\\na chi-square distribution for one degree of freedom, with each term\\nhaving a weight of $1/2$. In the following we will refer to this\\nmixture as a half chi-square distribution or $\\\\half \\\\chi^2_1$.\\nFrom Eq.~(\\\\ref{eq:fq0muprimewald}) the corresponding cumulative\\ndistribution is found to be\\n\\\\begin{equation}\\nF(q_0 | \\\\mu^{\\\\prime}) = \\\\Phi \\\\left( \\\\sqrt{q_0} - \\\\frac{\\\\mu^{\\\\prime}}{\\\\sigma} \\n\\\\right) \\\\;.\\n\\\\end{equation}\\nThe important special case $\\\\mu^{\\\\prime} = 0$ is therefore simply\\n\\\\begin{equation}\\nF(q_0 | 0) = \\\\Phi \\\\Big( \\\\sqrt{q_0} \\\\Big)\\n\\\\;.\\n\\\\end{equation}\\nThe $p$-value of the $\\\\mu=0$ hypothesis is \\n\\\\begin{equation}\\np_0 = 1 - F(q_0 | 0) \\\\;, \\n\\\\end{equation}\\nand therefore for the significance gives the simple formula\\n\\\\begin{equation}\\nZ = \\\\Phi^{-1}(1 - p_0) = \\\\sqrt{q_0} \\\\;.\\n\\\\end{equation}\\n\\\\subsubsection{Asymptotic Formulas for $\\\\tilde q_{\\\\mu}$}\\nFor a sufficiently large data sample, the pdf $f(\\\\tilde{q}_{\\\\mu} | \\\\mu)$ is found to approach\\n\\\\begin{eqnarray}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) & = & \\n\\\\Phi \\\\left( \\\\frac{\\\\mu^{\\\\prime} - \\\\mu}{\\\\sigma} \\\\right) \\n\\\\delta (\\\\tilde{q}_{\\\\mu}) \\\\nonumber \\\\\\\\*[0.3 cm] \\n& + &\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\n\\\\exp \\\\left[ -\\\\frac{1}{2} \\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} -\\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)^2 \\\\right]\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2} )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\n\\\\end{array}\\n\\\\right.\\n\\\\;.\\n\\\\end{eqnarray}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is therefore\\n\\\\begin{equation}\\nf(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\frac{1}{2} \\\\delta (\\\\tilde{q}_{\\\\mu}) +\\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\frac{1}{2} \\\\frac{1}{\\\\sqrt{2 \\\\pi}} \\\\frac{1}{\\\\sqrt{\\\\tilde{q}_{\\\\mu}}}\\ne^{- \\\\tilde{q}_{\\\\mu}/2}\\n& 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\\\\\\\*[0.5 cm]\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi} \\\\sigma} \\\\exp \\\\left[\\n-\\\\frac{1}{2} \\\\frac{ (\\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2 )^2 }\\n{(2 \\\\mu/\\\\sigma)^2} \\\\right] \\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe corresponding cumulative distribution is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu^{\\\\prime}) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\left( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} - \\n\\\\frac{\\\\mu - \\\\mu^{\\\\prime}}{\\\\sigma} \\\\right)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^{2} \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} - \\n(\\\\mu^2 - 2 \\\\mu \\\\mu^{\\\\prime})/\\\\sigma^{2}}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^{2} \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe special case $\\\\mu = \\\\mu^{\\\\prime}$ is\\n\\\\begin{equation}\\nF(\\\\tilde{q}_{\\\\mu}|\\\\mu) = \\n\\\\: \\\\left\\\\{ \\\\! \\\\! \\\\begin{array}{lll}\\n\\\\Phi\\\\Big( \\\\sqrt{\\\\tilde{q}_{\\\\mu}} \\\\Big)\\n& \\\\quad 0 < \\\\tilde{q}_{\\\\mu} \\\\le \\\\mu^2/\\\\sigma^2 \\n\\\\;, \\\\\\\\*[0.5 cm]\\n\\\\Phi \\\\left( \\\\frac{ \\\\tilde{q}_{\\\\mu} + \\\\mu^2/\\\\sigma^2}\\n{2\\\\mu/\\\\sigma} \\\\right)\\n& \\\\quad \\\\tilde{q}_{\\\\mu} > \\\\mu^2/\\\\sigma^2 \\\\;.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThe $p$-value of the hypothesized $\\\\mu$ is as before\\ngiven by one minus the cumulative distribution,\\n\\\\begin{equation}\\np_{\\\\mu} = 1 - F(\\\\tilde{q}_{\\\\mu} | \\\\mu) \\\\;.\\n\\\\end{equation}\\nAs when using $q_{\\\\mu}$, the upper limit on $\\\\mu$ at confidence level\\n$1 - \\\\alpha$ is found by setting $p_{\\\\mu} = \\\\alpha$ and solving for\\n$\\\\mu$, which reduces to the same result as found when using $q_{\\\\mu}$,\\nnamely,\\n\\\\begin{equation}\\n\\\\mu_{\\\\rm up} = \\\\hat{\\\\mu} + \\\\sigma \\\\Phi^{-1}(1 - \\\\alpha) \\\\;.\\n\\\\end{equation}\\nNote that because $\\\\sigma$ depends in general on $\\\\mu$,\\nEq.~(\\\\ref{eq:muuptilde}) must be solved numerically. \\n\\\\subsubsection{Expected $\\\\mathrm{CL}_s$ Limit and Bands}\\nFor the $CL_s$ method we need distributions for $\\\\tilde{q}_\\\\mu$ for the hypothesis at $\\\\mu$ and $\\\\mu=0$. We find\\n\\\\begin{equation}\\np'_{\\\\mu}=\\\\frac{1-\\\\Phi(\\\\sqrt{q_\\\\mu})}{\\\\Phi(\\\\sqrt{q_{\\\\mu,A}}-\\\\sqrt{q_{\\\\mu}})}\\n\\\\end{equation}\\nThe median and expected error bands will therefore be\\n\\\\begin{equation}\\n\\\\mu_{{up}+N}=\\\\sigma(\\\\Phi^{-1}(1-\\\\alpha \\\\Phi(N))+N)\\n\\\\end{equation} \\n\\\\noindent with \\n\\\\begin{equation}\\n\\\\sigma^2=\\\\frac{\\\\mu^2}{q_{\\\\mu,A}}\\n\\\\end{equation} \\n\\\\noindent $\\\\alpha=0.05$, $\\\\mu$ can be taken as $\\\\mu_{up}^{med}$ in the calculation of $\\\\sigma$.\\nNote that for $N=0$ we find the median limit \\n\\\\begin{equation}\\n\\\\mu_{up}^{med}=\\\\sigma \\\\Phi^{-1}(1-0.5\\\\alpha)\\n\\\\end{equation}\\nThe fact that $\\\\sigma$ (the variance of $\\\\hat{\\\\mu}$) defined in Eq.~\\\\ref{eq:sigmaofmu} in general depends on $\\\\mu$ complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above. The bands tend to be too narrow. A modified treatment of the bands taking into account the $\\\\mu$ dependence of $\\\\sigma$ is under development.\\n\", '\\\\section{Appendix A: Poisson cumulative distribution}\\nThe confidence level $\\\\alpha$ associated with a specific signal strength $\\\\mu$ is determined by the cumulative distribution function (CDF). There is a relation that links the CDF of the Poisson distribution to that of the $\\\\chi^{2}(x; k=2(n+1))$ distribution. By applying the definition of the confidence level, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha = 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) = \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{k/2+1} e^{-x^{2}/2}}{2^{k/2} \\\\Gamma(k/2)} = \\n\\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{2^{1+n}n!} dx.\\n\\\\end{equation}\\nBy applying integration by parts, where $u=\\\\frac{x^{n}}{n !}$, $du=\\\\frac{x^{n-1}}{(n-1)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n+1}}$, we obtained the following result:\\n\\\\begin{equation}\\n1-\\\\alpha=- \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{n! 2^{n}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{2^{n}(n-1)!} dx.\\n\\\\end{equation}\\nBy once again applying integration by parts, with $u=\\\\frac{x^{n-1}}{(n-1) !}$, $du=\\\\frac{x^{n-2}}{(n-2)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-1)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, the following expression is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} -\\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{(n-1)! 2^{(n-1)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{2^{(n-1)}(n-2)!} dx.\\n\\\\end{equation}\\nFinally, by integrating once more by parts, with $u=\\\\frac{x^{n-2}}{(n-2) !}$, $du=\\\\frac{x^{n-3}}{(n-3)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-2)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n-1}}$, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} -\\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{(n-2)! 2^{(n-2)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} + \\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx.\\n\\\\end{equation}\\nAt this stage, a pattern can be discerned in the results obtained after repeatedly applying integration by parts. Specifically, the evaluation of the integral as $x \\\\to \\\\infty$ equal zero, while for $x = 2\\\\lambda$, a term of the following form is obtained:\\n\\\\begin{equation}\\n\\\\frac{(\\\\lambda)^{n-i}e^{-\\\\lambda}}{(n-i)!}.\\n\\\\end{equation}\\nThus, after performing $n-1$ integration by parts, the following result is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+ \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{e^{-\\\\frac{x}{2}}}{2} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+e^{-\\\\lambda}\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}.\\n\\\\end{equation}\\nThis result corresponds to the cumulative distribution function (CDF) of the Poisson distribution. This distribution would then be related to the cumulative $\\\\chi^{2}$ distribution ($F_{\\\\chi^{2}}$) as follows:\\n\\\\begin{equation}\\n\\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}= 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)).\\n\\\\end{equation}\\n', '\\\\section{Least squares for Goodness of Fit}\\n\\\\subsection{The chi-squared distribution}\\nIt turns out that, if we repeated our experiment a large number of times, and certain conditions are satisfied, \\nthen $S_{min}$ will follow a $\\\\chi^2$ distribution with $\\\\nu = n - p$ degrees of freedom, where $n$ is the \\nnumber of data points, $p$ is the number of free parameters in the fit, and $S_{min}$ is the value of $S$ \\nfor the best values of the free parameters. For example, a straight line with free intercept and gradient fitted \\nto 12 data points would have $\\\\nu = 10$.\\nThe conditions for this to be true include:\\n\\\\begin{itemize}\\n\\\\item{the theory is correct:}\\n\\\\item{the data are unbiassed and asymptotic;}\\n\\\\item{the $y_i$ are Gaussian distributed about their true values;}\\n\\\\item{the estimates for $\\\\sigma_i$ are correct;$\\\\ \\\\ \\\\ \\\\ \\\\ $ etc. }\\n\\\\end{itemize}\\nUseful properties to know about the mathematical $\\\\chi^2$ distribution are that their mean is $\\\\nu$ and their variance is\\n$2\\\\nu$. Thus if a global fit to a lot of data has $S_{min}$ = 2200 and there are 2000 degrees of freedom, we can \\nimmediately estimate that this is equivalent to a fluctuation of 3.2$\\\\sigma$.\\nMore useful than plots of $\\\\chi^2$ distributions are those of the fractional tail area beyond a particular value \\nof $\\\\chi^2$ (see figs. \\\\ref{fig:chi_squared} and \\\\ref{fig:chi_sq_tail} respectively).\\nThe $\\\\chi^2$ goodness of fit test consists of\\n\\\\begin{itemize}\\n\\\\item{For the given theoretical form, find the best values of its free parameters, and hence $S_{min}$;}\\n\\\\item{Determine $\\\\nu$ from $n$ and $p$; and}\\n\\\\item{Use $S_{min}$ and $\\\\nu$ to obtain the tail probability $p$ \\\\footnote{If the conditions for $S_{min}$ to follow a \\n$\\\\chi^2$ distribution are satisfied, this simply involves using the tail probability of a $\\\\chi^2$ distribution. \\nIn other cases, it may be necessary to use Monte Carlo simulation to obtain the distribution of $S_{min}$;\\nthis could be tedious. }.} \\n\\\\end{itemize}\\nThen $p$ is the probability that, if the theory is correct, by random fluctuations we would have obtained a value of $S_{min}$ at least as large as the observed one. If this probability is smaller than some pre-defined level $\\\\alpha$, we reject the hypothesis that the model provides a good description of the data.\\n\\\\subsection{When $\\\\nu \\\\ne n - p$}\\nIf we add an extra parameter into our theoretical description, even if it is not really needed, we expect the value of $S_{min}$ to decrease slightly. (This contrasts with including a parameter which is really relevant, which can result in a dramatic reduction in $S_{min}$.) In determining $p$-values, this is allowed for by the reduction of $\\\\nu$. On average,\\na parameter which is not needed reduces $S_{min}$ by 1. But consider the following examples.\\n\\\\subsubsection{Small oscillatory term}\\nImaging we are fitting a histogram of a variable $\\\\phi$ by a distribution of the form\\n\\\\begin{equation}\\n\\\\frac{dy}{d\\\\phi} = N[ 1 + 10^{-6} cos(\\\\phi -\\\\phi_0)],\\n\\\\end{equation}\\nwhere the two parameters are the normaisation $N$ and the phase $\\\\phi_0$. Because of the factor $10^{-6}$ in front\\nof the cosine term, $\\\\phi_0$ will have a miniscule effect on the prediction, and so including this as a parameter has negligible effect on $S_{min}$; $\\\\phi_0$ is effectively not a free parameter.\\n\\\\subsubsection{Neutrino oscillations} \\nFor a scenario of two oscillating neutrino flavours, the probability $P$ of a neutrino of energy $E$ to remain the same flavour after\\na flight length $L$ is\\n\\\\begin{equation}\\nP = 1 - A sin^2(\\\\delta m^2 L/E)\\n\\\\end{equation}\\nwhere the two parameters are $\\\\delta m^2$, the difference in the mass-squareds of the two neutrino flavours, and \\n$A = sin^2 2\\\\theta$ with $\\\\theta$ being the mixing angle. However, since for small angles $\\\\alpha,\\\\ sin\\\\alpha\\\\approx \\\\alpha$, for small $\\\\delta m^2L/E$ the probability $P$ of eqn \\\\ref{neutrino} is approximately $1 - A(\\\\delta m^2 L/E)^2$. Thus the two parameters occur only as the product $A (\\\\delta m^2)^2$, and cannot be determined separately. Thus in that regime we have effectively just a single parameter.\\n\\\\vspace{0.2in}\\nIn both the above examples, an enormous amount of data would enable us to distinguish the small effects produced by the second \\nparameter; hence the requirement for asymptotic conditions. \\n\\\\subsection{Errors of First and Second Kind}\\nIn deciding in a Goodness of Fit test whether or not to reject the null hypothesis $H_0$ (e.g. that the data points lie on a \\nstraight line), there are two sorts of mistake we might make:\\n\\\\begin{itemize}\\n\\\\item{Error of the First Kind. This is when we reject $H_0$ when it is in fact true. The fraction of cases in which this happens \\nshould equal $\\\\alpha$, the cut on the $p$-value. }\\n\\\\item{Error of the Second Kind. This is when we do not reject $H_0$, even though some other hypothesis is true. The rate at which this happens depends on how similar $H_0$ and the alternative hypothesis are, the relative frequencies of the two hypotheses being true, etc.}\\n\\\\end{itemize}\\nAs $\\\\alpha$ increases the rates of Errors of the First and Second kinds go up and down respectively. \\nThese Errors correspond to a loss of efficiency and to an increase of contamination respectively.\\n\\\\subsection{Other Goodness of Fit tests}\\nThe $\\\\chi^2$ method is by no means the only one for testing Goodness of Fit.\\nIndeed whole books have been written on the subject\\\\cite{DAgostino}. Here\\nwe mention just one other, the Kolmogorov-Smirnov method (K-S), which has the \\nadvantage of working with individual observations. It thus can be used with \\nfewer observations than are required for the binned histograms in the $\\\\chi^2$\\napproach. \\nA cumulative plot is produced of the fraction of events as a function of the variable\\nof interest $x$. An example is shown in Fig.~\\\\ref{fig:K_S}. \\nThis shows the fraction of data events with $x$ smaller than any particular \\nvalue. It is thus a stepped plot, with the fraction going from zero at the extreme left, \\nto unity on the right hand side. Also on the plot is a curve showing the expected cumulative\\nfraction for some theory. The K-S method makes use of the largest (as a function of $x$) vertical \\ndiscrepancy $d$ between the data plot and the theoretical curve. Assuming the theory is true\\nand given the number of observations $N$, the probability $p_{KS}$ of obtaining $d$ at least as large \\nas the observed value can be calculated. The beauty of the K-S method is that this probability\\nis independent of the details of the theory. As in the $\\\\chi^2$ approach, the K-S probability \\ngives a numerical way of checking the compatibility of theory and data. If $p_{KS}$ is small, we \\nare likely to reject the theory as being a good description of the data.\\nSome features of the K-S method are:\\n\\\\begin{itemize}\\n\\\\item{The main advantage is that it can use a small number of observations.}\\n\\\\item{The calculation of the K-S probability depends on there being no adjustable parameters in the theory.\\nIf there are, it will be necessary for you to determine the expected distribution for $d$, presumably \\nby Monte Carlo simulation.}\\n\\\\item{It does not extend naturally to data of more than one dimension, because of there being no unique\\nway of producing an ordering in several dimensions.}\\n\\\\item{It is not very sensitive to deviations in the tails of distributions, which is where searches for\\nnew physics are often concentrated e.g. high mass or transverse momentum. Fortunately variants of K-S exist, \\nwhich put more emphasis on discrepancies in the tails.}\\n\\\\item{Instead of comparing a data cumulative distribution with a theoretical curve, it can alternatively be\\ncompared with another distribution. This can be from a simulation of a theory, or with another data set. The\\nlatter could be to check that two data sets are compatible. \\nThe calculation of the K-S probability now requires the maximum discrepancy $d$, and the numbers of events \\n$N_1$ and $N_2$ in each of the two distributions being compared.}\\n\\\\end{itemize}\\n', \"\\\\section{Estimation}\\nWhat statisticians call `estimation',\\nphysicists would generally call `measurement'.\\nSuppose you\\nknow the probability (density) function $P(x;a)$ \\nand you \\ntake a set of data $\\\\{x_i\\\\}$. What is the best value for $a$? (Sometimes one wants to estimate a property (e.g. the mean) rather than a parameter, but \\nthis is relatively uncommon, and the methodology is the same.) \\n$x_i$ may be single values, or pairs, or higher-dimensional.\\nThe unknown\\n$a$ may be a single parameter or several. If it has more than one component, these are sometimes split into `parameters of interest' and `nuisance parameters'.\\nThe {\\\\em estimator} is defined very broadly:\\nan estimator $\\\\hat a(x_1\\\\dots x_N)$ is a function of the data that gives a value for the parameter $a$. There is no `correct' estimator, but some are better than others. A perfect estimator would be:\\n\\\\begin{itemize}\\n\\\\item\\nConsistent. $\\\\hat a(x_1 \\\\dots x_N) \\\\to a$ as $ N \\\\to \\\\infty $,\\n\\\\item\\nUnbiased: $\\\\langle \\\\hat a \\\\rangle = a $,\\n\\\\item\\nEfficient: $\\\\langle (\\\\hat a - a)^2 \\\\rangle$ is as small as possible,\\n\\\\item \\nInvariant: $\\\\hat f(a) = f(\\\\hat a)$.\\n\\\\end{itemize}\\nNo estimator is perfect---these 4 goals are incompatible. In particular the second and the fourth; if\\nan estimator $\\\\hat a$ is unbiased for $a$ then\\n$\\\\sqrt{\\\\hat a}$ is not an unbiased estimator of $\\\\sqrt a$.\\n\\\\subsection{Bias}\\nSuppose we estimate the mean by taking the obvious\\\\footnote{Note the difference between $\\\\langle x \\\\rangle$ which is an average over a PDF and $\\\\overline x$ \\nwhich denotes the average over a particular sample: both are called `the mean $x$'.} $\\\\hat \\\\mu = \\\\xbar$\\n$\\\\left< \\\\hat \\\\mu \\\\right> = \\\\left< {1 \\\\over N } \\\\sum x_i \\\\right> = {1 \\\\over N } \\\\sum \\\\mu = \\\\mu$. \\nSo there is no bias. This expectation value of this estimator of $\\\\mu$ is just $\\\\mu$ itself. By contrast suppose\\nwe estimate the variance by the apparently obvious\\n$\\\\hat V = \\\\xsqbar-\\\\xbar^2$.\\nThen $\\\\left< \\\\hat V \\\\right> = \\\\left< \\\\xsqbar \\\\right> - \\\\left< \\\\xbar^2 \\\\right>$.\\nThe first term is just $\\\\left< x^2 \\\\right>$. To make sense of the second term, note that $\\\\left< x \\\\right> = \\\\left< \\\\xbar \\\\right>$ and add and subtract $\\\\left< x \\\\right>^2$ to get\\n$\\\\left< \\\\hat V \\\\right> = \\\\left< x^2 \\\\right> - \\\\left< x \\\\right>^2 - (\\\\left< \\\\xbar^2 \\\\right>- \\\\left< \\\\xbar \\\\right>^2)$\\n$\\\\left< \\\\hat V \\\\right> =V(x)-V(\\\\xbar)=V-{V \\\\over N}={N-1 \\\\over N} V$.\\nSo the estimator is biased! $\\\\hat V$ will, on average, give too small a value.\\nThis bias, like any known bias, can be corrected for. \\nUsing $\\\\hat V = {N \\\\over N-1} (\\\\xsqbar-\\\\xbar^2)$ corrects the bias. The familiar estimator for\\nthe standard deviation follows:\\n$\\\\hat \\\\sigma=\\\\sqrt{\\\\sum_i (x_i-\\\\xbar)^2 \\\\over N-1}$. \\n(Of course this gives a biased estimate of $\\\\sigma$. But $V$ is generally more important in this context.)\\n\\\\subsection {Efficiency}\\nSomewhat surprisingly, there is a limit to the efficiency of an estimator: the\\n{\\\\em minimum variance bound} (MVB),\\nalso known as the {\\\\em Cramér-Rao bound}.\\nFor any unbiased estimator $\\\\hat a(x)$, the variance is bounded \\n\\\\begin{equation}V(\\\\hat a)\\\\geq\\n-{1 \\\\over \\\\left< {d^2 \\\\ln L \\\\over da^2}\\\\right>}\\n={1 \\\\over \\\\left<\\n\\\\left({d \\\\ln L \\\\over da }\\\\right) ^2\\n\\\\right>}\\n\\\\quad.\\n\\\\end{equation}\\n$L$ is the likelihood (as introduced in Section~\\\\ref{sec:likelihood}) of a sample of independent measurements, i.e. the\\nprobability for the whole data sample for a particular value of $a$.\\nIt is just the product of the individual probabilities:\\n$L(a;x_1,x_2,...x_N)=P(x_1;a)P(x_2;a)...P(x_N;a)$.\\nWe will write $L(a;x_1,x_2,...x_N)$ as $L(a;x)$ for simplicity.\\n\\\\begin{proof}{Proof of the MVB}\\nUnitarity requires $\\\\int P(x;a)\\\\, dx = \\\\int L(a;x) \\\\, dx =1$\\nDifferentiate wrt $a$: \\\\qquad \\\\begin{equation}\\n0=\\\\int {dL \\\\over da} \\\\, dx = \\\\int L {d \\\\ln L \\\\over da} \\\\, dx = \\\\left< {d \\\\ln L \\\\over da} \\\\right>\\n\\\\end{equation}\\nIf $\\\\hat a$ is unbiased \\n$\\\\left< \\\\hat a\\\\right> = \\\\int \\\\hat a(x) P(x;a) \\\\, dx = \\\\int \\\\hat a(x) L(a;x) \\\\, dx =a$\\nDifferentiate wrt $a$: \\\\qquad $1=\\\\int \\\\hat a(x) {dL \\\\over da} \\\\, dx = \\\\int \\\\hat a L {d \\\\ln L \\\\over da} \\\\, dx $\\nSubtract Eq.~\\\\ref{eq:one} multiplied by $a$, and get $\\\\int (\\\\hat a - a){d \\\\ln L \\\\over da} L dx =1$\\nInvoke the Schwarz inequality $\\\\int u^2 \\\\, dx \\\\int v^2 \\\\, dx \\\\geq \\\\left( \\\\int u v \\\\, dx \\\\right)^2 $ with $u\\\\equiv (\\\\hat a - a) \\\\sqrt L, v\\\\equiv {d \\\\ln L \\\\over da} \\\\sqrt L$\\nHence $\\\\int (\\\\hat a - a)^2 L \\\\, dx \\\\int \\\\left( {d \\\\ln L \\\\over da}\\\\right)^2 L \\\\, dx \\\\geq 1$\\n\\\\begin{equation} \\n\\\\left< (\\\\hat a - a)^2 \\\\right> \\\\geq 1/\\\\left<\\\\left( {d ln L \\\\over da }\\\\right)^2 \\\\right>\\n\\\\end{equation}\\n\\\\end{proof}\\nDifferentiating Eq.~\\\\ref{eq:one} again gives\\n${ d \\\\over da} \\\\int L { d \\\\ln L \\\\over da} \\\\, dx = \\\\int {d L \\\\over da} \\\\, {d \\\\ln L \\\\over da} \\\\, dx + \\\\int L {d^2 \\\\ln A \\\\over da^2} \\\\, dx\\n=\\n\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>+\\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>=0$,\\nhence \\n$\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>= - \\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>$.\\nThis is the {\\\\em Fisher information} referred to in Section~\\\\ref{sec:Jeffreys}. Note how it is intrinsically positive.\\n\\\\subsection{Maximum likelihood estimation}\\nThe {\\\\em maximum likelihood} (ML) estimator just does what it says: $a$ is adjusted to maximise the\\nlikelihood of the sample\\n(for practical reasons one actually maximises the log likelihood, which is a sum rather than a product).\\n\\\\begin{equation}\\n{\\\\rm Maximise } \\\\ln L = \\\\sum_i \\\\ln {P(x_i;a)}\\n\\\\quad,\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\left. {d \\\\ln L \\\\over d a } \\\\right|_{\\\\hat a}=0\\n\\\\quad.\\n\\\\end{equation}\\nThe \\nML estimator is very commonly used. It is not only simple and intuitive, it has lots of nice properties.\\n\\\\begin{itemize}\\n\\\\item\\nIt is consistent.\\n\\\\item\\nIt is biased, but bias falls like $1/N$.\\n\\\\item\\nIt is efficient for the large $N$.\\n\\\\item\\nIt is invariant---doesn't matter if you reparametrize $a$. \\n\\\\end{itemize}\\nA particular maximisation problem may be solved in 3 ways, depending on the complexity\\n\\\\begin{enumerate}\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} algebraically,\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} numerically, and\\n\\\\item Solve Eq.~\\\\ref{eq:logL} numerically.\\n\\\\end{enumerate}\\n\\\\subsection{Least squares}\\n{\\\\em Least squares estimation} follows from maximum likelihood estimation.\\nIf you have \\nGaussian measurements of $y$ taken at various $x$ values, with measurement error $\\\\sigma$, and a prediction $y=f(x;a)$\\nthen the Gaussian probability\\n\\\\centerline{$P(y;x,a)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-(y-f(x,a))^2/2 \\\\sigma^2}$}\\ngives the log likelihood\\n\\\\centerline{$\\\\ln L = - \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over 2 \\\\sigma_i^2} + {\\\\rm constants}$.}\\nTo maximise $\\\\ln L$, you minimise $\\\\chi^2 = \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over \\\\sigma_i^2} $, hence the name `least squares'.\\nDifferentiating gives the {\\\\em normal equations}:\\n$\\\\sum { \\\\left(y_i - f(x_i;a)\\\\right) \\\\over \\\\sigma_i^2}f'(x_i;a) =0$.\\nIf $f(x;a)$ is linear in $a$ then these can be solved exactly. Otherwise an iterative method has to be used.\\n\\\\subsection{Straight line fits}\\nAs a particular instance of least squares estimation, suppose the function is $y=mx+c$, and assume all $\\\\sigma_i$ are the same (the extension to the general case is straightforward).\\nThe normal equations are then $\\\\sum (y_i - m x_i -c) x_i = 0$ and $\\\\sum (y_i-m x_i - c ) =0$\\\\ , for which the solution, shown in Fig.~\\\\ref{fig:slfit}, is\\n\\\\noindent $m={\\\\overline{xy} - \\\\overline x \\\\ , \\\\overline y \\\\over \\\\xsqbar - \\\\xbar^2}$\\\\ , $c=\\\\overline y - m \\\\xbar$ \\\\ .\\nStatisticians call this {\\\\em regression}. Actually there is a subtle difference, as shown in Fig.~\\\\ref{fig:regression}.\\nThe straight line fit considers well-defined $x$ values and $y$ values with measurement errors---if it were not for those\\nerrors then presumably the values would line up perfectly, with no scatter. The scatter in regression is not caused by measurement errors, but by the fact that the variables are linked only loosely. \\nThe history of regression started with Galton, who measured the heights of fathers and their (adult) sons.\\nTall parents tend to have tall children so there is a correlation. Because the height of a son depends\\nnot just on his paternal genes but on many factors (maternal genes, diet, childhood illnesses $\\\\dots$), the points\\ndo not line up exactly---and using a high accuracy laser interferometer to do the measurements, rather than a simple\\nruler, would not change anything. \\nGalton, incidentally, used this to show that although \\ntall fathers tend to have tall sons, they are not that tall. An outstandingly tall father will have (on average) quite tall children, and only tallish grandchildren. He called this \\n`Regression towards mediocrity', hence the name.\\nIt is also true that tall sons tend to have tall fathers---but not that tall---and only tallish grandfathers. Regress works in both directions!\\nThus for regression there is always an ambiguity as to whether to plot $x$ against $y$ or $y$ against $x$.\\nFor a straight line fit as we usually meet them this does not arise: one variable is precisely specified and we call that one $x$, and the one with measurement errors is $y$. \\n\\\\subsection{Fitting histograms}\\nWhen fitting a histogram the error is given by Poisson statistics for the number of events in each bin.\\nThere are \\n4 methods of approaching this problem---in order of increasing accuracy and decreasing speed. It is assumed that the bin width $W$ is narrow, so that $f(x_i,a)=\\\\int_{x_i}^{x_i+W} P(x,a)\\\\, dx$ can be approximated by \\n$f_i(x_i;a)=P(x_i;a) \\\\times W$. $W$ is almost always the same for all bins,\\nbut the rare cases of variable bin width can easily be included.\\n\\\\begin{enumerate}\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2 \\\\over n_i}$. This is the simplest but clearly breaks if $n_i=0$.\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2\\\\over f_i}$ . Minimising the Pearson $\\\\chi^2$ (which {\\\\em is}\\nvalid here) avoids the division-by-zero problem. It assumes that the Poisson distribution can be approximated by a Gaussian.\\n\\\\item Maximise $\\\\ln L = \\\\sum \\\\ln(e^{-f_i} f_i^{n_i} / n_i!) \\\\sim \\\\sum n_i \\\\ln f_i - f_i$. This, known as {\\\\em binned maximum likelihood}, remedies that assumption.\\n\\\\item Ignore bins and maximise the total likelihood. Sums run over $N_{events}$ not $N_{bins}$. So if you have large data samples this is much slower. You have to use it for sparse data, but of course in such cases the sample is small and the\\ntime penalty is irrelevant.\\n\\\\end{enumerate}\\nWhich method to use is something you have to decide on a case by case basis. \\nIf you have bins with zero entries then the first method is ruled out\\n(and removing such bins from the fit introduces bias so this should not be done).\\nOtherwise, in my experience, the improvement in adopting a more complicated method tends to be small.\\n\", \"\\\\section{Kinematic Fitting}\\nEarlier we had the example of estimating the number of married people on \\nan island, and saw that introducing theoretical information could improve\\nthe accuracy of our answer. Here we use the same idea in the context of\\nestimating the momenta and directions of objects produced in a high energy\\ninteraction. The theory we use is that energy and momentum are conserved\\nbetween the inital state collison and the observed objects in the reaction.\\nThe reaction can be either at a collider or with a stationary target. We \\ndenote it by $a + b \\\\rightarrow c + d + e$, but the number of final state \\nobjects can be arbitrary. We assume for the time being the energy and \\nmomenta of all the objects are measured\\\\footnote{For objects like charged \\nparticles whose momenta are determined from their trajectories in a \\nmagnetic field, the energy is determined from the momentum by using the \\nrelevant particle mass.}.\\nThe technique is to consider all possible configurations of the particles'\\nkinematic variables that conserve momentum and energy, and to choose that \\nconfiguration that is closest to the measured variables. The degree of\\ncloseness is defined by the weighted sum of squares of the discrepancies $S$,\\ntaking the uncertainties and correlations into account. If the uncertainties \\non the kinematic quantities $m_i$ were uncorrelated, \\n\\\\begin{equation}\\nS = \\\\Sigma (f_i - m_i)^2/\\\\sigma_i^2\\n\\\\end{equation}\\nwhere the summation is over the 4 components for all the objects in the \\nreaction, $m_i$ are the measured values and $f_i$ are the corresponding \\nfitting quantities. Because of correlations, however, this becomes\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (f_i - m_i) E_{ij} (f_j - m_j) \\n\\\\end{equation}\\nwhere there is now a double summation over the components, and $E_{ij}$ is the \\n$(i,j)^{th}$ component of the inverse covariance matrix for the measured \\nquantities\\\\footnote{The main correlations are among the 4 components of a single\\nobject, rather than between different objects.}. \\nThe procedure then consists in varying $f$ in order to minimise $S$, subject to\\nthe energy and momentum constraints. This usually involves Lagrange Multipliers.\\nThe result of this procedure is to produce a set of fitted values of all the \\nkinematic quantities, which will have smaller uncertainties than the measured ones.\\nThis is an example of incorporating theory to improve the results. Thus if the objects \\nare jets, their directions are usually quite well determined, but their energies less so. \\nThe fitting procedure enables the accurately determined jet directions to help \\nreduce the uncertainties on the jet energies.\\nThe fitting procedure \\nalso provides $S_{min}$, which is a measure of how well the best $f_i$ agree with the $m_i$.\\nIn the case described, the distribution of $S_{min}$ is approximately $\\\\chi^2$ \\nwith 4 degrees of freedom (because of the 4 constraints).\\nIf $S_{min}$ is too large, then our assumed hypothesis for the reaction may be \\nincorrect; for example, there might have been an extra object produced in the \\ncollision that was undetected (e.g. a neutrino, or a charged particle which passed \\nthrough an uninstrumented region of our detector).\\nSince we have 4 constraint equations, we can also allow for up to 4 missing kinematic\\nquantities. Examples include an undetected neutrino in the final state (3 unmeasured \\nmomentum components), a wide-band neutrino beam of known direction (1 missing variable),\\netc. With $m$ missing variables in an interaction involving a single vertex, \\n$S_{min}$ should have a $\\\\chi^2$ distribution with $4-m$ degrees of freedom.\\nKinematic fitting can be extended to more complicated event topologies including production\\nand decay vertices, reactions involving particles of well known mass which decay \\npromptly (e.g. $\\\\psi \\\\rightarrow \\\\mu^+ \\\\mu^-$), etc. \\n\\\\subsection{Example of a simplified kinematic fit}\\nConsider a non-relativistic elastic scattering of two equal mass objects, for example a slow \\nanti-proton hitting a stationary proton. \\nFor simplicity, the measured angles $\\\\theta_1^m \\\\pm \\\\sigma$ and $\\\\theta_2^m \\\\pm \\\\sigma$ that \\nthe outgoing particles make with the direction\\nof travel of the incident anti-proton are assumed to have the same uncorrelated \\nuncertainties $\\\\sigma$. As a result of energy and momentum conservation, the angles must\\nsatisfy the constraint\\n\\\\begin{equation}\\n\\\\theta_1^t + \\\\theta_2^t = \\\\pi/2\\n\\\\end{equation}\\nwhere the superscipt $t$ denotes the true value. There are 3 further constraints \\nbut for simplicity we shall ignore them.\\nTo find our best estimates of $\\\\theta_1^t$ and $\\\\theta_2^t$, we must minimise\\n\\\\begin{equation}\\nS = (\\\\theta_1^t - \\\\theta_1^m)^2/\\\\sigma^2 + (\\\\theta_2^t - \\\\theta_2^m)^2/\\\\sigma^2\\n\\\\end{equation}\\nsubject to the constraint \\\\ref{constraint}. By using Lagrange Multipliers or by \\neliminating $\\\\theta_2^t$ and then minimising $S$, this yields\\n\\\\begin{equation}\\n\\\\begin{split}\\n\\\\theta_1^t &= \\\\theta_1^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m) \\\\\\\\ \\n\\\\theta_2^t &= \\\\theta_2^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m)\\n\\\\end{split}\\n\\\\end{equation}\\nThat is, the best estimate of each true value is obtained by adding to the corresponding measured value\\nhalf the amount by which the measured values fail to satisfy the constraint \\n\\\\ref{constraint}. \\nThe uncertainties on the fitted estimates of the angles are easily obtained by \\npropagation of the uncertainties $\\\\sigma$ on the measured angles vias eqns. \\n\\\\ref{fitted}, and are both equal to $\\\\sigma/\\\\sqrt 2$. \\nWe thus have an example \\nof the promised outcome that kinematic fitting improves the accuracy of our \\nmeasurements. The factor of $\\\\sqrt 2$ improvement can easily be understood in that \\nwe have two independent estimates of $\\\\theta_1^t$, the first being the \\noriginal measurement $\\\\theta_1^m$, and the other coming from the measurement \\n$\\\\theta_2^m$ via the constraint \\\\ref{constraint}. However, even with uncorrelated uncertainties\\non the measured angles, the fitted ones would be anti-correlated.\\n\", '\\\\section{Errors}\\n\\\\subsection{Errors from likelihood}\\nFor large $N$, the $\\\\ln L(a,x)$ curve is a parabola, as shown in Fig.~\\\\ref{fig:ML}.\\nAt the maximum, a Taylor expansion gives $\\\\ln L(a)=\\\\ln L(\\\\hat a)+{1 \\\\over 2} (a-\\\\hat a)^2 {d^2 \\\\ln L \\\\over da^2}$ $\\\\dots$\\nThe maximum likelihood estimator saturates the MVB, so \\n\\\\begin{equation}\\nV_{\\\\hat a}=-1/\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>\\n\\\\qquad \\\\sigma_{\\\\hat a}=\\\\sqrt{-{1 \\\\over {d^2 \\\\ln L \\\\over da^2}}}\\n\\\\quad.\\n\\\\end{equation}\\nWe approximate the expectation value $\\\\left< d^2 \\\\ln L \\\\over da^2 \\\\right>$ by the actual value in this case\\n$ \\\\left. d^2 \\\\ln L \\\\over da^2 \\\\right|_{a=\\\\hat a}$ (for a discussion of the introduced inaccuracy, see Ref.~\\\\cite{DeltaML}).\\nThis can be read off the curve, as also shown in Fig.~\\\\ref{fig:ML}. The maximum gives the estimate.\\nYou then draw a line $1\\\\over 2$ below that (of course nowadays this is done within the code, not with pencil and ruler, but the visual image is still valid). This line $\\\\ln L(a)=\\\\ln L(\\\\hat a)-{1 \\\\over 2}$ intersects the likelihood curve at the points\\n$a=\\\\hat a \\\\pm \\\\sigma_{\\\\hat a}$. \\nIf you are working with $\\\\chi^2$, $L\\\\propto e^{-{1 \\\\over 2}\\\\chi^2}$ so the line is $\\\\Delta \\\\chi^2=1$.\\nThis gives $\\\\sigma$, or 68\\\\\\n', \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\"}},\n",
       "       {'entity_name': 'importance sampling', 'entity_type': 'analysis_technique', 'description': 'A statistical technique used to populate the extreme tails of a test statistic distribution by generating toy data from a different model, known as the importance density, rather than solely from the null hypothesis.', 'relevant_passages': {'\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Importance Sampling}\\n[The following section has been adapted from text written primarily by Sven Kreiss, Alex Read, and myself for the ATLAS Higgs combination. It is reproduced here for convenience. ]\\nTo claim a discovery, it is necessary to populate a small tail of a test statistic distribution. Toy Monte-Carlo techniques use the model $\\\\F_{\\\\rm tot}$ to generate toy data $\\\\data_{toy}$. For every pseudo-experiment (toy), the test statistic is calculated and added to the test statistic distribution. Building this distribution from toys is independent of the assumptions that go into the asymptotic calculation that describes this distribution with an analytic expression. Recently progress has been made using Importance Sampling to populate the extreme tails of the test statistic distribution, which is much more computationally intensive with standard methods. The presented algorithms are implemented in \\\\roostats\\\\ \\\\texttt{ToyMCSampler}.\\n\\\\subsubsection{Naive Importance Sampling}\\nAn ensemble of \"standard toys\" is generated from a model representing the Null hypothesis with $\\\\mu=0$ and the nuisance parameters $\\\\vec\\\\theta$ fixed at their profiled values to the observed data $\\\\nuisObs$, written\\\\\\\\ \\\\mbox{$\\\\F_{\\\\rm tot}(\\\\datasim,\\\\globs|\\\\mu=0,\\\\nuisObs)$}. With importance sampling however, the underlying idea is to generate toys from a different model, called the importance density. A valid importance density is for example the same model with a non-zero value of $\\\\mu$. The simple Likelihood ratio is calculated for each toy and used as a weight.\\n\\\\[\\n{\\\\rm weight} = \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)} {\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)}\\n\\\\]\\nThe weighted distribution is equal to a distribution of unweighted toys generated from the Null. The choice of the importance density is a delicate issue. Michael Woodroofe presented a prescription for creating a well behaved importance density~\\\\cite{Woodroofe}. Unfortunately, this method is impractical for models as large as the combined Higgs models. An alternative approach is shown below.\\n\\\\subsubsection{Phase Space Slicing}\\nThe first improvement from naive importance sampling is the idea of taking toys from both, the null density and the importance density. There are various ways to do that. Simply stitching two test statistic distributions together at an arbitrary point has the disadvantage that the normalizations of both distributions have to be known.\\nInstead, it is possible to select toys according to their weights. First, toys are generated from the Null and the simple Likelihood ratio is calculated. If it is larger than one, the toy is kept and otherwise rejected. Next, toys from the importance density are generated. Here again, the simple Likelihood ratio is calculated but this time the toy is rejected when the Likelihood ratio is larger than one and kept when it is smaller than one. If kept, the toy\\'s weight is the simple Likelihood ratio which is smaller than one by this prescription.\\nIn the following section, this idea is restated such that it generalizes to multiple importance densities.\\n\\\\subsubsection{Multiple Importance Densities}\\nThe above procedure for selecting and reweighting toys that were generated from both densities can be phrased in the following way:\\n\\\\begin{itemize}\\n\\\\item A toy is generated from a density with $\\\\mu=\\\\mu\\'$ and the Likelihoods $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)$ and $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)$ are calculated.\\n\\\\item The toy is veto-ed when the Likelihood with $\\\\mu=\\\\mu\\'$ is not the largest. Otherwise, the toy is used with a weight that is the ratio of the Likelihoods.\\n\\\\end{itemize}\\nThis can be generalized to any number of densities with $\\\\mu_i=\\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}$. For the toys generated from model $i$:\\n\\\\begin{align}\\n\\\\textrm{veto:}& \\\\textrm{ if } \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs) \\\\neq \\\\max\\\\left\\\\{ \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_j,\\\\nuisObs) : \\\\mu_j = \\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}\\\\right\\\\} \\\\\\\\\\n\\\\textrm{weight} &= \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)}{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs)}\\n\\\\end{align}\\nThe number of importance densities has to be known when applying the vetos. It should not be too small to cover the parameter space appropriately and it should not be too large, because too many importance densities lead to too many vetoed toys which decreases overall efficiency. The value and error of $\\\\hat{\\\\mu}$ from a fit to data can be used to estimate the required number of importance densities for a given target overlap of the distributions.\\nThe sampling efficiency in the tail can be further improved by generating a larger number of toys for densities with larger values of $\\\\mu$. For example, for $n$ densities, one can generate $2^k / 2^n = 2^{k-n}$ of the overall toys per density $k$ with $k=0, \\\\ldots, n-1$. The toys have to be re-weighted for example by $2^{n-1} / 2^k$ resulting in a minimum re-weight factor of one. The current implementation of the error calculation for the p-value is independent of an overall scale in the weights.\\nThe method using multiple importance densities is similar to Michael Woodroofe\\'s \\\\cite{Woodroofe} prescription of creating a suitable importance density with an integral over $\\\\mu$. In the method presented here, the integral is approximated by a sum over discrete values of $\\\\mu$. Instead of taking the sum, a mechanism that allows for multiple importance densities is introduced.\\n'}},\n",
       "       {'entity_name': 'multiple importance densities', 'entity_type': 'analysis_technique', 'description': 'An advanced technique in importance sampling that involves generating toys from multiple models with different parameter values, allowing for improved sampling efficiency and better coverage of the parameter space.', 'relevant_passages': {'\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{Importance Sampling}\\n[The following section has been adapted from text written primarily by Sven Kreiss, Alex Read, and myself for the ATLAS Higgs combination. It is reproduced here for convenience. ]\\nTo claim a discovery, it is necessary to populate a small tail of a test statistic distribution. Toy Monte-Carlo techniques use the model $\\\\F_{\\\\rm tot}$ to generate toy data $\\\\data_{toy}$. For every pseudo-experiment (toy), the test statistic is calculated and added to the test statistic distribution. Building this distribution from toys is independent of the assumptions that go into the asymptotic calculation that describes this distribution with an analytic expression. Recently progress has been made using Importance Sampling to populate the extreme tails of the test statistic distribution, which is much more computationally intensive with standard methods. The presented algorithms are implemented in \\\\roostats\\\\ \\\\texttt{ToyMCSampler}.\\n\\\\subsubsection{Naive Importance Sampling}\\nAn ensemble of \"standard toys\" is generated from a model representing the Null hypothesis with $\\\\mu=0$ and the nuisance parameters $\\\\vec\\\\theta$ fixed at their profiled values to the observed data $\\\\nuisObs$, written\\\\\\\\ \\\\mbox{$\\\\F_{\\\\rm tot}(\\\\datasim,\\\\globs|\\\\mu=0,\\\\nuisObs)$}. With importance sampling however, the underlying idea is to generate toys from a different model, called the importance density. A valid importance density is for example the same model with a non-zero value of $\\\\mu$. The simple Likelihood ratio is calculated for each toy and used as a weight.\\n\\\\[\\n{\\\\rm weight} = \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)} {\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)}\\n\\\\]\\nThe weighted distribution is equal to a distribution of unweighted toys generated from the Null. The choice of the importance density is a delicate issue. Michael Woodroofe presented a prescription for creating a well behaved importance density~\\\\cite{Woodroofe}. Unfortunately, this method is impractical for models as large as the combined Higgs models. An alternative approach is shown below.\\n\\\\subsubsection{Phase Space Slicing}\\nThe first improvement from naive importance sampling is the idea of taking toys from both, the null density and the importance density. There are various ways to do that. Simply stitching two test statistic distributions together at an arbitrary point has the disadvantage that the normalizations of both distributions have to be known.\\nInstead, it is possible to select toys according to their weights. First, toys are generated from the Null and the simple Likelihood ratio is calculated. If it is larger than one, the toy is kept and otherwise rejected. Next, toys from the importance density are generated. Here again, the simple Likelihood ratio is calculated but this time the toy is rejected when the Likelihood ratio is larger than one and kept when it is smaller than one. If kept, the toy\\'s weight is the simple Likelihood ratio which is smaller than one by this prescription.\\nIn the following section, this idea is restated such that it generalizes to multiple importance densities.\\n\\\\subsubsection{Multiple Importance Densities}\\nThe above procedure for selecting and reweighting toys that were generated from both densities can be phrased in the following way:\\n\\\\begin{itemize}\\n\\\\item A toy is generated from a density with $\\\\mu=\\\\mu\\'$ and the Likelihoods $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)$ and $\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu\\',\\\\nuisObs)$ are calculated.\\n\\\\item The toy is veto-ed when the Likelihood with $\\\\mu=\\\\mu\\'$ is not the largest. Otherwise, the toy is used with a weight that is the ratio of the Likelihoods.\\n\\\\end{itemize}\\nThis can be generalized to any number of densities with $\\\\mu_i=\\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}$. For the toys generated from model $i$:\\n\\\\begin{align}\\n\\\\textrm{veto:}& \\\\textrm{ if } \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs) \\\\neq \\\\max\\\\left\\\\{ \\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_j,\\\\nuisObs) : \\\\mu_j = \\\\{0, \\\\mu\\', \\\\mu\\'\\', \\\\ldots\\\\}\\\\right\\\\} \\\\\\\\\\n\\\\textrm{weight} &= \\\\frac{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=0,\\\\nuisObs)}{\\\\F_{\\\\rm tot}( \\\\data_{\\\\rm toy}, \\\\globs_{\\\\rm toy} | \\\\mu=\\\\mu_i,\\\\nuisObs)}\\n\\\\end{align}\\nThe number of importance densities has to be known when applying the vetos. It should not be too small to cover the parameter space appropriately and it should not be too large, because too many importance densities lead to too many vetoed toys which decreases overall efficiency. The value and error of $\\\\hat{\\\\mu}$ from a fit to data can be used to estimate the required number of importance densities for a given target overlap of the distributions.\\nThe sampling efficiency in the tail can be further improved by generating a larger number of toys for densities with larger values of $\\\\mu$. For example, for $n$ densities, one can generate $2^k / 2^n = 2^{k-n}$ of the overall toys per density $k$ with $k=0, \\\\ldots, n-1$. The toys have to be re-weighted for example by $2^{n-1} / 2^k$ resulting in a minimum re-weight factor of one. The current implementation of the error calculation for the p-value is independent of an overall scale in the weights.\\nThe method using multiple importance densities is similar to Michael Woodroofe\\'s \\\\cite{Woodroofe} prescription of creating a suitable importance density with an integral over $\\\\mu$. In the method presented here, the integral is approximated by a sum over discrete values of $\\\\mu$. Instead of taking the sum, a mechanism that allows for multiple importance densities is introduced.\\n'}},\n",
       "       {'entity_name': 'powerconstrained limits pcl', 'entity_type': 'analysis_technique', 'description': 'An approach in frequentist statistics that maintains the standard procedure for setting limits while adding a requirement based on the sensitivity of a parameter point, addressing Type II error.', 'relevant_passages': {\"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{One-sided intervals, CLs, power-constraints, and Negatively Biased Relevant Subsets}\\nParticle physicists regularly set upper-limits on cross sections and other parameters that are bounded to be non-negative. Standard frequentist confidence intervals should nominally cover at the stated value. The implication that a 95\\\\\\nTwo main approaches have been proposed to protect from excluding signals to which we do not consider ourselves sensitive. The first is the CLs procedure introduced by Read and described above~\\\\cite{Read2,Read1,CLsWikipedia}. The CLs procedure produce intervals that over-cover -- meaning that the intervals cover the true value more than the desired level. The coverage for small values of the cross-section approaches 100\\\\\\nAn alternate approach called power-constrained limits (PCL) is to leave the standard frequentist procedure unchanged while adding an additional requirement for a parameter point to be considered `excluded'. The additional requirement is directly a measure of the sensitivity of to that parameter point based on the notion of power (or Type II error). This approach makes the coverage of the procedure manifest~\\\\cite{2011arXiv1105.3166C}.\\nSurprisingly, one-sided upper limits on a bounded parameter are a subtle topic that has led to debates among the experts of statistics in the collaborations and a string of interesting articles from statisticians. The discussion is beyond the scope of the current version of these notes, but the interested reader is invited and encouraged to read~\\\\cite{Mandelkern2002} and the responses from notable statisticians on the topic. More recently Cousins tried to formalize the sensitivity problem in terms of a concept called Negatively Biased Relevant Subsets (NBRS)~\\\\cite{2011arXiv1109.2023C}. While the power-constrained limits do not formally emit NBRS, it is an interesting insight. Even more recently, Vitells has found interesting connections with CLs and the work of Birnbaum~\\\\cite{Birnbaum:1962,CLsWikipedia}. This connection is significant since statisticians have primarily seen CLs as an ad hoc procedure mixing the notion of size and power with no satisfying properties.\"}},\n",
       "       {'entity_name': 'negatively biased relevant subsets nbrs', 'entity_type': 'statistics_concept', 'description': 'A concept introduced to formalize the sensitivity problem in statistical analyses, particularly in the context of setting limits in particle physics.', 'relevant_passages': {\"\\\\section{Frequentist Statistical Procedures}\\n\\\\subsection{One-sided intervals, CLs, power-constraints, and Negatively Biased Relevant Subsets}\\nParticle physicists regularly set upper-limits on cross sections and other parameters that are bounded to be non-negative. Standard frequentist confidence intervals should nominally cover at the stated value. The implication that a 95\\\\\\nTwo main approaches have been proposed to protect from excluding signals to which we do not consider ourselves sensitive. The first is the CLs procedure introduced by Read and described above~\\\\cite{Read2,Read1,CLsWikipedia}. The CLs procedure produce intervals that over-cover -- meaning that the intervals cover the true value more than the desired level. The coverage for small values of the cross-section approaches 100\\\\\\nAn alternate approach called power-constrained limits (PCL) is to leave the standard frequentist procedure unchanged while adding an additional requirement for a parameter point to be considered `excluded'. The additional requirement is directly a measure of the sensitivity of to that parameter point based on the notion of power (or Type II error). This approach makes the coverage of the procedure manifest~\\\\cite{2011arXiv1105.3166C}.\\nSurprisingly, one-sided upper limits on a bounded parameter are a subtle topic that has led to debates among the experts of statistics in the collaborations and a string of interesting articles from statisticians. The discussion is beyond the scope of the current version of these notes, but the interested reader is invited and encouraged to read~\\\\cite{Mandelkern2002} and the responses from notable statisticians on the topic. More recently Cousins tried to formalize the sensitivity problem in terms of a concept called Negatively Biased Relevant Subsets (NBRS)~\\\\cite{2011arXiv1109.2023C}. While the power-constrained limits do not formally emit NBRS, it is an interesting insight. Even more recently, Vitells has found interesting connections with CLs and the work of Birnbaum~\\\\cite{Birnbaum:1962,CLsWikipedia}. This connection is significant since statisticians have primarily seen CLs as an ad hoc procedure mixing the notion of size and power with no satisfying properties.\"}},\n",
       "       {'entity_name': 'markov chain monte carlo mcmc', 'entity_type': 'analysis_technique', 'description': 'A class of algorithms used to sample from probability distributions by constructing a Markov chain that has the desired distribution as its equilibrium distribution. MCMC methods are widely utilized for estimating posterior distributions in Bayesian statistics.', 'relevant_passages': {\"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\", \"\\\\section{Upper Limits including systematic uncertainties, Bayesian approach}\\nThe inclusion of systematic effects in the calculation of upper limits, experimental sensitivity, and observation requires a Bayesian approach. This strategy extends the likelihood function by including typically Gaussian distributions to model effects such as efficiency, luminosity, Monte Carlo event estimation, and others~\\\\cite{lista2016practical,cranmer2015practical,junk1999confidence}. In particular, to establish the reconstruction efficiency of background events, a nuisance parameter (\\\\( \\\\epsilon \\\\)) centered on the expected value of \\\\( b \\\\) can be included. Thus, the parameter of the Poisson distribution is defined by:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu,\\\\epsilon) = \\\\mu s + \\\\epsilon b,\\n\\\\end{equation}\\nwhere the efficiency follows a binomial distribution \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\). The standard deviation of the likelihood adjusts the uncertainty of the number of background events across the observable spectrum, typically ranging from 5 to 20\\\\\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon) = \\\\prod_{i=1}^{N-channels}\\n\\\\frac{ e^{-(\\\\mu s_{i} + \\\\epsilon b_{i})} (\\\\mu s_{i} + \\\\epsilon b_{i})^{n_{i}} }{n_{i}!}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^{2}}} e^{ -\\\\frac{ (1-\\\\epsilon)^{2} } {2\\\\sigma^{2}} }.\\n\\\\end{equation}\\nThe non-informative prior distribution naturally extends to:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu,\\\\epsilon) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\ \\\\text{and} \\\\ 0 < \\\\epsilon < \\\\epsilon^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nUsing Bayes' theorem, the posterior distribution is obtained.\\n\\\\begin{equation}\\nP(\\\\mu, \\\\epsilon / \\\\bm{x}) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon)}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThis means that to establish the upper limits of \\\\( \\\\mu \\\\) or the experimental sensitivity, the posterior must be marginalized to find the profile \\\\( P(\\\\mu | \\\\bm{x}) \\\\). This is a standard probability calculation and requires numerical integration or sampling of the posterior distribution using, for example, the Markov Chain Monte Carlo (MCMC) algorithm~\\\\cite{raftery1992practical, wang2023recent}. In any case, the probability profile is given by:\\n\\\\begin{equation}\\nP(\\\\mu,\\\\bm{x}) = \\\\int_{0}^{\\\\infty} P(\\\\mu, \\\\epsilon / \\\\bm{x}) d\\\\epsilon = \\\\frac{ \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\epsilon}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThe marginalization process correctly propagates the effect of systematic uncertainty in the upper limits. In general, the correlation shifts the limit values to higher values, thereby restricting the sensitivity of a model in the experiment or the exclusion power in an experimental study~\\\\cite{conway2005calculation}. As mentioned previously, the expected and observed upper limits are defined over the marginal distribution:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = \\\\int_{0}^{\\\\mu_{up}} P(\\\\mu,x) d\\\\mu. = 0.95\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:19}] shows the posterior distribution as a function of the signal strength \\\\( \\\\mu \\\\) and the efficiency in estimating background events (\\\\( b \\\\)), for the channel with \\\\( n=105 \\\\), \\\\( b=100 \\\\), \\\\( s=10 \\\\), and a systematic uncertainty of \\\\( \\\\sigma=0.1 \\\\) for the background events~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/UpperLimitSystematic.ipynb}{Source code}}. Similarly, Figure~[\\\\ref{fig:20}] shows the marginal distribution obtained using the double Gaussian quadrature method~\\\\cite{golub1969calculation}.\\nBy varying the systematic uncertainty, it is possible to find the observed upper limits and the correlation effect between parameters. Table~[\\\\ref{tb:3}] shows the behavior of the observed upper limit as a function of \\\\( \\\\sigma \\\\) for the numerical approximation (Gaussian quadrature) and for the sampling generated by the Metropolis algorithm, as well as the correlation coefficient indicating how uncertainty limits the exclusion power of the model. It is important to note that for high-dimensional posterior distributions, there are no quadrature rules that allow for accurately estimating the marginal distribution. In such cases, the Metropolis algorithms and optimization methods have been widely applied~\\\\cite{atlas2012observation,cms2012observation}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & Gaussian quadrature & MCMC algorithm & Correlation coef\\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 & -0.32\\\\\\\\\\n0.10 & 3.34 & 3.31 & -0.54\\\\\\\\\\n0.15 & 4.09 & 4.13 & -0.68\\\\\\\\\\n0.20 & 4.91 & 4.66 & -0.77\\\\\\\\\\n0.25 & 5.79 & 5.80 & -0.88\\\\\\\\ \\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nIn particular, the \\\\texttt{emcee} package was used for sampling the extended posterior distribution shown above~\\\\cite{foreman2013emcee, Bocklund2019ESPEI}. Figure~[\\\\ref{fig:21}] shows the corner plot of the posterior distribution along with the marginal distributions associated with the signal strength \\\\( \\\\mu \\\\) and the reconstruction efficiency \\\\( \\\\epsilon \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/MetropolisSamplingBayes.ipynb}{Source code}}. Additionally, the maximum likelihood estimators of the posterior are shown; these parameters are required for the profile likelihood method presented in the following section.\\n\", \"\\\\section{Upper Limits for one channel experiment}\\n\\\\subsection{Bayesian upper limits}\\nSince the upper limits obtained through the frequentist approach can lead to non-physical statistical boundaries, alternative approaches can improve the results. One of the most promising strategies is based on Bayes' theorem:\\n\\\\begin{equation}\\nP(\\\\bm{\\\\theta}/x) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\bm{\\\\theta})\\\\Pi(\\\\bm{\\\\theta})}{P(\\\\bm{x})}.\\n\\\\end{equation}\\nWhere $P(\\\\bm{\\\\theta}|\\\\bm{x})$ represents the probability that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true given the set of observations $\\\\bm{x}$, and is known as the \\\\textit{posterior distribution}. $\\\\mathcal{L}(\\\\bm{x}|\\\\bm{\\\\theta})$, known as the \\\\textit{likelihood function}, describes the probability of observing $\\\\bm{x}$ given that the hypothesis parameterized by $\\\\bm{\\\\theta}$ is true. On the other hand, $\\\\Pi(\\\\bm{\\\\theta})$ is the \\\\textit{prior distribution}, which reflects the probability that the hypothesis $\\\\bm{\\\\theta}$ is true before the observations are made, and $P(\\\\bm{x})$ is the \\\\textit{total probability} of observing $\\\\bm{x}$ across all hypotheses. In optimization processes, this latter distribution is considered a normalization factor for the posterior distribution~\\\\cite{wang2023recent}. From a parameter estimation perspective, sampling from the posterior distribution generally requires robust methods, such as the Metropolis-Hastings algorithm~\\\\cite{chib1995understanding}.\\nBy incorporating an appropriate prior distribution, such as one with a minimum at $\\\\mu=0$, the coverage problem present in the frequentist approach is corrected. This implies that, for no value of the parameter of interest $\\\\mu$, is the null hypothesis excluded. An unbiased distribution is given by:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nThe value of $\\\\mu^{max}$ is chosen to obtain p-values consistent with the critical region (i.e., $\\\\alpha = 0.05$). However, the degree of subjectivity in selecting the prior distribution introduces ambiguity in the calculation of credible confidence intervals, as well as in the upper limits. In general, it has been observed that Bayesian parameter estimation leads to less restrictive upper limits, which are dependent on the choice of the prior distribution. This characteristic has limited the use of Bayesian upper limits in physics analyses with real observations~\\\\cite{cms2012observation,atlas2012observation}.\\nReturning to the simplified model with no observation, $b \\\\approx n = 0$, and $s=1$, the Bayesian upper limit for this new model is $\\\\mu_{up} = 2.999$ at $95\\\\\\n\\\\begin{equation}\\nP(\\\\bm{x}) = \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu)\\\\Pi(\\\\mu) d \\\\mu.\\n\\\\end{equation}\\nAlthough both methods seem to lead to the same results for the estimation of upper limits, varying the background component and the number of observed events reveals an adjustment in the upper limits that corrects the coverage issue in the estimation. Figure~[\\\\ref{fig:6}] shows the behavior of the upper limit as a function of the background component and the number of observations. Note how the null hypothesis is not excluded when the prior distribution is correctly chosen. However, manipulating the prior distribution can result in a significant shift in the upper limit value. For this reason, the Bayesian method is not widely used in the analysis of real data.\\nAdditionally, it is possible to estimate the upper limit using the Markov Chain Monte Carlo (MCMC) technique~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/1Channel/Bayesian/MetropolisSampling.ipynb}{Source Code}}. This method allows sampling from the posterior distribution based on Markov processes, as described in various sources~\\\\cite{chib1995understanding, raftery1992practical}. In general, this approach enables the construction of the marginal posterior function for the calculation of upper limits, the estimation of standard errors ($2\\\\sigma$), and the necessary parameter estimation in methods such as the profile of maximum likelihood, discussed later. Figure~[\\\\ref{fig:7}] shows the sampling of the posterior distribution for the toy model, leading to an approximation of the upper limit using the $P_{95}$ percentile.\\nThe MCMC algorithm is widely used for estimation in real multichannel experiments and incorporates systematic effects. The development of these ideas requires non-Bayesian approaches that do not depend on the choice of the prior distribution. An initial non-Bayesian approach that protects the null hypothesis is known as the modified frequentist method~\\\\cite{read2002presentation, cms2022portrait}.\\n\"}},\n",
       "       {'entity_name': 'metropolishastings algorithm', 'entity_type': 'analysis_technique', 'description': 'An algorithm used within Markov Chain Monte Carlo methods to generate samples from a probability distribution by proposing new samples and accepting or rejecting them based on their likelihood.', 'relevant_passages': {\"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\"}},\n",
       "       {'entity_name': 'sensitivity analysis', 'entity_type': 'analysis_technique', 'description': 'A method used to determine how the variation in the output of a model can be attributed to different variations in the inputs, particularly in the context of prior selection in Bayesian analysis.', 'relevant_passages': {\"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\"}},\n",
       "       {'entity_name': 'jeffreyss prior', 'entity_type': 'statistics_concept', 'description': 'A non-informative prior used in Bayesian statistics that is invariant under reparametrization of the parameters and is derived from the Fisher information matrix.', 'relevant_passages': {\"\\\\section{Bayesian Procedures}\\n[This section is far from complete. Some key practical issues and references to other literature are given.]\\nUnsurprisingly, Bayesian procedures are based on Bayes's theorem as in Eq.~\\\\ref{Eq:Bayes} and Eq.~\\\\ref{eq:urprior}. The Bayesian approach requires one to provide a prior over the parameters, which can be seen either as an advantage or a disadvantage~\\\\cite{DAgostiniInference,Cousins:1994yw}. In practical terms, one typically wants to build the posterior distribution for the parameter of interest. This typically requires integrating, or \\\\textit{marginalizing}, over all the nuisance parameters as in Eq.~\\\\ref{eq:credible}. These integrals can be over very high dimensional posteriors with complicated structure. One of the most powerful algorithms for this integration is Markov Chain Monte Carlo, described below. In terms of the prior one can either embrace the subjective Bayesian approach~\\\\cite{Jaynes:2003fk} or take a more 'objective' approach in which the prior is derived from formal rules. For instance, Jeffreys's Prior~\\\\cite{JeffreysPrior} or their generalization in terms of Reference Priors~\\\\cite{Demortier:2010sn}. \\nGiven the logical importance of the choice of prior, it is generally recommended to try a few options to see how the result numerically depends on the choice of priors (i.e.. sensitivity analysis). This leads me to a few great quotes from prominent statisticians:\\n``Sensitivity analysis is at the heart of scientific Bayesianism'' --Michael Goldstein\\n``Perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions'' -Brad Efron\\n``Meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification'' -- Michael Goldstein\\n``Objective Bayesian analysis is the best frequentist tool around'' --Jim Berger\\n\\\\subsection{Hybrid Bayesian-Frequentist methods}\\nIt is worth mentioning that in particle physics there has been widespread use of a hybrid Bayesian-Frequentist approach in which one marginalizes nuisance parameters. Perhaps the most well known example is due to a paper by Cousins and Highland~\\\\cite{CousinsHighland:1991qz}. In some instances one obtains a Bayesian-averaged model that depends only on the parameters of interest\\n\\\\begin{equation}\\n\\\\bar{\\\\F}(\\\\data | \\\\vec\\\\alpha_{\\\\rm poi}) = \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\eta(\\\\vec\\\\alpha_{\\\\rm nuis}) \\\\; d\\\\vec\\\\alpha_{\\\\rm nuis}\\n\\\\end{equation}\\nand then proceeds with the typical frequentist methodology for calculating p-values and constructing confidence intervals. Note, in this approach the constraint terms that are appended to $\\\\F_{\\\\rm sim}$ of Eq.~\\\\ref{Eq:simultaneous} to obtain $\\\\F_{\\\\rm tot}$ of Eq.~\\\\ref{Eq:ftot} are interpreted as in Eq.~\\\\ref{eq:urprior} and $\\\\eta(\\\\vec\\\\alpha_{\\\\rm nuts})$ is usually a uniform prior. Furthermore, the global observables or auxiliary measurements $a_p$ are typically left fixed to their nominal or observed values and not randomized.\\nIn other variants the full model without constraints $\\\\F_{\\\\rm sim}(\\\\data | \\\\vec\\\\alpha)$ is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing (or randomizing) the nuisance parameters as in Eq.~\\\\ref{eq:urprior}. See the following references for more details \\\\cite{Conrad:2005zm,Tegenfeldt:2004dk,Conrad:2002ur,Conrad:2002kn,Rolke:2004mj,PhysRevD.67.118101,Demortier:2007zz,Cousins:2008zz}. \\nThe shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability. Thus it is hard to define exactly what the p-values and intervals mean in a formal sense.\\n\\\\subsection{Markov Chain Monte Carlo and the Metropolis-Hastings Algorithm}\\nThe Metropolis-Hastings algorithm is used to construct a Markov chain $\\\\{\\\\vec\\\\alpha_i\\\\}$, where the samples $\\\\vec\\\\alpha_i$ are proportional to the target posterior density or likelihood function. The algorithm requires a proposal function $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')$ that gives the probability density to propose the point $\\\\vec\\\\alpha$ given that the last point in the chain is $\\\\vec\\\\alpha'$. Note, the density only depends on the last step in the chain, thus it is considered a Markov process. At each step in the algorithm, a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain. Even when the proposal density function is not symmetric, Metropolis Hastings maintains `detailed balance' when constructing the Markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density. That is, given the current point $\\\\vec\\\\alpha$, proposed point $\\\\vec\\\\alpha'$, likelihood function $L$, and proposal density function $Q$, we visit $\\\\vec\\\\alpha'$ if and only if\\n\\\\begin{equation}\\n\\\\displaystyle \\\\frac{L(\\\\vec\\\\alpha')}{L(\\\\vec\\\\alpha)} \\\\frac{Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')}{Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)} \\\\geq Rand[0,1]\\n\\\\end{equation}\\nNote, if the proposal density is symmetric, $Q(\\\\vec\\\\alpha | \\\\vec\\\\alpha')=Q(\\\\vec\\\\alpha' | \\\\vec\\\\alpha)$, then the ratio of the proposal densities can be neglected (which can be computationally expensive). Above we have written the algorithm to sample the likelihood function $L(\\\\vec\\\\alpha)$, but typically one would use the posterior $\\\\pi(\\\\vec\\\\alpha)$. Within \\\\roostats\\\\ the Metropolis-Hastings algorithm is implemented with the \\\\texttt{MetropolisHastings} class, which returns a \\\\texttt{MarkovChain}. Another powerful tool is the Bayesian Analysis Toolkit (BAT)~\\\\cite{Caldwell:2009ve}. Note, one can use a \\\\roofit\\\\ / \\\\roostats\\\\ model in the BAT environment.\\nNote, an alternative to Markov Chain Monte Carlo is the nested sampling approach of Skilling~\\\\cite{skilling:395} and the \\\\texttt{MultiNest} implementation~\\\\cite{Feroz:2008xx}.\\nLastly, we mention that sampling algorithms associated to Bayesian belief networks and graphical models may offer enormous advantages to both MCMC and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model.\\n\\\\subsection{Jeffreys's and Reference Prior}\\nOne of the great advances in Bayesian methodology was the introduction of Jeffreys's rule for selecting a prior based on a formal rule~\\\\cite{JeffreysPrior}. The rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters. The rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the Fisher information matrix, which we first encountered in Eq.~\\\\ref{Eq:expfisher}.\\n\\\\begin{equation}\\n\\\\pi(\\\\vec\\\\alpha) = \\\\sqrt{\\\\det \\\\Sigma^{-1}_{pp'}(\\\\vec\\\\alpha)} = \\\\sqrt{ \\\\det \\\\left[ \\\\int \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha) \\\\; \\\\frac{-\\\\partial^2 \\\\log \\\\F_{\\\\rm tot}(\\\\data | \\\\vec\\\\alpha)}{\\\\partial\\\\alpha_p\\\\alpha_{p'}} \\\\; d\\\\data \\\\right]}\\\\\\n\\\\end{equation}\\nWhile the right-most form of the prior looks daunting with complex integrals over partial derivatives, the Asimov data described in Sec.~\\\\ref{S:Asimov} and Ref.~\\\\cite{asimov} provide a convenient way to calculate the Fisher information. Fig.~\\\\ref{fig:JeffreysPriorGaussian} and \\\\ref{fig:JeffreysPriorPoisson} show examples of \\\\roostats\\\\ numerical algorithm for calculating Jeffreys's prior compared to analytic results on a simple Gaussian and a Poisson model.\\nUnfortunately, Jeffreys's prior does not behave well in multidimensional problems. Based on a similar information theoretic approach, Bernardo and Berger have developed the Reference priors~\\\\cite{Berger:1992ys,Berger:1992vn,Berger:1989kx,Bernardo:1979uq} and the associated Reference analysis. While attractive in many ways, the approach is fairly difficult to implement. Recently, there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics~\\\\cite{Demortier:2010sn,Casadei:2011hx}.\\n\\\\subsection{Likelihood Principle}\\nFor those interested in the deeper and more philosophical aspects of statistical inference, the likelihood principle is incredibly interesting. This section will be expanded in the future, but for now I simply suggest searching on the internet, the Wikipedia article, and Ref.~\\\\cite{Birnbaum:1962}. In short the principle says that all inference should be based on the likelihood function of the observed data. Frequentist procedures violate the likelihood principle since p-values are tail probabilities associated to hypothetical outcomes (not the observed data). Generally, Bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle. Somewhat ironically, the objective Bayesian procedures such as Reference priors and Jeffreys's prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes.\\n\"}},\n",
       "       {'entity_name': 'unfolding in particle physics', 'entity_type': 'analysis_technique', 'description': 'A statistical technique in particle physics aimed at correcting measured distributions for detector effects to recover the true underlying distribution of particles. This process is often laborious and performed for each observable separately, but advancements in machine learning methods can accelerate the unfolding process.', 'relevant_passages': {\"\\\\section{Unfolding}\\nAnother topic for the future. The basic aim of unfolding is to try to correct distributions back to the true underlying distribution before detector 'smearing'. For now, see \\\\cite{Prosper:1306523,DAgostini1995487,Adye:2011gm,Malaescu:2011yg,Blobel:2002pu,Hocker:1995kb,Choudalakis2012,Tikhonov}.\\n\", \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that the modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was no real reason to insure that the correlations are all correct. \\nA commonplace, and often implicit, belief is that that although the correlations in the synthetic data may not be exactly the same as the correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using the synthetic data would be to train on real data. Unfortunately, while the data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. And even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. Thus there are two ways to proceed. First, we can try to to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to get truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the faction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, recall that these simulations have different components. The short-distance simulation, which produces of order 1000 particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely exciting, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. An proof-of-principle implementation of this idea is OminFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then the mapping can be inverted to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up my many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background an some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, were some supervised training is used to guide data-driven estimation technique is a very promising area for future development of ML for particle physics. \\n\", \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was not a strong motivation to ensure that the correlations were all correct. \\nA commonplace, and often implicit, belief is that, although correlations in the synthetic data may not be exactly the same as correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using synthetic data would be to train on real data. Unfortunately, while synthetic data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. Even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. There are two ways to proceed. First, we can try to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to gather truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the fraction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments in probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, as discussed above these simulations have different components. The short-distance simulation, which produces hundreds of particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely appealing, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. A proof-of-principle implementation of this idea is OmniFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then one can try to invert the mapping to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up by many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting (refining the event selection) on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background in some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, where some supervised training is used to guide a data-driven estimation technique, is a very promising area for future development of ML for particle physics. \\n\"}},\n",
       "       {'entity_name': 'statistical measures of central tendency and dispersion', 'entity_type': 'statistics_concept', 'description': 'A collection of statistical concepts that include measures of central tendency such as sample mean and weighted average, as well as measures of dispersion like sample variance, standard deviation, and variance. These measures quantify the average values and the spread of data points in a sample, while also incorporating concepts like covariance and correlation to analyze relationships between variables and their uncertainties.', 'relevant_passages': {\"\\\\section{Least squares: Basic idea}\\nAs a specific example, we will consider fitting a straight line $y = a + bx$ to some data, which consist of a series on $n$ data\\npoints, each of which specifies $(x_i, y_i \\\\pm \\\\sigma_i)$ i.e. at precisely known $x_i$, the $y$ co-ordinate is measured\\nwith an uncertainty $\\\\sigma_i$. The $\\\\sigma_i$ are assumed to be uncorrelated. The more general case could involve \\n\\\\begin{itemize}\\n\\\\item{a more complicated functional form than linear;}\\n\\\\item{multidimensional $x$ and/or $y$;}\\n\\\\item{correlations among the $\\\\sigma_i$; and}\\n\\\\item{uncertainties on the $x_i$ values.}\\n\\\\end{itemize} \\nIn Particle Physics, we often deal with a histogram of some physical quantity $x$ (e.g. mass, angle, \\ntransverse momentum, etc.), in which case $y$ is simply the number of counts for that $x$ bin. Another possiblity\\nis that $y$ and $x$ are both physical quantities e.g. we have a two-dimensional plot showing the recession velocities \\nof galaxies as a function their distance. \\nThere are two statistical issues: Are our data consistent with the theory i.e. a straight line? And what are \\nthe best estimates of the parameters, the intercept and the gradient? The former is a Goodness of Fit \\nissue, while the latter is Parameter Determination. The Goodness of Fit is more fundamental, in that\\nif the data are not consistent with the hypothesis, the parameter values are meaningless. However, we will first\\nconsider Parameter Detemination, since checking the quality of the fit requires us to use the best straight\\nline. \\nThe data statistic used for both questions is $S$, the weighted sum of squared discrepancies\\\\footnote{Many people \\nrefer to this as $\\\\chi^2$. I prefer S, because otherwise a discussion about whether or not $\\\\chi^2$ follows the\\nmathematical $\\\\chi^2$ distribution sounds confusing.}\\n\\\\begin{equation}\\nS = \\\\Sigma (y_i^{th} - y_i^{obs})^2/\\\\sigma_i^2 = \\\\Sigma (a + bx_i - y_i^{obs})^2 / \\\\sigma_i^2\\n\\\\end{equation}\\nwhere $y_i^{th} = a + bx_i$ is the predicted value of $y$ at $x_i$, and $y_i^{obs}$ is the observed value. In the \\nexpression for $S$, we regard the data $(x_i, y_i \\\\pm \\\\sigma_i)$ as being fixed, and the parameters $a$\\nand $b$ as being variable.\\nIf for specific values of $a$ and $b$ the predicted values of $y$ and the corresponding observed ones are all close \\n(as measured in terms of the \\nuncertainties $\\\\sigma$), then $S$ will be `small', while significant discrepancies result in large $S$. Thus, according\\nto the least squares method, the best values of the parameters are those that minimise $S$, and the width of the $S$\\ndistribution determines their uncertainties. For a good fit, the value of $S_{min}$ should be `small'. A more quantative\\ndiscussion of `small' appears below. \\nTo determine the best values of $a$ and $b$, we need to set the first derivatives of $S$ with respect to $a$ and $b$ both\\nequal to zero. This leads to two simultaneous linear equations for $a$ and $b$ \\\\footnote{The derivatives are linear in the \\nparameters, because the functional form is linear in them. This would also be true for more complicated situations \\nsuch as a higher order polynomial (Yes, with respect to the coefficients, a $10^{th}$ order polynomial is linear),\\na series of inverse powers, Fourier series, etc.} \\nwhich are readily solved, to yield\\n\\\\begin{equation}\\n\\\\begin{split}\\na&=\\\\frac{<x^2><y> - <xy><x>}{<x^2> - <x>^2} \\\\\\\\\\nb&=\\\\frac {<xy> - <x> <y>}{<x^2> - <x>^2} \\n\\\\end{split}\\n\\\\end{equation} \\nwhere $<f>\\\\ = \\\\Sigma (f_i/\\\\sigma_i^2) / \\\\Sigma (1/\\\\sigma_i^2)$ i.e it is the weighted average of the quantity inside the\\nbrackets. If the positions of the data points are such that $<x>\\\\ = 0$, then $a=\\\\ <y>$, i.e. the height of the\\nbest fit line at the weighted centre of gravity of the data points is just the weighted average of the $y$ values. \\nIt is also essential to calculate the uncertainties $\\\\sigma_a$ and $\\\\sigma_b$ on the parameters and their correlation\\ncoefficient $\\\\rho = cov/(\\\\sigma_x \\\\sigma_y)$, where $cov$ is their covariance. The elements of the inverse\\ncovariance matrix $M$ are given by\\n\\\\begin{equation}\\n\\\\begin{split}\\nM_{aa} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a^2} = \\\\Sigma (1/\\\\sigma_i^2) \\\\\\\\\\nM_{ab} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a \\\\ \\\\partial b} = \\\\Sigma(x_i/\\\\sigma_i^2) \\\\\\\\\\nM_{bb} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial b^2} = \\\\Sigma(x_i^2/\\\\sigma_i^2) \\\\\\\\\\n\\\\end{split}\\n\\\\end{equation}\\nThe covariance matrix is obtained by inverting $M$. Since the covariance is proportional to $-< x >$, if the \\ndata are centred around $x = 0$, the uncertainties on $a$ and $b$ will be uncorrelated. That is \\none reason why track parameters are usually specified at the centre of the track, rather than at its starting point.\\n\\\\subsection{Correlated uncertainties on data}\\nSo far we have considered that the uncertainties on the data are uncorrelated, but this is not always the case; correlations can \\narise from some common systematic. Then instead of the first equation of (\\\\ref{eqn:S}), we use\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (y_i^{th} - y_i^{obs})E_{ij} (y_j^{th} - y_j^{obs})\\n\\\\end{equation}\\nwhere the double summation is over $i$ and $j$, and $E$ is the inverse covariance matrix\\\\footnote{We use the symbol $E$ for the inverse covariance matrix of the measured variables $y$, and $M$ for that of the output parameters (e.g. $a$ and $b$\\nfor the straight line fit).}\\nfor the uncertainties on the \\n$y_i$. For the special case of uncorrelated uncertainties, the only non-zero elements of $E$ are the diagonal ones \\n$E_{ii} = 1/\\\\sigma_i^2$ and then \\neqn. (\\\\ref{correlated_S}) reduces to (\\\\ref{eqn:S}).\\nThis new equation for $S$ can then be minimised to give the best values of the parameters, and $S_{min}$ can be used in a Goodness of Fit test. As before, if $y^{th}$ is linear in the parameters, their best estimates can be obtained by solving simultaneous linear equations, without the need for a minimisation programme.\\n\", \"\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Descriptive Statistics}\\nSuppose we have a sample of $N$ data $X = x_1, x_2, \\\\cdots, x_N$. It is often useful to summarize these data with a few numbers called statistics. \\nA \\\\textbf{statistic} is any number that can be calculated from the data and known parameters. For example, $t = (x_1 + x_N)/2$ is a statistic, but if the value of $\\\\theta$ is unknown $t = (x_1 - \\\\theta)^2$ is not. However, a word of caution is in order: we particle physicists are prone to misuse the jargon of professional statisticians. For example, we tend to refer to \\\\emph{any} function of the data as a statistic including those that contain unknown parameters. \\nThe two most important statistics are\\n\\\\begin{align}\\n&\\\\textrm{the {\\\\bf sample mean} (or average)} & \\\\bar{x} & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i, \\\\\\\\\\n&\\\\textrm{and the {\\\\bf sample variance}} & s^2 & = \\\\frac{1}{N} \\\\sum_{i=1}^N (x_i - \\\\bar{x})^2,\\n\\\\nonumber\\\\\\\\\\n& & & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i^2 - \\\\bar{x}^2, \\\\nonumber\\\\\\\\\\n& & & = \\\\overline{x^2} - \\\\bar{x}^2.\\n\\\\end{align}\\nThe sample average is a measure of the center of the distribution of the data, while the sample variance is a measure of its spread. Statistics that merely characterize the data are\\ncalled \\\\textbf{descriptive statistics}, of which the sample average and variance are the most\\nimportant. If we order the data, say from the smallest value to the largest, we can compute another interesting statistic $t_k \\\\equiv x_{(k)}$, where $1 \\\\leq k \\\\leq N$ and $x_{(k)}$ denotes the datum at the $k^\\\\text{th}$ position. The statistic $t_k$ is called the $k^\\\\text{th}$ \\\\textbf{order statistic} and is a measure of the value of outlying data.\\nThe average and variance, Eqs.~(\\\\ref{eq:xbar}) and (\\\\ref{eq:xvar}), are numbers that can always be calculated given a data sample $X$. But now we consider numbers that cannot be calculated from the data alone. Imagine the repetition, infinitely many times, of whatever data generating system yielded our data sample $X$ thereby creating an infinite sequence of data sets. We shall refer to the data generating system as an experiment and the\\ninfinite sequence as an infinite ensemble. The latter, together with all the mathematical\\noperations we may wish to apply to it, are \\nabstractions. After all, it is not possible to realize an infinite ensemble. The ensemble and\\nall the operations on it exist in the same sense that the number $\\\\pi$ exists along with all valid\\nmathematical operations on $\\\\pi$.\\nThe most common operation to perform on an ensemble is to compute the average of the statistics. This \\\\textbf{ensemble average} suggests several potentially useful characteristics of the ensemble, which we list below.\\n\\\\begin{align}\\n& \\\\textrm{Ensemble average} & & <x> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean} & \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Error} & \\\\epsilon & = x - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Bias} & b & = <x> - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Variance} & V & = <(x - <x>)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Standard deviation} & \\\\sigma & = \\\\sqrt{V} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean square error} & \\\\text{MSE} & = <(x - \\\\mu)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Root MSE} & \\\\textrm{RMS} & = \\\\sqrt{\\\\textrm{MSE}}\\n\\\\end{align}\\nNotice that none of these numbers can be calculated in practice because the data\\nrequired to do so do not concretely exist. Even\\nin an experiment simulated on a computer, there are very few of these numbers we can\\ncalculate. If we know the mean $\\\\mu$, perhaps because we have chosen its value --- for example, we may have chosen the mass of the Higgs boson in our simulation, we can certainly calculate the error $\\\\epsilon$ for any simulated datum $x$. But, we can only \\\\emph{approximate} the\\nensemble average $< x >$, bias $b$, variance $V$, and MSE, since our virtual ensemble is\\nalways finite. The point is this: the numbers that characterize the infinite ensemble are also abstractions,\\nalbeit useful ones. For example, the MSE is the most widely used\\nmeasure of the closeness of an ensemble of numbers to some parameter $\\\\mu$. The square root of the MSE is called the root mean square (RMS)\\\\footnote{Sometimes, the RMS and standard deviation are using interchangeably. However, the RMS is computed with respect to $\\\\mu$, while the standard deviation is computed with respect to the ensemble average $<x>$. The RMS and standard deviations are identical only if the bias is zero.}. The MSE can be written as\\n\\\\begin{align}\\n\\\\textrm{MSE} & = V + b^2. \\\\\\\\ & \\\\framebox{\\\\textbf{Exercise 1: } \\\\textrm{Show this}}\\\\nonumber\\n\\\\end{align}\\nThe MSE is the sum of the variance and the square of the bias, \\na very important result with practical consequences. For example, suppose that $\\\\mu$ represents the mass of the Higgs boson and $x$ represents some (typically very complicated) statistic that is considered an \\\\textbf{estimator} of the mass. An estimator is any\\nfunction, which when data are entered into it, yields an \\\\textbf{estimate} of the quantity\\nof interest, which we may take to be a measurement. \\nWords are important; ``bias'' is a case in point. It is an unfortunate choice\\nfor the difference $<x> - \\\\mu$ because the word ``bias'' biases attitudes towards bias! Something that, or someone who, is biased is surely bad and needs to be corrected. Perhaps.\\nBut, it would be wasteful of data to make the bias zero if the net effect is to make the MSE larger than an MSE in which the bias is non-zero. The price for achieving $b = 0$ in our\\nexample would be not only throwing away expensive data --- which is bad enough --- but also measuring a mass that is more likely to be further away from the Higgs boson mass. This may, or may not, be what we want to achieve. \\nAs noted, many of the numbers listed in Eq.~(\\\\ref{eq:ensemble}) cannot be calculated because\\nthe information needed is unknown. This is\\ntrue, in particular, of the bias. However, sometimes it is possible to relate the bias to another ensemble quantity. Consider the ensemble average of the sample variance, Eq.~(\\\\ref{eq:xvar}),\\n\\\\begin{align}\\n<s^2> & = < \\\\overline{x^2} > - <\\\\bar{x}^2>, \\\\nonumber\\\\\\\\\\n& = V - \\\\frac{V}{N}, \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 2a:} Show this} \\\\nonumber \\n\\\\end{align}\\nThe sample variance has a bias of $b = - V/N$, which many argue should be\\ncorrected. Unfortunately, we cannot calculate the bias because it depends on an unknown parameter, namely, the variance $V$. \\nHowever, if we replace the sample variance by $s^{\\\\prime 2} = c s^2$,where the\\ncorrection factor $c = N/(N-1)$, we find that for the corrected variance estimator $s^{\\\\prime 2}$ the bias is zero. Surely the world is now a better place? Well, not necessarily. \\nConsider the ratio of $\\\\textrm{MSE}^\\\\prime$ to $\\\\textrm{MSE}$, where $\\\\textrm{MSE}^\\\\prime = <(s^{\\\\prime 2} - V)^2>$, $\\\\textrm{MSE} = <\\\\delta^2>$ with $\\\\delta = s^2 - V$, and\\n$b = -V / N$,\\n\\\\begin{align*}\\n\\\\textrm{MSE}^\\\\prime / \\\\textrm{MSE} \\n& = < (c s^2 - V)^2 > / < \\\\delta^2 > , \\\\\\\\\\n& = c^2 < (s^2 - V/c)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 < (\\\\delta - b)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 (1 - b^2 / <\\\\delta^2> ), \\\\\\\\\\n& = c^2 \\\\left[ 1 - b^2 / (b^2 + <s^4> - (V+b)^2) \\\\right].\\n\\\\end{align*} \\nFrom this we deduce that if $<s^4>/[ (V+b)^2 + b^2 / (c^2 - 1)] > 1$, the unbiased\\nestimate will be further away on average from $V$ than the biased estimate. This is the case,\\nfor example, for a uniform distribution.\\n\\\\centerline{ \\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 2b:} Use the method {Rndm()} of the {\\\\tt Root}\\\\\\\\ class {\\\\tt TRandom3} to verify that $\\\\textrm{MSE}^\\\\prime > \\\\textrm{MSE}$. }} \\n}\\n\", \"\\\\section{Errors}\\n\\\\subsection {Combining errors}\\nHaving obtained---by whatever means---errors $\\\\sigma_x, \\\\sigma_y...$ \\nhow does one combine them to get errors on derived quantities $f(x,y...), g(x,y,...)$?\\nSuppose $f=Ax+By+C$, with $A,B$ and $C$ constant.\\nThen it is easy to show that \\n\\\\begin{align}\\nV_f&= \\\\notag\\n\\\\left< (f - \\\\left< f\\\\right>)^2\\\\right>\\\\\\\\&= \\\\notag\\n\\\\left< (Ax+By+C - \\\\left< Ax+By+C\\\\right>)^2\\\\right> \\\\\\\\\\n&= \\\\notag\\nA^2(\\\\left<x^2\\\\right> - \\\\left<x\\\\right>^2)\\n+B^2(\\\\left<y^2\\\\right> - \\\\left<y\\\\right>^2)\\n+2AB(\\\\left<xy\\\\right> - \\\\left<x\\\\right>\\\\left< y \\\\right>)\\\\\\\\\\n&=A^2 V_x + B^2 V_y + 2AB\\\\, {\\\\rm Cov}_{xy}\\n\\\\quad.\\n\\\\end{align}\\nIf $f$ is not a simple linear function of $x$ and $y$ then one can use a first order Taylor expansion to\\napproximate it about a central value $f_0(x_0,y_0)$\\n\\\\begin{equation}\\nf(x,y)\\\\approx f_0 \\n+ \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) (x-x_0)\\n+ \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) (y-y_0)\\n\\\\end{equation}\\n\\\\noindent and application of Eq.~\\\\ref{eq:COE0} gives\\n\\\\begin{equation}\\nV_f=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 V_x+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 V_y+\\n2 \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) {\\\\rm Cov}_{xy}\\n\\\\end{equation}\\n\\\\noindent writing the more familiar $\\\\sigma^2$ \\ninstead of $V$ this is equivalent to\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n+ 2 \\\\rho \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) \\\\sigma_x \\\\sigma_y\\n\\\\quad.\\n\\\\end{equation}\\nIf $x$ and $y$ are independent, which is often but not always the case, this reduces to what is often known as the\\n`combination of errors' formula\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n\\\\quad.\\n\\\\end{equation}\\nExtension to more than two variables is trivial: an extra squared term is added for each and\\nan extra covariance term for each of the variables (if any) with which it is correlated.\\nThis can be expressed in language as {\\\\it errors add in quadrature}. This is a friendly fact, as\\nthe result is smaller than you would get from arithmetic addition. If this puzzles you, it may be helpful to think \\nof this as allowing for the possibility that a positive fluctuation in one variable may be cancelled by a negative fluctuation in\\nthe other. \\nThere are a couple of special cases we need to consider. \\nIf $f$ is a simple product, $f=Axy$, then Eq.~\\\\ref{eq:COE2} gives\\n$$\\\\sigma_f^2=(Ay)^2 \\\\sigma_x^2+ (Ax)^2 \\\\sigma_y^2 \\\\ ,$$\\nwhich, dividing by $f^2$, can be written as\\n\\\\begin{equation}\\n\\\\left({ \\\\sigma_f \\\\over f }\\\\right)^2 =\\\\left( {\\\\sigma_x \\\\over x }\\\\right)^2 +\\n\\\\left({ \\\\sigma_y \\\\over y }\\\\right)^2.\\n\\\\end{equation}\\nFurthermore this also applies if $f$ is a simple quotient, \\n$f=Ax/y$ or $Ay/x$ or even $A/(xy)$.\\nThis is very elegant, but it should not be overemphasised. Equation~\\\\ref{eq:COE3}\\nis not fundamental: it only applies in certain cases (products or quotients). Equation~\\\\ref{eq:COE2} is\\nthe fundamental one, and Eq.~\\\\ref{eq:COE3} is just a special case of it.\\nFor example: if you measure the radius of a cylinder as $r=123 \\\\pm 2$ mm and the height as $h=456 \\\\pm 3$ mm\\nthen the volume $\\\\pi r^2 h$ is $\\\\pi \\\\times 123^2 \\\\times 456 = 21673295 \\\\ {\\\\rm mm}^3$ \\nwith error $\\\\sqrt{(2 \\\\pi r h)^2 \\\\times \\\\sigma_r^2 + ( \\\\pi r^2 )^2 \\\\times \\\\sigma_h^2}=719101$,\\nso one could write it as $v=(216.73 \\\\pm 0.72) \\\\times 10^5\\\\ {\\\\rm mm}^3$.\\nThe surface area $2 \\\\pi r^2 + 2\\\\pi r h$ is $ 2 \\\\pi \\\\times 123^2 + 2 \\\\pi \\\\times 123 \\\\times 456 = 447470\\\\ {\\\\rm mm}^2$\\nwith error $\\\\sqrt{(4\\\\pi r + 2 \\\\pi h)^2 \\\\sigma_r^2 + (2 \\\\pi r)^2 \\\\sigma_h^2 }= 9121 \\\\ {\\\\rm mm}^2 $---so one could write the result \\nas $a=(447.5 \\\\pm 9.1) \\\\times 10^3 \\\\ {\\\\rm mm}^2$.\\nA full error analysis has to include the treatment of the covariance terms---if only to show that they can be ignored.\\nWhy should the $x$ and $y$ in Eq.~\\\\ref{eq:COE1} be correlated? \\nFor direct measurements very often (but not always) they will not be.\\nHowever the interpretation of results is generally a multistage process. \\nFrom raw numbers of events one computes branching ratios (or cross sections...), from which one computes matrix elements (or particle masses...). Many quantities of interest to theorists are expressed as ratios of experimental numbers. \\nAnd in this interpretation there is plenty of scope for correlations to creep into the analysis.\\nFor example, an experiment might measure a cross section $\\\\sigma(pp \\\\to X) $ from a number of observed events $N$ in the decay channel $X \\\\to \\\\mu^+\\\\mu^-$. One would \\nuse a formula\\n$$\\\\sigma={N \\\\over B \\\\eta {\\\\cal L}} \\\\ ,$$\\nwhere $\\\\eta$ is the efficiency for detecting and reconstructing \\nan event, $B$ is the branching ratio for $X \\\\to \\\\mu^+\\\\mu^-$, and ${\\\\cal L}$ is the integrated luminosity.\\nThese will all have errors, and the above prescription can be applied.\\nHowever it might also use the $X \\\\to e^+e^-$ channel and then use\\n$$\\\\sigma'={N' \\\\over B' \\\\eta' {\\\\cal L}} \\\\ .$$\\nNow $\\\\sigma$ and $\\\\sigma'$ are clearly correlated; even though $N$ and $N'$ are \\nindependent, the same ${\\\\cal L}$ appears in both. If the estimate of ${\\\\cal L}$ is on the high side, that will push both $\\\\sigma$ and $\\\\sigma'$ downwards, and vice versa. \\nOn the other hand, if a second experiment did the same measurement it would have its own $N$, $\\\\eta$ and ${\\\\cal L}$, but would be correlated with the first\\nthrough using the same branching ratio (taken, presumably, from the Particle Data Group).\\nTo calculate correlations between results we need the equivalent of Eq.~\\\\ref{eq:COE0}\\n\\\\begin{align}\\n{\\\\rm Cov}_{fg} &= \\\\notag \\\\left< (f-\\\\langle f \\\\rangle )(g-\\\\langle g \\\\rangle ) \\\\right> \\\\\\\\\\n&=\\\\left({\\\\partial f \\\\over \\\\partial x} \\\\right) \\\\left( { \\\\partial g \\\\over \\\\partial x} \\\\right) \\\\sigma_x^2\\n\\\\quad,\\n\\\\end{align} \\nThis can all be combined in the general formula which encapsulates all of the ones above\\n\\\\begin{equation}\\n{\\\\bf V_f} = {\\\\bf G V_x \\\\tilde G}\\n\\\\quad,\\n\\\\end{equation}\\nwhere ${\\\\bf V_x}$ is the covariance matrix of the primary quantities (often, as pointed out earlier, this is diagonal),\\n${\\\\bf V_f}$ is the covariance matrix of secondary quantities, and \\n\\\\begin{equation}\\nG_{ij}={\\\\partial f_i \\\\over \\\\partial x_j}\\n\\\\quad.\\n\\\\end{equation}\\nThe {\\\\bf G} matrix is rectangular but need not be square. \\nThere may be more---or fewer---derived quantities than primary quantities.\\nThe matrix algebra of ${\\\\bf G}$ and its transpose ${\\\\bf \\\\tilde G}$ \\nensures that the numbers of rows and columns match for Eq.~\\\\ref{eq:COE4}.\\nTo show how this works, we go back to our earlier example of a cylinder.\\n$v$ and $a$ are correlated: if $r$ or $h$ fluctuate upwards (or downwards), that makes both volume and area larger\\n(or smaller). The matrix ${\\\\bf G}$ is\\n\\\\begin{equation}\\n{\\\\bf G}=\\\\left( \\n\\\\begin{matrix}\\n2 \\\\pi r h & \\\\pi r^2 \\\\\\\\\\n2 \\\\pi (2 r + h) &\\n2 \\\\pi r\\n\\\\end{matrix}\\n\\\\right)\\n=\\\\left( \\n\\\\begin{matrix}\\n352411 & 47529 \\\\\\\\\\n4411 & 773\\n\\\\end{matrix}\\n\\\\right)\\n\\\\quad,\\n\\\\end{equation} \\n\\\\noindent the variance matrix $V_x$ is\\n\\\\begin{equation*}\\n{\\\\bf V_x}=\\\\left( \\n\\\\begin{matrix}\\n4 & 0\\\\\\\\\\n0& 9\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\n\\\\noindent and Eq.~\\\\ref{eq:COE4} gives\\n\\\\begin{equation*}\\n{\\\\bf V_f}=\\\\left( \\n\\\\begin{matrix}\\n517.1 \\\\times 10^9 & 6.548 \\\\times 10^9\\\\\\\\\\n6.548 \\\\times 10^9 & 83.20 \\\\times 10^6\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\nfrom which one obtains, as before,\\n$\\\\sigma_v= 719101, \\\\sigma_a=9121$ but also $\\\\rho=0.9983$.\\nThis can be used to provide a useful example of why correlation matters. Suppose\\nyou want to know the volume to surface ratio, $z=v/a$, of this cylinder. \\nDivision gives $z=21673295/447470=48.4352$ mm.\\nIf we just use Eq.~\\\\ref{eq:COE2} for the error, this gives $\\\\sigma_z=1.89$ mm. \\nIncluding the correlation term, as in Eq.~\\\\ref{eq:COE2a}, reduces this to\\n$0.62$ mm---three times smaller. It makes a big difference.\\nWe can also check that this is correct, because the ration ${v \\\\over a}$ can be written as\\n$\\\\pi r^2 h \\\\over 2 \\\\pi r^2 + 2 \\\\pi r h$, and applying the uncorrelated errors of the original $r$ and $h$ to this also gives\\nan error of $0.62$ mm.\\nAs a second, hopefully helpful, example we consider a simple straight line fit, $y=mx+c$.\\nAssuming that all the $N$ $y$ values are measured with the same error $\\\\sigma$,\\nleast squares estimation gives the well known results\\n\\\\begin{equation}\\nm={\\\\overline{xy} - \\\\overline x \\\\, \\\\overline y \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\qquad\\nc={\\\\overline{y}\\\\, \\\\overline{x^2} - \\\\overline {xy} \\\\, \\\\overline x \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\quad.\\n\\\\end{equation}\\nFor simplicity we write $D=1/(\\\\overline{x^2}-\\\\overline x^2)$. The differentials are\\n\\\\begin{equation*}\\n{\\\\partial m \\\\over \\\\partial y_i}={D \\\\over N} (x_i-\\\\overline x) \\\\qquad\\n{\\\\partial c \\\\over \\\\partial y_i}={D \\\\over N} (\\\\overline{x^2}- x_i\\\\overline x)\\n\\\\quad,\\n\\\\end{equation*}\\n\\\\noindent from which, remembering that the $y$ values are uncorrelated,\\n\\\\begin{align*}\\nV_m=\\\\sigma^2\\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)^2=\\\\sigma^2 {D \\\\over N}\\n\\\\\\\\\\nV_c=\\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (\\\\overline{x^2}- x_i\\\\overline x)^2 =\\\\sigma^2 \\\\overline {x^2} {D \\\\over N}\\\\\\\\\\n{\\\\rm Cov}_{mc}= \\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)(\\\\overline{x^2}-x_i \\\\overline x)=-\\\\sigma^2 \\\\overline x {D \\\\over N}\\n\\\\end{align*}\\n\\\\noindent from which the correlation between $m$ and $c$ is just $\\\\rho=- \\\\overline x / \\\\sqrt{\\\\overline{x^2}}$.\\nThis makes sense. Imagine you're fitting a straight line through a set of points with a range of positive $x$ values (so $\\\\overline x$ is positive). If the rightmost point happened to be a bit higher, that would push the slope $m$ up and the intercept $c$ down. Likewise if the leftmost point happened to be too high that would push the slope down and the intercept up. There is a negative correlation between the two fitted quantities.\\nDoes it matter? Sometimes. Not if you're just interested in the slope---or the constant. But suppose you intend to use them to find the expected value of $y$ at some extrapolated $x$. Equation~\\\\ref{eq:COE2a} gives\\n\\\\begin{equation*}\\ny=m x + c \\\\pm \\\\sqrt {x^2 \\\\sigma_m^2 + \\\\sigma_c^2 + 2 x \\\\rho \\\\sigma_m \\\\sigma_c}\\n\\\\end{equation*}\\nand if, for a typical case where $\\\\overline x$ is positive so $\\\\rho$ is negative, you leave out the correlation term you will overestimate your error.\\nThis is an educational example because this correlation can be avoided. Shifting to a co-ordinate system in which\\n$\\\\overline x$ is zero ensures that the quantities are uncorrelated. This is \\nequivalent to rewriting the well-known $y=mx+c$ formula as $y=m(x-\\\\overline x)+c'$, where\\n$m$ is the same as before and $c'=c+m \\\\overline x$. $m$ and $c'$ are now uncorrelated, and \\nerror calculations involving them become a lot simpler. \\n\", \"\\\\section{Likelihood}\\nThe likelihood function is very widely used in many statistics applications. In this \\nSection, we consider it just for Parameter Determination. An important feature of the \\nlikelihood approach is that it can be used with {\\\\bf unbinned} data, and\\nhence can be applied in situations where there are not enough individual observations\\nto construct a histogram for the $\\\\chi^2$ approach. \\nWe start by assuming that we wish to fit our data $x$, using a model $f(x;\\\\mu)$ \\nwhich has one or more free parameters $\\\\mu$, whose value(s) we need to determine. \\nThe function $f$ is known as the `probability distribution' ($pdf$) and \\nspecifies the probability (or probability density, for the data having continuous as\\nopposed to discrete values) for obtaining different values of the data, when the parameter(s)\\nare specified. Without this \\nit is impossible to apply the likelihood (or many other) approaches. \\nFor example $x$ could be observations of a variable of interest within some \\nrange, and $f$ could be\\nany function such as a straight line, with gradient and intercept as parameters.\\nBut we will start with an angular distribution\\n\\\\begin{equation}\\ny(\\\\cos\\\\theta;\\\\beta) = \\\\frac{d\\\\ p}{d\\\\cos\\\\theta} = N(1+\\\\beta \\\\cos^2\\\\theta)\\n\\\\end{equation}\\nHere $\\\\theta$ is the angle at which a particle is observed, $dp/d\\\\cos\\\\theta$ is the $pdf$\\nspecifying the probability density for observing a decay at any $\\\\cos\\\\theta$, $\\\\beta$ is\\nthe parameter we want to determine, and $N$ is the crucial nomalisation factor \\nwhich ensures that the probability of observing a given decay at any $\\\\cos\\\\theta$\\nin the whole range from $-1$ to $+1$ is unity. In this case $N = 1/(2(1+\\\\beta/3))$. \\nThe data consists of $N$ decays, with their individual observations $\\\\cos\\\\theta_i$.\\nAssuming temporarily that the value of the parameter $\\\\beta$ is specified,\\nthe probability density $y_1$ of observing the first decay at $\\\\cos\\\\theta_1$ is\\n\\\\begin{equation}\\ny_1 = N (1+\\\\beta \\\\cos^2\\\\theta_1) = 0.5 (1+\\\\beta \\\\cos^2\\\\theta_1)/(1 + \\\\beta/3),\\n\\\\end{equation}\\nand similarly for the rest of the $N$ observations. Since the individual observations\\nare independent, the overall probability $P(\\\\beta)$ of observing the complete data set\\nof $N$ events is given by the product of the individual probabilities\\n\\\\begin{equation}\\nP(\\\\beta) = \\\\Pi y_i = \\\\Pi \\\\ 0.5 (1+\\\\beta \\\\cos^2\\\\theta_i)/(1 + \\\\beta/3) \\n\\\\end{equation}\\nWe imagine that this is computed for all values of the parameter $\\\\beta$; \\nthen this is known as the likelihood function ${\\\\it L}(\\\\beta)$.\\nThe likelihood method then takes as the estimate of $\\\\beta$ that value which \\nmaximises the likelihood. That is, it is the value which maximises (with respect to \\n$\\\\beta$) the probability density of observing the given data set. Conversely \\nwe rule out values of $\\\\beta$ for which ${\\\\it L}(\\\\beta)$ is very small. The\\nuncertainty on $\\\\beta$ is related to the width of the ${\\\\it L}(\\\\beta)$ \\ndistribution (see later). \\nIt is often convenient to consider the logarithm of the likelihood\\n\\\\begin{equation} \\n{\\\\it l} = \\\\ln{\\\\it L} = \\\\Sigma \\\\ln y_i\\n\\\\end{equation}\\nOne reason for this is that, for a large number of observations \\nsome fraction could have small $y_i$. Then the likelihood, involving the product of the\\n$y_i$, could be very small and may underflow the computer's range for real numbers.\\nIn contrast, {\\\\it l} involves a sum rather than a product, and $\\\\ln y_i$ rather than \\n$y_i$, and so produces a gentler number.\\n\\\\subsection{Likelihood and $pdf$}\\nThe procedure for constructing the likelihood is first to write down the $pdf$, and then to insert into that \\nexpression the observed data values in order to evaluate their product, which is the likelihood. Thus both \\nthe $pdf$ and the likelihood involve the data $x$ and the parameter(s) $\\\\mu$. The difference is that the $pdf$ is a function of $x$ for fixed values of $\\\\mu$, while the likelihood is a function of $\\\\mu$ given the fixed observed \\ndata $x_{obs}$. \\nThus for a Poisson distribution, the probability of observing $n$ events when the rate $\\\\mu$ is specified is \\n\\\\begin{equation}\\nP(n;\\\\mu) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $n$, while the likelihood is\\n\\\\begin{equation}\\nL(\\\\mu;n) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $\\\\mu$ for the fixed observed number $n$.\\n\\\\subsection{Intuitive example: Location and width of peak}\\nWe consider a \\nsituation where we are studying a resonant state which would result in a bump in the mass distribution of its decay particles.\\nWe assume that the bump can be parametrised as a simple Breit-Wigner\\n\\\\begin{equation}\\ny(m;M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nwhere $y$ is the probability density of obtaining a mass $m$ if the location and width the state are $M_0$ and $\\\\Gamma$,\\nthe parameters we want to determine. It is essential that $y$ is normalised, i.e. its integral over all physical values of \\n$m$ is unity; hence the normalisation factor of $\\\\Gamma/(2\\\\pi)$. The data consists of $n$ observations of $m$, as shown in fig. \\\\ref{fig:L_for_Resonance}.\\nAssume for the moment that we know $M_0$ and $\\\\Gamma$. Then the probability density for observing the $i^{th}$\\nevent with mass $m_i$ is\\n\\\\begin{equation}\\ny_i(M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nSince the events are independent, the probability density for observing the whole data sample is\\n\\\\begin{equation}\\ny_{all}(M_0,\\\\Gamma) =\\\\Pi \\\\ \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nand this is known as the likelihood $L(M_0,\\\\Gamma)$. Then the best values for the parameters are taken as\\nthe combination that maximises the probability density for the whole data sample i.e. $L(M_0,\\\\Gamma)$. \\nParameter values for which $L$ is very small compared to its maximum value are rejected, and the uncertainties \\non the parameters are related to the width of the distribution of $L$; we will be more specific later.\\nThe curve in\\nfig. \\\\ref{fig:L_for_Resonance}(left) shows the expected probability distribution for fixed parameter values. The way $L$ is calculated involves\\nmultiplying the heights of the curve at all the observed $m_i$ values. If we now consider varying $M_0$, this moves the curve bodily to the left or right without changing its shape or normalisation. So to determine the best value of $M_0$, we need to find where to locate the curve so that the product of the heights is a maximum; it is plausibe that the peak will be located where the majority of events are to be found.\\nNow we will consider how the optimum value of $\\\\Gamma$ is obtained. A small $\\\\Gamma$ results in a narrow curve, so the masses in the tail will make an even smaller contribution to the product in eqn. \\\\ref{product}, and hence reduce the likelihood. But a large $\\\\Gamma$ is not good, because not only is the width larger, but because of the normalisation condition, the peak height is reduced, and so the observations in the peak region make a smaller contribution to the likelihood. The optimal \\n$\\\\Gamma$ involves a trade-off between these two effects.\\nOf course, in finding the optimal of values of the two parameters, in general it is necessary to find the maximum of the \\nlikelihood as a function of the two parameters, rather than maximising with respect to just one, and then with respect to the other and then stopping (see section \\\\ref{More_variables}).\\n\\\\subsection{Uncertainty on parameter}\\nWith a large amount of data, the likelihood as a function of a parameter $\\\\mu$ is \\noften approximately Gaussian. In that case, ${\\\\it l}$ is an upturned parabola. Then\\nthe following definitions of $\\\\sigma_\\\\mu$, the uncertainty on $\\\\mu_{best}$, \\nyield identical answers:\\n\\\\begin{itemize}\\n\\\\item{The RMS of the likelihood distribution.}\\n\\\\item{[$-\\\\frac{d^2 {\\\\it l}}{d \\\\mu^2}]^{-1/2}$. If you remember that \\nthe second derivative of the log likelihood function is involved because it \\ncontrols the width of the ${\\\\it l}$ distribution, a mneumonic helps \\nyou remember the formula for $\\\\sigma_\\\\mu$: Since $\\\\sigma_\\\\mu$ must have the \\nsame units as $\\\\mu$, the second derivative must appear to the power $-1/2$. But because the\\nlog of the likelihood has a maximum, the second derivative is negative, so the minus \\nsign is necessary before we take the square root.}\\n\\\\item{It is the distance in $\\\\mu$ from the maximum in order to decrease ${\\\\it l}$ by half a unit\\nfrom its maximum value. i.e.\\n\\\\begin{equation}\\n{\\\\it l} (\\\\mu_{best} + \\\\sigma_{\\\\mu}) = {\\\\it l}_{max} - 0.5 \\n\\\\end{equation}\\n}\\n\\\\end{itemize} \\nIn situations where the likelihood is not Gaussian in shape, these three definitions no longer agree.\\nThe third one is most commonly used in that case. Now the upper and lower ends of the intervals can \\nbe asymmetric with respect to the central value. It is a mistake to believe that this method \\nprovides intervals which have a $68\\\\parameter\\\\footnote{Unfortunately, this incorrect statement occurs in my book\\\\cite{LL_book}. It is \\ncorrected in a separate update\\\\cite{LL_book_update}.}.\\nSymmetric uncertainties are easier to work with than asymmetric ones. It is thus sometimes better to quote the \\nuncertainty on a function of the first variable you think of. For example, for a charged particle in a magnetic field,\\nthe reciprocal of the momentum has a nearly symmetric uncertainty. Especially for high\\nmomentum tracks, the upper uncertainty on the momentum can be much larger than the lower one \\ne.g. $1.0\\\\ ^{+1.5}_{-0.4}$ TeV.\\n\\\\subsection{Coverage}\\nAn important feature of any statistical method for estimating a range for some parameter $\\\\mu$ at a \\nspecified confidence level $\\\\alpha$ is its coverage $C$. If the procedure is applied many times, \\nthese ranges will vary because of statistical fluctuations in the observed data. Then $C$ is defined as\\nthe fraction of ranges which contain the true value $\\\\mu_{true}$; it can vary with $\\\\mu_{true}$. \\nIt is very\\nimportant to realise that coverage is a property of the {\\\\bf statistical procedure} and does not apply\\nto your particular measurement. An ideal plot of coverage as a function of $\\\\mu$ would have $C$ constant \\nat its nominal value $\\\\alpha$. For a Poisson counting experiment, figure \\\\ref{fig:PoissonCoverage} shows $C$ as a \\nfunction of the Poisson parameter $\\\\mu$, when the observed number of counts $n$ is used to determine a range \\nfor $\\\\mu$ via the change in log-likelihood being 0.5. The coverage is far from constant at small $\\\\mu$.\\nIf $C$ is smaller than $\\\\alpha$, this is known as undercoverage. Certainly frequentists would regard this \\nas unfortunate; it means that people reading an article containing parameters determined this way are \\nlikely to place more than justified reliance on the quoted range. Methods using the Neyman construction \\nto determine parameter ranges by construction do not have undercoverage. \\nCoverage involves a statement about $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$. This is to be interpreted as a\\nprobability statement about how often the ranges $\\\\mu_l$ to $\\\\mu_u$ contain the (unknown but constant) true\\nvalue $\\\\mu_{true}$. This is a frequentist statement; Bayesians do not want to consider the ensemble of possible\\nresults if the measurement procedure were to be repeated. Thus Bayesians would regard the statement\\nabout $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$ as describing what fraction of their estimated \\nposterior probability density for $\\\\mu_{true}$ would be \\nbetween the fixed values $\\\\mu_l$ and $\\\\mu_u$, derived from their actual measurement.\\n\\\\subsection{More than one parameter}\\nFor the case of just one parameter $\\\\mu$, the likelihood best estimate $\\\\hat{\\\\mu}$ is given \\nby the value of $\\\\mu$ which maximises $L$. Its uncertainty $\\\\sigma_\\\\mu$ is determined either from \\n\\\\begin{equation}\\n1/\\\\sigma_\\\\mu^2 = -d^2\\\\ln L/d\\\\mu^2 ;\\n\\\\end{equation} \\nof by finding how far $\\\\hat{\\\\mu}$ would have to be changed in order to reduce $\\\\ln L$ by 0.5.\\nWhen we have two or more parameters $\\\\beta_i$ the rule for finding the best estimates $\\\\hat{\\\\beta_i}$\\nis still to maximise $L$.\\nFor the uncertainties and their correlations, the generalisation of equation \\\\ref{error} is to construct\\nthe inverse covariance matrix ${\\\\bf M}$, whose elements are given by\\n\\\\begin{equation}\\nM_{ij} = -\\\\frac{\\\\partial^2 \\\\ln L} {\\\\partial \\\\beta_i\\\\ \\\\partial \\\\beta_j} \\n\\\\end{equation} \\nThen the inverse of $\\\\bf{M}$ is the covariance matrix, whose diagonal elements are the variances of $\\\\beta_i$,\\nand whose off-diagonal ones are the covariances.\\nAlternatively (and more common in practice), the uncertainty on a specific $\\\\beta_j$ can be obtained \\nby using the profile likelihood $L_{prof}(\\\\beta_j)$.\\nThis is the likelihood as a function of the specific $\\\\beta_j$, where for each value of $\\\\beta_j,\\\\ L$ has been remaximised\\nwith respect to all the other $\\\\beta$. Then $L_{prof}(\\\\beta_j)$ is used with the `reduce $\\\\ln L_{prof}$ = 0.5' rule\\nto obtain the uncertainty on $\\\\beta_j$. This is equivalent to determining the contour in $\\\\beta$-space where \\n$\\\\ln L = \\\\ln L_{max} - 0.5$, and finding the values $\\\\beta_{j,1}$ and $\\\\beta_{j,2}$ on the contour which are \\nfurthest from $\\\\hat{\\\\beta_j.}$ Then the (probably asymmetric) upper and lower uncertainties on $\\\\beta_j$ are \\ngiven by $\\\\beta_{j,2}- \\\\hat{\\\\beta_j}$ and $\\\\hat{\\\\beta_j} - \\\\beta_{j,1}$ respectively.\\nBecause these are likelihood methods of obtaining the intervals, these estimates of uncertainities provide only\\n{\\\\bf nominal} regions of 68\\\\the region within \\nthe contour described in the previous paragraph for the multidimensional $\\\\beta$ space will have less than 68\\\\overage. To achieve that, the $`0.5'$ in the rule for how much $\\\\ln L$ has to be reduced from its maximum \\nmust be replaced by a larger number, whose value depends on the dimensionality of $\\\\beta$.\\n\", \"\\\\section{Gaussian distribution}\\nThe Gaussian or normal distribution (shown in Fig.~\\\\ref{fig:Gaussians}) is of widespread usage in data analysis. Under suitable conditions, in\\na repeated series of measurements $x$ with accuracy $\\\\sigma$ when the true value of the quantity\\nis $\\\\mu$, the distribution of $x$ is given by a Gaussian\\\\footnote{However, it is often the case \\nthat such a distribution has heavier tails than the Gaussian.}. A mathematical motivation is given by the \\nCentral Limit Theorem, which states that the sum of\\na large number of variables with (almost) any distributions is approximately Gaussian. \\nFor the Gaussian, the probability density $y(x)$ of an observation $x$ is given by\\n\\\\begin{equation}\\ny(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi} \\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\n\\\\end{equation}\\nwhere the parameters $\\\\mu$ and $\\\\sigma$ are respectively the centre and width of the distribution.\\nThe factor $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$ is required to normalise the area under the curve, so that $y(x)$\\ncan be directly interpreted as a probability density.\\nThere are several properties of $\\\\sigma$:\\n\\\\begin{itemize}\\n\\\\item{The mean value of $x$ is $\\\\mu$, and the standard deviation of its distribution is $\\\\sigma$. Since the usual symbol for \\nstandard deviation is $\\\\sigma$, this leads to the formula $\\\\sigma = \\\\sigma$ (which is not so trivial as it seems, since the \\ntwo $\\\\sigma$s have different meanings). This explains the curious factor of 2 in the denominator of the exponential,\\nsince without it, the two types of $\\\\sigma$ would not be equal.}\\n\\\\item{The value of $y$ at the $\\\\mu \\\\pm \\\\sigma$ is equal to the peak height multiplied by $e^{-0.5}$ = 0.61.\\nIf we are prepared to overlook the difference between 0.61 and 0.5, $\\\\sigma$ is the half-width of the distribution at\\n`half' the peak height.}\\n\\\\item{The fractional area in the range $x = \\\\mu - \\\\sigma$ to $\\\\mu + \\\\sigma$ is 0.68. Thus for a series of unbiassed, independent Gaussian \\ndistributed measurements}, about 2/3 are expected to lie within $\\\\sigma$ of the true value.\\n\\\\item{The peak height of $y$ at $x=\\\\mu$ is $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$. It is reasonable that this is proportional to \\n$1/\\\\sigma$ as the width is proportional to $\\\\sigma$, so $\\\\sigma$ cancels out in the product\\nof the height and width, as is \\nrequired for a distribution normalised to unity.}\\n\\\\end{itemize}\\nFor deciding whether an experimental measurement is consistent with a theory, more useful than the Gaussian distribution \\nitself is its tail area beyond $r$, a number of standard deviations from the central value (see Fig.~\\\\ref{fig:Gauss_tail}). \\nThis gives the probability of obtaining a result as extreme as ours or more so as a consequence of statistical fluctuations, \\nassuming that the theory is correct (and that our measurement is unbiassed, it is Gaussian distributed, etc.). If this \\nprobability is small, the measurement and the theory may be inconsistent. \\nFigure~\\\\ref{fig:Gauss_tail} has two different vertical scales, the left one for the probability of a fluctuation in a specific \\ndirection, and the right side for a fluctuation in either direction. Which to use depends on the particular\\nsituation. For example if we were performing a neutrino oscillation disappearance experiment, we would be looking for \\na reduction in the number of events as compared with the no-oscillation scenario, and hence would be interested in \\njust the single-sided tail. In contrast searching for any deviation from the Standard Model expectation, maybe the two-sided tails would be more relevant. \\n\", \"\\\\section{Combining experiments} \\nSometimes different experiments will measure the same physical quantity. It is then reasonable to ask what is our\\nbest information available when these experiments are combined. It is a general rule that it is better to use the\\n{\\\\bf DATA} for the experiments and then perform a combined analysis, rather than simply combine the {\\\\bf RESULTS}.\\nHowever, combining the results is a simpler procedure, and access to the original data is not always possible. \\nFor a series of unbiassed, uncorrelated measurements $x_i$ of the same physical quantity,\\nthe combined value $\\\\hat{x} \\\\pm \\\\hat{\\\\sigma}$ is given by weighting each measurement by $w_i$,\\nwhich is proportional to the inverse of the square of its uncertainty i.e. \\n\\\\begin{equation}\\n\\\\hat{x} = \\\\Sigma w_i x_i, \\\\ \\\\ \\\\ \\\\ w_i =(1/\\\\sigma_i^2)/\\\\Sigma (1/\\\\sigma_j^2) \\\\end{equation}\\nwith the uncertainty $\\\\hat{\\\\sigma}$ on the combined value being given by\\n\\\\begin{equation}\\n1/\\\\hat{\\\\sigma}^2 = \\\\Sigma 1/\\\\sigma_i^2\\n\\\\end{equation}\\nThis ensures that the uncertainty on the combination is at least as small as the \\nsmallest uncertainty of the individual measurements. It should be remembered that the combined uncertainty takes no \\naccount of whether or not the individual measurements are consistent with each other.\\nIn an informal sense, $1/\\\\sigma_i^2$ is the information content of a measurement. Then each $x_i$ is weighted\\nproportionally to its information content. Also the equation for\\n$\\\\hat{\\\\sigma^2}$ says that the information content of the combination is the sum of the information contents\\nof the individual measurements.\\nAn example demonstrates that care is needed in applying the formulae. Consider counting the number of high\\nenergy cosmic rays being recorded by a large counter system for two consecutive one-week periods, with the number of counts being\\n$100 \\\\pm 10$ and $1 \\\\pm 1$ \\\\footnote{It is vital to be aware that it is a crime (punishable by a forcible transfer\\nto doing a doctorate on Astrology) to combine such discrepant measurements. It seems likely that someone turned off\\nthe detector between the two runs; or there was a large background in the first measurement which was eliminated\\nfor the second; etc. The only reason for my using such discrepant numbers is to produce a dramatically stupid\\nresult. The effect would have been present with measurements like $100 \\\\pm 10$ and $81 \\\\pm 9$.}.\\n(See section \\\\ref{Poisson} for the choice of uncertainties). Unthinking application of the formulae\\nfor the combined result give the ridiculous $2 \\\\pm 1$. What has gone wrong?\\nThe answer is that we are supposed to use the {\\\\bf true} accuracies of the individual measurements to assign the weights. \\nHere we have used the {\\\\bf estimated} accuracies. Because the estimated uncertainty \\ndepends on the estimated rate, a downward fluctuation in the measurement results in an underestimated uncertainty,\\nan overestimated weight, and a downward bias in the combination. In our example, the combination should assume \\nthat the true rate was the same in the two measurements which used the same detector and which \\nlasted the same time as each other, and hence their\\ntrue accuracies are (unknown but) equal. So the two measurements should each be given a weight of 0.5, which \\nyields the sensible combined result of $50.5 \\\\pm 5$ counts. \\n\\\\subsection{\\\\bf BLUE}\\nA method of combining correlated results is the `{\\\\bf B}est {\\\\bf L}inear {\\\\bf U}nbiassed {\\\\bf E}stimate' ({\\\\bf BLUE}). \\nWe look for the best linear unbiassed combination\\n\\\\begin{equation}\\nx_{BLUE} = \\\\Sigma w_i x_i,\\n\\\\end{equation} \\nwhere the weights are chosen to give the smallest uncertainty $\\\\sigma_{BLUE}$ on \\n$x_{BLUE}$. Also for the combination to be unbiassed, the weights must add up to unity.\\nThey are thus determined by minimising $\\\\Sigma\\\\Sigma w_i w_j E^{-1}_{ij}$, subject to the constraint\\n$\\\\Sigma w_i = 1$; here $E$ is the covariance matrix for the correlated measurements. \\nThe $BLUE$ procedure just described is equivalent to the $\\\\chi^2$ approach for checking whether\\na correlated set of measurements are consistent with a common value. The advantage of $BLUE$ is that \\nit provides the weights for each measurement in the combination. It thus enables us to calculate the contribution \\nof various sources of uncertainty in the individual measurements to the uncertainty on the combined result.\\n\\\\subsection{Why weighted averaging can be better than simple averaging}\\nConsider a remote island whose inhabitants are very conservative, and no-one leaves or arrives \\nexcept for some anthropologists who wish to determine the number of married people there.\\nBecause the islanders are very traditional, it is necessary to send two teams of anthropologists,\\none consisting of males to interview the men, and the other of females for the women. There are too\\nmany islanders to interview them all, so each team interviews a sample and then extrapolates. \\nThe first team estimates the number of married men as $10,000 \\\\pm 300$. The second, who \\nunfortunately have less funding and so can interview only a smaller sample, have a \\nlarger statistical uncertainty; they estimate $9,000 \\\\pm 900$ married women. Then how many \\nmarried people are there on the island? \\nThe simple approach is to add the numbers of married men and women, to give $19,000 \\\\pm 950$\\nmarried people. But if we use some theoretical input, maybe we can improve the accuracy of \\nour estimate. So if we assume that the islanders are monogamous, the numbers of married men and \\nwomen should be equal, as they are both estimates of the number of married couples. The weighted \\naverage is $9,900 \\\\pm 285$ married couples and hence $19,800 \\\\pm 570$ married people.\\nThe contrast in these results is not so much the difference in the estimates, but that \\nincorporating the assumption of monogamy and hence using the weighted average gives a smaller \\nuncertainty on the answer. Of course, if our assumption is incorrect, this answer will be biassed.\\nA Particle Physics example incorporating the same idea of theoretical input reducing the \\nuncertainty of a measurement can be found in the `Kinematic Fitting' section of Lecture 2.\\n\", \"\\\\section{Outline}\\nThis series of five lectures deals with practical aspects of statistical issues that arise in typical \\nHigh Energy Physics analyses. The topics are:\\n\\\\begin{itemize}\\n\\\\item{Introduction. This is largely a reminder of topics which you should have encountered as \\nundergraduates. Some of them are looked at in novel ways, and will hopefully provide new insights.}\\n\\\\item{Least Squares and Likelihoods. We deal with two different methods for parameter determination.\\nLeast Squares is also useful for Goodness of Fit testing, while likelihood ratios play a crucial role in\\nchoosing between two hypotheses.}\\n\\\\item{Bayes and Frequentism. These are two fundamental and very different approaches to statistical\\nsearches. They disagree even in their views on `What is probability?'}\\n\\\\item{Searches for New Physics. Many statistical issues arise in searches for New Physics. These may \\nresult in discovery claims, or alternatively in exclusion of theoretical models in some region of their\\nparameter space (e.g. mass ranges).} \\n\\\\item{Learning to love the covariance matrix. This is relevant for dealing with the possible correlations\\nbetween uncertainties on two or more quantities. The covariance matrix takes care of all these\\ncorrelations, so that you do not have to worry about each situation separately.\\nThis was an unscheduled lecture which was included at the request of several students.}\\n\\\\end{itemize}\\nLectures 3 to 5 are not included in these proceedings but can be found elsewhere~\\\\cite{Lecture3,Lecture4,Lecture5}. \\nThe material in these lectures follows loosely that in my book \\\\cite{LL_book}, together with some \\nsignificant updates (see ref. \\\\cite{LL_book_update}). \\n\", \"\\\\section{Probability distributions and their properties} \\nWe have to make a simple distinction between two sorts of data: \\\\emph{integer} data and \\\\emph{real-number} data\\\\footnote{Other branches of science have to include a third, \\\\emph{categorical} data, but we will ignore that.}.\\nThe first covers results which are of their nature whole numbers: the numbers of kaons produced in \\na collision, or the number of entries falling into some bin of a histogram.\\nGenerically let's call such numbers $r$. They have probabilities $P(r)$ which are dimensionless.\\nThe second covers results whose values are real (or floating-point) numbers. There are lots of these:\\nenergies, angles, invariant masses $\\\\dots$\\nGenerically let's call such numbers $x$, and they have probability density functions $P(x)$\\nwhich have \\ndimensions of $[x]^{-1}$, so $\\\\int_{x_1}^{x_2} P(x) dx$ or $P(x)\\\\, dx$ are probabilities.\\nYou will also sometimes meet the cumulative distribution $C(x)=\\\\int_{-\\\\infty}^x P(x') \\\\, dx'$.\\n\\\\subsection{Expectation values}\\nFrom $P(r)$ or $P(x)$ one can form the expectation value\\n\\\\begin{equation}\\n\\\\langle f \\\\rangle =\\\\sum_r f(r) P(r) \\\\qquad {\\\\rm or} \\\\qquad \\\\langle f \\\\rangle = \\\\int f(x) P(x) \\\\, dx \\n\\\\quad,\\n\\\\end{equation}\\nwhere the sum or integral is taken as appropriate.\\nSome authors write this as\\n$E(f)$, but I personally prefer the angle-bracket notation. You may think it looks too much like quantum mechanics,\\nbut in fact it's quantum mechanics which looks like statistics: an expression like $\\\\langle \\\\psi | \\\\hat Q | \\\\psi \\\\rangle$\\nis the average value of an operator $\\\\hat Q$ in some state $\\\\psi$, where `average value' has exactly the same \\nmeaning and significance. \\n\\\\subsubsection{Mean and standard deviation} \\nIn particular the {\\\\em mean}, often written $\\\\mu$, is given by \\n$ \\\\langle r \\\\rangle = \\\\sum_r r P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle x \\\\rangle =\\\\int x P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent Similarly one can write \\nhigher {\\\\em moments} \\n$\\\\mu_k = \\\\langle r^k \\\\rangle = \\\\sum_r r^k P(r)\\\\qquad {\\\\rm or } \\\\qquad \\\\langle x^k \\\\rangle =\\\\int x^k P(x) \\\\, dx \\\\quad,$ \\n\\\\noindent and {\\\\em central moments} \\n$\\\\mu'_k = \\\\langle (r-\\\\mu)^k \\\\rangle = \\\\sum_r (r-\\\\mu)^k P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle (x-\\\\mu)^k \\\\rangle =\\\\int (x-\\\\mu)^k P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent The second central moment is known as the \\n{\\\\em variance} \\n$\\\\mu'_2=V= \\\\sum_r (r-\\\\mu)^2 P(r) = \\\\langle r^2 \\\\rangle - \\\\langle r \\\\rangle ^2$\\n\\\\qquad \\nor \\\\qquad $ \\\\int (x-\\\\mu)^2 P(x) \\\\, dx = \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2$\\n\\\\noindent It is easy to show that $\\\\langle (x-\\\\mu)^2 \\\\rangle =\\\\langle x^2 \\\\rangle -\\\\mu^2$. The {\\\\em standard deviation} is just the square root of the variance $\\\\sigma=\\\\sqrt{V}$.\\nStatisticians usually use variance, perhaps because formulae come out simpler. Physicists usually use standard deviation,\\nperhaps because it has the same dimensions as the variable being studied, and can be drawn as an error bar on a plot. \\nYou may also meet {\\\\em skew}, which is $\\\\gamma=\\\\langle (x-\\\\mu)^3 \\\\rangle /\\\\sigma^3$ and {\\\\em kurtosis}, $h=\\\\langle (x-\\\\mu)^4 \\\\rangle /\\\\sigma^4 -3$.\\nDefinitions vary, so be careful. Skew is a dimensionless measure of the asymmetry of a distribution. Kurtosis is\\n(thanks to that rather arbitrary looking 3 in the definition) \\nzero for a Gaussian distribution (see Section~\\\\ref{sec:measurement}): positive kurtosis indicates a\\nnarrow core with a wide tail, negative kurtosis indicates the tails are reduced. \\n\\\\subsubsection{Covariance and correlation}\\nIf your data are \\n2-dimensional pairs $(x,y)$,\\nthen besides forming $\\\\langle x \\\\rangle, \\\\langle y \\\\rangle, \\\\sigma_x$ etc., you can also form the \\n{\\\\em Covariance}\\n${\\\\rm Cov}(x,y)=\\\\langle (x-\\\\mu_x)(y-\\\\mu_y) \\\\rangle = \\\\langle xy \\\\rangle - \\\\langle x \\\\rangle \\\\langle y \\\\rangle \\\\quad.$\\nExamples are shown in Fig.~\\\\ref{fig:covariance}. If there is a tendency for positive fluctuations in $x$ to be associated with positive fluctuations in $y$ (and therefore negative with negative) then\\nthe product $(x_i-\\\\overline x)(y_i-\\\\overline y)$ tends to be positive and the covariance is greater than 0. A negative covariance, as in the 3rd plot, happens if a positive fluctuation in one variable is associated with a negative fluctuation in the other.\\nIf the variables are independent then a positive variation in $x$ is equally likely to be associated with a positive or a negative variation in $y$ and the covariance is zero, as in the first plot. However the converse is not always the case, there can be two-dimensional distributions where the covariance is zero, but the two variables are not independent, as is shown in the fourth plot.\\nCovariance is useful, but it has dimensions. Often one uses the \\n{\\\\em correlation}, which is just\\n\\\\begin{equation}\\n\\\\rho={{\\\\rm Cov}(x,y)\\\\over \\\\sigma_x \\\\sigma_y}\\n\\\\quad.\\n\\\\end{equation}\\nIt is easy to show that\\n$\\\\rho$ lies between 1 (complete correlation) and -1 (complete anticorrelation). \\n$\\\\rho=0$ if $x$ and $y$ are independent.\\nIf there are more than two variables---the alphabet runs out so let's call them \\n$(x_1,x_2,x_3\\\\dots x_n)$---\\nthen these generalise to the \\n{\\\\em covariance matrix}\\n${\\\\bf V}_{ij}=\\\\langle x_i x_j \\\\rangle - \\\\langle x_i \\\\rangle \\\\langle x_j \\\\rangle$ \\n\\\\noindent and the {\\\\em \\ncorrelation matrix}\\n${\\\\bf \\\\rho}_{ij}={{\\\\bf V}_{ij} \\\\over \\\\sigma_i \\\\sigma_j} \\\\quad.$\\n\\\\noindent The diagonal of $\\\\bf V$ is $\\\\sigma_i^2$. \\nThe\\ndiagonal of $\\\\bf \\\\rho$ is 1.\\n\\\\subsection{Binomial, Poisson and Gaussian}\\nWe now move from considering the general properties of distributions to considering three specific ones.\\nThese are the ones you will most commonly meet for the distribution of the original data\\n(as opposed to quantities constructed from it). Actually the first, the binomial, is not nearly as common as the second, the Poisson; and the third, the Gaussian, is overwhelmingly more common. However it is \\nuseful to consider all three as concepts are built up from the simplest to the more sophisticated.\\n\\\\subsubsection {The binomial distribution }\\nThe binomial distribution is easy to understand as it basically describes the familiar\\\\footnote{Except, as it happens, in Vietnam, where coins have been completely replaced by banknotes.} tossing of coins. \\nIt describes the number $r$ of successes in $N$ trials, each with probability $p$ of success.\\n$r$ is discrete so the process is described by a probability distribution\\n\\\\begin{equation}\\nP(r;p,N)={N! \\\\over r! (N-r)!} p^r q^{N-r} \\n\\\\quad,\\n\\\\end{equation}\\nwhere $ q \\\\equiv 1-p$.\\nSome examples are shown in Fig.~\\\\ref{fig:binom}.\\nThe distribution has \\nmean $\\\\mu=Np$, variance $V=Npq$, and standard deviation $\\\\sigma=\\\\sqrt{Npq}$.\\n\\\\subsubsection {The Poisson distribution}\\nThe Poisson distribution also describes the probability of some discrete number $r$,\\nbut rather than a fixed number of `trials' it considers a random rate $\\\\lambda$: \\n\\\\begin{equation}\\nP(r;\\\\lambda)=e^{-\\\\lambda}{\\\\lambda^r \\\\ \\\\over r!}\\n\\\\quad.\\n\\\\end{equation}\\nIt is linked to the binomial---the Poisson is the \\nlimit of the binomial---as $N\\\\to \\\\infty$, $p \\\\to 0$ with $np=\\\\lambda=constant$. Figure~\\\\ref{fig:poisson} shows various examples. It has mean $\\\\mu=\\\\lambda$, variance $V=\\\\lambda$, and standard deviation $\\\\sigma=\\\\sqrt{\\\\lambda}=\\\\sqrt{\\\\mu}$.\\nThe clicks of a Geiger counter are the standard illustration of a Poisson process.\\nYou will meet it a lot as it applies to event counts---on their own or in histogram bins.\\nTo help you think about the Poisson, here is a simple question (which\\ndescribes a situation \\nI have seen in practice, more than once, from people who ought to know better).\\n\\\\\\n\\\\hrule\\n\\\\\\nYou need to know the efficiency of your PID system for positrons.\\nYou find 1000 data events where 2 tracks have a combined mass of 3.1~GeV ($J/\\\\psi)$ and the negative track is \\nidentified as an $e^-$ (`Tag-and-probe' technique).\\nIn 900 events the $e^+$ is also identified. In 100 events it is not. The efficiency is 90\\\\\\nWhat about the error?\\nColleague A says $\\\\sqrt{900}=30$ so efficiency is $90.0 \\\\pm 3.0 $\\\\\\ncolleague B says $\\\\sqrt{100}=10$ so efficiency is $90.0 \\\\pm 1.0 $\\\\\\nWhich is right?\\n\\\\\\n\\\\hrule\\n\\\\\\nPlease think about this before turning the page...\\n\\\\vfill\\\\eject\\n{Neither---both are wrong}. This is binomial not Poisson: $p=0.9, N=1000$.\\n\\\\noindent The error is $\\\\sqrt{Npq}=\\\\sqrt{1000 \\\\times 0.9 \\\\times 0.1}$ (or $\\\\sqrt{1000 \\\\times 0.1 \\\\times 0.9}$) =$\\\\sqrt{90} = 9.49$ so the efficiency is $90.0 \\\\pm 0.9$ \\\\\\n\\\\subsubsection{The Gaussian distribution}\\nThis is by far the most important statistical distribution.\\nThe probability density function (PDF) for a variable $x$ is given by the formula\\n\\\\begin{equation} \\nP(x;\\\\mu,\\\\sigma)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-{(x-\\\\mu)^2 \\\\over 2 \\\\sigma^2}}\\n\\\\quad.\\n\\\\end{equation}\\nPictorially this is shown in Fig.~\\\\ref{fig:gauss}.\\nThis is sometimes called the `bell curve', though in fact a real bell does not have flared edges like that.\\nThere is (in contrast to the Poisson and binomial) \\nonly one \\nGaussian curve, as $\\\\mu$ and $\\\\sigma$ are just location and scale parameters.\\nThe mean is $\\\\mu$ and the standard deviation is $\\\\sigma$. The \\nSkew is zero, as it is symmetric, and the kurtosis is zero by construction.\\nIn statistics, and most disciplines, this is known as the {\\\\em normal distribution}. Only in physics is it known as `The Gaussian'---perhaps because the word `normal' already has so many meanings. \\nThe reason for the importance of the Gaussian is the {\\\\em central limit theorem} (CLT) that states:\\nif the variable $X$ is the sum of $N$ variables $x_1,x_2\\\\dots x_N$ then:\\n\\\\begin{enumerate}\\n\\\\item Means add: $ \\\\langle X \\\\rangle = \\\\langle x_1 \\\\rangle + \\\\langle x_2 \\\\rangle + \\\\dots \\\\langle x_N \\\\rangle$, \\n\\\\item Variances add: $V_X=V_1+V_2 +\\\\dots V_N$,\\n\\\\item If the variables $x_i$ are independent and identically distributed (i.i.d.) then $P(X)$ tends to a Gaussian for large $N$.\\n\\\\end{enumerate}\\n(1) is obvious, (2) is pretty obvious, and means that standard deviations add in quadrature, and that the standard deviation of an average falls like $1\\\\over \\\\sqrt N$, (3) applies whatever the form of the original $P(x)$.\\nBefore proving this, it is helpful to see a demonstration to convince yourself that the implausible assertion in (3)\\nactually does happen.\\nTake a uniform distribution from 0 to 1, as shown in the top left subplot of Fig.~\\\\ref{fig:CLT}. It is flat. Add two such numbers and the distribution is triangular, between 0 and 2, as shown in the top right.\\nWith 3 numbers, at the bottom left, it gets curved. With 10 numbers, at the bottom right, it looks pretty Gaussian. The proof follows. \\n\\\\begin{proof}\\nFirst, introduce the characteristic function $\\\\langle e^{i k x} \\\\rangle = \\\\int e^{i k x } P(x) \\\\, dx = \\\\tilde P(k)$.\\nThis can usefully be thought of as an expectation value and as a Fourier transform, FT.\\nExpand the exponential as a series\\n$\\\\langle e^{i k x} \\\\rangle = \\\\langle 1+ikx+{(ikx)^2 \\\\over 2!}+{(ikx)^3 \\\\over 3!}\\\\dots \\\\rangle = 1 + ik \\\\langle x \\\\rangle +(ik)^2{\\\\langle x^2 \\\\rangle \\\\over 2!} + (ik^3) {\\\\langle x^3 \\\\rangle \\\\over 3!} \\\\dots$. \\nTake the logarithm and use the expansion $\\\\ln(1+z)=z-{z^2 \\\\over 2 } + {z^3 \\\\over 3} \\\\dots$\\nThis gives a power series in $(ik)$, where the coefficient ${\\\\kappa_r \\\\over r!}$ of $(ik)^r$ is made up of expectation values of $x$ of total power $r$\\n$\\\\kappa_1= \\\\langle x \\\\rangle, \\\\kappa_2= \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2 =, \\\\kappa_3= \\\\langle x^3 \\\\rangle -3 \\\\langle x^2 \\\\rangle \\\\langle x \\\\rangle +2 \\\\langle x \\\\rangle^3 \\\\dots$ \\nThese are called the semi-invariant cumulants of Thi\\\\`ele . Under a change of scale $\\\\alpha$, $\\\\kappa_r \\\\to \\\\alpha^r \\\\kappa_r$. Under a change in location only $\\\\kappa_1$ changes.\\nIf $X$ is the sum of i.i.d. random variables, $x_1+x_2+x_3...$, then $P(X)$ is the convolution of $P(x)$ with itself $N$ times.\\nThe FT of a convolution is the product of the individual FTs,\\nthe logarithm of a product is the sum of the logarithms,\\nso $P(X)$ has cumulants $K_r=N \\\\kappa_r$.\\nTo make graphs commensurate, you need to scale the $X$ axis by the\\nstandard deviation, which grows like $\\\\sqrt{N}$. The cumulants of the scaled graph are $K'_r = N^{1-r/2} \\\\kappa_r$. \\nAs $N \\\\to \\\\infty$, these vanish for $r>2$, leaving a quadratic.\\nIf the log is a quadratic, the exponential is a Gaussian. So $\\\\tilde P(X)$ is Gaussian.\\nAnd finally, the inverse FT of a Gaussian is also a Gaussian.\\n\\\\end{proof} \\nEven if the distributions are not identical, the CLT tends to apply, unless one (or two) dominates.\\nMost `errors' fit this, being compounded of many different sources.\\n\"}},\n",
       "       {'entity_name': 'order statistic', 'entity_type': 'statistics_concept', 'description': 'A statistic that represents the value of the k-th smallest observation in a sample, providing insight into the distribution of the data.', 'relevant_passages': {\"\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Descriptive Statistics}\\nSuppose we have a sample of $N$ data $X = x_1, x_2, \\\\cdots, x_N$. It is often useful to summarize these data with a few numbers called statistics. \\nA \\\\textbf{statistic} is any number that can be calculated from the data and known parameters. For example, $t = (x_1 + x_N)/2$ is a statistic, but if the value of $\\\\theta$ is unknown $t = (x_1 - \\\\theta)^2$ is not. However, a word of caution is in order: we particle physicists are prone to misuse the jargon of professional statisticians. For example, we tend to refer to \\\\emph{any} function of the data as a statistic including those that contain unknown parameters. \\nThe two most important statistics are\\n\\\\begin{align}\\n&\\\\textrm{the {\\\\bf sample mean} (or average)} & \\\\bar{x} & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i, \\\\\\\\\\n&\\\\textrm{and the {\\\\bf sample variance}} & s^2 & = \\\\frac{1}{N} \\\\sum_{i=1}^N (x_i - \\\\bar{x})^2,\\n\\\\nonumber\\\\\\\\\\n& & & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i^2 - \\\\bar{x}^2, \\\\nonumber\\\\\\\\\\n& & & = \\\\overline{x^2} - \\\\bar{x}^2.\\n\\\\end{align}\\nThe sample average is a measure of the center of the distribution of the data, while the sample variance is a measure of its spread. Statistics that merely characterize the data are\\ncalled \\\\textbf{descriptive statistics}, of which the sample average and variance are the most\\nimportant. If we order the data, say from the smallest value to the largest, we can compute another interesting statistic $t_k \\\\equiv x_{(k)}$, where $1 \\\\leq k \\\\leq N$ and $x_{(k)}$ denotes the datum at the $k^\\\\text{th}$ position. The statistic $t_k$ is called the $k^\\\\text{th}$ \\\\textbf{order statistic} and is a measure of the value of outlying data.\\nThe average and variance, Eqs.~(\\\\ref{eq:xbar}) and (\\\\ref{eq:xvar}), are numbers that can always be calculated given a data sample $X$. But now we consider numbers that cannot be calculated from the data alone. Imagine the repetition, infinitely many times, of whatever data generating system yielded our data sample $X$ thereby creating an infinite sequence of data sets. We shall refer to the data generating system as an experiment and the\\ninfinite sequence as an infinite ensemble. The latter, together with all the mathematical\\noperations we may wish to apply to it, are \\nabstractions. After all, it is not possible to realize an infinite ensemble. The ensemble and\\nall the operations on it exist in the same sense that the number $\\\\pi$ exists along with all valid\\nmathematical operations on $\\\\pi$.\\nThe most common operation to perform on an ensemble is to compute the average of the statistics. This \\\\textbf{ensemble average} suggests several potentially useful characteristics of the ensemble, which we list below.\\n\\\\begin{align}\\n& \\\\textrm{Ensemble average} & & <x> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean} & \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Error} & \\\\epsilon & = x - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Bias} & b & = <x> - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Variance} & V & = <(x - <x>)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Standard deviation} & \\\\sigma & = \\\\sqrt{V} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean square error} & \\\\text{MSE} & = <(x - \\\\mu)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Root MSE} & \\\\textrm{RMS} & = \\\\sqrt{\\\\textrm{MSE}}\\n\\\\end{align}\\nNotice that none of these numbers can be calculated in practice because the data\\nrequired to do so do not concretely exist. Even\\nin an experiment simulated on a computer, there are very few of these numbers we can\\ncalculate. If we know the mean $\\\\mu$, perhaps because we have chosen its value --- for example, we may have chosen the mass of the Higgs boson in our simulation, we can certainly calculate the error $\\\\epsilon$ for any simulated datum $x$. But, we can only \\\\emph{approximate} the\\nensemble average $< x >$, bias $b$, variance $V$, and MSE, since our virtual ensemble is\\nalways finite. The point is this: the numbers that characterize the infinite ensemble are also abstractions,\\nalbeit useful ones. For example, the MSE is the most widely used\\nmeasure of the closeness of an ensemble of numbers to some parameter $\\\\mu$. The square root of the MSE is called the root mean square (RMS)\\\\footnote{Sometimes, the RMS and standard deviation are using interchangeably. However, the RMS is computed with respect to $\\\\mu$, while the standard deviation is computed with respect to the ensemble average $<x>$. The RMS and standard deviations are identical only if the bias is zero.}. The MSE can be written as\\n\\\\begin{align}\\n\\\\textrm{MSE} & = V + b^2. \\\\\\\\ & \\\\framebox{\\\\textbf{Exercise 1: } \\\\textrm{Show this}}\\\\nonumber\\n\\\\end{align}\\nThe MSE is the sum of the variance and the square of the bias, \\na very important result with practical consequences. For example, suppose that $\\\\mu$ represents the mass of the Higgs boson and $x$ represents some (typically very complicated) statistic that is considered an \\\\textbf{estimator} of the mass. An estimator is any\\nfunction, which when data are entered into it, yields an \\\\textbf{estimate} of the quantity\\nof interest, which we may take to be a measurement. \\nWords are important; ``bias'' is a case in point. It is an unfortunate choice\\nfor the difference $<x> - \\\\mu$ because the word ``bias'' biases attitudes towards bias! Something that, or someone who, is biased is surely bad and needs to be corrected. Perhaps.\\nBut, it would be wasteful of data to make the bias zero if the net effect is to make the MSE larger than an MSE in which the bias is non-zero. The price for achieving $b = 0$ in our\\nexample would be not only throwing away expensive data --- which is bad enough --- but also measuring a mass that is more likely to be further away from the Higgs boson mass. This may, or may not, be what we want to achieve. \\nAs noted, many of the numbers listed in Eq.~(\\\\ref{eq:ensemble}) cannot be calculated because\\nthe information needed is unknown. This is\\ntrue, in particular, of the bias. However, sometimes it is possible to relate the bias to another ensemble quantity. Consider the ensemble average of the sample variance, Eq.~(\\\\ref{eq:xvar}),\\n\\\\begin{align}\\n<s^2> & = < \\\\overline{x^2} > - <\\\\bar{x}^2>, \\\\nonumber\\\\\\\\\\n& = V - \\\\frac{V}{N}, \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 2a:} Show this} \\\\nonumber \\n\\\\end{align}\\nThe sample variance has a bias of $b = - V/N$, which many argue should be\\ncorrected. Unfortunately, we cannot calculate the bias because it depends on an unknown parameter, namely, the variance $V$. \\nHowever, if we replace the sample variance by $s^{\\\\prime 2} = c s^2$,where the\\ncorrection factor $c = N/(N-1)$, we find that for the corrected variance estimator $s^{\\\\prime 2}$ the bias is zero. Surely the world is now a better place? Well, not necessarily. \\nConsider the ratio of $\\\\textrm{MSE}^\\\\prime$ to $\\\\textrm{MSE}$, where $\\\\textrm{MSE}^\\\\prime = <(s^{\\\\prime 2} - V)^2>$, $\\\\textrm{MSE} = <\\\\delta^2>$ with $\\\\delta = s^2 - V$, and\\n$b = -V / N$,\\n\\\\begin{align*}\\n\\\\textrm{MSE}^\\\\prime / \\\\textrm{MSE} \\n& = < (c s^2 - V)^2 > / < \\\\delta^2 > , \\\\\\\\\\n& = c^2 < (s^2 - V/c)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 < (\\\\delta - b)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 (1 - b^2 / <\\\\delta^2> ), \\\\\\\\\\n& = c^2 \\\\left[ 1 - b^2 / (b^2 + <s^4> - (V+b)^2) \\\\right].\\n\\\\end{align*} \\nFrom this we deduce that if $<s^4>/[ (V+b)^2 + b^2 / (c^2 - 1)] > 1$, the unbiased\\nestimate will be further away on average from $V$ than the biased estimate. This is the case,\\nfor example, for a uniform distribution.\\n\\\\centerline{ \\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 2b:} Use the method {Rndm()} of the {\\\\tt Root}\\\\\\\\ class {\\\\tt TRandom3} to verify that $\\\\textrm{MSE}^\\\\prime > \\\\textrm{MSE}$. }} \\n}\\n\"}},\n",
       "       {'entity_name': 'mean square error and root mean square error', 'entity_type': 'statistics_concept', 'description': 'A set of statistical measures that quantify the differences between estimated values and actual values. Mean Square Error (MSE) represents the average of the squares of these errors, while Root Mean Square Error (RMSE) is the square root of MSE, providing a measure of the magnitude of the errors. Both metrics are commonly used to assess the accuracy of predictions in statistical analyses and are particularly relevant in contexts such as model evaluation and parameter estimation.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Outlier detection}\\nThe above methods focus on detecting new physics as overdensities in very specific regions of the kinematic phase space; this paradigm is similar to a traditional bump hunt, often performed in HEP searches for novel particles. However, new physics signatures are equally likely to manifest themselves as unexpected events in the tail of distributions. This type of events may be identified using out-of-distribution detection algorithms. The prime example of such an algorithm is the auto-encoder\\\\cite{lecun1987phd, ballard1987modular, hinton1993autoencoders}, which is especially popular in high-energy physics applications\\\\cite{Radovic:2018dip, albertsson2019machine, Jawahar:2021vyu, tsan2021particle, Finke_2021, Laguarta:2023evo, Vaslin:2023lig, Anzalone:2023ugq, Bohm:2023ihd}.\\n\\\\subsubsection{Self-supervised methods}\\nSelf-supervised learning\\\\cite{balestriero2023cookbook} is a form of unsupervised learning where the data provides the supervision. In general, a part of the data is initially withheld from the model and the task of the network is to reproduce this data. Consequently, the network learns a meaningful representation of the data to solve this problem. The self-supervised learning workflow usually involves two stages: first, generating a set of supervisory signals from the input data; and second, employing these signals for a supervised~task. Self-supervised learning can be seen as a hybrid approach that lies somewhere between unsupervised and supervised learning. In high-energy physics, the most used type of self-supervised model is by far the auto-encoder.\\nThe standard Auto-Encoder (AE) model consists of two neural networks: the encoder and the decoder. The encoder maps the input data to a \\\\textit{latent space} of a lower dimensionality. For example, a particle that is represented by 64 features (transverse momentum, azimuthal angle, etc.) is reduced to a 16 feature representation. In contrast, the objective of the decoder is to reconstruct the input features from the latent space features. The ultimate goal of the AE training is to minimise the difference between the input and reconstructed data. This difference can be quantified by employing various loss functions. The Mean Squared Error (MSE) loss function is the most basic example of quantifying the input-output discrepancy:\\n\\\\begin{equation}\\nL_\\\\mathrm{MSE} = (x - f(z,\\\\theta))^2 \\n\\\\end{equation}\\nwhere $x$ is the input data, $z$ is the latent space data, and $\\\\theta$ are the weights of the decoder. This reconstruction loss is propagated through both the decoder and the encoder. Thus, the latent space and the reconstructed data evolve simultaneously.\\nThe extent to which the auto-encoder latent space follows a statistical distribution is referred to as the latent space \\\\textit{regularity}. The latent space of the standard auto-encoder does not follow any particular distribution. The regularity of the standard AE depends on the input features, the dimension of the latent space, and the encoder architecture. Thus, the encoder will shape the latent space such that it facilitates the reconstruction task, thus minimising the MSE loss from \\\\autoref{eq:vanillaloss}. In contrast, the Variational Auto-encoder (VAE)\\\\cite{kingma2014autoencoding} is an extension of the conventional auto-encoder described above, which models the latent representation to approximate a given probability distribution. This is typically a Gaussian distribution, described by a mean and a variance; however, many alternatives exist\\\\cite{joo2019dirichlet, patrini2019sinkhorn, Cerri_2019, Dillon_2021, Cheng_2023}, and the choice of latent space distribution ultimately depends on the task. \\nThe main idea of variational inference is to deﬁne a parametrised family of distributions and to search within it for the best approximation of the chosen prior distribution. The ``best approximation\\'\\' is defined as the element of the aforementioned family of distributions that minimises a pre-deﬁned function that measures the dissimilarity between the trial approximation and the prior. The function that is most commonly employed for this task is the Kullback-Leibler\\\\cite{Joyce2011} (KL) divergence, defined as\\n\\\\begin{equation}\\n\\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma}) = -\\\\frac{1}{2}\\\\sum_i \\\\left ( \\\\log(\\\\sigma_i^2) - \\\\sigma_i^2 -\\\\mu_i^2 +1 \\\\right)~,\\n\\\\end{equation}\\nfor the specific case of comparing a parametrised Gaussian distribution $\\\\mathrm{N}(\\\\vec{\\\\mu},\\\\vec{\\\\sigma})$ with $\\\\mathrm{N}(1, 0)$. A broader discussion on the KL divergence is found in Ref.\\\\,\\\\citen{paisley2012variational}. Note that the KL divergence is a somewhat unstable dissimilarity metric. Hence, more robust alternatives exist, such as the Wasserstein distance, which led to the creation of an AE architecture with the same name\\\\cite{tolstikhin2019wasserstein}. Variations on the Wasserstein AE have also been applied in a high-energy physics context\\\\cite{Komiske_2019, Komiske_2020}.\\nThe VAE loss consists of two components: the reconstruction loss, conventionally the MSE, and the KL divergence term. The latter encourages the VAE to produce a latent space that follows a well-defined prior distribution, regularising the model. Thus, the VAE loss can be written schematically as\\n\\\\begin{equation}\\n{\\\\cal L} = (1-\\\\beta) \\\\mathrm{MSE}(\\\\mathrm{Output}, \\\\mathrm{Input}) + \\\\beta \\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma})~,\\n\\\\end{equation}\\nwhere MSE labels the reconstruction loss, $\\\\mathrm{D_\\\\mathrm{{KL}}}$ is the KL regularization term, and $\\\\beta\\\\in[0, 1]$ is a hyperparameter that balances the effect of the two loss components.\\nThe weakly supervised methods from the previous sections aim to learn the likelihood ratio and thus can identify anomalies. In contrast, self-supervised models only learn the probability density of the background. Hence, an event may be labeled as anomalous if its probability to be associated with the learned latent distribution is very low. Additionally, the learned distribution exists in a lower dimensional embedded space. This stops the model from memorizing the input and is a form of lossy compression. Therefore, the model is generally capable of reconstructing events it is frequently exposed to during its training, but it fails at reconstructing events that are rare in the training set. The difference between the input data and its reconstructed counterpart may then be used to define an anomaly score: a high MSE is expected for anomalous data and a low MSE is expected for typical events. An illustration of this paradigm is shown in Figure~\\\\ref{fig:ae}. There exist several studies in HEP where AEs and VAEs are used for detecting new physics as outliers in the data~\\\\cite{Farina:2018fyg, Heimel:2018mkt,Blance:2019ibf,Hajer:2018kqm,Roy:2019jae,Cheng_2023}. For~example, this type of workflow was used to search for new physics in the two-body invariant mass spectrum of two jets or a jet and a lepton with the ATLAS Experiment in Ref.~\\\\citen{ATLAS:2023ixc}. Therein, a selection on an auto-encoder output is used to suppress the background and define signal regions with a high signal-to-background ratio. The auto-encoder output for data and for a range of potential new physics signatures is shown in Figure~\\\\ref{fig:atlasae}.\\nAs mentioned in the beginning of this section, autoencoders are efficient for event-by-event outlier detection and are not expected to perform well in finding overdensities. This makes them complimentary to the weakly supervised methods. Furthermore, an additional problem that auto-encoders have is discussed in Ref.~\\\\citen{obstructions}. In the aforementioned work it is demonstrated that the connection between large MSE and anomalies is not completely clear: for data sets with a nontrivial topology, there will always be points that wrongly are classified as anomalous. Conventionally, this can be mitigated by using VAEs and classifying anomalous events using the regularized latent space. An alternative method of circumventing this issue is based on the so called normalised AE \\\\cite{yoon2023autoencoding}, which is located at the boundary between self-supervised and unsupervised learning. This newer type of AE architecture uses energy-based models as an alternative to the likelihood ratio or the MSE. Thus, the normalised AE avoids classifying genuinely complex albeit standard events as anomalous. For more details on this last kind of AE and its possible application to HEP, see Ref.\\\\,\\\\citen{dillon2023normalized}. As mentioned earlier, diffusion models are also being explored as an alternative method to perform density estimation, similar to variational autoencoders, utilizing the learned density as a permutation-invariant anomaly detection score ~\\\\cite{mikuni2023highdimensional}.\\nAs mentioned earlier, diffusion models are also increasingly being investigated as an alternative approach for density estimation. This method parallels the use of variational autoencoders, leveraging the learned density to create a permutation-invariant score for anomaly detection, as detailed in Ref.~\\\\citen{mikuni2023highdimensional}.\\n\\\\subsubsection{Unsupervised Methods}\\nUnsupervised anomaly detection methods usually perform some type of data clustering. They include models such as Support Vector Machines~\\\\cite{boser1992training}, Isolation Forests~\\\\cite{isoforest}, and Gaussian Mixture Models\\\\cite{vanBeekveld:2020txa, Kuusela_2012}. An application using SVMs for anomaly detection in particle physics is discussed in Section~\\\\ref{sec:qml}. An example of unsupervised clustering for collider physics is presented in Ref.~\\\\citen{ucluster}. Therein, the Unsupervised Clustering algorithm, or UCluster, uses an attention-based Graph Neural Network known as \"ABC net\"~\\\\cite{abcnet} to create a latent space in which points sharing similar properties are placed close to each other. This is achieved by combining a clustering objective and a classification task during training. The produced embedding is shown to be capable of clustering together events that contain a new physics signal. A benefit of this method is that it naturally provides a way of performing background estimation. For each identified cluster, the nearest cluster within the embedding space can be used as a background model. The anomalous signal remains\\nlocalized in a particular cluster. Therefore, the nearest clusters are signal free, as shown in Figure~\\\\ref{fig:ucluster}. \\nA model-independent search method, based on Gaussian Mixture Models (GMMs), is introduced in Ref.~\\\\citen{Kuusela_2012}. Within this methodology, a GMM is being used to model the background. Then, in order to avoid any dependence on a signal hypothesis, deviations from this model are identified by fitting a mixture of the background model and a number of additional Gaussians to the observed data. This allows to search for any potential deviation from the background expectation without developing a model for the signal a priori.\\nFinally, decision trees have also been explored in anomaly detection for searches. For example, in Ref.~\\\\citen{roche2023nanosecond}, a tree-based autoencoder is trained through a self-supervised paradigm on background data and then evaluated on the ADC challenge data~\\\\cite{adcchallenge}. Their unsupervised counterpart, isolation forests, have been less prominent in particle physics, but they have been applied for accelerator control~\\\\cite{Halilovic:2665985}.\\nA key challenge with outlier detection methods, as discussed in Ref.~\\\\citen{golling2023massive}, is their tendency to generate anomaly scores closely correlated to the variable of interest. This may lead to undesired sculpting effects, complicating bump-hunt like searches. To address this, strategies such as decorrelating the latent space from the variable of interest or tailoring the anomaly metric to be conditional on the jet mass \\\\cite{Cheng_2023} should be explored. However, efforts in these areas remain limited.\\n', \"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\", \"\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Descriptive Statistics}\\nSuppose we have a sample of $N$ data $X = x_1, x_2, \\\\cdots, x_N$. It is often useful to summarize these data with a few numbers called statistics. \\nA \\\\textbf{statistic} is any number that can be calculated from the data and known parameters. For example, $t = (x_1 + x_N)/2$ is a statistic, but if the value of $\\\\theta$ is unknown $t = (x_1 - \\\\theta)^2$ is not. However, a word of caution is in order: we particle physicists are prone to misuse the jargon of professional statisticians. For example, we tend to refer to \\\\emph{any} function of the data as a statistic including those that contain unknown parameters. \\nThe two most important statistics are\\n\\\\begin{align}\\n&\\\\textrm{the {\\\\bf sample mean} (or average)} & \\\\bar{x} & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i, \\\\\\\\\\n&\\\\textrm{and the {\\\\bf sample variance}} & s^2 & = \\\\frac{1}{N} \\\\sum_{i=1}^N (x_i - \\\\bar{x})^2,\\n\\\\nonumber\\\\\\\\\\n& & & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i^2 - \\\\bar{x}^2, \\\\nonumber\\\\\\\\\\n& & & = \\\\overline{x^2} - \\\\bar{x}^2.\\n\\\\end{align}\\nThe sample average is a measure of the center of the distribution of the data, while the sample variance is a measure of its spread. Statistics that merely characterize the data are\\ncalled \\\\textbf{descriptive statistics}, of which the sample average and variance are the most\\nimportant. If we order the data, say from the smallest value to the largest, we can compute another interesting statistic $t_k \\\\equiv x_{(k)}$, where $1 \\\\leq k \\\\leq N$ and $x_{(k)}$ denotes the datum at the $k^\\\\text{th}$ position. The statistic $t_k$ is called the $k^\\\\text{th}$ \\\\textbf{order statistic} and is a measure of the value of outlying data.\\nThe average and variance, Eqs.~(\\\\ref{eq:xbar}) and (\\\\ref{eq:xvar}), are numbers that can always be calculated given a data sample $X$. But now we consider numbers that cannot be calculated from the data alone. Imagine the repetition, infinitely many times, of whatever data generating system yielded our data sample $X$ thereby creating an infinite sequence of data sets. We shall refer to the data generating system as an experiment and the\\ninfinite sequence as an infinite ensemble. The latter, together with all the mathematical\\noperations we may wish to apply to it, are \\nabstractions. After all, it is not possible to realize an infinite ensemble. The ensemble and\\nall the operations on it exist in the same sense that the number $\\\\pi$ exists along with all valid\\nmathematical operations on $\\\\pi$.\\nThe most common operation to perform on an ensemble is to compute the average of the statistics. This \\\\textbf{ensemble average} suggests several potentially useful characteristics of the ensemble, which we list below.\\n\\\\begin{align}\\n& \\\\textrm{Ensemble average} & & <x> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean} & \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Error} & \\\\epsilon & = x - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Bias} & b & = <x> - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Variance} & V & = <(x - <x>)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Standard deviation} & \\\\sigma & = \\\\sqrt{V} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean square error} & \\\\text{MSE} & = <(x - \\\\mu)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Root MSE} & \\\\textrm{RMS} & = \\\\sqrt{\\\\textrm{MSE}}\\n\\\\end{align}\\nNotice that none of these numbers can be calculated in practice because the data\\nrequired to do so do not concretely exist. Even\\nin an experiment simulated on a computer, there are very few of these numbers we can\\ncalculate. If we know the mean $\\\\mu$, perhaps because we have chosen its value --- for example, we may have chosen the mass of the Higgs boson in our simulation, we can certainly calculate the error $\\\\epsilon$ for any simulated datum $x$. But, we can only \\\\emph{approximate} the\\nensemble average $< x >$, bias $b$, variance $V$, and MSE, since our virtual ensemble is\\nalways finite. The point is this: the numbers that characterize the infinite ensemble are also abstractions,\\nalbeit useful ones. For example, the MSE is the most widely used\\nmeasure of the closeness of an ensemble of numbers to some parameter $\\\\mu$. The square root of the MSE is called the root mean square (RMS)\\\\footnote{Sometimes, the RMS and standard deviation are using interchangeably. However, the RMS is computed with respect to $\\\\mu$, while the standard deviation is computed with respect to the ensemble average $<x>$. The RMS and standard deviations are identical only if the bias is zero.}. The MSE can be written as\\n\\\\begin{align}\\n\\\\textrm{MSE} & = V + b^2. \\\\\\\\ & \\\\framebox{\\\\textbf{Exercise 1: } \\\\textrm{Show this}}\\\\nonumber\\n\\\\end{align}\\nThe MSE is the sum of the variance and the square of the bias, \\na very important result with practical consequences. For example, suppose that $\\\\mu$ represents the mass of the Higgs boson and $x$ represents some (typically very complicated) statistic that is considered an \\\\textbf{estimator} of the mass. An estimator is any\\nfunction, which when data are entered into it, yields an \\\\textbf{estimate} of the quantity\\nof interest, which we may take to be a measurement. \\nWords are important; ``bias'' is a case in point. It is an unfortunate choice\\nfor the difference $<x> - \\\\mu$ because the word ``bias'' biases attitudes towards bias! Something that, or someone who, is biased is surely bad and needs to be corrected. Perhaps.\\nBut, it would be wasteful of data to make the bias zero if the net effect is to make the MSE larger than an MSE in which the bias is non-zero. The price for achieving $b = 0$ in our\\nexample would be not only throwing away expensive data --- which is bad enough --- but also measuring a mass that is more likely to be further away from the Higgs boson mass. This may, or may not, be what we want to achieve. \\nAs noted, many of the numbers listed in Eq.~(\\\\ref{eq:ensemble}) cannot be calculated because\\nthe information needed is unknown. This is\\ntrue, in particular, of the bias. However, sometimes it is possible to relate the bias to another ensemble quantity. Consider the ensemble average of the sample variance, Eq.~(\\\\ref{eq:xvar}),\\n\\\\begin{align}\\n<s^2> & = < \\\\overline{x^2} > - <\\\\bar{x}^2>, \\\\nonumber\\\\\\\\\\n& = V - \\\\frac{V}{N}, \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 2a:} Show this} \\\\nonumber \\n\\\\end{align}\\nThe sample variance has a bias of $b = - V/N$, which many argue should be\\ncorrected. Unfortunately, we cannot calculate the bias because it depends on an unknown parameter, namely, the variance $V$. \\nHowever, if we replace the sample variance by $s^{\\\\prime 2} = c s^2$,where the\\ncorrection factor $c = N/(N-1)$, we find that for the corrected variance estimator $s^{\\\\prime 2}$ the bias is zero. Surely the world is now a better place? Well, not necessarily. \\nConsider the ratio of $\\\\textrm{MSE}^\\\\prime$ to $\\\\textrm{MSE}$, where $\\\\textrm{MSE}^\\\\prime = <(s^{\\\\prime 2} - V)^2>$, $\\\\textrm{MSE} = <\\\\delta^2>$ with $\\\\delta = s^2 - V$, and\\n$b = -V / N$,\\n\\\\begin{align*}\\n\\\\textrm{MSE}^\\\\prime / \\\\textrm{MSE} \\n& = < (c s^2 - V)^2 > / < \\\\delta^2 > , \\\\\\\\\\n& = c^2 < (s^2 - V/c)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 < (\\\\delta - b)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 (1 - b^2 / <\\\\delta^2> ), \\\\\\\\\\n& = c^2 \\\\left[ 1 - b^2 / (b^2 + <s^4> - (V+b)^2) \\\\right].\\n\\\\end{align*} \\nFrom this we deduce that if $<s^4>/[ (V+b)^2 + b^2 / (c^2 - 1)] > 1$, the unbiased\\nestimate will be further away on average from $V$ than the biased estimate. This is the case,\\nfor example, for a uniform distribution.\\n\\\\centerline{ \\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 2b:} Use the method {Rndm()} of the {\\\\tt Root}\\\\\\\\ class {\\\\tt TRandom3} to verify that $\\\\textrm{MSE}^\\\\prime > \\\\textrm{MSE}$. }} \\n}\\n\"}},\n",
       "       {'entity_name': 'bias', 'entity_type': 'statistics_concept', 'description': 'A systematic error in statistical estimation that represents the difference between the expected value of an estimator and the true value of the parameter being estimated. This error can lead to consistent overestimation or underestimation, thereby affecting the accuracy of statistical estimates.', 'relevant_passages': {'\\\\section{Inference}\\n\\\\subsection{Estimate of Gaussian parameters}\\nIf we have $n$ independent measurements $\\\\vec{x}=(x_1,\\\\cdots,x_n)$ all modeled (exactly or approximatively)\\nwith the same Gaussian PDF,\\nwe can write the negative of twice the logarithm of the likelihood function as follows:\\n\\\\begin{equation}\\n-2\\\\ln L(\\\\vec{x}; \\\\mu) = \\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2} + n(\\\\ln 2\\\\pi + 2\\\\ln\\\\sigma)\\\\,.\\n\\\\end{equation}\\nThe first term, $\\\\sum_{i=1}^n \\\\frac{(x_i-\\\\mu)^2}{\\\\sigma^2}$, is an example of $\\\\chi^2$ variable (see Sec.~\\\\ref{sec:binnedSamples}).\\nAn analytical minimization of $-2\\\\ln L$ with respect to $\\\\mu$,\\nassuming $\\\\sigma^2$ is known, gives the {\\\\it arithmetic mean} as maximum likelihood estimate of $\\\\mu$:\\n\\\\begin{equation}\\n\\\\hat{\\\\mu} = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i\\\\,.\\n\\\\end{equation}\\nIf $\\\\sigma^2$ is also unknown, the maximum likelihood estimate of $\\\\sigma^2$ is:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2} = \\\\frac{1}{n} \\\\sum_{i=1}^m(x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\nThe estimate in Eq.~(\\\\ref{eq:sigma2MLestimate}) can be demonstrated to have an unpleasant feature, called {\\\\it bias}, that\\nwill be discussed in Sec.~\\\\ref{sec:bias}.\\n', \"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\", \"\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Descriptive Statistics}\\nSuppose we have a sample of $N$ data $X = x_1, x_2, \\\\cdots, x_N$. It is often useful to summarize these data with a few numbers called statistics. \\nA \\\\textbf{statistic} is any number that can be calculated from the data and known parameters. For example, $t = (x_1 + x_N)/2$ is a statistic, but if the value of $\\\\theta$ is unknown $t = (x_1 - \\\\theta)^2$ is not. However, a word of caution is in order: we particle physicists are prone to misuse the jargon of professional statisticians. For example, we tend to refer to \\\\emph{any} function of the data as a statistic including those that contain unknown parameters. \\nThe two most important statistics are\\n\\\\begin{align}\\n&\\\\textrm{the {\\\\bf sample mean} (or average)} & \\\\bar{x} & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i, \\\\\\\\\\n&\\\\textrm{and the {\\\\bf sample variance}} & s^2 & = \\\\frac{1}{N} \\\\sum_{i=1}^N (x_i - \\\\bar{x})^2,\\n\\\\nonumber\\\\\\\\\\n& & & = \\\\frac{1}{N} \\\\sum_{i=1}^N x_i^2 - \\\\bar{x}^2, \\\\nonumber\\\\\\\\\\n& & & = \\\\overline{x^2} - \\\\bar{x}^2.\\n\\\\end{align}\\nThe sample average is a measure of the center of the distribution of the data, while the sample variance is a measure of its spread. Statistics that merely characterize the data are\\ncalled \\\\textbf{descriptive statistics}, of which the sample average and variance are the most\\nimportant. If we order the data, say from the smallest value to the largest, we can compute another interesting statistic $t_k \\\\equiv x_{(k)}$, where $1 \\\\leq k \\\\leq N$ and $x_{(k)}$ denotes the datum at the $k^\\\\text{th}$ position. The statistic $t_k$ is called the $k^\\\\text{th}$ \\\\textbf{order statistic} and is a measure of the value of outlying data.\\nThe average and variance, Eqs.~(\\\\ref{eq:xbar}) and (\\\\ref{eq:xvar}), are numbers that can always be calculated given a data sample $X$. But now we consider numbers that cannot be calculated from the data alone. Imagine the repetition, infinitely many times, of whatever data generating system yielded our data sample $X$ thereby creating an infinite sequence of data sets. We shall refer to the data generating system as an experiment and the\\ninfinite sequence as an infinite ensemble. The latter, together with all the mathematical\\noperations we may wish to apply to it, are \\nabstractions. After all, it is not possible to realize an infinite ensemble. The ensemble and\\nall the operations on it exist in the same sense that the number $\\\\pi$ exists along with all valid\\nmathematical operations on $\\\\pi$.\\nThe most common operation to perform on an ensemble is to compute the average of the statistics. This \\\\textbf{ensemble average} suggests several potentially useful characteristics of the ensemble, which we list below.\\n\\\\begin{align}\\n& \\\\textrm{Ensemble average} & & <x> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean} & \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Error} & \\\\epsilon & = x - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Bias} & b & = <x> - \\\\mu \\\\nonumber\\\\\\\\\\n& \\\\textrm{Variance} & V & = <(x - <x>)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Standard deviation} & \\\\sigma & = \\\\sqrt{V} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Mean square error} & \\\\text{MSE} & = <(x - \\\\mu)^2> \\\\nonumber\\\\\\\\\\n& \\\\textrm{Root MSE} & \\\\textrm{RMS} & = \\\\sqrt{\\\\textrm{MSE}}\\n\\\\end{align}\\nNotice that none of these numbers can be calculated in practice because the data\\nrequired to do so do not concretely exist. Even\\nin an experiment simulated on a computer, there are very few of these numbers we can\\ncalculate. If we know the mean $\\\\mu$, perhaps because we have chosen its value --- for example, we may have chosen the mass of the Higgs boson in our simulation, we can certainly calculate the error $\\\\epsilon$ for any simulated datum $x$. But, we can only \\\\emph{approximate} the\\nensemble average $< x >$, bias $b$, variance $V$, and MSE, since our virtual ensemble is\\nalways finite. The point is this: the numbers that characterize the infinite ensemble are also abstractions,\\nalbeit useful ones. For example, the MSE is the most widely used\\nmeasure of the closeness of an ensemble of numbers to some parameter $\\\\mu$. The square root of the MSE is called the root mean square (RMS)\\\\footnote{Sometimes, the RMS and standard deviation are using interchangeably. However, the RMS is computed with respect to $\\\\mu$, while the standard deviation is computed with respect to the ensemble average $<x>$. The RMS and standard deviations are identical only if the bias is zero.}. The MSE can be written as\\n\\\\begin{align}\\n\\\\textrm{MSE} & = V + b^2. \\\\\\\\ & \\\\framebox{\\\\textbf{Exercise 1: } \\\\textrm{Show this}}\\\\nonumber\\n\\\\end{align}\\nThe MSE is the sum of the variance and the square of the bias, \\na very important result with practical consequences. For example, suppose that $\\\\mu$ represents the mass of the Higgs boson and $x$ represents some (typically very complicated) statistic that is considered an \\\\textbf{estimator} of the mass. An estimator is any\\nfunction, which when data are entered into it, yields an \\\\textbf{estimate} of the quantity\\nof interest, which we may take to be a measurement. \\nWords are important; ``bias'' is a case in point. It is an unfortunate choice\\nfor the difference $<x> - \\\\mu$ because the word ``bias'' biases attitudes towards bias! Something that, or someone who, is biased is surely bad and needs to be corrected. Perhaps.\\nBut, it would be wasteful of data to make the bias zero if the net effect is to make the MSE larger than an MSE in which the bias is non-zero. The price for achieving $b = 0$ in our\\nexample would be not only throwing away expensive data --- which is bad enough --- but also measuring a mass that is more likely to be further away from the Higgs boson mass. This may, or may not, be what we want to achieve. \\nAs noted, many of the numbers listed in Eq.~(\\\\ref{eq:ensemble}) cannot be calculated because\\nthe information needed is unknown. This is\\ntrue, in particular, of the bias. However, sometimes it is possible to relate the bias to another ensemble quantity. Consider the ensemble average of the sample variance, Eq.~(\\\\ref{eq:xvar}),\\n\\\\begin{align}\\n<s^2> & = < \\\\overline{x^2} > - <\\\\bar{x}^2>, \\\\nonumber\\\\\\\\\\n& = V - \\\\frac{V}{N}, \\\\nonumber\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 2a:} Show this} \\\\nonumber \\n\\\\end{align}\\nThe sample variance has a bias of $b = - V/N$, which many argue should be\\ncorrected. Unfortunately, we cannot calculate the bias because it depends on an unknown parameter, namely, the variance $V$. \\nHowever, if we replace the sample variance by $s^{\\\\prime 2} = c s^2$,where the\\ncorrection factor $c = N/(N-1)$, we find that for the corrected variance estimator $s^{\\\\prime 2}$ the bias is zero. Surely the world is now a better place? Well, not necessarily. \\nConsider the ratio of $\\\\textrm{MSE}^\\\\prime$ to $\\\\textrm{MSE}$, where $\\\\textrm{MSE}^\\\\prime = <(s^{\\\\prime 2} - V)^2>$, $\\\\textrm{MSE} = <\\\\delta^2>$ with $\\\\delta = s^2 - V$, and\\n$b = -V / N$,\\n\\\\begin{align*}\\n\\\\textrm{MSE}^\\\\prime / \\\\textrm{MSE} \\n& = < (c s^2 - V)^2 > / < \\\\delta^2 > , \\\\\\\\\\n& = c^2 < (s^2 - V/c)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 < (\\\\delta - b)^2 > / < \\\\delta^2>, \\\\\\\\\\n& = c^2 (1 - b^2 / <\\\\delta^2> ), \\\\\\\\\\n& = c^2 \\\\left[ 1 - b^2 / (b^2 + <s^4> - (V+b)^2) \\\\right].\\n\\\\end{align*} \\nFrom this we deduce that if $<s^4>/[ (V+b)^2 + b^2 / (c^2 - 1)] > 1$, the unbiased\\nestimate will be further away on average from $V$ than the biased estimate. This is the case,\\nfor example, for a uniform distribution.\\n\\\\centerline{ \\n\\\\framebox{\\\\parbox{0.5\\\\textwidth}{\\\\textbf{Exercise 2b:} Use the method {Rndm()} of the {\\\\tt Root}\\\\\\\\ class {\\\\tt TRandom3} to verify that $\\\\textrm{MSE}^\\\\prime > \\\\textrm{MSE}$. }} \\n}\\n\"}},\n",
       "       {'entity_name': 'marginalization techniques', 'entity_type': 'analysis_technique', 'description': 'A set of statistical methods used to compute marginal probabilities by integrating over nuisance parameters or other variables in a joint probability distribution. This approach is commonly applied in both general probability calculations and Bayesian analysis to simplify models and focus on parameters of interest.', 'relevant_passages': {\"\\\\section{Upper Limits including systematic uncertainties, Bayesian approach}\\nThe inclusion of systematic effects in the calculation of upper limits, experimental sensitivity, and observation requires a Bayesian approach. This strategy extends the likelihood function by including typically Gaussian distributions to model effects such as efficiency, luminosity, Monte Carlo event estimation, and others~\\\\cite{lista2016practical,cranmer2015practical,junk1999confidence}. In particular, to establish the reconstruction efficiency of background events, a nuisance parameter (\\\\( \\\\epsilon \\\\)) centered on the expected value of \\\\( b \\\\) can be included. Thus, the parameter of the Poisson distribution is defined by:\\n\\\\begin{equation}\\n\\\\lambda(\\\\mu,\\\\epsilon) = \\\\mu s + \\\\epsilon b,\\n\\\\end{equation}\\nwhere the efficiency follows a binomial distribution \\\\( \\\\epsilon \\\\sim \\\\mathcal{N}(1, \\\\sigma) \\\\). The standard deviation of the likelihood adjusts the uncertainty of the number of background events across the observable spectrum, typically ranging from 5 to 20\\\\\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon) = \\\\prod_{i=1}^{N-channels}\\n\\\\frac{ e^{-(\\\\mu s_{i} + \\\\epsilon b_{i})} (\\\\mu s_{i} + \\\\epsilon b_{i})^{n_{i}} }{n_{i}!}\\n\\\\frac{1}{\\\\sqrt{2 \\\\pi \\\\sigma^{2}}} e^{ -\\\\frac{ (1-\\\\epsilon)^{2} } {2\\\\sigma^{2}} }.\\n\\\\end{equation}\\nThe non-informative prior distribution naturally extends to:\\n\\\\begin{equation}\\n\\\\Pi(\\\\mu,\\\\epsilon) = \\n\\\\begin{cases} \\n1 & 0<\\\\mu<\\\\mu^{max} \\\\ \\\\text{and} \\\\ 0 < \\\\epsilon < \\\\epsilon^{max} \\\\\\\\\\n0 & \\\\text{otherwise }.\\n\\\\end{cases}\\n\\\\end{equation}\\nUsing Bayes' theorem, the posterior distribution is obtained.\\n\\\\begin{equation}\\nP(\\\\mu, \\\\epsilon / \\\\bm{x}) = \\\\frac{\\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon)}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThis means that to establish the upper limits of \\\\( \\\\mu \\\\) or the experimental sensitivity, the posterior must be marginalized to find the profile \\\\( P(\\\\mu | \\\\bm{x}) \\\\). This is a standard probability calculation and requires numerical integration or sampling of the posterior distribution using, for example, the Markov Chain Monte Carlo (MCMC) algorithm~\\\\cite{raftery1992practical, wang2023recent}. In any case, the probability profile is given by:\\n\\\\begin{equation}\\nP(\\\\mu,\\\\bm{x}) = \\\\int_{0}^{\\\\infty} P(\\\\mu, \\\\epsilon / \\\\bm{x}) d\\\\epsilon = \\\\frac{ \\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\epsilon}\\n{\\\\int_{0}^{\\\\infty}\\\\int_{0}^{\\\\infty} \\\\mathcal{L}(\\\\bm{x}/\\\\mu,\\\\epsilon)\\\\Pi(\\\\mu,\\\\epsilon) d\\\\mu d\\\\epsilon }.\\n\\\\end{equation}\\nThe marginalization process correctly propagates the effect of systematic uncertainty in the upper limits. In general, the correlation shifts the limit values to higher values, thereby restricting the sensitivity of a model in the experiment or the exclusion power in an experimental study~\\\\cite{conway2005calculation}. As mentioned previously, the expected and observed upper limits are defined over the marginal distribution:\\n\\\\begin{equation}\\nCLs(\\\\mu_{up}) = \\\\int_{0}^{\\\\mu_{up}} P(\\\\mu,x) d\\\\mu. = 0.95\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:19}] shows the posterior distribution as a function of the signal strength \\\\( \\\\mu \\\\) and the efficiency in estimating background events (\\\\( b \\\\)), for the channel with \\\\( n=105 \\\\), \\\\( b=100 \\\\), \\\\( s=10 \\\\), and a systematic uncertainty of \\\\( \\\\sigma=0.1 \\\\) for the background events~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/UpperLimitSystematic.ipynb}{Source code}}. Similarly, Figure~[\\\\ref{fig:20}] shows the marginal distribution obtained using the double Gaussian quadrature method~\\\\cite{golub1969calculation}.\\nBy varying the systematic uncertainty, it is possible to find the observed upper limits and the correlation effect between parameters. Table~[\\\\ref{tb:3}] shows the behavior of the observed upper limit as a function of \\\\( \\\\sigma \\\\) for the numerical approximation (Gaussian quadrature) and for the sampling generated by the Metropolis algorithm, as well as the correlation coefficient indicating how uncertainty limits the exclusion power of the model. It is important to note that for high-dimensional posterior distributions, there are no quadrature rules that allow for accurately estimating the marginal distribution. In such cases, the Metropolis algorithms and optimization methods have been widely applied~\\\\cite{atlas2012observation,cms2012observation}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccc}\\n\\\\hline\\n$\\\\sigma$ & Gaussian quadrature & MCMC algorithm & Correlation coef\\\\\\\\\\n\\\\hline\\n\\\\multicolumn{1}{c}{} & \\\\multicolumn{3}{c}{$\\\\mu_{up}(95\\\\ 0.05 & 2.80 & 2.71 & -0.32\\\\\\\\\\n0.10 & 3.34 & 3.31 & -0.54\\\\\\\\\\n0.15 & 4.09 & 4.13 & -0.68\\\\\\\\\\n0.20 & 4.91 & 4.66 & -0.77\\\\\\\\\\n0.25 & 5.79 & 5.80 & -0.88\\\\\\\\ \\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Upper limits of the signal strength at 95\\\\ \\n\\\\end{center}\\n\\\\end{table}\\nIn particular, the \\\\texttt{emcee} package was used for sampling the extended posterior distribution shown above~\\\\cite{foreman2013emcee, Bocklund2019ESPEI}. Figure~[\\\\ref{fig:21}] shows the corner plot of the posterior distribution along with the marginal distributions associated with the signal strength \\\\( \\\\mu \\\\) and the reconstruction efficiency \\\\( \\\\epsilon \\\\)~\\\\footnote{\\\\href{https://github.com/asegura4488/StatsHEP/blob/main/Systematic/Bayesian/MetropolisSamplingBayes.ipynb}{Source code}}. Additionally, the maximum likelihood estimators of the posterior are shown; these parameters are required for the profile likelihood method presented in the following section.\\n\", \"\\\\section{Errors}\\n\\\\subsection{Errors in 2 or more dimensions}\\nFor 2 (or more) dimensions, one plots the log likelihood and defines regions using contours in $\\\\Delta \\\\ln L$ (or $\\\\Delta \\\\chi^2\\\\equiv - 2 \\\\Delta \\\\ln L$). An example is given in Fig.~\\\\ref{fig:CMS}.\\nThe link between the $\\\\Delta \\\\ln L$ values and the significance changes. \\nIn 1D, there is a 68\\\\In 2D, a 1$\\\\sigma$ square would give a probability $0.68^2 = 47\\\\If one rounds off the corners and draws a 1$\\\\sigma$ contour \\nat $\\\\Delta \\\\ln L=-\\\\half$ this falls to 39\\\\To retrieve the full 68\\\\ $\\\\Delta \\\\chi^2=2.27$.\\nFor 95\\\\\\nThe necessary value is obtained from the $\\\\chi^2$ distribution---described later. It can be found by the R function {\\\\tt qchisq(p,n)} or the Root function {\\\\tt TMath::ChiSquareQuantile(p,n)}, where the desired \\nprobability {\\\\tt p} and number of degrees of freedom {\\\\tt n} are the arguments given.\\n\\\\subsubsection{Nuisance parameters}\\nIn the example of Fig.~\\\\ref{fig:CMS}, both $C_V$ and $C_F$ are interesting.\\nBut in many cases one is interested only in one (or some) of the quantities and the others\\nare `nuisance parameters' that one would like to remove, reducing the dimensionality of the \\nquoted result. There are two methods of doing this, one (basically) frequentist and one Bayesian.\\nThe frequentist uses the {\\\\it profile likelihood} technique. Suppose that there are two parameters,\\n$a_1$ and $a_2$, where $a_2$ is a nuisance parameter, and so one wants to reduce the \\njoint likelihood function $L(x;a_1,a_2)$ to some function $\\\\hat L(a_1)$.\\nTo do this one scans across the values of $a_1$ and inserts $\\\\hat{\\\\hat a}_2(a_1)$, the value of $a_2$ which maximises the likelihood for that particular $a_1$\\n\\\\begin{equation}\\n\\\\hat L (x,a_1)=L(a_1,\\\\hat{\\\\hat a}_2(a_1))\\n\\\\end{equation}\\n\\\\noindent and the location of the maximum and the $\\\\Delta \\\\ln L=\\\\-\\\\half$ errors are\\nread off as usual. \\nTo see why this works---though this is not a very rigorous motivation---suppose one had a likelihood function\\nas shown in Fig.~\\\\ref{fig:profile}.\\nThe horizontal axis is for the parameter of interest, $a_1$, and the vertical for the nuisance parameter $a_2$.\\nDifferent values of $a_2$ give different results (central and errors) for $a_1$.\\nIf it is possible to transform to $a_2'(a_1,a_2)$ so that $L$ factorises, then we can write\\n$L(a_1,a_2')=L_1(a_1)L_2(a_2')$: this is shown in the plot on the right.\\nWe suppose that this is indeed possible. In the case here, and other not-too-complicated cases, it clearly is, although\\nit will not be so in more complicated topologies with multiple peaks. \\nThen using the transformed graph, \\nwhatever the value of $a_2'$, one would get the same result for $a_1$.\\nThen one can present this result for $a_1$, independent of anything about $a_2'$.\\nThere is no need to factorise explicitly: the\\npath of central $a_2'$ value as a function of $a_1$ (the central of the 3 lines on the right hand plot)\\nis the path of the peak, and that path can be located in the first plot (the transformation only stretches the $a_2$ axis, it does not change the heights).\\nThe Bayesian method uses the technique called {\\\\it marginalisation}, which \\njust integrates over $a_2$.\\nFrequentists can not do this as they are not allowed to integrate likelihoods over the parameter:\\n$\\\\int P(x;a)\\\\, dx$ is fine, but $\\\\int P(x;a)\\\\, da$ is off limits.\\nNevertheless this can be a very helpful alternative to profiling, specially for many nuisance parameters.\\nBut if you use it you must be aware that this is strictly Bayesian.\\nReparametrizing $a_2$ (or choosing a different prior) will give different results for $a_1$.\\nIn many cases, where the effect of the nuisance parameter is small, this does not\\nhave a big effect on the result.\\n\", '\\\\section{Lecture 1: Descriptive Statistics, Probability and Likelihood}\\n\\\\subsection{Probability}\\nWhen the weather forecast specifies that there is a 80\\\\have an intuitive sense of what this means. Likewise, most people have an intuitive \\nunderstanding of what it means to say that there is a 50-50 chance for a tossed coin to land\\nheads up. Probabilistic ideas are thousands of years old, but, starting in the sixteenth century these ideas were formalized into increasingly rigorous mathematical theories of probability. \\nIn the theory formulated by Kolmogorov in 1933, $\\\\Omega$ is\\nsome fixed mathematical space, $E_1, E_2, \\\\cdots \\\\subset \\\\Omega$ are subsets (called events) defined in some\\nreasonable way\\\\footnote{If $E_1, E_2, \\\\cdots$ are meaningful subsets of $\\\\Omega$, so to is the complement $\\\\overline{E}_1, \\\\overline{E}_2, \\\\cdots$ of each, as are\\ncountable unions and intersections of these subsets.}, and $P(E_j)$ is a number \\nassociated with subset $E_j$. These numbers satisfy the\\n\\\\begin{align*}\\n\\\\textbf{Kolmogorov Axioms} \\\\\\\\\\n& \\\\quad 1. \\\\quad P(E_j) \\\\geq 0 \\\\\\\\\\n& \\\\quad 2. \\\\quad P(E_1 + E_2 + \\\\cdots) = P(E_1) + P(E_2) + \\\\cdots\\n\\\\quad\\\\textrm{for disjoint subsets} \\\\\\\\\\n& \\\\quad 3. \\\\quad P(\\\\Omega) = 1.\\n\\\\end{align*}\\nConsider two subsets $A = E_1$ and $B = E_2$. The quantity $AB$ means $A$ \\\\emph{and} $B$, while $A + B$ means $A$ \\\\emph{or}\\n$B$, with associated probabilities $P(AB)$ and $P(A+B)$, respectively. Kolmogorov assumed, not unreasonably given the intuitive origins of probability, that probabilities sum to unity; hence the axiom $P(\\\\Omega) = 1$. However, this assumption can be dropped so that probabilities remain meaningful even if $P(\\\\Omega) = \\\\infty$~\\\\cite{Taraldsen}. \\nFigure~\\\\ref{fig:venn} suggests another probability, namely, the number $P(A|B) = P(AB) / P(B)$, called the \\\\textbf{conditional probability} of\\n$A$ given $B$. This permits statements such as: ``the probability that this track was\\ncreated by an electron given the measured track parameters\" or ``the probability to observe\\n17 events given that the mean background is 3.8 events\".\\nConditional probability is a very powerful idea, but the term itself is misleading. It implies that there are two kinds of\\nprobability: conditional and unconditional. In fact, \\\\emph{all} probabilities are conditional in\\nthat they always depend on a specific set of conditions, namely, those that\\ndefine the space $\\\\Omega$. It is entirely possible to embed a family of subsets of $\\\\Omega$ \\ninto another space $\\\\Omega^\\\\prime$ which assigns to each family member a different\\nprobability $P^\\\\prime$. A probability is defined only relative to some space of possibilities $\\\\Omega$.\\n$A$ and $B$ are said to be mutually exclusive if $P(AB) = 0$, that is, if the truth\\nof one denies the truth of the other. They are said to be exhaustive if $P(A) + P(B) = 1$. \\nFigure~\\\\ref{fig:venn} suggests the theorem\\n\\\\begin{align}\\nP(A + B) = P(A) + P(B) - P(AB), \\\\\\\\\\n\\\\framebox{\\\\textbf{Exercise 3:} Prove theorem}\\\\nonumber\\n\\\\end{align}\\nwhich can be deduced from the rules given above. Another useful theorem is \\nan immediate consequence of the commutativity of ``anding\" \\n$P(AB) = P(BA)$ and the definition of $P(A|B)$,\\nnamely,\\n\\\\begin{align}\\n\\\\textbf{Bayes Theorem} \\\\nonumber\\\\\\\\\\n&P(B|A ) = \\\\frac{P(A|B) P(B)}{P(A)}, \\n\\\\end{align} \\nwhich provides a way to convert the probability $P(A|B)$ to the probability $P(B|A)$. \\nUsing Bayes theorem, we can, for example,\\ndeduce the probability $P(e|x)$ that a particle is an electron, $e$, given a set of measurements, $x$, from the\\nprobability $P(x|e)$ of a set of measurements given that the particle is an electron.\\n\\\\subsubsection{Probability Distributions}\\nIn this section, we illustrate the use of these rules to derive more complicated \\nprobabilities. First we start with a definition:\\n\\\\begin{quote}\\nA \\\\textbf{Bernoulli trial}, named after the Swiss mathematician Jacob Bernoulli (1654 -- 1705), is an experiment with only two possible outcomes: $S = \\\\textrm{success}$ or $F = \\\\textrm{failure}$. \\n\\\\end{quote}\\n\\\\begin{quote}\\n\\\\paragraph*{Example} Each collision between protons at the Large Hadron Collider (LHC) is a Bernoulli trial in which something interesting happens ($S$) or does not ($F$). Let $p$ be\\nthe probability of a success, which is assumed to be the \\\\emph{same for each trial}. Since $S$\\nand $F$ are exhaustive, the probability of a failure is $1 - p$. For a given order $O$ of $n$\\nproton-proton collisions and exactly $k$ successes, and therefore exactly $n - k$ failures, the probability $P(k, O , n, p)$ is given by\\n\\\\begin{align}\\nP(k, O, n, p) = p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nIf the order $O$ of successes and failures is judged to be irrelevant, we can eliminate the order\\nfrom the problem by summing over all possible orders,\\n\\\\begin{align}\\nP(k, n, p) = \\\\sum_O P(k, O, n, p) = \\\\sum_O p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nThis procedure is called \\\\textbf{marginalization}. It is one of the most important operations in probability calculations. Every term in the sum in Eq.~(\\\\ref{eq:Pkn}) is identical and there\\nare $\\\\binom{n}{k}$ of them. This yields the \\\\textbf{binomial distribution},\\n\\\\begin{align}\\n\\\\textrm{Binomial(k, n, p)} \\\\equiv \\\\binom{n}{k} p^k (1 - p)^{n - k}.\\n\\\\end{align}\\nBy definition, the mean number of successes $a$ is given by\\n\\\\begin{align}\\na & = \\\\sum_{k=0}^n k \\\\, \\\\textrm{Binomial(k, n, p)}, \\\\nonumber\\\\\\\\\\n& = p n. \\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 4:} Show this} \\\\nonumber\\n\\\\end{align}\\nAt the LHC $n$ is a number in the trillions, while for successes of interest such as the creation of a Higgs boson\\nthe probability $p << 1$. In this case, it proves convenient to consider the \\nlimit $p \\\\rightarrow 0, n \\\\rightarrow \\\\infty$ in such a way that $a$ remains constant. In\\nthis limit\\n\\\\begin{align}\\n\\\\textrm{Binomial(k, n, p)} & \\\\rightarrow e^{-a} a^k / k! , \\\\nonumber\\\\\\\\\\n& \\\\equiv \\\\textrm{Poisson}(k, a).\\\\\\\\\\n& \\\\framebox{\\\\textbf{Exercise 5:} Show this} \\\\nonumber\\n\\\\end{align}\\n\\\\end{quote}\\n\\\\bigskip\\n\\\\noindent\\nBelow we list the most common probability distributions.\\n\\\\begin{align}\\n&\\\\textbf{Discrete distributions}\\\\nonumber\\\\\\\\\\n& \\\\textrm{Binomial}(k, n, p) & \\\\binom{n}{k} p^k (1 - p^{n-k} \\\\nonumber\\\\\\\\\\n& \\\\textrm{Poisson}(k, a) & a^k \\\\exp(-a) / k! \\\\nonumber\\\\\\\\\\n& \\\\textrm{Multinomial}(k, n, p) & \\\\frac{n!}{k_1!\\\\cdots k_K!} \\\\prod_{i=1}^K p_i^{k_i}, \\\\quad \\\\sum_{i=1}^K p_i = 1, \\\\sum_{i=1}^K k_i = n \\\\nonumber\\\\\\\\\\n&\\\\textbf{Continuous densities}\\\\nonumber\\\\\\\\\\n& \\\\textrm{Uniform}(x, a) & 1 / a \\\\nonumber\\\\\\\\\\n& \\\\textrm{Gaussian}(x, \\\\mu, \\\\sigma) & \\\\exp[-(x - \\\\mu)^2 / (2 \\\\sigma^2)] / (\\\\sigma \\\\sqrt{2\\\\pi}) \\\\nonumber\\\\\\\\\\n&\\\\textrm{(also known as the Normal density)}\\\\nonumber\\\\\\\\\\n&\\\\textrm{LogNormal}(x, \\\\mu, \\\\sigma) & \\\\exp[-(\\\\ln x - \\\\mu)^2 / (2 \\\\sigma^2)] / (x \\\\sigma \\\\sqrt{2\\\\pi})\\n\\\\nonumber\\\\\\\\\\n& \\\\textrm{Chisq}(x, n) & x^{n/2 -1} \\\\exp(-x /2) / [2^{n/2} \\\\Gamma(n/2)] \\\\nonumber\\\\\\\\\\n& \\\\textrm{Gamma}(x, a, b) & x^{a -1} a^b \\\\exp(- a x) / \\\\Gamma(b) \\\\nonumber\\\\\\\\\\n&\\\\textrm{Exp}(x, a) & a \\\\exp(- a x) \\\\nonumber\\\\\\\\\\n&\\\\textrm{Beta}(x, n, m) & \\\\frac{\\\\Gamma(n+m)}{\\\\Gamma(m) \\\\, \\\\Gamma(n)}\\nx^{n-1} \\\\, (1 - x)^{m-1} \\n\\\\end{align}\\nParticle physicists tend to use the term probability distribution for both discrete and \\ncontinuous functions, such as the Poisson and Gaussian distributions, respectively. But, strictly speaking, the continuous functions are probability \\\\emph{densities}, not probability\\ndistributions. In order to compute a probability from a density we need to integrate the density \\nover a finite set in $x$.\\n\\\\paragraph*{Discussion}\\nProbability is the foundation for models of non-deterministic data generating mechanisms,\\nsuch as particle collisions at the LHC. A \\\\textbf{probability model} is the probability\\ndistribution together with all the assumptions on which the distribution is based. For example,\\nsuppose we wish to count, during a given period of time, the number of entries $N$ in a given\\ntransverse momentum ($p_\\\\text{T}$) bin due to particles created in proton-proton collisions\\nat the LHC; that is, suppose we wish to perform a counting experiment. If we assume that \\nthe probability to obtain a count in this bin is very small and that the number of proton-proton collisions is very large, then it is common practice to use a Poisson distribution to model the\\ndata generating mechanism, which yields the bin count $N$. If we have multiple independent bins, we \\nmay choose to model the data generating mechanism as a product of Poisson distributions. Or,\\nperhaps, we may prefer to model the possible counts conditional on a fixed total count in which case\\na multinomial distribution would be appropriate.\\nSo far, we have assumed the meaning of the word probability to be self-evident. However, \\nthe meaning of probability~\\\\cite{Daston} has been the subject of debate for more than two centuries and\\nthere is no sign that the debate will end anytime soon. Probability, in spite of its intuitive\\nbeginnings, is an\\nabstraction. Therefore, for it to be of practical use it must be \\\\emph{interpreted}. The two most\\nwidely used interpretations of probability are:\\n\\\\begin{enumerate}\\n\\\\item \\\\textbf{degree of belief} in, or plausibility of, a proposition, for example, \\n``It will snow at CERN on December 18th\", and the\\n\\\\item \\\\textbf{relative frequency} of outcomes in an \\\\emph{infinite} ensemble of\\ntrials, for example, the relative frequency of Higgs boson \\ncreation in an infinite number of proton-proton collisions. \\n\\\\end{enumerate} \\nThe first interpretation is the older, while the second was championed by influential\\nmathematicians and logicians starting in the mid-nineteenth century and became \\nthe dominant interpretation. Of the two interpretations, however,\\nthe older is the more general in that it encompasses the latter and can be\\nused in contexts in which the latter makes no sense. The relative frequency, or \\\\textbf{frequentist}, interpretation is useful for situations in which one can contemplate counting the number of times $k$ a given outcome is realized in $n$ trials, as in the example of\\na counting experiment. The relative frequency $r = k / n$ is expected to converge, in a \\nsubtle but well-defined\\nsense, to some number $p$ that satisfies the rules of probability. It should noted, however, that \\nthe numbers $k/n$ and $p$ are conceptually distinct. The former is something we can actually calculate, while there is no \\\\emph{finite} operational way to calculate the latter from data. \\nThe probability $p$, even when interpreted as a relative frequency, remains an abstraction. \\nOn the other hand, the\\ndegrees of belief, which is the basis of the \\\\emph{Bayesian} approach to statistics (see Lecture 2), are just that: the\\ndegree to which a rational being \\\\emph{ought} to believe in the veracity of a given statement. The word ``ought\" in the last sentence is important: probability theory, with probabilities interpreted\\nas degrees of belief, is \\\\emph{not} a model of how human beings actually reason in situations\\nof uncertainty; rather probability theory when interpreted this way is a normative theory in\\nthat it specifies how an idealized\\nreasoning being, or system, ought to reason when faced with uncertainty. \\nThere is a school of thought that argues that degrees of belief should be an individual\\'s own assessment of\\nher or his degree of belief in a statement, which are then to be updated using the probability rules. The problem with this position is that it presupposes probability theory to be a model of\\nhuman reasoning, which we argue it is not --- a position confirmed by numerous psychological experiments.\\nIt is perhaps better to think of degrees of belief as numbers that inform one\\'s reasoning rather than as numbers\\nthat describe it, and relative frequencies as numbers that characterize stochastic data generation\\nmechanisms. Both are probabilities and both are useful. \\n', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\nIn this lecture, we introduce the Bayesian approach to inference starting with a\\ndescription of its salient features and ending with a detailed example, again using the top quark\\ndiscovery data from D\\\\O. \\nThe main point to be understood about the Bayesian approach is that it is merely applied\\nprobability theory (see Sec.~\\\\ref{sec:prob}).\\nA method is Bayesian if\\n\\\\begin{itemize}\\n\\\\item it is based on the degree of belief interpretation of probability and\\n\\\\item it uses Bayes theorem\\n\\\\begin{align}\\np(\\\\theta, \\\\omega | D) & = \\\\frac{p(D|\\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega)}{p(D)}, \\\\\\\\\\n\\\\textrm{where} \\\\nonumber\\\\\\\\\\nD & = \\\\textrm{ observed data}, \\\\nonumber \\\\\\\\\\n\\\\theta & = \\\\textrm{ parameters of interest}, \\\\nonumber\\\\\\\\\\n\\\\omega & = \\\\textrm{ nuisance parameters}, \\\\nonumber\\\\\\\\\\np(\\\\theta, \\\\omega| D) & = \\\\textrm{posterior density}, \\\\nonumber\\\\\\\\\\n\\\\pi(\\\\theta, \\\\omega) & = \\\\emph{prior density (or prior for short)}. \\\\nonumber\\n\\\\end{align}\\n\\\\end{itemize}\\nfor \\\\emph{all} inferences. The result of a Bayesian inference is the posterior density\\n$p(\\\\theta, \\\\omega | D$ from which, if desired, various summaries can be extracted. The parameters can be discrete or continuous and nuisance parameters are eliminated by \\nmarginalization,\\n\\\\begin{align}\\np(\\\\theta | D) & = \\\\int p(\\\\theta, \\\\omega | D ) \\\\, d\\\\omega, \\\\\\\\\\n& \\\\propto \\\\int p(D | \\\\theta, \\\\omega) \\\\, \\\\pi(\\\\theta, \\\\omega) \\\\, d\\\\omega. \\\\nonumber\\n\\\\end{align}\\nThe function $\\\\pi(\\\\theta, \\\\omega)$, called the prior, encodes whatever information we have\\nabout the parameters $\\\\theta$ and $\\\\omega$ independently of the data $D$. A key\\nfeature of the Bayesian approach is recursion; the use of\\nthe posterior density $p(\\\\theta, \\\\omega|D)$ or one, or more, of its marginals as the prior in a subsequent analysis. \\nThese simple rules yield an extremely powerful and general inference model. Why\\nthen is the Bayesian approach not more widely used in particle physics? The \\nanswer is partly historical: the frequentist approach was dominant at the dawn of particle physics. It is also partly the widespread perception that the Bayesian\\napproach is too subjective to be useful for scientific work. However, there is published\\nevidence\\nthat this view is mistaken, witness the success of Bayesian methods in \\nhigh-profile analyses in particle physics such as the discovery of single top quark\\nproduction at the Tevatron~\\\\cite{Abazov:2009ii, Aaltonen:2009jj}.\\n', \"\\\\section{Upper limits}\\n\\\\subsection{Limits in the presence of background}\\nThis is where it gets tricky.\\nTypically an experiment may observe $N_D$ events, with an expected background $N_B$ and efficiency $\\\\eta$, and wants to present results for $N_S={N_D-N_B \\\\over \\\\eta}$.\\nUncertainties in $\\\\eta$ and $N_B$ are handled by profiling or marginalising.\\nThe problem is that the \\n{\\\\it actual number} of background events is not $N_B$ but Poisson in $N_B$.\\nSo in a straightforward case, if you observe twelve events, with expected background 3.4 and $\\\\eta=1$\\nit is obviously sensible to say $N_S=8.6$\\n(though the error is $\\\\sqrt{12}$ not $\\\\sqrt{8.6}$)\\nBut suppose, with the same background, you see four events, three events or zero events.\\nCan you say $N_S=0.6$? or $-0.4$? Or $-3.4$???\\nWe will look at four methods of handling this, considering as an example the observation of three events with expected background 3.40 and wanting to present the 95\\\\ \\n\\\\subsubsection{Method 1: Pure frequentist}\\n$N_D-N_B$ is an unbiased estimator of $N_S$ and its properties are known.\\nQuote the result. Even if it is non-physical.\\nThe argument for doing so\\nis that\\nthis is needed for balance: if there is really no signal, approximately half of the experiments will give positive values and half negative. \\nIf the negative results are not published, but the positive ones are, the world average will be spuriously high.\\nFor a 95\\\\clearly one of them. So what?\\nA counter-argument is that if\\n$N_D<N_B$, we {\\\\it know} that the background has fluctuated downwards. But this cannot be incorporated \\ninto the formalism.\\nAnyway, the upper limit from 3 is 7.75, as $\\\\sum_0^3 e^{-7.75}7.75^r/r! = 0.05$, and the \\n95\\\\\\n\\\\subsubsection{Method 2: Go Bayesian}\\nAssign a uniform prior to $N_S$, for $N_S>0$, zero for $N_S<0$.\\nThe posterior is then just the likelihood, $P(N_S | N_D,N_B)=e^{-(N_S+N_B)}{(N_S+N_B)^{N_D} \\\\over N_D!}$.\\nThe required limit is obtained from integrating $\\\\int_0^{N_{hi}} P(N_S)\\\\, dN_S = 0.95$\\nwhere\\n$P(N_S)\\\\propto e^{-(N_s+3.40)}{(N_s+3.4)^3 \\\\over 3!}$; this is illustrated in Fig.~\\\\ref{fig:Bayeslimit}\\nand the value of the limit is \\n5.21.\\n\\\\subsubsection{Method 3: Feldman-Cousins}\\nThis---called `the unified approach' by Feldman and Cousins~\\\\cite{FC}---takes a step backwards\\nand considers the ambiguity in the use of confidence belts. \\nIn principle, if you decide to work at, say, 90\\\\This is shown in Fig.~\\\\ref{fig:FC1}.\\nIn practice, if you happen to get a low result you would quote an upper limit, but if you get a high result you would quote a central limit.\\nThis, which they call `flip-flopping', is illustrated in the plot by a break shown here for $r=10$. \\nNow the confidence belt is the green one for $r< 10$ and the red one for $r\\\\geq 10$. The\\nprobability of lying in the band is no longer 90\\\\Flip-flopping invalidates the Frequentist construction, leading to undercoverage. \\nThey show how to avoid this. You draw the plot slightly differently:\\n$r \\\\equiv N_D$ is still the horizontal variable, but as the vertical variable you use $N_S$. \\n(This means a different plot for any different $N_B$, whereas the previous Poisson plot is universal, but this is not a problem.)\\nThis is to be filled using $P(r;N_s)=e^{-(N_s+N_B)}{(N_S+N_B)^r \\\\over r!}$\\\\ .\\nFor each $N_S$ you define a region $R$ such that $\\\\sum_{r\\\\epsilon R}P(r;N_s) \\\\geq 90\\\\You have a choice of strategy that goes beyond `central' or `upper limit': one \\nplausible suggestion would be to\\nrank $r$ by probability and take them in order until the desired total probability content is achieved (which would, incidentally, give the shortest interval).\\nHowever this has the drawback that outcomes with $r < N_B$ will have small probabilities and be excluded for all $N_S$, so that, if such a result does occur, one cannot say anything constructive, just `This was unlikely'. \\nAn improved form of this suggestion is that for each $N_S$, considering each $r$ you compare $P(r;N_S)$ with the largest possible value obtained by varying $N_S$. This is easier than it sounds because this highest value is either at $N_S=r-N_B$ (if $r\\\\geq N_B$) or $N_S=0$ (if $r\\\\leq N_B$ ).\\nRank on the ratio $P(r;N_S)/P(r;N^{best}_S)$ and again take them in order till their sum gives the desired probability.\\nThis gives a band as shown in Fig.~\\\\ref{fig:FC2}, which has $N_B=3.4$. You can see that \\n`flip-flopping' occurs naturally: for small values of $r$ one just has an upper limit, whereas for larger values, above $r=7$, one obtains a lower limit as well. Yet there is a single band, and the coverage is\\ncorrect (i.e. it does not undercover).\\nIn the case we are considering, $r=3$, just an upper limit is given, at $4.86$. \\nLike other good ideas, this has not found universal favour. Two arguments are raised against the method.\\nFirst, that it deprives the physicist of the choice of whether to publish an upper limit or a range. \\nIt could be embarrassing if you\\nlook for something weird and are `forced' to publish a non-zero result. \\nBut this is actually the point, and in such cases one can always explain\\nthat the limits should not be taken as implying that the quantity actually is nonzero.\\nSecondly, if two experiments with different $N_B$ get the same small $N_D$, the one with the higher $N_B$ will quote a smaller limit on $N_S$. The worse experiment gets the better result, which is clearly unfair!\\nBut this is not comparing like with like: for a `bad' experiment with large background to get a small number of events is much less likely than it is for a `good' low background experiment.\\n\\\\subsubsection {Method 4: $CL_s$}\\nThis is a modification of the standard frequentist approach to include the \\nfact, as mentioned above, that a small observed signal implies a downward \\nfluctuation in background~\\\\cite{Read}. Although presented here using just numbers of events, the method is usually extended to use the full likelihood of the result, as will be discussed in Section~\\\\ref{subsection:Extension}.\\nDenote the (strict frequentist) \\nprobability of getting a result this small (or less) from $s+b$ events as \\n$CL_{s+b}$, and the equivalent probability from pure background as \\n$CL_b$ (so \\n$CL_b=CL_{s+b}$ for $s=0$).\\nThen introduce\\n\\\\begin{equation}\\nCL_s={CL_{s+b} \\\\over CL_b}\\n\\\\quad.\\n\\\\end{equation}\\nLooking at Fig.~\\\\ref{fig:CLS}, the $CL_{s+b}$ curve shows that if $s+b$ is small then the probability of getting three events or less is high, near 100\\\\the probability of only getting three events or less is only 5\\\\\\nThe point $s+b=3.4$ corresponds to $s=0$, at which the probability $CL_b$ is 56\\\\incorporate this by renormalizing the (blue) $CL_{s+b}$ curve to have a maximum of 100\\\\physically sensible region, dividing it by 0.56 to get the (green) $CL_s$ curve.\\nThis is treated in the same way as the $CL_{s+b}$ curve, reading off the point at $s+b=8.61$ where it falls to 5\\\\This is larger than the strict frequentist limit: the method over-covers (which, as we have seen, is allowed if not encouraged)\\nand is, in this respect `conservative'\\\\footnote{`Conservative' is a misleading word. It is used by people \\ndescribing their analyses to \\nimply safety and caution, whereas it usually entails cowardice and sloppy thinking.}. This is the same value as the Bayesian Method 2, as it makes the same assumptions. \\n$CL_s$ is not frequentist, just `frequentist inspired'. In terms of statistics there is perhaps little in its favour. But it has an intuitive appeal, and is widely used.\\n\\\\subsubsection{Summary so far}\\nGiven three observed events, and an expected background of 3.4 events, what is\\nthe 95\\\\Possible answers are shown in table~\\\\ref{tab:summary}.\\n\\\\begin{table}[h]\\n\\\\begin{center}\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nStrict Frequentist & 4.35 \\\\\\\\\\nBayesian (uniform prior) & 5.21 \\\\\\\\\\nFeldman-Cousins & 4.86 \\\\\\\\\\n$CL_s$ & 5.21 \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{ Upper limits from different methods}\\n\\\\end{table}\\nWhich is `right'? Take your pick!\\nAll are correct. (Well, not wrong.). The golden rule is to say what you are doing, and if possible give the raw numbers. \\n\\\\subsubsection{Extension: not just counting numbers}\\nThese examples have used \\nsimple counting experiments. But a simple number does not (usually) exploit the full information.\\nConsider the illustration in Fig.~\\\\ref{fig:beyondsimple}. One is searching for (or putting an upper limit on) some broad resonance around 7~GeV. One could count the number of events inside some window\\n(perhaps 6 to 8~GeV?) and subtract the estimated background. This might work with high statistics, as in the left, but would be pretty useless with small numbers, as in the right. It is clearly not optimal \\njust to count an event as `in', whether it is at 7.0 or 7.9, and to treat an event as `out', if it is at 8.1 or \\n10.1.\\nIt is better to calculate the \\nLikelihood $\\\\ln L_{s+b}=\\\\sum_i \\\\ln{N_s S(x_i)+N_b B(x_i)} \\\\quad;\\\\quad \\\\ln{ L_b}=\\\\sum_i \\\\ln{N_b B(x_i)}$.\\nThen, for example using $CL_s$, you can work with $L_{s+b}/L_b$, or $-2 \\\\ln{(L_{s+b}/L_b)}$.\\nThe confidence/probability quantities can be found from simulations, or sometimes from data.\\n\\\\subsubsection{Extension: From numbers to masses}\\nLimits on numbers of events can readily be translated into limits on branching ratios,\\n$BR={N_s \\\\over N_{total}}$,\\nor limits on cross sections,\\n$\\\\sigma={N_s \\\\over \\\\int {\\\\cal L} dt}$\\\\ .\\nThese may translate to limits on other, theory, parameters.\\nIn the Higgs search (to take an example) the cross section depends on the mass, $M_H$---and so does the detection efficiency---which may require changing strategy (hence different backgrounds). This leads to the need\\nto basically repeat the analysis for all (of many) $M_H$ values. This can be presented in two ways. \\nThe first is shown in Fig.~\\\\ref{fig:significanceplot}, taken from Ref.~\\\\cite{ATLAS1}. For each $M_H$ (or whatever is being studied) you search for a signal and plot the $CL_s$ (or whatever limit method you prefer) significance \\nin a {\\\\it Significance Plot}. \\nSmall values indicate that it is unlikely to get a signal this large just from background.\\nOne often also plots the expected (from MC) significance, assuming the signal hypothesis is true. This is a measure of a `good experiment'. In this case there is a discovery level \\ndrop at $M_H \\\\approx 125$~GeV, which exceeds the expected significance, though not by much: ATLAS were lucky but not incredibly lucky.\\nThe second method is---for some reason---known as the green-and-yellow plot.\\nThis is basically the same data, but fixing $CL$ at a chosen value: in Fig.~\\\\ref{fig:greenandyellow} it is 95\\\\ You find the limit on signal strength, at this confidence level, and interpret it as \\na limit on the cross section $\\\\sigma / \\\\sigma_{SM}$.\\nAgain, as well as plotting the actual data one also plots the expected (from MC) limit, with variations.\\nIf there is no signal, 68\\\\ \\nSo this figure shows the experimental result as a black line. Around 125~GeV the 95\\\\is more than the Standard Model prediction indicating a discovery. There are peaks between 200 and 300~GeV, but they do not approach the SM value, indicating that they are just fluctuations. The value rises at 600~GeV, but the green (and yellow) bands rise also, showing that the experiment is not\\nsensitive for such high masses: basically it sees nothing but would expect to see nothing.\"}},\n",
       "       {'entity_name': 'central intervals', 'entity_type': 'analysis_technique', 'description': 'A type of confidence interval defined by the Neyman construction, where the probability of observing values below or above the interval is equal.', 'relevant_passages': {\"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\"}},\n",
       "       {'entity_name': 'rootn interval', 'entity_type': 'analysis_technique', 'description': 'A simple method for constructing confidence intervals based on the square root of the observed count, providing a quick approximation for interval boundaries.', 'relevant_passages': {\"\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Confidence Intervals}\\nThe confidence interval is a concept best explained by example. Consider an experiment\\nthat observes $D$ events with expected (that is, mean) signal $s$ and no background. Neyman devised a way to make statements of the form\\n\\\\begin{align}\\ns \\\\in [ l(D), \\\\, u(D) ],\\n\\\\end{align}\\nwith the \\\\emph{a priori} guarantee that at least a fraction $p$ of them will be true, as\\nrequired by the frequentist principle. A procedure for constructing such\\nintervals is called a \\\\textbf{Neyman construction}. The frequentist principle \\nmust hold for any ensemble of experiments, not necessarily all making the same kind of\\nobservations and statements. For simplicity, however, we shall presume the\\nexperiments to be of the same kind and to be completely specified by a single unknown\\nparameter $s$. The\\nNeyman construction is illustrated in Fig.~\\\\ref{fig:neyman}. \\nThe construction proceeds as follows. Choose a value of $s$ and use some rule to find\\nan interval in the space of observations (or, more generally, a region), for example, the\\ninterval defined by the two vertical lines in the center of the figure, such that the probability to obtain a count in this interval is $f \\\\geq p$, where $p$ is the desired confidence level. We move to another\\nvalue of $s$ and repeat the procedure. The procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range. When this is done, as illustrated in Fig.~\\\\ref{fig:neyman}, the intervals of probability content\\n$f$ will form a band in the Cartesian product of the parameter space and the observation space.\\nThe upper edge of this band defines the curve $u(D)$, while the lower edge defines the curve\\n$l(D)$. These curves are the product of the Neyman construction.\\nFor a given value of the parameter of interest $s$, the interval with probability content $f$ in the space\\nof observations\\nis not unique; different rules for choosing the interval will, in general, yield different intervals. Neyman suggested choosing the interval so that the probability to obtain an observation below or above\\nthe interval are the same. The Neyman rule yields the so-called \\\\textbf{central intervals}. \\nOne virtue of central intervals is that their boundaries can be more efficiently calculated by\\nsolving the equations,\\n\\\\begin{align}\\nP(x \\\\leq D | u) & = \\\\alpha_L, \\\\nonumber\\\\\\\\\\nP(x \\\\geq D | l) & = \\\\alpha_R,\\n\\\\end{align}\\na mathematical fact that becomes clear after staring at Fig.~\\\\ref{fig:neyman} long enough.\\nAnother rule was suggested by Feldman and Cousins~\\\\cite{FC}. For our example, the Feldman-Cousins\\nrule requires that the potential observations $\\\\{D\\\\}$ be ordered in descending order, $D_{(1)}, D_{(2)}, \\\\cdots$, of the likelihood ratio $p(D | s) / p(D | \\\\hat{s})$, where\\n$\\\\hat{s}$ is the maximum likelihood estimator (see Sec.~\\\\ref{sec:profile}) of the parameter $s$.\\nOnce ordered, we compute the running sum $f = \\\\sum_j p(D_{(j)} | s)$ until $f$ equals or just exceeds the desired\\nconfidence level $p$. This rules does not guarantee that the potential observations $D$ are\\ncontiguous, but this does not matter because we simply take the minimum element of the set $\\\\{ D_{(j)} \\\\}$ to be\\nthe lower bound of the interval and its maximum element to be the upper bound. \\nAnother simple rule is the mode-centered rule: order $D$ in descending order of $p(D| s)$ and \\nproceed as with the Feldman-Cousins rule. \\nIn principle, absent criteria for choosing a rule, there is nothing\\nto prevent the use of \\\\emph{ordering rules} randomly chosen for different values of $s$! \\nFigure~\\\\ref{fig:ciwidths} compares the widths of the\\nintervals $[l(D), u(D)]$ for three different ordering rules, central, Feldman-Cousins, and mode-centered as a function of the count $D$. It is instructive to compare these widths with those provided by\\nthe well-known root(N) interval, $l(D) = D - \\\\sqrt{D}$ and $u(D) = D + \\\\sqrt{D}$. Of the three sets of intervals, the ones suggested by Neyman are the widest, the Feldman-Cousins and mode-centered ones are of similar width, while the root(N) intervals are the shortest. So why are we going through\\nall the trouble of the Neyman construction? We shall return to this question shortly.\\nHaving completed the Neyman construction and found the curves $u(D)$ and $l(D)$ \\nwe can use the latter to make statements of\\nthe form $s \\\\in [l(D), \\\\, u(D)]$: for a given observation $D$, we simply read off\\nthe interval $[l(D), u(D)]$ from the curves. For example, suppose in Fig.~\\\\ref{fig:neyman} that the true value of $s$ is\\nrepresented \\nby the horizontal line that intersects the curves $u(D)$ and $l(D)$ and which therefore defines\\nthe interval demarcated by the two vertical lines. If the observation $D$ happens to fall in the interval to the left of the left vertical line, or to the right of the right vertical line, then the interval\\n$[l(D), \\\\, u(D)]$ will not bracket $s$. However, if $D$ falls between the two vertical\\nlines, the interval $[l(D), \\\\, u(D)]$ will bracket $s$. Moreover, by virtue of the Neyman construction, a fraction $f$ of the intervals $[l(D), \\\\, u(D)]$ will bracket the value of $s$ whatever its value happens to be, which brings us back to the question about the root(N) intervals. Figure~\\\\ref{fig:coverage} shows the coverage probability over the parameter space of $s$. As expected,\\nthe three rules, Neyman's, that of Feldman-Cousins, and the mode-centered, satisfy the condition coverage probability $\\\\geq$ confidence level over all values of $s$ that are\\npossible \\\\emph{a priori}; that is, the intervals cover. However, the root(N) intervals do not and indeed fail badly for $ s < 2$.\\nHowever, notice that the coverage probability of the root(N) intervals bounces around the (68\\\\seem that using the root(N) intervals may not be that bad after all. Whether it is or not depends entirely on one's\\nattitude towards the frequentist principle. Some will lift mountains and carry them to the Moon \\nin order to achieve exact coverage, \\nwhile others, including the author, is entirely happy with coverage that bounces around a little.\\n\\\\paragraph*{Discussion}\\nWe may summarize the content of the Neyman construction with\\na statement of the form:\\nthere is a probability of at least $p$ that\\n$s \\\\in [l(D), \\\\, u(D)]$. But it would be a misreading of the statement to presume it is about\\nthat particular interval. It is not because $p$, as noted, is a property of the ensemble to which this \\nstatement belongs. The precise statement is this: $s \\\\in [l(D), \\\\, u(D)]$ is a member of an (infinite) ensemble of statements a fraction $f \\\\geq p$ of which are true. This mathematical fact is the\\nprincipal reason why the frequentist approach is described as objective; the probability $p$ is something for which there seems, in principle, to be an operational definition: we just count how many\\nstatements of the form $s \\\\in [l(D), \\\\, u(D)]$ are true and divide by the total number of\\nstatements. Unfortunately, in the real world this procedure cannot be realized because\\nin general\\nwe are not privy to which statements are true and, even if we came\\ndown from a mountain with the requisite knowledge, we would need\\nto examine an infinite number of statements, which is impossible. Nevertheless, the\\nNeyman\\nconstruction is a\\nremarkable procedure that always yields exact coverage for any problem that\\ndepends on a \\\\emph{single} unknown parameter.\\nMatters quickly become less tidy, however, when a probability model contains more than\\none unknown\\nparameter. In almost every particle physics experiment there is background that is usually not\\nknown precisely. Consequently, even for the simplest experiment we must contend with\\nat least two parameters, the expected signal $s$ and the expected background $b$,\\nneither of which is known. Neyman required a procedure to cover whatever the value of \\\\emph{all} the parameters be they known or unknown.\\nThis is a very tall order, which cannot be met in general. In practice, we resort to\\napproximations, the most widely used of which is the profile likelihood to which we now turn.\\n\"}},\n",
       "       {'entity_name': 'hypothesis testing', 'entity_type': 'analysis_technique', 'description': 'A statistical method used to determine the validity of a hypothesis by comparing observed data against a null hypothesis. This technique assesses whether there is sufficient evidence in a sample to infer that a specific condition holds for the entire population, often involving the comparison of data compatibility with both null and alternative hypotheses, and is essential for making claims about new physics.', 'relevant_passages': {\"\\\\section{Introduction}\\nIn a particle accelerator, fundamental physical processes unfold, producing a variety of particles that characterize the final state of each event. These particles are reconstructed through detectors, which assign kinematic and dynamic variables to each event, such as the particle's position, energy deposition, and transverse momentum. These physical observables are typically defined by counting events in various observation channels~\\\\cite{read2002presentation,cowan2014statistics}. For example, measuring the invariant mass of a system within the 100-300 GeV range, with a 10 GeV resolution, yields 20 observation channels, each characterized by its event count.\\nFrom a physical perspective, various processes can contribute to the event count in any given observation channel. These processes may include known phenomena, as predicted by the Standard Model, or new physical processes beyond the model's current scope~\\\\cite{feldman1998unified, lista2016practical}. A central aspect of experimental physics analysis is determining which known processes contribute to a specific observable and identifying potential new processes to explain any deviations from established theories.\\nTo evaluate whether reconstructed events align with existing theories or if they indicate the need for alternative explanations, inferential statistical tools, such as parameter estimation and hypothesis testing, are employed~\\\\cite{cowan2014statistics, cranmer2015practical,barlow2019practical}. In this report, we describe and implement, using Python and RooFit~\\\\cite{verkerke2006roofit,schott2012roostats}, a range of statistical methods used in high-energy physics to estimate the sensitivity of new models and to define exclusion limits for these models.\\nThe development of particle physics has led to three main fields: theory, phenomenology, which links theory to experiments, and experimental work. A primary goal of phenomenology is to guide and optimize experimental searches for new particles based on theoretical models~\\\\cite{conway2005calculation, cranmer2015practical}. This strategy generally involves evaluating a model's sensitivity, identifying a kinematic region (signal region) with the highest potential for discovering new physics. Additionally, exclusion limits are established, defining the parameter space where the model is ruled out based on the expected event counts, typically at a 95\\\\\\nIn the experimental phase, the upper limit is calculated based on observed event counts rather than expected ones. After observation, two outcomes are possible: (1) the data conform to the established model, or (2) the data exhibit a discrepancy that cannot be explained by statistical fluctuations alone. In the first scenario, where data align with existing theories~\\\\cite{cowan2014statistics, casadei2011statistical}, the observed and expected limits are similar, providing no substantial evidence to support new theories, thus excluding them within a specific parameter region.\\nIn the second scenario, any significant discrepancy between data and theory is evaluated using the $5\\\\sigma$ threshold (corresponding to a $p$-value of $2.5 \\\\times 10^{-7}$)~\\\\cite{cowan2014statistics,jme2010cms}, which measures the probability of observing such data (or more extreme results) under the assumption that the current theory is correct. If this threshold is exceeded, a discovery can be claimed. Figure[\\\\ref{fig:1}] illustrates the general research framework in high-energy physics, highlighting the role of statistical tools in excluding models or detecting new physics.\\nIn the statistical modeling of new physical theories and their observations, a key source of uncertainty arises from systematic errors. These errors encompass factors related to the characteristics of the accelerator, particle detectors, and intrinsic model parameters, such as parton distribution functions (PDFs)~\\\\cite{junk1999confidence, cranmer2015practical}. Systematic uncertainties typically reduce the ability to exclude models and to detect new physics. Therefore, incorporating systematic effects is essential for achieving more accurate estimates\\\\cite{barlow2002systematic, read2002presentation, lista2016practical}.\\nThis document presents various models for estimating upper limits and significance, organized into two conceptual categories: single-channel and multichannel experiments, both with and without sources of systematic uncertainty. First, for single-channel experiments without systematic effects, we describe the frequentist and Bayesian approaches traditionally applied in experiments such as the former LEP collider and, more recently, the Large Hadron Collider (LHC)~\\\\cite{read2002presentation, atlas2012observation}. Next, we extend these methods to multichannel experiments, where the increase in dimensionality necessitates optimization strategies and Monte Carlo methods. Finally, we examine systematic effects that require a Bayesian approach, as well as the derivation of profile likelihoods through optimization processes\\\\cite{conway2005calculation}.\\n\", \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", '\\\\section{Lecture 2: The Frequentist and Bayesian Approaches}\\n\\\\subsection{The Frequentist Approach}\\n\\\\subsubsection{Hypothesis Tests}\\nIt is hardly possible in experimental particle physics to avoid the testing of hypotheses, testing\\nthat invariably leads to decisions. For example, electron identification entails hypothesis testing; given data\\n$D$ we ask: is this particle an isolated electron or is it not an isolated electron? Then we\\ndecide whether or not it is and proceed on the basis of the decision that has been made.\\nIn the discovery of the Higgs boson, we had to test whether, given the data available in early summer 2012, the Standard Model without a Higgs boson, a somewhat ill-founded background-only model, or\\nthe Standard Model with a Higgs boson, the background $+$ signal model, was\\nthe preferred hypothesis. We decided that the latter model was preferred and announced the\\ndiscovery of a new boson. Given the ubiquity of hypothesis testing, it is important to have\\na grasp of the methods that have been invented to implement it.\\nOne method was due to Fisher~\\\\cite{Fisher}, another was\\ninvented by Neyman, and a third (Bayesian) method was proposed by Sir Harold Jeffreys, all around the same time.\\nToday, we tend to merge the approaches of Fisher and Neyman, and we hardly ever\\nuse the method of Jeffreys even though in several respects the method of Jeffreys and their modern variants are arguably more natural. In particle physics, we regard our\\nFisher/Neyman\\nhybrid as sacrosanct, witness the near-religious adherence to the $5\\\\sigma$ discovery rule. However, the pioneers disagreed strongly with\\neach other about how to test hypotheses, which suggests that the topic is considerably more subtle than it seems. We first describe the method of Fisher, then follow with a description of the method of\\nNeyman. For concreteness, we consider the problem of deciding between a background-only\\nmodel and a background $+$ signal model.\\n\\\\paragraph{Fisher\\'s Approach} In Fisher\\'s approach, we construct a \\\\textbf{null hypothesis}, often denoted by $H_0$,\\nand \\\\emph{reject} it should some measure be judged\\nsmall enough to cast doubt on the validity of this hypothesis. In our\\nexample, the null hypothesis is the background-only model, for example, the SM without a Higgs boson. The measure is called a \\\\textbf{p-value} and is defined by\\n\\\\begin{align}\\n\\\\textrm{p-value}(x_0) = P( x > x_0| H_0), \\n\\\\end{align}\\nwhere $x$ is a statistic designed so that large values indicate\\ndeparture from the null hypothesis. This is illustrated in Fig.~\\\\ref{fig:pvalue1}, which shows\\nthe location of the observed value $x_0$ of $x$. The p-value is the probability that $x$ could\\nhave been higher than the $x$ actually observed.\\nIt is argued that a small p-value implies that either the null hypothesis is false or something rare has occurred. If \\nthe p-value is extremely\\nsmall, say $\\\\sim 3 \\\\times 10^{-7}$, then of the two possibilities the most common response\\nis to presume the null to be false. If we apply this method to the D\\\\O\\\\ top quark discovery data, and \\nneglect the uncertainty in null hypothesis, we find\\n\\\\begin{align*}\\n\\\\textrm{p-value} & = \\\\sum_{D=17}^\\\\infty \\\\textrm{Poisson}(D, 3.8) = 5.7 \\\\times 10^{-7}.\\n\\\\end{align*}\\nIn order to report a more intuitive number, the common\\npractice is to map the p-value to the $Z$ scale defined by \\n\\\\begin{align}\\nZ & = \\\\sqrt{2} \\\\, \\\\textrm{erf}^{-1}(1 - 2\\\\textrm{p-value}).\\n\\\\end{align}\\nThis is the number of Gaussian standard deviations\\naway from the mean\\\\footnote{$\\\\textrm{erf}(x) = \\\\frac{1}{\\\\sqrt{\\\\pi}} \\\\int_{-x}^x \\\\exp(-t^2) \\\\, dt$ is the error funtion.}. \\nA p-value of $5.7 \\\\times 10^{-7}$ corresponds to a $Z$ of $4.9\\\\sigma$. The $Z$-value can be\\ncalculated using the {\\\\tt Root} function $$Z = \\\\textrm{\\\\tt -TMath::NormQuantile(p-value)}.$$\\n\\\\paragraph{Neyman\\'s Approach}\\nIn Neyman\\'s approach \\\\emph{two} hypotheses are considered, the null hypothesis $H_0$ and\\nan alternative hypothesis $H_1$. This is illustrated in Fig.~\\\\ref{fig:neymantest1}. In our\\nexample, the null is the same as before but the alternative hypothesis is the SM with a Higgs boson. \\nAgain, one generally chooses $x$ so that large values would cast doubt on \\nthe validity of $H_0$. However, the Neyman test is specifically designed to\\nrespect the frequentist principle, which is done as follows. A \\\\emph{fixed} probability $\\\\alpha$ is\\nchosen, which corresponds to some threshold value $x_\\\\alpha$ defined by\\n\\\\begin{align}\\n\\\\alpha & = P( x > x_\\\\alpha | H_0),\\n\\\\end{align}\\ncalled the significance (or size) of the test. Should the observed value $x_0 > x_\\\\alpha$, or\\nequivalently, p-value($x_0$) $< \\\\alpha$, the hypothesis $H_0$ is rejected in favor of the\\nalternative. \\nIn \\nparticle physics, in addition to applying the Neyman hypothesis test, we also report the\\np-value. This is sensible because there is a more information in the p-value than merely reporting the fact that a null hypothesis was rejected at a significance level of $\\\\alpha$. \\nThe Neyman method satisfies the frequentist principle by construction. Since the significance of the test is fixed, $\\\\alpha$ is the relative frequency with which true\\nnull hypotheses would be rejected and is called the \\\\textbf{Type I} error rate. \\nHowever, since\\nwe have specified an alternative hypothesis there is more that can be said. Figure~\\\\ref{fig:neymantest1} shows that we can also calculate\\n\\\\begin{align}\\n\\\\beta & = P( x \\\\leq x_\\\\alpha | H_1),\\n\\\\end{align}\\nwhich is the relative frequency with which we would reject the hypothesis $H_1$ if it is true.\\nThis mistake is called a \\\\textrm{Type II} error. The quantity $1 - \\\\beta$ is called the\\n\\\\textbf{power} of the test and is the relative frequency with which we would accept the hypothesis\\n$H_1$ if it is true. Obviously, for a given $\\\\alpha$ we want to maximize the power. Indeed, this\\nis the basis of the Neyman-Pearson lemma (see for example Ref.~\\\\cite{James}), which asserts that given two simple hypotheses --- that is, hypotheses in which all parameters have well-defined values --- the optimal statistic $t$ to use in the hypothesis test is the likelihood ratio\\n$t = p(x|H_1) / p(x | H_0)$. \\nMaximizing the power seems sensible. Consider\\nFig.~\\\\ref{fig:neymantest2}. The significance of the test in this figure is the same as \\nthat in Fig.~\\\\ref{fig:neymantest1}, so the Type I error rate is identical. However, the Type II error\\nrate is much greater in Fig.~\\\\ref{fig:neymantest2} than in Fig.~\\\\ref{fig:neymantest1}, that is, the power\\nof the test is considerably weaker in the former. In that case, there may be no compelling reason to reject the null since the alternative is not that much better. This insight was one source\\nof Neyman\\'s disagreement with Fisher. Neyman objected to possibility that one might reject a null hypothesis regardless\\nof whether it made sense to do so. Neyman insisted that the task is always one of\\ndeciding between competing hypotheses. Fisher\\'s counter argument was that an alternative\\nhypothesis may not be available, but we may nonetheless wish to know whether the only\\nhypothesis that is available is worth keeping. As we shall see, the Bayesian approach\\nalso requires an alternative, in agreement with\\nNeyman, but in a way that neither he nor Fisher agreed with!\\nWe have assumed that the hypotheses $H_0$ and $H_1$ are simple, that is, fully specified. \\nUnfortunately, most of the hypotheses that arise in realistic particle physics analyses are not of this kind. In the Higgs boson discovery analyses by ATLAS and CMS the probability models depend on many nuisance parameters for which only estimates are available. Consequently, neither the background-only nor the background $+$ signal hypotheses are fully specified.\\nSuch hypotheses are called\\n\\\\textbf{compound hypotheses}. \\nIn order\\nto illustrate how hypothesis testing proceeds in this case, we again turn again to the top discovery example.\\n\\\\begin{quote}\\n\\\\paragraph*{Example}\\nAs we saw in Sec.~\\\\ref{sec:profile}, the standard way to handle nuisance\\nparameters in the frequentist approach is to replace them by their conditional MLEs and thereby reduce the likelihood \\nfunction to the profile likelihood. In the top discovery example, we obtain a function $p_{PL}(D | s)$ that depends on the single\\nparameter, $s$. We now treat this function as if it were a likelihood and\\ninvoke both the Neyman-Pearson lemma, which suggests the use of likelihood ratios, and Wilks\\' theorem to motivate the use of the \\nfunction $t(x, s)$ given in Eq.~(\\\\ref{eq:wilks}) to distinguish between two hypotheses:\\nthe hypothesis $H_1$ in which $s = \\\\hat{s} = N - B$ and the hypothesis $H_0$ in which $s \\\\neq \\\\hat{s}$, for example,\\nthe background-only hypothesis $s = 0$. In the context of testing, $t(x, s)$ is called\\na \\\\textbf{test statistic}, which, unlike a statistic as we have defined it (see Sec.~\\\\ref{sec:statistics}), usually depends on at least one unknown parameter.\\nIn principle, the next step is the computationally arduous task of simulating the distribution\\nof the statistic $t(x, s)$. The task is arduous because \\\\emph{a priori} the probability density \\n$p(t| s, b)$ can depend on \\\\emph{all} the parameters\\nthat exist in the original likelihood. If this is really the case, then after all this effort we seem to have achieved a pyrrhic victory! But, this is where Wilks\\' theorem saves the day, at least approximately. We can avoid the burden of simulating $t(x, s)$ because the latter is\\napproximately a $\\\\chi^2$ variate.\\nUsing $N = 17$ and $s = 0$, we find $t_0 = t(N=17, s = 0) = 4.6$. According to the\\nresults shown in\\nFig.~(\\\\ref{fig:toppl})(a), $N = 17$ may \\ncan be considered ``a lot of data\"; therefore, we may use $t_0$ to implement a hypothesis test by comparing $t_0$ with a fixed value\\n$t_\\\\alpha$ corresponding to the significance level $\\\\alpha$ of the test. \\n\\\\end{quote', '\\\\section{Lecture 3: The Bayesian Approach}\\n\\\\subsection{Model Selection}\\nConceptually, hypothesis testing in the Bayesian approach (also called model selection)\\nproceeds exactly the same way as any other Bayesian calculation: we compute the \\nposterior density,\\n\\\\begin{align}\\np(\\\\theta, \\\\omega, H | D) & = \\\\frac{p(D | \\\\theta, \\\\omega, H) \\\\, \\\\pi(\\\\theta, \\\\omega, H)} {p(D)},\\n\\\\end{align}\\nand marginalize it with respect to all parameters except the ones that label\\nthe hypotheses or models, $H$, \\n\\\\begin{align}\\np(H | D ) & = \\\\int p(\\\\theta, \\\\omega, H | D) \\\\, d\\\\theta \\\\, d\\\\omega.\\n\\\\end{align}\\nEquation~(\\\\ref{eq:pHD}) is\\nthe probability of hypothesis $H$ given the observed data $D$.\\nIn principle, the parameters $\\\\omega$ could also depend on $H$. For example, suppose\\nthat $H$ labels different parton distribution function (PDF) models, say CT10, MSTW, and\\nNNPDF, then $\\\\omega$ would indeed depend on the PDF model and should be written\\nas $\\\\omega_H$.\\nIt is usually more convenient to arrive at the probability $p(H|D)$ in stages.\\n\\\\begin{enumerate}\\n\\\\item Factorize the prior in the most convenient form,\\n\\\\begin{align}\\n\\\\pi(\\\\theta, \\\\omega_H, H) & = \\\\pi(\\\\theta, \\\\omega_H | H) \\\\, \\\\pi(H), \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\theta |\\\\omega_H, H) \\\\, \\\\pi(\\\\omega_H | H) \\\\, \\\\pi(H),\\\\\\\\\\n\\\\textrm{or} \\\\nonumber\\\\\\\\\\n& = \\\\pi(\\\\omega_H |\\\\theta, H) \\\\, \\\\pi(\\\\theta | H) \\\\, \\\\pi(H).\\n\\\\end{align}\\nOften, we can assume that the parameters of interest $\\\\theta$ are independent,\\n\\\\emph{a priori}, of both the nuisance\\nparameters $\\\\omega_H$ and the model label $H$, in which case we can write,\\n$\\\\pi(\\\\theta, \\\\omega_H, H) = \\\\pi(\\\\theta) \\\\, \\\\pi(\\\\omega_H|H) \\\\, \\\\pi(H)$.\\n\\\\item Then, for each hypothesis, $H$, compute the function\\n\\\\begin{align}\\np(D | H ) = \\\\int p(D | \\\\theta, \\\\omega_H, H) \\\\, \\\\pi(\\\\theta, \\\\omega | H) \\\\, d\\\\theta \\\\,\\nd\\\\omega.\\n\\\\end{align}\\n\\\\item Then, compute the probability of each hypothesis,\\n\\\\begin{align}\\np(H | D ) =\\\\frac{p(D | H) \\\\, \\\\pi(H)} {\\\\sum_H p(D | H) \\\\, \\\\pi(H)}.\\n\\\\end{align} \\n\\\\end{enumerate}\\nClearly, in order to compute $p(H | D)$ it is necessary to specify the priors $\\\\pi(\\\\theta, \\\\omega | H)$ and $\\\\pi(H)$. With some effort, it is possible to arrive at an acceptable form for\\n$\\\\pi(\\\\theta, \\\\omega | H)$, however, it is highly unlikely that consensus could ever be reached on the discrete prior\\n$\\\\pi(H)$. At best, one may be able to adopt a convention. For example, if by convention two hypotheses $H_0$ and $H_1$ are to be regarded as equally likely, \\\\emph{a priori},\\nthen it would make sense to assign $\\\\pi(H_0) = \\\\pi(H_1) = 0.5$.\\nOne way to circumvent the specification of the prior $\\\\pi(H)$ is to compare the probabilities,\\n\\\\begin{align}\\n\\\\frac{p(H_1 | D )}{p(H_0 | D)} =\\\\left[ \\\\frac{p(D | H_1)}{p(D | H_0} \\\\right] \\\\,\\n\\\\frac{ \\\\pi(H_1)} {\\\\pi(H_0)}.\\n\\\\end{align}\\nand use only the term in brackets, called the global \\\\textbf{Bayes factor}, $B_{10}$, as a way to\\ncompare hypotheses. The Bayes factor specifies by how much the relative probabilities\\nof two hypotheses changes as a result of incorporating new data, $D$. The word global \\nindicates that we have marginalized over all the parameters of the two models. The \\\\emph{local}\\nBayes factor, $B_{10}(\\\\theta)$ is defined by\\n\\\\begin{align}\\nB_{10}(\\\\theta) & = \\\\frac{p(D| \\\\theta, H_1)}{p(D| H_0)}, \\\\\\\\\\n\\\\textrm{where}, \\\\nonumber\\\\\\\\\\np(D| \\\\theta, H_1) & \\\\equiv \\\\int p(D | \\\\theta, \\\\omega_{H_1}, H_1) \\\\, \\\\pi(\\\\omega_{H_1} | H_1) \\\\, d\\\\omega_{H_1},\\n\\\\end{align}\\nare the \\\\textbf{marginal} or integrated likelihoods in which we have assumed the \\\\emph{a priori}\\nindependence of $\\\\theta$ and $\\\\omega_{H_1}$. We have further assumed\\nthat the marginal likelihood $H_0$ is independent of $\\\\theta$, which is a very\\ncommon situation. For example, $\\\\theta$ could be the expected signal count $s$,\\nwhile $\\\\omega_{H_1} = \\\\omega$ could be the expected background $b$. In this case, the\\nhypothesis $H_0$ is a special case of $H_1$, namely, it is the same as $H_1$ with $s = 0$. An hypothesis that is a special case of another \\nis said to be \\\\textbf{nested} in the more general hypothesis. The Bayesian example, discussed below, will\\nmake this clearer. \\nThere is a subtlety that may be missed: because of the way we have\\ndefined $p(D|\\\\theta, H)$, we\\nneed to multiply $p(D| \\\\theta, H)$ by the prior $\\\\pi(\\\\theta)$ and then integrate with respect\\nto $\\\\theta$ in order to calculate $p(D | H)$.\\n\\\\subsubsection{A Word About Priors}\\nConstructing a prior for nuisance parameters is generally neither controversial (for most parameters) nor problematic. Such difficulties as do arise occur when the priors must, of necessity,\\ndepend on expert judgement. For example, one theorist may\\ninsist that a uniform prior within a finite interval is a reasonable prior for the factorization scale in a QCD calculation, while in the expert judgement of another the interval should be twice as large.\\nClearly, in this case, there is no getting around the fact that the prior for this parameter is \\nunavoidably subjective. However, once a choice is made, a prior $\\\\pi(\\\\omega_H|H)$ that integrates to one can be constructed.\\nThe Achilles heal of the Bayesian approach is the need to specify the prior $\\\\pi(\\\\theta)$,\\nfor the parameters of interest,\\nat the start of the inference chain when we know almost nothing\\nabout these parameters. Careless specification of this prior can yield\\nresults that are unreliable or even nonsensical. The mandatory requirement is that \\nthe posterior density be proper, that is integrate to unity. Ideally, the same should hold\\nfor priors. A very extensive literature exists on the topic of prior specification\\nwhen the available information is extremely limited. However, a discussion of this\\ntopic is beyond the scope of these lectures; but, we shall make a few remarks.\\nFor model selection, we need to proceed with caution because\\nBayes factor are sensitive to the choice of priors and therefore less robust than posterior densities. Suppose that the prior $\\\\pi(\\\\theta) = C f(\\\\theta)$, where $C$ is a normalization\\nconstant. The global Bayes factor for the two hypotheses $H_1$ and $H_0$ can be written as\\n\\\\begin{align}\\nB_{10} = C \\\\frac{\\\\int p(D | \\\\theta, H_1) \\\\, f(\\\\theta) \\\\, d\\\\theta}{p(D | H_0)}.\\n\\\\end{align}\\nTherefore, if the constant $C$ is ill defined, typically because $\\\\int f(\\\\theta) \\\\, d\\\\theta = \\\\infty$,\\nthe Bayes factor will likewise be ill defined. For this reason, it is generally recommended\\nthat an improper prior not be used for parameters $\\\\theta$ that occur only in one hypothesis, here $H_1$. However, for parameters that are common to all hypotheses, it is permissible to\\nuse improper priors because the ill defined constant cancels in the Bayes factor.\\nThe discussion so far has been somewhat abstract. The next section therefore works through a detailed example of a possible Bayesian analysis of the D\\\\O\\\\ top discovery data.\\n', '\\\\section{Upper Limits for one channel experiment}\\n\\nIn scientific research, experiments are designed to collect data, and theories or models are developed to explain those observations. In general, the falsification of theories is based on hypothesis testing. Hypothesis tests determine, with a given confidence level ($CL$), whether the observed data provide sufficient evidence to reject an initial hypothesis, called the null hypothesis ($H_{0}$), in favor of an alternative hypothesis ($H_{1}$). The null hypothesis ($H_{0}$) is considered true until observations indicate otherwise; in such a case, the initial explanation is rejected, and the new theory ($H_{1}$) is accepted~\\\\cite{sinervo2002signal}. Both the frequentist and Bayesian approaches applied here yield robust upper limit estimations, adaptable to various experimental setups, making them vital tools for model testing and exclusion.\\nIn high-energy physics (HEP), the null hypothesis ($H_{0}$) refers to all known physical processes, which are summarized in what is known as the Standard Model. The alternative hypothesis ($H_{1}$) represents potential models that could explain new observations that the accepted model cannot account for, such as supersymmetry, extra dimensions, among others~\\\\cite{cowan2011asymptotic,florez2016probing}.\\nAdditionally, hypothesis testing requires the selection of a confidence level in terms of statistical significance ($\\\\alpha$).\\n\\\\begin{equation}\\nCL = 1 - \\\\alpha.\\n\\\\end{equation}\\nWhere $\\\\alpha$ (type I error) is the probability of rejecting the null hypothesis when it is true. By convention, model exclusion in particle physics is done for a value of $\\\\alpha = 0.05$, which corresponds to a confidence level ($CL$) of $95\\\\\\n\\\\begin{equation}\\n\\\\alpha = \\\\int_{P}^{\\\\infty} \\\\frac{1}{2\\\\pi} e^{-x^{2}/2} dx.\\n\\\\end{equation}\\nWhere $P$ is the percentile of the distribution, for which the type I error is $\\\\alpha = 0.05$. Figure~[\\\\ref{fig:2}] shows the standard normal distribution; the shaded area represents the type I error for the percentile $P_{95} \\\\approx 1.645$, which corresponds to the model exclusion condition at the $3\\\\sigma$ level.\\nThe confidence level for an observation is significantly higher. In general, the discovery threshold is set at $5\\\\sigma$, where the type I error is $\\\\alpha = 2.86 \\\\times 10^{-7}$. Table~[\\\\ref{tb:1}] summarizes different confidence levels and their interpretation in high-energy physics (HEP)~\\\\cite{lista2016practical,cranmer2015practical,cowan2011asymptotic}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccl}\\n\\\\hline\\n$\\\\alpha$ & $CL [\\\\ \\\\hline\\n$0.1586$ & $84.13$ & $1\\\\sigma$ & $H_{1}$ no excluded \\\\\\\\\\n$0.05$ & $95.00$ & $1.645\\\\sigma$ & $H_{1}$ excluded (Model exclusion) \\\\\\\\\\n$0.0227$ & $97.72$ & $2\\\\sigma$ & $H_{1}$ excluded \\\\\\\\ \\n$1.349 \\\\times 10^{-3}$ & $99.86$ & $3\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$3.167 \\\\times 10^{-5}$ & $99.99$ & $4\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$2.8665 \\\\times 10^{-7}$ & $99.99997$ & $5\\\\sigma$ & $H_{0}$ excluded (Observation) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Significance at different observation points. The exclusion of the alternative hypothesis requires a result statistical consistent with the background-only hypothesis ($H_{0}$), while confirmation of the observation requires compatibility with the signal + background hypothesis ($H_{1}$).}\\n\\\\end{center}\\n\\\\end{table}\\n'}},\n",
       "       {'entity_name': 'central limit theorem', 'entity_type': 'statistics_concept', 'description': 'A fundamental theorem in probability and statistics stating that the distribution of the sum (or average) of a large number of independent and identically distributed random variables will approximate a Gaussian distribution, regardless of the original distributions of the variables.', 'relevant_passages': {'\\\\section{Probability theory}\\n\\\\subsection{Commonly used distributions}\\nBelow a few examples of probability distributions are reported \\nthat are frequently used in physics and more in general in statistical\\napplications.\\n\\\\subsubsection{Gaussian distribution}\\nA {\\\\it Gaussian} or {\\\\it normal} distribution is given by:\\n\\\\begin{equation}\\ng(x;\\\\mu,\\\\sigma) = \\\\frac{1}{\\\\sigma\\\\sqrt{2\\\\pi}}e^{-{(x-\\\\mu)^2}/{2\\\\sigma^2}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu$ and $\\\\sigma$ are parameters equal to the average value and standard deviation of $x$, respectively.\\nIf $\\\\mu=0$ and $\\\\sigma=1$, a Gaussian distribution is also called {\\\\it standard normal distribution}.\\nAn example of\\nGaussian PDF is shown in Fig.~\\\\ref{fig:gaussianPdf}.\\nProbability values corresponding to intervals $[\\\\mu-n\\\\sigma, \\\\mu+n\\\\sigma]$ for a Gaussian\\ndistribution are frequently used as reference, and are reported in Tab.~\\\\ref{tab:GaussianInt}.\\n\\\\begin{table}\\n\\\\caption{Probabilities for a Gaussian PDF corresponding to an interval $[\\\\mu-n\\\\sigma, \\\\mu+n\\\\sigma]$.}\\n\\\\centering\\n\\\\begin{tabular}{cc}\\\\hline\\\\hline\\n$n$ & Prob.\\\\\\\\\\\\hline\\n1 & 0.683 \\\\\\\\\\n2 & 0.954 \\\\\\\\\\n3 & 0.997 \\\\\\\\\\n4 & $1-6.5\\\\times10^{-5}$ \\\\\\\\\\n5 & $1-5.7\\\\times10^{-7}$ \\\\\\\\\\\\hline\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\nMany random variables in real experiments follow, at least approximately, a Gaussian distribution.\\nThis is mainly due to the {\\\\it central limit theorem} that allows to\\napproximate the sum of multiple random variables, regardless of their individual distributions,\\nwith a Gaussian distribution.\\nGaussian PDFs are frequently used to model detector resolution.\\n\\\\subsubsection{Poissonian distribution}\\nA {\\\\it Poissonian} distribution for an integer non-negative random variable $n$ is:\\n\\\\begin{equation}\\nP(n;\\\\nu) = \\\\frac{\\\\nu^n}{n!}e^{-\\\\nu}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\nu$ is a parameter equal to the average value of $n$.\\nThe variance of $n$ is also equal to $\\\\nu$.\\nPoissonian distributions model the number of occurrences of random event uniformly distributed\\nin a measurement range whose rate is known. Examples are the number of rain drops falling in a given area\\nand in a given time interval or the number of cosmic rays crossing a detector in a given time \\ninterval. \\nPoissonian distributions may be approximated with a Gaussian distribution\\nhaving $\\\\mu=\\\\nu$ and $\\\\sigma=\\\\sqrt{\\\\nu}$ for sufficiently large values of $\\\\nu$.\\nExamples of Poissonian distributions are shown in Fig.~\\\\ref{fig:poissonianPdf} with\\nsuperimposed Gaussian distributions as comparison.\\n\\\\subsubsection{Binomial distribution}\\nA {\\\\it binomial} distribution gives the probability to achieve $n$ successful outcomes\\non a total of $N$ independent trials whose individual probability of success is $p$.\\nThe binomial probability is given by:\\n\\\\begin{equation}\\nP(n;N,p) = \\\\frac{N!}{n!(N-n)!}p^n(1-p)^{N-p}\\\\,.\\n\\\\end{equation}\\nThe average value of $n$ for a binomial variable is:\\n\\\\begin{equation}\\n\\\\left<n\\\\right>= N\\\\,p\\n\\\\end{equation}\\nand the variance is:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[n] = N\\\\,p\\\\,(1-p)\\\\,.\\n\\\\end{equation}\\nA typical example of binomial process in physics is the case of a detector\\nwith efficiency $p$, where $n$ is the number of {\\\\it detected} particles\\nover a total number of particles $N$ that {\\\\it crossed} the detector.\\n', \"\\\\section{Gaussian distribution}\\nThe Gaussian or normal distribution (shown in Fig.~\\\\ref{fig:Gaussians}) is of widespread usage in data analysis. Under suitable conditions, in\\na repeated series of measurements $x$ with accuracy $\\\\sigma$ when the true value of the quantity\\nis $\\\\mu$, the distribution of $x$ is given by a Gaussian\\\\footnote{However, it is often the case \\nthat such a distribution has heavier tails than the Gaussian.}. A mathematical motivation is given by the \\nCentral Limit Theorem, which states that the sum of\\na large number of variables with (almost) any distributions is approximately Gaussian. \\nFor the Gaussian, the probability density $y(x)$ of an observation $x$ is given by\\n\\\\begin{equation}\\ny(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi} \\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\n\\\\end{equation}\\nwhere the parameters $\\\\mu$ and $\\\\sigma$ are respectively the centre and width of the distribution.\\nThe factor $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$ is required to normalise the area under the curve, so that $y(x)$\\ncan be directly interpreted as a probability density.\\nThere are several properties of $\\\\sigma$:\\n\\\\begin{itemize}\\n\\\\item{The mean value of $x$ is $\\\\mu$, and the standard deviation of its distribution is $\\\\sigma$. Since the usual symbol for \\nstandard deviation is $\\\\sigma$, this leads to the formula $\\\\sigma = \\\\sigma$ (which is not so trivial as it seems, since the \\ntwo $\\\\sigma$s have different meanings). This explains the curious factor of 2 in the denominator of the exponential,\\nsince without it, the two types of $\\\\sigma$ would not be equal.}\\n\\\\item{The value of $y$ at the $\\\\mu \\\\pm \\\\sigma$ is equal to the peak height multiplied by $e^{-0.5}$ = 0.61.\\nIf we are prepared to overlook the difference between 0.61 and 0.5, $\\\\sigma$ is the half-width of the distribution at\\n`half' the peak height.}\\n\\\\item{The fractional area in the range $x = \\\\mu - \\\\sigma$ to $\\\\mu + \\\\sigma$ is 0.68. Thus for a series of unbiassed, independent Gaussian \\ndistributed measurements}, about 2/3 are expected to lie within $\\\\sigma$ of the true value.\\n\\\\item{The peak height of $y$ at $x=\\\\mu$ is $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$. It is reasonable that this is proportional to \\n$1/\\\\sigma$ as the width is proportional to $\\\\sigma$, so $\\\\sigma$ cancels out in the product\\nof the height and width, as is \\nrequired for a distribution normalised to unity.}\\n\\\\end{itemize}\\nFor deciding whether an experimental measurement is consistent with a theory, more useful than the Gaussian distribution \\nitself is its tail area beyond $r$, a number of standard deviations from the central value (see Fig.~\\\\ref{fig:Gauss_tail}). \\nThis gives the probability of obtaining a result as extreme as ours or more so as a consequence of statistical fluctuations, \\nassuming that the theory is correct (and that our measurement is unbiassed, it is Gaussian distributed, etc.). If this \\nprobability is small, the measurement and the theory may be inconsistent. \\nFigure~\\\\ref{fig:Gauss_tail} has two different vertical scales, the left one for the probability of a fluctuation in a specific \\ndirection, and the right side for a fluctuation in either direction. Which to use depends on the particular\\nsituation. For example if we were performing a neutrino oscillation disappearance experiment, we would be looking for \\na reduction in the number of events as compared with the no-oscillation scenario, and hence would be interested in \\njust the single-sided tail. In contrast searching for any deviation from the Standard Model expectation, maybe the two-sided tails would be more relevant. \\n\", \"\\\\section{Probability distributions and their properties} \\nWe have to make a simple distinction between two sorts of data: \\\\emph{integer} data and \\\\emph{real-number} data\\\\footnote{Other branches of science have to include a third, \\\\emph{categorical} data, but we will ignore that.}.\\nThe first covers results which are of their nature whole numbers: the numbers of kaons produced in \\na collision, or the number of entries falling into some bin of a histogram.\\nGenerically let's call such numbers $r$. They have probabilities $P(r)$ which are dimensionless.\\nThe second covers results whose values are real (or floating-point) numbers. There are lots of these:\\nenergies, angles, invariant masses $\\\\dots$\\nGenerically let's call such numbers $x$, and they have probability density functions $P(x)$\\nwhich have \\ndimensions of $[x]^{-1}$, so $\\\\int_{x_1}^{x_2} P(x) dx$ or $P(x)\\\\, dx$ are probabilities.\\nYou will also sometimes meet the cumulative distribution $C(x)=\\\\int_{-\\\\infty}^x P(x') \\\\, dx'$.\\n\\\\subsection{Expectation values}\\nFrom $P(r)$ or $P(x)$ one can form the expectation value\\n\\\\begin{equation}\\n\\\\langle f \\\\rangle =\\\\sum_r f(r) P(r) \\\\qquad {\\\\rm or} \\\\qquad \\\\langle f \\\\rangle = \\\\int f(x) P(x) \\\\, dx \\n\\\\quad,\\n\\\\end{equation}\\nwhere the sum or integral is taken as appropriate.\\nSome authors write this as\\n$E(f)$, but I personally prefer the angle-bracket notation. You may think it looks too much like quantum mechanics,\\nbut in fact it's quantum mechanics which looks like statistics: an expression like $\\\\langle \\\\psi | \\\\hat Q | \\\\psi \\\\rangle$\\nis the average value of an operator $\\\\hat Q$ in some state $\\\\psi$, where `average value' has exactly the same \\nmeaning and significance. \\n\\\\subsubsection{Mean and standard deviation} \\nIn particular the {\\\\em mean}, often written $\\\\mu$, is given by \\n$ \\\\langle r \\\\rangle = \\\\sum_r r P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle x \\\\rangle =\\\\int x P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent Similarly one can write \\nhigher {\\\\em moments} \\n$\\\\mu_k = \\\\langle r^k \\\\rangle = \\\\sum_r r^k P(r)\\\\qquad {\\\\rm or } \\\\qquad \\\\langle x^k \\\\rangle =\\\\int x^k P(x) \\\\, dx \\\\quad,$ \\n\\\\noindent and {\\\\em central moments} \\n$\\\\mu'_k = \\\\langle (r-\\\\mu)^k \\\\rangle = \\\\sum_r (r-\\\\mu)^k P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle (x-\\\\mu)^k \\\\rangle =\\\\int (x-\\\\mu)^k P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent The second central moment is known as the \\n{\\\\em variance} \\n$\\\\mu'_2=V= \\\\sum_r (r-\\\\mu)^2 P(r) = \\\\langle r^2 \\\\rangle - \\\\langle r \\\\rangle ^2$\\n\\\\qquad \\nor \\\\qquad $ \\\\int (x-\\\\mu)^2 P(x) \\\\, dx = \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2$\\n\\\\noindent It is easy to show that $\\\\langle (x-\\\\mu)^2 \\\\rangle =\\\\langle x^2 \\\\rangle -\\\\mu^2$. The {\\\\em standard deviation} is just the square root of the variance $\\\\sigma=\\\\sqrt{V}$.\\nStatisticians usually use variance, perhaps because formulae come out simpler. Physicists usually use standard deviation,\\nperhaps because it has the same dimensions as the variable being studied, and can be drawn as an error bar on a plot. \\nYou may also meet {\\\\em skew}, which is $\\\\gamma=\\\\langle (x-\\\\mu)^3 \\\\rangle /\\\\sigma^3$ and {\\\\em kurtosis}, $h=\\\\langle (x-\\\\mu)^4 \\\\rangle /\\\\sigma^4 -3$.\\nDefinitions vary, so be careful. Skew is a dimensionless measure of the asymmetry of a distribution. Kurtosis is\\n(thanks to that rather arbitrary looking 3 in the definition) \\nzero for a Gaussian distribution (see Section~\\\\ref{sec:measurement}): positive kurtosis indicates a\\nnarrow core with a wide tail, negative kurtosis indicates the tails are reduced. \\n\\\\subsubsection{Covariance and correlation}\\nIf your data are \\n2-dimensional pairs $(x,y)$,\\nthen besides forming $\\\\langle x \\\\rangle, \\\\langle y \\\\rangle, \\\\sigma_x$ etc., you can also form the \\n{\\\\em Covariance}\\n${\\\\rm Cov}(x,y)=\\\\langle (x-\\\\mu_x)(y-\\\\mu_y) \\\\rangle = \\\\langle xy \\\\rangle - \\\\langle x \\\\rangle \\\\langle y \\\\rangle \\\\quad.$\\nExamples are shown in Fig.~\\\\ref{fig:covariance}. If there is a tendency for positive fluctuations in $x$ to be associated with positive fluctuations in $y$ (and therefore negative with negative) then\\nthe product $(x_i-\\\\overline x)(y_i-\\\\overline y)$ tends to be positive and the covariance is greater than 0. A negative covariance, as in the 3rd plot, happens if a positive fluctuation in one variable is associated with a negative fluctuation in the other.\\nIf the variables are independent then a positive variation in $x$ is equally likely to be associated with a positive or a negative variation in $y$ and the covariance is zero, as in the first plot. However the converse is not always the case, there can be two-dimensional distributions where the covariance is zero, but the two variables are not independent, as is shown in the fourth plot.\\nCovariance is useful, but it has dimensions. Often one uses the \\n{\\\\em correlation}, which is just\\n\\\\begin{equation}\\n\\\\rho={{\\\\rm Cov}(x,y)\\\\over \\\\sigma_x \\\\sigma_y}\\n\\\\quad.\\n\\\\end{equation}\\nIt is easy to show that\\n$\\\\rho$ lies between 1 (complete correlation) and -1 (complete anticorrelation). \\n$\\\\rho=0$ if $x$ and $y$ are independent.\\nIf there are more than two variables---the alphabet runs out so let's call them \\n$(x_1,x_2,x_3\\\\dots x_n)$---\\nthen these generalise to the \\n{\\\\em covariance matrix}\\n${\\\\bf V}_{ij}=\\\\langle x_i x_j \\\\rangle - \\\\langle x_i \\\\rangle \\\\langle x_j \\\\rangle$ \\n\\\\noindent and the {\\\\em \\ncorrelation matrix}\\n${\\\\bf \\\\rho}_{ij}={{\\\\bf V}_{ij} \\\\over \\\\sigma_i \\\\sigma_j} \\\\quad.$\\n\\\\noindent The diagonal of $\\\\bf V$ is $\\\\sigma_i^2$. \\nThe\\ndiagonal of $\\\\bf \\\\rho$ is 1.\\n\\\\subsection{Binomial, Poisson and Gaussian}\\nWe now move from considering the general properties of distributions to considering three specific ones.\\nThese are the ones you will most commonly meet for the distribution of the original data\\n(as opposed to quantities constructed from it). Actually the first, the binomial, is not nearly as common as the second, the Poisson; and the third, the Gaussian, is overwhelmingly more common. However it is \\nuseful to consider all three as concepts are built up from the simplest to the more sophisticated.\\n\\\\subsubsection {The binomial distribution }\\nThe binomial distribution is easy to understand as it basically describes the familiar\\\\footnote{Except, as it happens, in Vietnam, where coins have been completely replaced by banknotes.} tossing of coins. \\nIt describes the number $r$ of successes in $N$ trials, each with probability $p$ of success.\\n$r$ is discrete so the process is described by a probability distribution\\n\\\\begin{equation}\\nP(r;p,N)={N! \\\\over r! (N-r)!} p^r q^{N-r} \\n\\\\quad,\\n\\\\end{equation}\\nwhere $ q \\\\equiv 1-p$.\\nSome examples are shown in Fig.~\\\\ref{fig:binom}.\\nThe distribution has \\nmean $\\\\mu=Np$, variance $V=Npq$, and standard deviation $\\\\sigma=\\\\sqrt{Npq}$.\\n\\\\subsubsection {The Poisson distribution}\\nThe Poisson distribution also describes the probability of some discrete number $r$,\\nbut rather than a fixed number of `trials' it considers a random rate $\\\\lambda$: \\n\\\\begin{equation}\\nP(r;\\\\lambda)=e^{-\\\\lambda}{\\\\lambda^r \\\\ \\\\over r!}\\n\\\\quad.\\n\\\\end{equation}\\nIt is linked to the binomial---the Poisson is the \\nlimit of the binomial---as $N\\\\to \\\\infty$, $p \\\\to 0$ with $np=\\\\lambda=constant$. Figure~\\\\ref{fig:poisson} shows various examples. It has mean $\\\\mu=\\\\lambda$, variance $V=\\\\lambda$, and standard deviation $\\\\sigma=\\\\sqrt{\\\\lambda}=\\\\sqrt{\\\\mu}$.\\nThe clicks of a Geiger counter are the standard illustration of a Poisson process.\\nYou will meet it a lot as it applies to event counts---on their own or in histogram bins.\\nTo help you think about the Poisson, here is a simple question (which\\ndescribes a situation \\nI have seen in practice, more than once, from people who ought to know better).\\n\\\\\\n\\\\hrule\\n\\\\\\nYou need to know the efficiency of your PID system for positrons.\\nYou find 1000 data events where 2 tracks have a combined mass of 3.1~GeV ($J/\\\\psi)$ and the negative track is \\nidentified as an $e^-$ (`Tag-and-probe' technique).\\nIn 900 events the $e^+$ is also identified. In 100 events it is not. The efficiency is 90\\\\\\nWhat about the error?\\nColleague A says $\\\\sqrt{900}=30$ so efficiency is $90.0 \\\\pm 3.0 $\\\\\\ncolleague B says $\\\\sqrt{100}=10$ so efficiency is $90.0 \\\\pm 1.0 $\\\\\\nWhich is right?\\n\\\\\\n\\\\hrule\\n\\\\\\nPlease think about this before turning the page...\\n\\\\vfill\\\\eject\\n{Neither---both are wrong}. This is binomial not Poisson: $p=0.9, N=1000$.\\n\\\\noindent The error is $\\\\sqrt{Npq}=\\\\sqrt{1000 \\\\times 0.9 \\\\times 0.1}$ (or $\\\\sqrt{1000 \\\\times 0.1 \\\\times 0.9}$) =$\\\\sqrt{90} = 9.49$ so the efficiency is $90.0 \\\\pm 0.9$ \\\\\\n\\\\subsubsection{The Gaussian distribution}\\nThis is by far the most important statistical distribution.\\nThe probability density function (PDF) for a variable $x$ is given by the formula\\n\\\\begin{equation} \\nP(x;\\\\mu,\\\\sigma)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-{(x-\\\\mu)^2 \\\\over 2 \\\\sigma^2}}\\n\\\\quad.\\n\\\\end{equation}\\nPictorially this is shown in Fig.~\\\\ref{fig:gauss}.\\nThis is sometimes called the `bell curve', though in fact a real bell does not have flared edges like that.\\nThere is (in contrast to the Poisson and binomial) \\nonly one \\nGaussian curve, as $\\\\mu$ and $\\\\sigma$ are just location and scale parameters.\\nThe mean is $\\\\mu$ and the standard deviation is $\\\\sigma$. The \\nSkew is zero, as it is symmetric, and the kurtosis is zero by construction.\\nIn statistics, and most disciplines, this is known as the {\\\\em normal distribution}. Only in physics is it known as `The Gaussian'---perhaps because the word `normal' already has so many meanings. \\nThe reason for the importance of the Gaussian is the {\\\\em central limit theorem} (CLT) that states:\\nif the variable $X$ is the sum of $N$ variables $x_1,x_2\\\\dots x_N$ then:\\n\\\\begin{enumerate}\\n\\\\item Means add: $ \\\\langle X \\\\rangle = \\\\langle x_1 \\\\rangle + \\\\langle x_2 \\\\rangle + \\\\dots \\\\langle x_N \\\\rangle$, \\n\\\\item Variances add: $V_X=V_1+V_2 +\\\\dots V_N$,\\n\\\\item If the variables $x_i$ are independent and identically distributed (i.i.d.) then $P(X)$ tends to a Gaussian for large $N$.\\n\\\\end{enumerate}\\n(1) is obvious, (2) is pretty obvious, and means that standard deviations add in quadrature, and that the standard deviation of an average falls like $1\\\\over \\\\sqrt N$, (3) applies whatever the form of the original $P(x)$.\\nBefore proving this, it is helpful to see a demonstration to convince yourself that the implausible assertion in (3)\\nactually does happen.\\nTake a uniform distribution from 0 to 1, as shown in the top left subplot of Fig.~\\\\ref{fig:CLT}. It is flat. Add two such numbers and the distribution is triangular, between 0 and 2, as shown in the top right.\\nWith 3 numbers, at the bottom left, it gets curved. With 10 numbers, at the bottom right, it looks pretty Gaussian. The proof follows. \\n\\\\begin{proof}\\nFirst, introduce the characteristic function $\\\\langle e^{i k x} \\\\rangle = \\\\int e^{i k x } P(x) \\\\, dx = \\\\tilde P(k)$.\\nThis can usefully be thought of as an expectation value and as a Fourier transform, FT.\\nExpand the exponential as a series\\n$\\\\langle e^{i k x} \\\\rangle = \\\\langle 1+ikx+{(ikx)^2 \\\\over 2!}+{(ikx)^3 \\\\over 3!}\\\\dots \\\\rangle = 1 + ik \\\\langle x \\\\rangle +(ik)^2{\\\\langle x^2 \\\\rangle \\\\over 2!} + (ik^3) {\\\\langle x^3 \\\\rangle \\\\over 3!} \\\\dots$. \\nTake the logarithm and use the expansion $\\\\ln(1+z)=z-{z^2 \\\\over 2 } + {z^3 \\\\over 3} \\\\dots$\\nThis gives a power series in $(ik)$, where the coefficient ${\\\\kappa_r \\\\over r!}$ of $(ik)^r$ is made up of expectation values of $x$ of total power $r$\\n$\\\\kappa_1= \\\\langle x \\\\rangle, \\\\kappa_2= \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2 =, \\\\kappa_3= \\\\langle x^3 \\\\rangle -3 \\\\langle x^2 \\\\rangle \\\\langle x \\\\rangle +2 \\\\langle x \\\\rangle^3 \\\\dots$ \\nThese are called the semi-invariant cumulants of Thi\\\\`ele . Under a change of scale $\\\\alpha$, $\\\\kappa_r \\\\to \\\\alpha^r \\\\kappa_r$. Under a change in location only $\\\\kappa_1$ changes.\\nIf $X$ is the sum of i.i.d. random variables, $x_1+x_2+x_3...$, then $P(X)$ is the convolution of $P(x)$ with itself $N$ times.\\nThe FT of a convolution is the product of the individual FTs,\\nthe logarithm of a product is the sum of the logarithms,\\nso $P(X)$ has cumulants $K_r=N \\\\kappa_r$.\\nTo make graphs commensurate, you need to scale the $X$ axis by the\\nstandard deviation, which grows like $\\\\sqrt{N}$. The cumulants of the scaled graph are $K'_r = N^{1-r/2} \\\\kappa_r$. \\nAs $N \\\\to \\\\infty$, these vanish for $r>2$, leaving a quadratic.\\nIf the log is a quadratic, the exponential is a Gaussian. So $\\\\tilde P(X)$ is Gaussian.\\nAnd finally, the inverse FT of a Gaussian is also a Gaussian.\\n\\\\end{proof} \\nEven if the distributions are not identical, the CLT tends to apply, unless one (or two) dominates.\\nMost `errors' fit this, being compounded of many different sources.\\n\"}},\n",
       "       {'entity_name': 'law of total probability', 'entity_type': 'statistics_concept', 'description': 'A fundamental rule that relates marginal probabilities to conditional probabilities. It allows the calculation of the probability of an event by considering all possible ways that event can occur, partitioning the sample space into disjoint subsets.', 'relevant_passages': {\"\\\\section{Probability theory}\\n\\\\subsection{Bayes theorem}\\nConsidering two events $A$ and $B$, using Eq.~(\\\\ref{eq:condProb}) twice, we can write:\\n\\\\begin{eqnarray}\\nP(A|B) & = & \\\\frac{P(A\\\\cap B)}{P(B)} \\\\,,\\\\\\\\\\nP(B|A) & = & \\\\frac{P(A\\\\cap B)}{P(A)} \\\\,,\\n\\\\end{eqnarray}\\nfrom which the following equation derives:\\n\\\\begin{equation}\\nP(A|B)P(B) = P(B|A)P(A)\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:bayesInterm}) can be written in the following form, that\\ntakes the name of {\\\\it Bayes theorem}:\\n\\\\begin{equation}\\n\\\\boxed{\\nP(A|B) = \\\\frac{P(B|A)P(A)}{P(B)}\\\\,.\\n}\\n\\\\end{equation}\\nIn Eq.~(\\\\ref{eq:BayesTheorem}), $P(A)$ has the role of {\\\\it prior} probability and\\n$P(A|B)$ has the role of {\\\\it posterior} probability.\\nBayes theorem, that has its validity in any probability approach, including the frequentist one,\\ncan also be used to assign a posterior probability to a claim $H$ that is necessarily not a random\\nevent, given a corresponding prior probability $P(H)$ and the observation of an event $E$ whose\\nprobability, if $H$ is true, is given by $P(E|H)$:\\n\\\\begin{equation}\\nP(H|E) = \\\\frac{P(E|H)P(H)}{P(E)}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:BayesRule}) is the basis of Bayesian approach to probability. It\\ndefines in a {\\\\it rational way} a role to modify one's prior\\nbelief in a claim $H$ given the observation of $E$.\\nThe following problem is an example of application of\\nBayes theorem in a frequentist environment.\\nImagine you have a particle identification detector that identifies muons with high efficiency,\\nsay $\\\\varepsilon=95\\\\Given a particle in a data sample that is identified as a muon, what is the probability that\\nit is really a muon? The answer to this question can't be given unless we know more information\\nabout the composition of the sample, i.e.: what is the fraction of muons and pions in the data sample.\\nUsing Bayes theorem, we can write:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{P(+|\\\\mu)P(\\\\mu)}{P(+)}\\\\,,\\n\\\\end{equation}\\nwhere `$+$' denotes a positive muon identification, $P(\\\\mu|+)=\\\\varepsilon$ is the probability to positively\\nidentify a muon, $P(\\\\mu)$ is the fraction of muons in our sample ({\\\\it purity}) and\\n$P(+)$ is the probability to positively identify a particle randomly chosen from our sample.\\nIt's possible to decompose $P(+)$ as:\\n\\\\begin{equation}\\nP(+) = P(+|\\\\mu) P(\\\\mu) + P(+|\\\\pi) P(\\\\pi)\\\\,,\\n\\\\end{equation}\\nwhere $P(+|\\\\pi)=\\\\delta$ is the probability to positively identify a pion and $P(\\\\pi)=1-P(\\\\mu)$ is the\\nfraction of pions in our samples, that we suppose is only made of muons and pions.\\nEq.~(\\\\ref{eq:pPlusInt}) is a particular case of the {\\\\it law of total probability} which\\nallows to decompose the probability of an event $E_0$ as:\\n\\\\begin{equation}\\nP(E_0) = \\\\sum_{i=1}^n P(E_0|A_i) P(A_i)\\\\,,\\n\\\\end{equation}\\nwhere the sets $A_i$ are all pairwise disjoint and constitute a partition of the sample space.\\nUsing the decomposition from Eq.~(\\\\ref{eq:pPlusInt}) in\\nEq.~(\\\\ref{eq:pMuPlusInt}), one gets:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{\\\\varepsilon P(\\\\mu)}{\\\\varepsilon P(\\\\mu) + \\\\delta P(\\\\pi)}\\\\,.\\n\\\\end{equation}\\nIf we assume that our sample contains a fraction $P(\\\\mu)=4\\\\pions, we have:\\n\\\\begin{equation}\\nP(\\\\mu|+) = \\\\frac{0.95 \\\\cdot 0.04}{0.95 \\\\cdot 0.04 + 0.05\\\\cdot 0.96}\\\\simeq 0.44\\\\,.\\n\\\\end{equation}\\nIn this case, even if the selection efficiency is very high, given the low sample purity,\\na particle positively identified as a muon has a probability less than 50\\\\\\n\"}},\n",
       "       {'entity_name': 'signal strength and signal region', 'entity_type': 'statistics_concept', 'description': 'Concepts used in statistical models to analyze signals in particle physics. Signal strength quantifies the ratio of observed signal yield to theoretical predictions, while signal region refers to a defined area in parameter space that maximizes the expected number of signal events, enhancing the sensitivity of searches for new physics.', 'relevant_passages': {\"\\\\section{Experimental sensitivity using the $\\\\mathcal{Q}$ estimator}\\nThe experimental sensitivity to detect a new physics signal depends on multiple factors, including the accelerator's integrated luminosity, data quality, detector triggers, simulations, and accurate estimation of events associated with known physics (background)~\\\\cite{barlow2002systematic}. Given the vast parameter space and variety of theories, there arises a need to identify a specific search region to efficiently focus experimental efforts towards generating new discoveries~\\\\cite{casadei2011statistical}.\\nPhenomenology in high-energy physics (HEP) defines this search region as the signal region, determined by the expected number of new physics events for certain observables, such as invariant mass, transverse mass, etc. A common strategy to identify this region involves maximizing the statistical significance of observing \\\\( n = b + \\\\mu s \\\\), with \\\\( \\\\mu = 1 \\\\) events consistent with the new theory, assuming the background-only hypothesis (\\\\( H_{0} \\\\)) is true. This expected number of events is referred to as Asimov data~\\\\cite{lista2016practical,cowan2011asymptotic}, and it is used to determine the parameter space window of the theory measurable in the experiment with a specific luminosity. The statistical estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), with expected value \\\\( n = s + b \\\\), is given by:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & 2( \\\\mu s - nLn(1 + \\\\frac{\\\\mu s}{b}) ) {} \\\\nonumber \\\\\\\\\\n\\\\mathcal{Q}(1) & = & 2( s - (s+b)Ln(1 + \\\\frac{s}{b}) ). {}\\n\\\\end{eqnarray}\\nFrom this value, \\\\( \\\\mathcal{Q}_{\\\\text{obs}}(1) \\\\) is calculated, which allows estimating the significance in the context of a distribution corresponding to background-only hypothesis:\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{\\\\text{obs}}(1)} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThe optimal signal region for the experimental search is determined by finding the expected number of new physics events that maximizes the statistical significance. Thus:\\n\\\\begin{equation}\\ns_{best} = \\\\max_{s} \\\\alpha(s).\\n\\\\end{equation}\\nThis value of \\\\( \\\\alpha \\\\) is converted into units of standard deviations from a \\\\( \\\\mathcal{N}(0,1) \\\\) distribution. For now, this estimate assumes that the number of background events is well-determined, with no associated statistical or systematic error. Systematic effects on the background can modify both the signal region and the upper limits of theoretical predictions, and must be taken into account in phenomenological studies. Figure~[\\\\ref{fig:12}] illustrates the distribution \\\\( f(\\\\mathcal{Q},0) \\\\) with \\\\( Q_{obs}(1) \\\\), which is used to estimate the significance of the new physics model with \\\\( s=10 \\\\) and \\\\( b=100 \\\\) background events. The value of \\\\( p_{0} \\\\) is \\\\( \\\\alpha = 0.1705 \\\\), which corresponds to \\\\( Z_{0} = 0.952 \\\\) standard deviations. This result can also be approximated using the estimator~\\\\cite{florez2016probing,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nZ_{0} = \\\\frac{s}{\\\\sqrt{s+b}} \\\\approx 0.953.\\n\\\\end{equation}\\nThis estimator is considered an approximate estimation of the signal significance and is valid when \\\\( s \\\\ll b \\\\) (Appendix~\\\\ref{sec:AppendixB}). It is worth noting that this estimator has been widely used in optimizing search regions beyond the Standard Model in the context of phenomenology and experimental analysis~\\\\cite{florez2016probing,allahverdi2016distinguishing,cms2012observation,atlas2012observation}.\\n\", '\\\\section{Inference}\\n\\\\subsection{Maximum likelihood estimates}\\nThe maximum likelihood method takes as best-fit values of the unknown parameter\\nthe values that maximize the likelihood function (defined Sec.~\\\\ref{sec:likeFun}).\\nThe maximization of the likelihood function can be performed analytically only in the simplest cases,\\nwhile a numerical treatment is needed in most of the realistic cases.\\n{\\\\sc Minuit}~\\\\cite{minuit} is historically the most widely used minimization software engine in High Energy Physics.\\n\\\\subsubsection{Extended likelihood function}\\nGiven a sample of $N$ measurements of the variables $\\\\vec{x}=(x_1, \\\\cdots, x_n)$, the likelihood function expresses the probability\\ndensity evaluated for our sample as a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\n\\\\prod_{i=1}^Nf(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,.\\n\\\\end{equation}\\nThe size $N$ of the sample is in many cases also a random variable. In those cases,\\nthe {\\\\it extended likelihood function} can be defined as:\\n\\\\begin{equation}\\nL(\\\\vec{x}_1,\\\\cdots,\\\\vec{x}_N) =\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) \\\\prod_{i=1}^N f(x_1^i,\\\\cdots,x_n^i;\\\\theta_1,\\\\cdots,\\\\theta_m)\\\\,,\\n\\\\end{equation}\\nwhere $P(N;\\\\theta_1,\\\\cdots,\\\\theta_m)$ is the distribution of $N$, and in practice is always a Poissonian \\nwhose expected rate parameter is a function of the unknown parameters $\\\\theta_1,\\\\cdots,\\\\theta_m$:\\n\\\\begin{equation}\\nP(N;\\\\theta_1,\\\\cdots,\\\\theta_m) = \\\\frac{\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)^N e^{-\\\\nu(\\\\theta_1,\\\\cdots,\\\\theta_m)}}{N!}\\\\,.\\n\\\\end{equation}\\nIn many cases, either with a standard or an extended likelihood function,\\nit may be convenient to use $-\\\\ln L$ or $-2\\\\ln L$ in the numerical treatment\\nrather than $L$, because\\nthe product of the various terms is transformed into the sum of the logarithms of\\nthose terms, which may have advantages in the computation.\\nFor a Poissonian process that is given by the sum of a signal plus a background process,\\nthe extended likelihood function may be written as:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\n\\\\frac{(s+b)^N e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nf_sP_s(x_i;\\\\vec{\\\\theta}) + f_b P_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $s$ and $b$ are the signal and background expected yields, respectively,\\n$f_s$ and $f_b$ are the fraction of signal and background events, namely:\\n\\\\begin{eqnarray}\\nf_s & = & \\\\frac{s}{s+b} \\\\,,\\\\\\\\\\nf_b & = & \\\\frac{b}{s+b} \\\\,,\\n\\\\end{eqnarray}\\nand $P_s$ and $P_b$ are the PDF of the variable $x$ for signal and background,\\nrespectively.\\nReplacing $f_s$ and $f_b$ into Eq.~(\\\\ref{eq:extLikSB}) gives:\\n\\\\begin{equation}\\nL(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) = \\\\frac{e^{-(s+b)}}{N!}\\n\\\\prod_{i=1}^N\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIt may be more convenient to use the negative logarithm of Eq.~(\\\\ref{eq:extLikInt}),\\nthat should be minimize in order to determine the best-fit values of $s$, $b$ and $\\\\vec{\\\\theta}$:\\n\\\\begin{equation}\\n-\\\\ln L(\\\\vec{x}; s, b, \\\\vec{\\\\theta}) =\\ns + b -\\\\sum_{i=1}^N\\\\ln\\\\left(\\nsP_s(x_i;\\\\vec{\\\\theta}) + bP_b(x_i;\\\\vec{\\\\theta})\\n\\\\right) +\\\\ln N!\\\\,.\\n\\\\end{equation}\\nThe last term $\\\\ln N!$ is a constant with respect to the fit parameters,\\nand can be omitted in the minimization.\\nIn many cases, instead of using $s$ as parameter of interest,\\nthe {\\\\it signal strength} $\\\\mu$ is introduced, defined by the following equation:\\n\\\\begin{equation}\\ns = \\\\mu s_0\\\\,,\\n\\\\end{equation}\\nwhere $s_0$ is the theory prediction for the signal yield $s$.\\n$\\\\mu=1$ corresponds to the nominal value of the theory prediction for the signal yield.\\nAn example of unbinned maximum likelihood fit is given in Fig.~\\\\ref{fig:sbFit},\\nwhere the data are fit with a model inspired to Eq.~(\\\\ref{eq:extLikInt}), with\\n$P_s$ and $P_b$ taken as a Gaussian and an exponential distribution, respectively.\\nThe observed variable has been called $m$ in that case because the spectrum resembles an invariant mass peak,\\nand the position of the peak at 3.1~GeV reminds a $\\\\mathrm{J}/\\\\psi$ particle.\\nThe two PDFs can be written as:\\n\\\\begin{eqnarray}\\nP_s(m) & = & \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma}e^{-\\\\frac{(m-\\\\mu)^2}{2\\\\sigma^2}}\\\\,,\\\\\\\\\\nP_b(m) & = & \\\\lambda e^{-\\\\lambda m}\\\\,.\\n\\\\end{eqnarray}\\nThe parameters $\\\\mu$, $\\\\sigma$ and $\\\\lambda$ are fit together with the\\nsignal and background yields $s$ and $b$. While $s$ is our {\\\\it parameter of interest},\\nbecause we will eventually determine a production cross section or branching fraction from\\nits measurement, the other additional parameters, that are not directly\\nrelated to our final measurement, are said {\\\\it nuisance parameters}.\\nIn general, nuisance parameters are needed to model background yield,\\ndetector resolution and efficiency, various parameters modeling the\\nsignal and background shapes, etc. Nuisance parameters are also important\\nto model {\\\\it systematic uncertainties}, as will be discussed more in\\ndetails in the following sections.\\n'}},\n",
       "       {'entity_name': 'consistency', 'entity_type': 'statistics_concept', 'description': 'A property of an estimator indicating that as the sample size increases, the estimator converges in probability to the true parameter value.', 'relevant_passages': {\"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\"}},\n",
       "       {'entity_name': 'unbiased estimator', 'entity_type': 'statistics_concept', 'description': 'An estimator whose expected value equals the true parameter value, resulting in zero bias.', 'relevant_passages': {\"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\"}},\n",
       "       {'entity_name': 'efficiency', 'entity_type': 'statistics_concept', 'description': \"A measure of an estimator's performance, defined as the ratio of the Cramér-Rao bound to the estimator's variance, indicating how well the estimator utilizes information.\", 'relevant_passages': {\"\\\\section{Inference}\\n\\\\subsection{Estimator properties}\\nThis section illustrates the main properties of estimators. Maximum likelihood estimators\\nare most frequently chosen because they have good performances for what concerns those properties.\\n\\\\subsubsection{Consistency}\\nFor large number of measurements, the estimator $\\\\hat{\\\\theta}$ should converge, in probability, to the true value of $\\\\theta$,\\n$\\\\theta^{\\\\mathrm{true}}$.\\nMaximum likelihood estimators are consistent.\\n\\\\subsubsection{Bias}\\nThe bias of a parameter is the average value of its deviation from the true value:\\n\\\\begin{equation}\\n\\\\mathbbm{b}[\\\\hat{\\\\theta}] = \\\\left< \\\\hat{\\\\theta} - \\\\theta^{\\\\mathrm{true}}\\\\right> = \\\\left<\\\\hat{\\\\theta}\\\\right> - \\\\theta^{\\\\mathrm{true}}\\\\,.\\n\\\\end{equation}\\nAn {\\\\it unbiased estimator} has $\\\\mathbbm{b}[\\\\theta]=0$.\\nMaximum likelihood estimators may have a bias, but the bias decreases with large number of measurements (if the model used in the fit is correct).\\nIn the case of the estimate of a Gaussian's $\\\\sigma^2$,\\nthe maximum likelihood estimate (Eq.~(\\\\ref{eq:sigma2MLestimate})) underestimates the true variance. \\nThe bias can be corrected for by applying a multiplicative factor:\\n\\\\begin{equation}\\n\\\\widehat{{\\\\sigma}^2}_{\\\\mathrm{unbias.}} = \\\\frac{n}{n-1}\\\\widehat{{\\\\sigma}^2}\\n=\\\\frac{1}{n-1}\\\\sum_{i=1}^n (x_i-\\\\hat{\\\\mu})^2\\\\,.\\n\\\\end{equation}\\n\\\\subsubsection{Efficiency}\\nThe variance of any consistent estimator is subject to a lower bound\\ndue to Cram\\\\'er~\\\\cite{Cramer} and Rao~\\\\cite{Rao}:\\n\\\\begin{equation}\\n\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}] \\\\ge \\\\frac{\\\\displaystyle\\n\\\\left(1 + \\\\frac{\\\\partial \\\\mathbbm{b}[\\\\theta] }{\\\\partial\\\\theta} \\\\right)^2\\n}{\\\\displaystyle\\n\\\\left<\\\\left(\\n\\\\frac{\\\\partial\\\\ln L(\\\\vec{x};\\\\theta)}{\\\\partial\\\\theta}\\n\\\\right)\\\\right>\\n} = \\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]\\\\,.\\n\\\\end{equation}\\nFor an unbiased estimator, the numerator in Eq.~(\\\\ref{eq:CramerRao}) is equal to one.\\nThe denominator in Eq.~(\\\\ref{eq:CramerRao}) is the Fisher information (Eq.~(\\\\ref{eq:FisherInformation})).\\nThe {\\\\it efficiency} of an estimator $\\\\hat{\\\\theta}$ is the ratio\\nof the Cram\\\\'er--Rao bound and the estimator's variance:\\n\\\\begin{equation}\\n\\\\varepsilon(\\\\hat{\\\\theta}) = \\\\frac{\\\\mathbbm{V}_{\\\\mathrm{CR}}[\\\\hat{\\\\theta}]}{\\\\mathbbm{V}\\\\mathrm{ar}[\\\\hat{\\\\theta}]}\\\\,.\\n\\\\end{equation}\\nThe efficiency for maximum likelihood estimators tends to one for large number of measurements.\\nIn other words, maximum likelihood estimates have, asymptotically, the smallest variance\\nof all possible consistent estimators.\\n\"}},\n",
       "       {'entity_name': 'clopperpearson method', 'entity_type': 'analysis_technique', 'description': 'A statistical method used to construct confidence intervals for a binomial proportion, providing exact coverage for discrete variables, though achieving exact coverage is often challenging.', 'relevant_passages': {\"\\\\section{Inference}\\n\\\\subsection{Binomial intervals}\\nThe Neyman's belt construction may only guarantee approximate coverage in case of a discrete\\nvariable $n$. This because the interval for a discrete variable is a set of integer values,\\n$\\\\{ n_{\\\\mathrm{min}}, \\\\cdots, n_{\\\\mathrm{max}}\\\\}$, and cannot be ``tuned'' like in\\na continuous case. The choice of the discrete interval should be such to provide\\n{\\\\it at least} the desired coverage (i.e.: it may {\\\\it overcover}).\\nFor a binomial distribution, the problem consists of finding the interval such that:\\n\\\\begin{equation}\\n\\\\sum_{n=n_{\\\\mathrm{min}}}^{n_{\\\\mathrm{max}}}\\n\\\\frac{N!}{n!(N-n)!} p^n (1-p)^{N-n} \\\\ge 1-\\\\alpha\\\\,.\\n\\\\end{equation}\\nClopper and Pearson~\\\\cite{clopper_pearson} solved the belt inversion problem for\\ncentral intervals.\\nFor an observed $n = k$, one has to find the lowest $p^{\\\\mathrm{lo}}$ and highest\\n$p^{\\\\mathrm{up}}$ such that:\\n\\\\begin{eqnarray}\\nP(n \\\\ge k | N, p^{\\\\mathrm{lo}}) & = & \\\\frac{\\\\alpha}{2}\\\\,, \\\\\\\\\\nP(n \\\\le k | N, p^{\\\\mathrm{up}}) & = & \\\\frac{\\\\alpha}{2}\\\\,.\\n\\\\end{eqnarray}\\nAn example of Neyman belt constructed using the Clopper--Pearson\\nmethod is shown in Fig.~\\\\ref{fig:CloPear}.\\nFor instance for $n = N$, Eq.~(\\\\ref{eq:CP1}) becomes:\\n\\\\begin{equation}\\nP(n\\\\ge N|N,p^{\\\\mathrm{lo}}) = P(n=N|N,p^{\\\\mathrm{lo}}) = (p^{\\\\mathrm{lo}})^N = \\\\frac{\\\\alpha}{2}\\\\,,\\n\\\\end{equation}\\nhence, for the specific case $N=10$:\\n\\\\begin{equation}\\np^{\\\\mathrm{lo}} = \\\\sqrt[10]{\\\\frac{\\\\alpha}{2}} = 0.83\\\\,\\\\text(1-\\\\alpha = 0.683), \\\\,0.74\\\\,(1-\\\\alpha = 0.90)\\\\,.\\n\\\\end{equation}\\nIn fact, in Fig.~\\\\ref{fig:CloPear}, the bottom line of the belt reaches\\nthe value $p=0.83$ for $n=10$.\\nA frequently used approximation, inspired by Eq.~(\\\\ref{eq:binomVar}) is:\\n\\\\begin{equation}\\n\\\\hat{p} = \\\\frac{n}{N},\\\\,\\\\,\\\\,\\\\sigma_{\\\\hat{p}} \\\\simeq \\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{N}}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:varEff}) gives $\\\\sigma_{\\\\hat{p}}=0$ for $n=0$ or $N=n$, which is\\nclearly an underestimate of the uncertainty on $\\\\hat{p}$. For this reason,\\nClopper--Pearson intervals should be preferred to the approximate\\nformula in Eq.~(\\\\ref{eq:varEff}).\\nClopper--Pearson intervals are often defined as ``exact'' in literature,\\nthough exact coverage is often impossible to achieve for discrete variables.\\nFigure~\\\\ref{fig:CloPearCov} shows the coverage of Clopper--Pearson intervals as a\\nfunction of $p$ for $N=10$ and $N=100$ for $1-\\\\alpha = 0.683$. A ``ripple'' structure\\nis present which, for large $N$, tends to gets closer to the nominal 68.3\\\\\\n\"}},\n",
       "       {'entity_name': '1σ and 2σ contours', 'entity_type': 'statistics_concept', 'description': 'Statistical regions in parameter space that correspond to one standard deviation (1σ) and two standard deviations (2σ) from the mean, representing confidence levels for the estimated parameters in particle physics analyses.', 'relevant_passages': {\"\\\\section{Inference}\\n\\\\subsection{Two-dimensional uncertainty contours}\\nIn more dimensions, i.e.: for the simultaneous determination of more unknown parameters from a fit,\\nit's still possible to determine multi-dimensional contours corresponding to $1\\\\sigma$ or $2\\\\sigma$\\nprobability level. It should be noted that the scan of $-2\\\\ln L$ in the multidimensional\\nspace, looking for an excursion of $+1$ with respect to the value at the minimum, may give\\nprobability levels smaller than the corresponding values in one dimension.\\nFor a Gaussian case in one dimension, the probability associated to an interval $[-n\\\\sigma,+n\\\\sigma]$ is\\ngiven, integrating Eq.~(\\\\ref{eq:GaussianPDF}), by:\\n\\\\begin{equation}\\nP_{1\\\\mathrm{D}}(n\\\\sigma)= \\\\sqrt{\\\\frac{2}{\\\\pi}}\\\\int_0^ne^{-\\\\frac{x^2}{2}}\\\\,\\\\mathrm{d}x = \\\\mathrm{erf}\\\\left(\\\\frac{n}{\\\\sqrt{2}}\\\\right)\\\\,.\\n\\\\end{equation}\\nFor a two-dimensional Gaussian distribution, i.e.: the product of two independent Gaussian PDF,\\nthe probability associated to the contour with elliptic shape for which $-2\\\\ln L$ increases by $+(n\\\\sigma)^2$ with respect to its\\nminimum is:\\n\\\\begin{equation}\\nP_{2\\\\mathrm{D}}(n\\\\sigma)= \\\\int_0^ne^{-\\\\frac{r^2}{2}}r\\\\,\\\\mathrm{d}r = 1 - e^{-\\\\frac{n^2}{2}}\\\\,.\\n\\\\end{equation}\\nTable.~\\\\ref{tab:GaussianInt1D2D} reports numerical values for Eq.~(\\\\ref{eq:GaussInt1D}) and\\nEq.~(\\\\ref{eq:GaussInt2D}) for various $n\\\\sigma$ levels.\\n\\\\begin{table}[htbp]\\n\\\\caption{Probabilities for 1D interval and 2D contours with different $n\\\\sigma$ levels..}\\n\\\\centering\\n\\\\begin{tabular}{ccc}\\\\hline\\\\hline\\n$n\\\\sigma$ & $P_{1\\\\mathrm{D}}$ & $P_{2\\\\mathrm{D}}$ \\\\\\\\\\\\hline\\n$1\\\\sigma$ & 0.6827 & 0.3934 \\\\\\\\\\n$2\\\\sigma$ & 0.9545 & 0.8647 \\\\\\\\\\n$3\\\\sigma$ & 0.9973 & 0.9889 \\\\\\\\\\n$1.515\\\\sigma$ & & 0.6827 \\\\\\\\\\n$2.486\\\\sigma$ & & 0.9545 \\\\\\\\\\n$3.439\\\\sigma$ & 0.9973 \\\\\\\\\\\\hline\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\nIn two dimensions, for instance, in order to recover a $1\\\\sigma$ probability level in one\\ndimension (68.3\\\\should be considered, and for a $2\\\\sigma$\\nprobability level in one dimension (95.5\\\\Usualy two-dimensional intervals corresponding to one or two sigma are reported,\\nwhose one-dimensional projection correspond to 68\\\\respectively.\\n\"}},\n",
       "       {'entity_name': 'best linear unbiased estimator', 'entity_type': 'analysis_technique', 'description': 'A statistical method for combining correlated measurements that minimizes the variance of the estimator while ensuring that it is unbiased. This technique seeks the best linear combination of results, making it particularly useful in scenarios where measurements are correlated and uncertainty needs to be minimized.', 'relevant_passages': {'\\\\section{Inference}\\n\\\\subsection{Combination of measurements}\\nThe simplest combination of two measurements can be performed when no correlation is present between them:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1 \\\\,,\\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2 \\\\,.\\n\\\\end{eqnarray}\\nThe following $\\\\chi^2$ can be built, assuming a Gaussian PDF model for the two measurements,\\nsimilarly to Eq.~(\\\\ref{eq:GausChi2}):\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\frac{(m-m_1)^2}{\\\\sigma_1^2} + \\\\frac{(m-m_2)^2}{\\\\sigma_2^2}\\\\,.\\n\\\\end{equation}\\nThe minimization of the $\\\\chi^2$ in Eq.~(\\\\ref{eq:Chi2Comb}) leads to the following equation:\\n\\\\begin{equation}\\\\\\n0 = \\\\frac{\\\\partial\\\\chi^2}{\\\\partial m} =\\n2 \\\\frac{(m-m_1)}{\\\\sigma_1^2} + 2 \\\\frac{(m-m_2)}{\\\\sigma_2^2}\\\\,,\\n\\\\end{equation}\\nwhich is solved by:\\n\\\\begin{equation}\\nm = \\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^2} + \\\\frac{m_2}{\\\\sigma_2^2}\\n}{\\n\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}\\n}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:wAvPart}) can also be written in form of {\\\\it weighted average}:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{w_1 m_1 + w_2 m_2}{w_1 + w_2}\\\\,,\\n\\\\end{equation}\\nwhere the weights $w_i$ are equal to $\\\\sigma_i^{-2}$.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}}\\\\,.\\n\\\\end{equation}\\nIn case $m_1$ and $m_2$ are correlated measurements, the $\\\\chi^2$ changes from Eq.~(\\\\ref{eq:Chi2Comb}) to the following, including a non-null correlation coefficient $\\\\rho$:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\left(\\n\\\\begin{array}{cc} m - m_1 & m - m_2 \\\\end{array}\\n\\\\right)\\\\left(\\\\begin{array}{cc}\\n\\\\sigma_1^2 & \\\\rho\\\\sigma_1\\\\sigma_2 \\\\\\\\\\n\\\\rho\\\\sigma_1\\\\sigma_2 & \\\\sigma_2^2\\n\\\\end{array}\\n\\\\right)^{-1}\\\\left(\\n\\\\begin{array}{c}\\nm - m_1 \\\\\\\\ m - m_2\\n\\\\end{array}\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIn this case, the minimization of the $\\\\chi^2$ defined by Eq.~(\\\\ref{eq:chi2BLUE}) gives:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\nm_1(\\\\sigma_2^2 -\\\\rho\\\\sigma_1\\\\sigma_2) + m_2(\\\\sigma_1^2 -\\\\rho\\\\sigma_1\\\\sigma_2)\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,,\\n\\\\end{equation}\\nwith uncertainty given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{\\n\\\\sigma_1^2\\\\sigma_2^2(1-\\\\rho)^2\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,.\\n\\\\end{equation}\\nThis solution is also called best linear unbiased estimator (BLUE)~\\\\cite{lyons_gibaut}\\nand can be generalized to more measurements.\\nAn example of application of the BLUE method is the world combination\\nof the top-quark mass measurements at LHC and Tevatron~\\\\cite{topMassComb}.\\nIt can be shown that, in case the uncertainties $\\\\sigma_1$ and $\\\\sigma_2$\\nare estimates that may depend on the assumed central value,\\na bias may arise, which can be mitigated by evaluating the uncertainties\\n$\\\\sigma_1$ and $\\\\sigma_2$ at the central value obtained with the combination,\\nthen applying the BLUE combination, iteratively, until the procedure converges~\\\\cite{LyonsMartinSaxon, ListaBLUE}.\\nImagine we can write the two measurements as:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1^\\\\prime \\\\pm \\\\sigma_C\\\\,, \\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2^\\\\prime \\\\pm \\\\sigma_C\\\\,,\\n\\\\end{eqnarray}\\nwhere $\\\\sigma_C^2 = \\\\rho \\\\sigma_1\\\\sigma_2$. This is the case where the two measurements\\nare affected by a statistical uncertainty, which is uncorrelated between the two measurements,\\nand a fully correlated systematic uncertainty.\\nIn those case, Eq.~(\\\\ref{eq:BLUE}) becomes:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{m_2}{\\\\sigma_2^{\\\\prime 2}}\\n}{\\n\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}\\n}\\\\,,\\n\\\\end{equation}\\ni.e.: it assumes again the form of a weighted average with weights $w_i = \\\\sigma^{\\\\prime -2}$\\ncomputed on the uncorrelated uncertainty contributions.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}} + \\\\sigma_C^2\\\\,,\\n\\\\end{equation}\\nwhich is the sum in quadrature of the uncertainty of the weighted average (Eq.~(\\\\ref{eq:wAvgErr})) and the\\ncommon uncertainty $\\\\sigma_C$~\\\\cite{Valassi:2013bga}.\\nIn a more general case, we may have $n$ measurements $m_1,\\\\cdots,m_n$\\nwith a $n\\\\times n$ covariance matrix $C_{ij}$.\\nThe expected values for $m_1, \\\\cdots, m_n$ are $M_1, \\\\cdots, M_n$ and\\nmay depend on some unknown parameters $\\\\vec{\\\\theta}=(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nFor this case, the $\\\\chi^2$ to be minimized is:\\n\\\\begin{eqnarray}\\n\\\\chi^2 & = & \\\\sum_{i,j=1}^n (m_i-M_i(\\\\vec{\\\\theta}))\\\\,C_{ij}^{-1}\\\\,(m_j-M_j(\\\\vec{\\\\theta})) \\\\\\\\\\n& = & \\\\left(\\\\begin{array}{ccc} m_1 - M_1(\\\\vec{\\\\theta}) & \\\\cdots & m_n - M_n(\\\\vec{\\\\theta})\\\\end{array} \\\\right)\\\\!\\\\!\\n\\\\left(\\n\\\\begin{array}{ccc} C_{11} & \\\\cdots & C_{1n} \\\\\\\\\\n\\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\nC_{n1} & \\\\cdots & C_{nn}\\n\\\\end{array}\\n\\\\right)^{\\\\!\\\\!\\\\!-1}\\\\!\\\\!\\\\!\\\\!\\\\!\\\\left(\\n\\\\begin{array}{c}\\nm_1 - M_1(\\\\vec{\\\\theta}) \\\\\\\\ \\\\cdots \\\\\\\\ m_n - M_n(\\\\vec{\\\\theta})\\n\\\\end{array}\\n\\\\right).\\n\\\\end{eqnarray}\\nAn example of application of such a combination of measurement\\nis given by fit of the Standard Model parameters using the\\nelectroweak precision measurements\\nat colliders~\\\\cite{LEP-2, Baak:2014ora}.', \"\\\\section{Combining experiments} \\nSometimes different experiments will measure the same physical quantity. It is then reasonable to ask what is our\\nbest information available when these experiments are combined. It is a general rule that it is better to use the\\n{\\\\bf DATA} for the experiments and then perform a combined analysis, rather than simply combine the {\\\\bf RESULTS}.\\nHowever, combining the results is a simpler procedure, and access to the original data is not always possible. \\nFor a series of unbiassed, uncorrelated measurements $x_i$ of the same physical quantity,\\nthe combined value $\\\\hat{x} \\\\pm \\\\hat{\\\\sigma}$ is given by weighting each measurement by $w_i$,\\nwhich is proportional to the inverse of the square of its uncertainty i.e. \\n\\\\begin{equation}\\n\\\\hat{x} = \\\\Sigma w_i x_i, \\\\ \\\\ \\\\ \\\\ w_i =(1/\\\\sigma_i^2)/\\\\Sigma (1/\\\\sigma_j^2) \\\\end{equation}\\nwith the uncertainty $\\\\hat{\\\\sigma}$ on the combined value being given by\\n\\\\begin{equation}\\n1/\\\\hat{\\\\sigma}^2 = \\\\Sigma 1/\\\\sigma_i^2\\n\\\\end{equation}\\nThis ensures that the uncertainty on the combination is at least as small as the \\nsmallest uncertainty of the individual measurements. It should be remembered that the combined uncertainty takes no \\naccount of whether or not the individual measurements are consistent with each other.\\nIn an informal sense, $1/\\\\sigma_i^2$ is the information content of a measurement. Then each $x_i$ is weighted\\nproportionally to its information content. Also the equation for\\n$\\\\hat{\\\\sigma^2}$ says that the information content of the combination is the sum of the information contents\\nof the individual measurements.\\nAn example demonstrates that care is needed in applying the formulae. Consider counting the number of high\\nenergy cosmic rays being recorded by a large counter system for two consecutive one-week periods, with the number of counts being\\n$100 \\\\pm 10$ and $1 \\\\pm 1$ \\\\footnote{It is vital to be aware that it is a crime (punishable by a forcible transfer\\nto doing a doctorate on Astrology) to combine such discrepant measurements. It seems likely that someone turned off\\nthe detector between the two runs; or there was a large background in the first measurement which was eliminated\\nfor the second; etc. The only reason for my using such discrepant numbers is to produce a dramatically stupid\\nresult. The effect would have been present with measurements like $100 \\\\pm 10$ and $81 \\\\pm 9$.}.\\n(See section \\\\ref{Poisson} for the choice of uncertainties). Unthinking application of the formulae\\nfor the combined result give the ridiculous $2 \\\\pm 1$. What has gone wrong?\\nThe answer is that we are supposed to use the {\\\\bf true} accuracies of the individual measurements to assign the weights. \\nHere we have used the {\\\\bf estimated} accuracies. Because the estimated uncertainty \\ndepends on the estimated rate, a downward fluctuation in the measurement results in an underestimated uncertainty,\\nan overestimated weight, and a downward bias in the combination. In our example, the combination should assume \\nthat the true rate was the same in the two measurements which used the same detector and which \\nlasted the same time as each other, and hence their\\ntrue accuracies are (unknown but) equal. So the two measurements should each be given a weight of 0.5, which \\nyields the sensible combined result of $50.5 \\\\pm 5$ counts. \\n\\\\subsection{\\\\bf BLUE}\\nA method of combining correlated results is the `{\\\\bf B}est {\\\\bf L}inear {\\\\bf U}nbiassed {\\\\bf E}stimate' ({\\\\bf BLUE}). \\nWe look for the best linear unbiassed combination\\n\\\\begin{equation}\\nx_{BLUE} = \\\\Sigma w_i x_i,\\n\\\\end{equation} \\nwhere the weights are chosen to give the smallest uncertainty $\\\\sigma_{BLUE}$ on \\n$x_{BLUE}$. Also for the combination to be unbiassed, the weights must add up to unity.\\nThey are thus determined by minimising $\\\\Sigma\\\\Sigma w_i w_j E^{-1}_{ij}$, subject to the constraint\\n$\\\\Sigma w_i = 1$; here $E$ is the covariance matrix for the correlated measurements. \\nThe $BLUE$ procedure just described is equivalent to the $\\\\chi^2$ approach for checking whether\\na correlated set of measurements are consistent with a common value. The advantage of $BLUE$ is that \\nit provides the weights for each measurement in the combination. It thus enables us to calculate the contribution \\nof various sources of uncertainty in the individual measurements to the uncertainty on the combined result.\\n\\\\subsection{Why weighted averaging can be better than simple averaging}\\nConsider a remote island whose inhabitants are very conservative, and no-one leaves or arrives \\nexcept for some anthropologists who wish to determine the number of married people there.\\nBecause the islanders are very traditional, it is necessary to send two teams of anthropologists,\\none consisting of males to interview the men, and the other of females for the women. There are too\\nmany islanders to interview them all, so each team interviews a sample and then extrapolates. \\nThe first team estimates the number of married men as $10,000 \\\\pm 300$. The second, who \\nunfortunately have less funding and so can interview only a smaller sample, have a \\nlarger statistical uncertainty; they estimate $9,000 \\\\pm 900$ married women. Then how many \\nmarried people are there on the island? \\nThe simple approach is to add the numbers of married men and women, to give $19,000 \\\\pm 950$\\nmarried people. But if we use some theoretical input, maybe we can improve the accuracy of \\nour estimate. So if we assume that the islanders are monogamous, the numbers of married men and \\nwomen should be equal, as they are both estimates of the number of married couples. The weighted \\naverage is $9,900 \\\\pm 285$ married couples and hence $19,800 \\\\pm 570$ married people.\\nThe contrast in these results is not so much the difference in the estimates, but that \\nincorporating the assumption of monogamy and hence using the weighted average gives a smaller \\nuncertainty on the answer. Of course, if our assumption is incorrect, this answer will be biassed.\\nA Particle Physics example incorporating the same idea of theoretical input reducing the \\nuncertainty of a measurement can be found in the `Kinematic Fitting' section of Lecture 2.\\n\"}},\n",
       "       {'entity_name': 'weighted average method', 'entity_type': 'analysis_technique', 'description': 'A statistical technique for combining measurements that assigns different weights to values based on their uncertainties, particularly using the inverse of their uncertainties to provide a more accurate estimate of a central value.', 'relevant_passages': {'\\\\section{Inference}\\n\\\\subsection{Combination of measurements}\\nThe simplest combination of two measurements can be performed when no correlation is present between them:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1 \\\\,,\\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2 \\\\,.\\n\\\\end{eqnarray}\\nThe following $\\\\chi^2$ can be built, assuming a Gaussian PDF model for the two measurements,\\nsimilarly to Eq.~(\\\\ref{eq:GausChi2}):\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\frac{(m-m_1)^2}{\\\\sigma_1^2} + \\\\frac{(m-m_2)^2}{\\\\sigma_2^2}\\\\,.\\n\\\\end{equation}\\nThe minimization of the $\\\\chi^2$ in Eq.~(\\\\ref{eq:Chi2Comb}) leads to the following equation:\\n\\\\begin{equation}\\\\\\n0 = \\\\frac{\\\\partial\\\\chi^2}{\\\\partial m} =\\n2 \\\\frac{(m-m_1)}{\\\\sigma_1^2} + 2 \\\\frac{(m-m_2)}{\\\\sigma_2^2}\\\\,,\\n\\\\end{equation}\\nwhich is solved by:\\n\\\\begin{equation}\\nm = \\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^2} + \\\\frac{m_2}{\\\\sigma_2^2}\\n}{\\n\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}\\n}\\\\,.\\n\\\\end{equation}\\nEq.~(\\\\ref{eq:wAvPart}) can also be written in form of {\\\\it weighted average}:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{w_1 m_1 + w_2 m_2}{w_1 + w_2}\\\\,,\\n\\\\end{equation}\\nwhere the weights $w_i$ are equal to $\\\\sigma_i^{-2}$.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^2} + \\\\frac{1}{\\\\sigma_2^2}}\\\\,.\\n\\\\end{equation}\\nIn case $m_1$ and $m_2$ are correlated measurements, the $\\\\chi^2$ changes from Eq.~(\\\\ref{eq:Chi2Comb}) to the following, including a non-null correlation coefficient $\\\\rho$:\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\left(\\n\\\\begin{array}{cc} m - m_1 & m - m_2 \\\\end{array}\\n\\\\right)\\\\left(\\\\begin{array}{cc}\\n\\\\sigma_1^2 & \\\\rho\\\\sigma_1\\\\sigma_2 \\\\\\\\\\n\\\\rho\\\\sigma_1\\\\sigma_2 & \\\\sigma_2^2\\n\\\\end{array}\\n\\\\right)^{-1}\\\\left(\\n\\\\begin{array}{c}\\nm - m_1 \\\\\\\\ m - m_2\\n\\\\end{array}\\n\\\\right)\\\\,.\\n\\\\end{equation}\\nIn this case, the minimization of the $\\\\chi^2$ defined by Eq.~(\\\\ref{eq:chi2BLUE}) gives:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\nm_1(\\\\sigma_2^2 -\\\\rho\\\\sigma_1\\\\sigma_2) + m_2(\\\\sigma_1^2 -\\\\rho\\\\sigma_1\\\\sigma_2)\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,,\\n\\\\end{equation}\\nwith uncertainty given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{\\n\\\\sigma_1^2\\\\sigma_2^2(1-\\\\rho)^2\\n}{\\n\\\\sigma_1^2 -2\\\\rho\\\\sigma_1\\\\sigma_2 + \\\\sigma_2^2\\n}\\\\,.\\n\\\\end{equation}\\nThis solution is also called best linear unbiased estimator (BLUE)~\\\\cite{lyons_gibaut}\\nand can be generalized to more measurements.\\nAn example of application of the BLUE method is the world combination\\nof the top-quark mass measurements at LHC and Tevatron~\\\\cite{topMassComb}.\\nIt can be shown that, in case the uncertainties $\\\\sigma_1$ and $\\\\sigma_2$\\nare estimates that may depend on the assumed central value,\\na bias may arise, which can be mitigated by evaluating the uncertainties\\n$\\\\sigma_1$ and $\\\\sigma_2$ at the central value obtained with the combination,\\nthen applying the BLUE combination, iteratively, until the procedure converges~\\\\cite{LyonsMartinSaxon, ListaBLUE}.\\nImagine we can write the two measurements as:\\n\\\\begin{eqnarray}\\nm & = & m_1 \\\\pm \\\\sigma_1^\\\\prime \\\\pm \\\\sigma_C\\\\,, \\\\\\\\\\nm & = & m_2 \\\\pm \\\\sigma_2^\\\\prime \\\\pm \\\\sigma_C\\\\,,\\n\\\\end{eqnarray}\\nwhere $\\\\sigma_C^2 = \\\\rho \\\\sigma_1\\\\sigma_2$. This is the case where the two measurements\\nare affected by a statistical uncertainty, which is uncorrelated between the two measurements,\\nand a fully correlated systematic uncertainty.\\nIn those case, Eq.~(\\\\ref{eq:BLUE}) becomes:\\n\\\\begin{equation}\\n\\\\hat{m} = \\\\frac{\\n\\\\frac{m_1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{m_2}{\\\\sigma_2^{\\\\prime 2}}\\n}{\\n\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}\\n}\\\\,,\\n\\\\end{equation}\\ni.e.: it assumes again the form of a weighted average with weights $w_i = \\\\sigma^{\\\\prime -2}$\\ncomputed on the uncorrelated uncertainty contributions.\\nThe uncertainty on $\\\\hat{m}$ is given by:\\n\\\\begin{equation}\\n\\\\sigma_{\\\\hat{m}}^2 = \\\\frac{1}{\\\\frac{1}{\\\\sigma_1^{\\\\prime 2}} + \\\\frac{1}{\\\\sigma_2^{\\\\prime 2}}} + \\\\sigma_C^2\\\\,,\\n\\\\end{equation}\\nwhich is the sum in quadrature of the uncertainty of the weighted average (Eq.~(\\\\ref{eq:wAvgErr})) and the\\ncommon uncertainty $\\\\sigma_C$~\\\\cite{Valassi:2013bga}.\\nIn a more general case, we may have $n$ measurements $m_1,\\\\cdots,m_n$\\nwith a $n\\\\times n$ covariance matrix $C_{ij}$.\\nThe expected values for $m_1, \\\\cdots, m_n$ are $M_1, \\\\cdots, M_n$ and\\nmay depend on some unknown parameters $\\\\vec{\\\\theta}=(\\\\theta_1,\\\\cdots,\\\\theta_m)$.\\nFor this case, the $\\\\chi^2$ to be minimized is:\\n\\\\begin{eqnarray}\\n\\\\chi^2 & = & \\\\sum_{i,j=1}^n (m_i-M_i(\\\\vec{\\\\theta}))\\\\,C_{ij}^{-1}\\\\,(m_j-M_j(\\\\vec{\\\\theta})) \\\\\\\\\\n& = & \\\\left(\\\\begin{array}{ccc} m_1 - M_1(\\\\vec{\\\\theta}) & \\\\cdots & m_n - M_n(\\\\vec{\\\\theta})\\\\end{array} \\\\right)\\\\!\\\\!\\n\\\\left(\\n\\\\begin{array}{ccc} C_{11} & \\\\cdots & C_{1n} \\\\\\\\\\n\\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\nC_{n1} & \\\\cdots & C_{nn}\\n\\\\end{array}\\n\\\\right)^{\\\\!\\\\!\\\\!-1}\\\\!\\\\!\\\\!\\\\!\\\\!\\\\left(\\n\\\\begin{array}{c}\\nm_1 - M_1(\\\\vec{\\\\theta}) \\\\\\\\ \\\\cdots \\\\\\\\ m_n - M_n(\\\\vec{\\\\theta})\\n\\\\end{array}\\n\\\\right).\\n\\\\end{eqnarray}\\nAn example of application of such a combination of measurement\\nis given by fit of the Standard Model parameters using the\\nelectroweak precision measurements\\nat colliders~\\\\cite{LEP-2, Baak:2014ora}.', \"\\\\section{Combining experiments} \\nSometimes different experiments will measure the same physical quantity. It is then reasonable to ask what is our\\nbest information available when these experiments are combined. It is a general rule that it is better to use the\\n{\\\\bf DATA} for the experiments and then perform a combined analysis, rather than simply combine the {\\\\bf RESULTS}.\\nHowever, combining the results is a simpler procedure, and access to the original data is not always possible. \\nFor a series of unbiassed, uncorrelated measurements $x_i$ of the same physical quantity,\\nthe combined value $\\\\hat{x} \\\\pm \\\\hat{\\\\sigma}$ is given by weighting each measurement by $w_i$,\\nwhich is proportional to the inverse of the square of its uncertainty i.e. \\n\\\\begin{equation}\\n\\\\hat{x} = \\\\Sigma w_i x_i, \\\\ \\\\ \\\\ \\\\ w_i =(1/\\\\sigma_i^2)/\\\\Sigma (1/\\\\sigma_j^2) \\\\end{equation}\\nwith the uncertainty $\\\\hat{\\\\sigma}$ on the combined value being given by\\n\\\\begin{equation}\\n1/\\\\hat{\\\\sigma}^2 = \\\\Sigma 1/\\\\sigma_i^2\\n\\\\end{equation}\\nThis ensures that the uncertainty on the combination is at least as small as the \\nsmallest uncertainty of the individual measurements. It should be remembered that the combined uncertainty takes no \\naccount of whether or not the individual measurements are consistent with each other.\\nIn an informal sense, $1/\\\\sigma_i^2$ is the information content of a measurement. Then each $x_i$ is weighted\\nproportionally to its information content. Also the equation for\\n$\\\\hat{\\\\sigma^2}$ says that the information content of the combination is the sum of the information contents\\nof the individual measurements.\\nAn example demonstrates that care is needed in applying the formulae. Consider counting the number of high\\nenergy cosmic rays being recorded by a large counter system for two consecutive one-week periods, with the number of counts being\\n$100 \\\\pm 10$ and $1 \\\\pm 1$ \\\\footnote{It is vital to be aware that it is a crime (punishable by a forcible transfer\\nto doing a doctorate on Astrology) to combine such discrepant measurements. It seems likely that someone turned off\\nthe detector between the two runs; or there was a large background in the first measurement which was eliminated\\nfor the second; etc. The only reason for my using such discrepant numbers is to produce a dramatically stupid\\nresult. The effect would have been present with measurements like $100 \\\\pm 10$ and $81 \\\\pm 9$.}.\\n(See section \\\\ref{Poisson} for the choice of uncertainties). Unthinking application of the formulae\\nfor the combined result give the ridiculous $2 \\\\pm 1$. What has gone wrong?\\nThe answer is that we are supposed to use the {\\\\bf true} accuracies of the individual measurements to assign the weights. \\nHere we have used the {\\\\bf estimated} accuracies. Because the estimated uncertainty \\ndepends on the estimated rate, a downward fluctuation in the measurement results in an underestimated uncertainty,\\nan overestimated weight, and a downward bias in the combination. In our example, the combination should assume \\nthat the true rate was the same in the two measurements which used the same detector and which \\nlasted the same time as each other, and hence their\\ntrue accuracies are (unknown but) equal. So the two measurements should each be given a weight of 0.5, which \\nyields the sensible combined result of $50.5 \\\\pm 5$ counts. \\n\\\\subsection{\\\\bf BLUE}\\nA method of combining correlated results is the `{\\\\bf B}est {\\\\bf L}inear {\\\\bf U}nbiassed {\\\\bf E}stimate' ({\\\\bf BLUE}). \\nWe look for the best linear unbiassed combination\\n\\\\begin{equation}\\nx_{BLUE} = \\\\Sigma w_i x_i,\\n\\\\end{equation} \\nwhere the weights are chosen to give the smallest uncertainty $\\\\sigma_{BLUE}$ on \\n$x_{BLUE}$. Also for the combination to be unbiassed, the weights must add up to unity.\\nThey are thus determined by minimising $\\\\Sigma\\\\Sigma w_i w_j E^{-1}_{ij}$, subject to the constraint\\n$\\\\Sigma w_i = 1$; here $E$ is the covariance matrix for the correlated measurements. \\nThe $BLUE$ procedure just described is equivalent to the $\\\\chi^2$ approach for checking whether\\na correlated set of measurements are consistent with a common value. The advantage of $BLUE$ is that \\nit provides the weights for each measurement in the combination. It thus enables us to calculate the contribution \\nof various sources of uncertainty in the individual measurements to the uncertainty on the combined result.\\n\\\\subsection{Why weighted averaging can be better than simple averaging}\\nConsider a remote island whose inhabitants are very conservative, and no-one leaves or arrives \\nexcept for some anthropologists who wish to determine the number of married people there.\\nBecause the islanders are very traditional, it is necessary to send two teams of anthropologists,\\none consisting of males to interview the men, and the other of females for the women. There are too\\nmany islanders to interview them all, so each team interviews a sample and then extrapolates. \\nThe first team estimates the number of married men as $10,000 \\\\pm 300$. The second, who \\nunfortunately have less funding and so can interview only a smaller sample, have a \\nlarger statistical uncertainty; they estimate $9,000 \\\\pm 900$ married women. Then how many \\nmarried people are there on the island? \\nThe simple approach is to add the numbers of married men and women, to give $19,000 \\\\pm 950$\\nmarried people. But if we use some theoretical input, maybe we can improve the accuracy of \\nour estimate. So if we assume that the islanders are monogamous, the numbers of married men and \\nwomen should be equal, as they are both estimates of the number of married couples. The weighted \\naverage is $9,900 \\\\pm 285$ married couples and hence $19,800 \\\\pm 570$ married people.\\nThe contrast in these results is not so much the difference in the estimates, but that \\nincorporating the assumption of monogamy and hence using the weighted average gives a smaller \\nuncertainty on the answer. Of course, if our assumption is incorrect, this answer will be biassed.\\nA Particle Physics example incorporating the same idea of theoretical input reducing the \\nuncertainty of a measurement can be found in the `Kinematic Fitting' section of Lecture 2.\\n\"}},\n",
       "       {'entity_name': 'misidentification probability', 'entity_type': 'statistics_concept', 'description': 'The probability of rejecting the alternative hypothesis when it is true, also known as type II error or false negative rate.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\"}},\n",
       "       {'entity_name': 'principal component analysis pca', 'entity_type': 'analysis_technique', 'description': 'A statistical technique used to reduce the dimensionality of data by transforming it into a new set of uncorrelated variables, while preserving as much variance as possible. It is commonly employed in high-energy physics to simplify data input for machine learning models.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", '\\\\section{Quantum Anomaly Detection}\\n\\\\subsection{Background}\\nIn recent years, \\\\ac{QML} has emerged as a new paradigm for data processing at the intersection of machine learning and quantum information processing. Quantum computing has the potential to address real-world challenges that are difficult or even intractable for classical computers~\\\\cite{arute_quantum_2019, zhong_quantum_2020, madsen_quantum_2022}. \\nSuch problems include prime number factoring~\\\\cite{shor97}, a problem at the basis of classical modern-day encryption, search in unstructured databases~\\\\cite{grover1996fast}, solving systems of equations~\\\\cite{Harrow_Hassidim_Lloyd_2009}, and simulations of quantum systems, enabling first-principle computation of chemical properties in atomic, molecular, and nuclear systems~\\\\cite{Kandala2017, Barkoutsos2018, Kiss2022}\\\\footnote{Please also refer to references therein, references provided are non-exhaustive.}.\\nInitially, applications of quantum computing in \\\\ac{ML} focused on investigating speedups in computationally expensive subroutines of learning algorithms, such as optimization and matrix inversion~\\\\cite{Lloyd_Mohseni_Rebentrost_2014, Wiebe_Braun_Lloyd_2012, Rebentrost_Mohseni_Lloyd_2014}. Through this scope, replacing classical subroutines with quantum algorithms provide provable speedups in terms of runtime complexity of the \\\\ac{ML} training. Nevertheless, such proofs frequently require large and fault tolerant quantum hardware. Namely, quantum computers with error correction schemes that are able to arbitrarily suppress the inherent logical error rates~\\\\cite{Acharya2023}. Such devices do not exist yet. Currently available quantum computers are noisy and have limited number of qubits of small decoherence times~\\\\cite{preskill_nisq_2018}. Hence, the size of the quantum circuits and the number of operations that can be carried out at present are limited. Quantum algorithms with too many operations for the device at hand, can be rendered useless, or at least equivalent to a classical computation, by the inherent hardware noise.\\nLately, studies have also investigated the potential of quantum computing to enhance fundamentally the learning model itself~\\\\cite{Biamonte2017, Schuld2018, Schuld_2018_book, Havlicek2019}. \\\\ac{QML} models have been shown to generalise well with few training data~\\\\cite{caro_generalization_2022}, to provide advantages over classical algorithms for specific types of learning problems~\\\\cite{Liu2021rigorous, Kubler2021, Huang2021, pirnay22_superpol, muser2023}, and are able to identify patterns in data that cannot be recognised efficiently with classical methods~\\\\cite{huangQA2022}. Mirroring classical models for classification tasks, \\\\ac{QML} algorithms can be coarsely grouped into two categories: \\\\textit{kernel-based} methods and \\\\textit{variational} learning approaches\\\\cite{Cerezo2022}.\\nIn the former category a main example are \\\\ac{QSVM}, where a classical \\\\ac{SVM}~\\\\cite{svmVapnik} is equipped with a \\\\textit{quantum kernel}. The values of the kernel are evaluated on a quantum computer through measurements~\\\\cite{Schuld_2019, Havlicek2019}. During training, (Q)SVMs find the hyperplane that separates different classes of data, maximizing the margin between them. These models can create non-linear decision boundaries in the data input space when equipped with kernel functions~\\\\cite{svmVapnik}. The kernels are constructed by feature maps that transform the data into a higher dimensional feature space, in which the classes can be more effectively separated by a linear decision boundary. In the case of a quantum kernel, the data is mapped to the Hilbert space spanned by the qubit states. The dimensionality of this space grows exponentially with the number of qubits, and hence, such models are difficult to simulate classically. After constructing the quantum kernel matrix from the measurements the loss function of the model is minimized on a classical device using quadratic programming techniques. In particle physics, SVMs can be used for supervised classification tasks~\\\\cite{vaiciulis2003, sforza2013, sahin2016}, although their use is not as prevalent as deep learning approaches or ensemble models such as boosted decision trees. Additionally, kernel machines have been extended to an unsupervised setting~\\\\cite{one_class_svm}, where the training data is assumed to contain mostly background events and an upper bound on the expected anomaly contamination is set using a hyperparameter.\\nThe latter category encompasses parametrised quantum circuits, also referred to as \\\\ac{QNN} or variational quantum circuits. These circuits are composed of gates whose parameters can be tuned iteratively to minimize a loss function using classical gradient-based learning techniques~\\\\cite{Benedetti2019, Mitarai2018}. The output of the circuit is an expectation value of an operator, that is sampled from a quantum computer. This approach allows for the training of quantum circuits to perform specific tasks, such as classification and generative modeling. Specific architectures of QNNs have been shown to be universal function approximators~\\\\cite{Pérez-Salinas2020}, and that they can be expressed as a Fourier series expansion~\\\\cite{Schuld_Sweke_Meyer_2021}. Contrary to (quantum) kernel machines, the loss function landscape of QNNs is non-convex, which can lead to trainability issues similar to the ones of the vanishing gradient problem in classical neural networks~\\\\cite{Cerezo2021, Wang2021, Holmes2022}. Nevertheless, the authors of Ref.~\\\\citen{thanasilp2022exponential} argue that under certain conditions, kernel-based models can also manifest similar problems in training. Additionally, some works provide a unified view of \\\\ac{QML}~\\\\cite{Jerbi2023}, while others claim that the kernel-based learning is more natural for \\\\ac{QML}~\\\\cite{Schuld_qml_is_kernels_2021}.\\nIn most current applications, one can treat the quantum computer as a specialized processing unit, that is part of the overall computation. The hybrid algorithms discussed above aim to leverage the different strengths of classical and quantum processing units while mitigating their corresponding weaknesses.\\n\\\\subsection{Applications in High Energy Physics} \\nStudies have assessed the potential of quantum computing and variational algorithms for simulations of lattice field theories~\\\\cite{atas2021, mildenberger2022probing, funcke2023review}, as an alternative to \\\\ac{MC} techniques~\\\\cite{Kiss_MC2022, Delgado_Hamilton_2022, Chang2021, Bravo2022}, and for parton showering simulations~\\\\cite{nachman_qshower2021, bepari2022}. Research along these lines is motivated by the question of whether quantum algorithms can provide a natural platform for simulating fundamental physics~\\\\cite{dimeglio2023quantum}. An additional motive is the prospect of quantum computers providing a more favorable computational complexity than currently available classical methods. Furthermore, \\\\ac{QML} models have been developed for solving reconstruction problems in the context of collider experiments~\\\\cite{Tuysuz_2020, Grossi2020, magano2022, Lerjarza2022, duckett2022}. \\n\\\\subsection{Supervised classification} \\nIn terms of classification tasks in a model-dependent setting, quantum computing was first considered in Ref.~\\\\citen{Mott:2017}. Therein, the training of a classifier for $H\\\\to\\\\gamma\\\\gamma$ events was mapped to a quantum annealing task. \\nSince then, studies have mainly focused on the design and implementation of supervised \\\\ac{QML} algorithms based on different \\\\ac{QSVM} and \\\\ac{QNN} architectures that are able classify \\\\ac{HEP} events by discriminating the signal distribution from the background distribution~\\\\cite{terashi2021, blance_quantum_2021, qmlHiggs2021, Guan_2021, Heredge2021, Chen2020, Chen2021, wu_2021_kernel, wu_2021_qnn}. Such quantum models, are often developed and assessed via computationally expensive quantum simulations on classical processors using limited number of qubits; typically up to 20. In these simulations, the algorithms can be investigated in an ideal noiseless environment. After the architecture has been chosen and its hyperparameters have converged to values that lead to good performance on the learning task at hand, the \\\\ac{QML} algorithms are tested by running experiments on real hardware via cloud-based platforms. \\nThe developed quantum models are typically benchmarked against classical models of similar complexity, that are trained on the same small data sets. So far, the number of training data points is at the order of $10^2$ to $10^4$. \\\\ac{HEP} datasets are frequently high dimensional, with number of features exceeding the order of presently available number qubits, posing a challenge for direct input and processing by data encoding circuits on current quantum devices. To address this challenge, a set of reduced features is typically used as an input to the \\\\ac{QML} models. This representation of reduced dimensionality is obtained by manual selection of physical variables~\\\\cite{terashi2021, blance_quantum_2021}, Principle Component Analysis (PCA)~\\\\cite{wu_2021_qnn, wu_2021_kernel, Schuhmacher23, Peixoto2023}, or autoencoders~\\\\cite{qmlHiggs2021, wozniak_belis_puljak23}. In the case of autoencoders, the compression of \\\\ac{HEP} events can be regarded as more representative since these models can, at least approximately, retain non-linear correlations of the input features in their latent space. Such higher order correlations are removed by definition in the case of PCA, and are potentially lost in manual feature selection or feature extraction based on univariate discrimination metrics~\\\\cite{qmlHiggs2021}.\\nSo far, in most studies regarding supervised models, the performance of the quantum algorithms is competitive and matches the performance of their classical counterparts. Numerical evidence suggests that \\\\ac{QML} algorithms might outperform classical models when the training datasets are small~\\\\cite{terashi2021, wu_2021_kernel, Guan_2021, Gianelle2022}. However, such a property has not been proven in general, yet.\\n\\\\subsection{Unsupervised new-physics searches} \\nRecently, different strategies were proposed for new-physics searches at the \\\\ac{LHC} using \\\\ac{QML} in the context of anomaly detection~\\\\cite{Blance_Spannowsky_2021, Ngairangbam_2022, alvi2023, wozniak_belis_puljak23, Schuhmacher23, Bermot2023, Bordoni:2023lad}. In Ref.~\\\\citen{Blance_Spannowsky_2021}, Gaussian Boson Sampling (GBS) is used to create a lower dimensional representation of \\\\ac{BSM} events where the Higgs boson decays into two pseudoscalar particles. GBS is classically difficult to simulate and can be implemented using continuous variable devices such as photonic quantum computers. This procedure is combined with a quantum version of the K-means clustering algorithm, Q-means, to detect anomalies. \\nK-means is a method that aims to partition an unlabeled dataset into K clusters in the feature space. Each cluster center, also called a centroid, is defined as the mean of the datapoints that belong to that cluster. Each datapoint is assigned to the nearest centroid, according to some distance measure, typically the Euclidean metric, which serves us the loss function of the algorithm. The cluster assignment and the coordinates of the centroids are iteratively updated, according to the loss minimization procedure, until the datapoints have converged to specific stable clusters. In the case of Q-means, the datapoints are embedded in the quantum Hilbert space, where the distance calculation occurs depending on the chosen quantum circuit. Additionally, depending on the design of the quantum model the minimisation of the loss can be accomplished with a quantum or classical algorithm. K-means has been applied to \\\\ac{HEP} for jet clustering~\\\\cite{Chekanov2006, Thaler2012, Stewart2015}, while Q-means and its variants have been applied also for unsupervised detection of new-physics events~\\\\cite{Blance_Spannowsky_2021, wozniak_belis_puljak23}.\\nA quantum autoencoder (QAE) is considered in Ref.~\\\\citen{Ngairangbam_2022} using four physical variables as an input. The authors demonstrate, both in quantum simulation and hardware, that the QAE is a promising approach for \\\\ac{BSM} scenarios involving a resonant heavy Higgs decaying to a pair of top quarks, and a \\\\ac{SM} Higgs decaying to two dark matter particles. The authors of Ref.~\\\\citen{Bordoni:2023lad} employ QAEs for the detection of long-lived particles and adapt the proposed methods for execution on real quantum hardware. Additionally, different architectures of \\\\ac{QNN} have been investigated in Ref.~\\\\citen{alvi2023}, using low dimensional (simulated) datasets involving Higgs-like scalar particles signatures as anomalies in a semi-supervised setting. The authors do not identify any region where the tested \\\\ac{QML} models present an advantage in performance or in terms of the needed size of the trained dataset.\\nRef.~\\\\citen{Schuhmacher23} proposes a simulation-assisted new-physics search where supervised quantum and classical \\\\ac{SVM}s are trained on a dataset that contains \\\\ac{SM} processes as background and an artificial set of anomalous events obtained from the \\\\ac{SM}-distributed features as signal. Specifically, the authors generate the distributions of the signal samples by a so-called scrambling process, in which the feature distributions of the background are smeared by the normal distribution, preserving energy and momentum conservation. Furthermore, it is demonstrated that the considered models are able to generalize to real signals such as Higgs and Graviton production events.\\nIn Ref.~\\\\citen{Bermot2023}, a quantum Generative Adversarial Network is designed to extract an anomaly score for each data input. The authors benchmark the proposed model and verify its efficacy in data sets where they treat the Higgs boson production and Graviton production as anomalies, respectively. Additionally, generative modeling in the context of Hamiltonian learning has been investigated for semi-leptonic top and dijet event production~\\\\cite{araz2023}. The anomaly score in Ref.~\\\\citen{araz2023} is constructed using the different properties of the time evolution of quantum states that represent background and signal data.\\nA new-physics search in dijet topologies is addressed in Ref.~\\\\citen{wozniak_belis_puljak23}, where an unsupervised quantum kernel machine and quantum clustering methods are designed to define a metric of typicality for QCD jets. The dijet events are described by 600 features --100 particle constituents per jet and three features per particle-- and the examined anomalies include two different Graviton scenarios and a \\\\ac{BSM} scalar boson production with the final state. The authors develop a convolutional autoencoder to produce a compressed representation of the \\\\ac{HEP} events, addressing the challenge of directly processing realistic high-dimensional data on current quantum devices. Consequently, the quantum anomaly detection algorithms use as an input the latent representation of the data that is generated by the encoder and are trained using QCD background events. For the proposed kernel-based anomaly detection model, this work identifies an advantage in performance of the quantum model over its classical counterpart.\\n\\\\subsection{Discussion \\\\& Outlook}\\nIn \\\\ac{HEP} applications so far, the quantum models are not designed to explicitly manifest an inductive bias towards the structure of the chosen (simulated) particle physics datasets. In the aforementioned studies, the model architectures, i.e., quantum circuits used for the implementation of \\\\ac{QNN}s and feature maps for the kernel methods, are constructed following ansätze in the \\\\ac{QML} literature that have desired properties such as expressiveness and hardware efficiency. \\nMany \\\\ac{QML} algorithms have been inspired by classical model architectures, such as autoencoders~\\\\cite{Romero_2017}, convolutional neural networks~\\\\cite{cong2019}, equivariant models~\\\\cite{Nguyen:2022lww}, and graph neural networks~\\\\cite{verdon2019quantum}. Despite drawing inspiration from classical models, these quantum counterparts may exhibit distinct properties and inductive biases~\\\\cite{Kubler2021,Bowles2023}. The studies presented in Sections~\\\\ref{sec:quantum_supervised} and~\\\\ref{sec:quantum_unsupervised} compare the performance of their proposed models to their classical counterparts for the task at hand. However, beyond promising results in specific problems and datasets, identifying precisely in which applications QML models could provide consistent benefits such as enhancement in model performance, or computational speed-ups, still remains an open question and an active area of research. Furthermore, due to limitations in current hardware, the behavior of \\\\ac{QML} models in the regime that is comparable to current state-of-the-art deep learning models, i.e., having millions or even billions of training samples and model parameters, is unknown.\\nIn general, the exploration of \\\\ac{QML} strategies for \\\\ac{HEP} data is, at least partly, motivated by the question of whether quantum models can exploit correlations and information existing in particle physics datasets leading to advantages in performance. It is important to note, that no studies, so far, have used quantum models for supervised or unsupervised classification in real data from \\\\ac{HEP} experiments.\\nThe data measured by the detectors and stored for the analysis of \\\\ac{HEP} experiments is classical. However, a quantum field theory framework is essential to predict and properly explain the outcome of such experiments. Furthermore, remnants of the initial quantum mechanical process --particle interaction-- are still present in the data. Specifically, measuring spin correlations between particles~\\\\cite{top_correlations_CMS2019}, observing entanglement between particles produced in proton collisions~\\\\cite{ATLAS:2023_entanglement, Cervera2017, Severi2022, Fabbrichesi2023} and violation of Bell inequalities~\\\\cite{Fabbrichesi2021, Afik_Nova_2022, Ghosh2023} in \\\\ac{LHC} data has been established. Measuring these first-principle quantities highlights that data from particle physics experiments cannot be described by classical local hidden-variable theories. In conclusion, the topics discussed above represent an active field of research and hold promise for classical and quantum data analysis algorithms that can enhance our ability to probe for new-physics.\\n'}},\n",
       "       {'entity_name': 'fisher discriminant', 'entity_type': 'analysis_technique', 'description': 'A method that uses a linear combination of input variables to maximize the separation between two classes while minimizing variance.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\"}},\n",
       "       {'entity_name': 'neural networks and deep learning techniques', 'entity_type': 'analysis_technique', 'description': 'A collection of computational models inspired by the human brain, including various architectures such as artificial neural networks, deep neural networks, convolutional neural networks, and recurrent neural networks. These techniques are utilized in particle physics for complex pattern recognition, classification tasks, and feature extraction from raw data, enhancing the analysis of particle interactions and event classification.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Reconstruction}\\nRegression algorithms are another type of supervised learning,\\nproviding continuous outputs which can have any numerical value within\\na range. They can be deployed for reconstruction purposes in HEP\\ne.g. when we want to make precise determinations of continuous\\nquantities like hit positions, track momenta or jet energies.\\nAt the intensity frontier advanced detectors collect record amounts of\\nluminosity at what would be considered ``medium'' energies by today's\\nstandards. One example is the Beijing Electron Positron Collider\\n(BEPCII) running at center of mass energies 2.0--4.6 GeV. The BESIII\\nexperiment has collected record size data samples in this\\n$\\\\tau$--charm region. Advanced ML techniques have been applied for\\nmany tasks~\\\\cite{BESIII}. One of them is cluster reconstruction for\\nthe cylindrical triple-GEM inner tracker, part of the 2019 upgrade to\\nthe aging inner drift chamber. The goal is to measure the drift\\ncathode layer position of ionizing particles from the readouts of the\\nanode strips, which is the first reconstruction step. Two methods are\\navailable: weighted by electric charge average position of the anode\\nstrips (Q~method), or time measurement using the drift gap as kind of\\nmicro time projection chamber (T~method). The two methods can be\\ncombined to improve the position resolution, but this combination is\\nmade difficult by the correlations to the incident angle. Here ML\\ntechniques come to the rescue: a XGBoost regressor is developed to\\nreconstruct the initial particle positions from the Q and T\\nreadouts. Substantial improvements over the charge centroid method are\\nreported.\\nML techniques are entering in full force the ``sister'' field of\\nparticle astrophysics. One development in the field of very high\\nenergy gamma-ray astronomy is the Cherenkov Telescope Array (CTA)\\nwhich will ultimately consists of 19 telescopes in the Northern and 99\\ntelescopes in the Southern hemisphere to cover the full sky. A\\ncolossal amount of data in the multi-petabyte range per year is\\nexpected. The telescope arrays are operated as a single instrument to\\nobserve extensive air showers originating from gammas or charged\\nparticles, and aim to separate them and measure basic characteristics\\nas energy, direction and impact point of the original particle. An\\nexploratory regression study~\\\\cite{GammaLearn} in this direction uses\\nCNN with the hope to extract more information directly from the raw\\ndata and outperform traditional approaches based on human-selected\\nfeatures.\\nThe main difficulty is that conventional CNNs are developed to process\\nrectangular images with regular pixel grids. The telescope outputs\\nhere have hexagonal pixels forming hexagonal images. One, not very\\nsatisfying approach is resampling, converting the image to a standard\\none, potentially losing some information about the neighbors. This\\nanalysis takes the more difficult route of reimplementing the\\nconvolutional and pooling operations of CNNs by building matrices of\\nneighbor indices, rearranging the data accordingly and then applying\\nthe general methods for convolution, or for pooling with different\\nfunctions depending on the task ({\\\\it softmax, average, max}).\\nThe next difficulty is to combine images from several telescopes to\\nobtain stereoscopic information. Traditional DL methods\\nonly deal with single images, sequentially in time. This is solved by\\nadding a convolution block for each telescope in the array, and\\nfeeding them all to the dense fully connected part of the network. The\\nexploratory study with four telescopes shows promise in the\\nmeasurements of energy, direction and impact point for incoming\\nparticles; additional work is needed to outperform traditional methods\\nand solve technical details before applying the developed algorithm on\\nreal data.\\nTracking detectors form the core of most collider experiments, and\\nsuccessful track reconstruction is mission critical for achieving\\ntheir goals. Reconstructing tracks is a combinatorial problem,\\ni.e. finding the measurements (hits) belonging to individual particles\\nentering the detectors from an often huge set of possible\\ncombinations. With the transition to the High-Luminosity LHC (HL-LHC)\\nthe complexity of this task will increase substantially. Traditional\\napproaches like track following (inside-out or outside-in) and Kalman\\nfilters do not scale favorably to very high hit densities, and are\\ntypically custom implemented for each experiment with large amount of\\nhuman efforts.\\nThe TrackML~\\\\cite{TrackML} project has the ambition to stimulate new\\napproaches and the development of new algorithms by exposing data from\\na virtual, but realistic HL-LHC tracking detector to data science and\\ncomputer experts outside of the HEP community. Production of\\ntop-antitop quark pairs is selected for the signal events, which are\\nthen merged with 200 soft interactions (pile-up events). Fast\\nsimulation is used to generate hits in the tracker from charged\\ntracks. The magnetic field is inhomogeneous, energy loss, hadronic\\ninteractions and multiple scattering are parameterized. The silicon\\ntracker consists of three parts: innermost pixel detector, followed by\\ntwo layers of short and long silicon strips providing hermetic\\ncoverage up to $|\\\\eta|\\\\ <\\\\ 3$. For each collision, about ten thousand\\ncharged particles, originating approximately from the center of the\\ndetector, produce about ten precise hits per track in three\\ndimensions.\\nThe challenge, running on the Kaggle platform~\\\\cite{TrackMLKaggle} and\\non Codalab in 2018--2019, is split in two phases: accuracy and\\nthroughput. The first phase is scored by a specially developed metric,\\nwhich puts high priority on efficiency of finding real hits belonging\\nto a particle and low fake rates. At least 50\\\\originate from the same simulated truth particle, with hits on the\\ninnermost layers, key for good vertex resolution, and on the outermost\\nlayers, key for long lever arms and thus for good momentum resolution,\\ngetting highest weights in the overall score. A random solution will\\nget a score of zero and a perfect reconstruction of all events in the\\ntest dataset, consisting of 125 simulated events, will get a score of\\none.\\nThe challenge attracted more than 650 participants. In the accuracy\\nphase the participants provide their reconstruction of the test\\ndataset to Kaggle where it is scored. In the throughput phase the\\nparticipants provide their algorithms and software, and it is run in a\\nconsistent environment (Docker containers on two i686 processor cores\\nand 4GB of memory) to measure both accuracy and runtime, which will be\\nvery important to handle the enormous datasets expected from the\\nHL-LHC.\\nWinners~\\\\cite{TrackMLWin} of the accuracy phase are teams (with\\nscores): top-quarks(0.92219), outrunner(0.90400) and Sergey\\nGorbunov(0.89416). While training can consume lots of computer\\nresources, where machine learning really shines is the speed of\\nreconstruction once the algorithms are trained. Winners of the\\nthroughput phase are teams sgorbuno (Sergei Gorbunov), fastrack\\n(Dmitry Emelyanov) and cloudkitchen (Marcel Kunze), who were able to\\ncombine high accuracy scores with speeds well below ten seconds per\\nevent, and even below one second for the first two.\\nThe TrackML challenge shows that ML techniques like representation\\nlearning, combinatorial optimization, clustering and even time series\\nprediction can be applied to tracking. The best solutions offer a\\nsynergy between model-based and data-based approaches, combining the\\nbest of both worlds: physical track models and machine learning, with\\nsensible trade-offs between complexity and performance.\\n\", \"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Classification and Event Selection}\\nThe difficulty in extracting small signals from the towering LHC\\nbackgrounds has helped to introduce ML techniques for classification\\npurposes. Classification algorithms are a type of supervised learning\\nwhere the outputs are restricted to a limited set of values, or\\nclasses like signals or backgrounds. The Higgs analyses are a prime\\nexample.\\nThe discovery of the Higgs boson in 2012 by the\\nCMS~\\\\cite{Chatrchyan:2012xdj} and ATLAS~\\\\cite{Aad:2012tfa}\\ncollaborations saw the first use of boosted decision trees in such a\\nhigh stakes search for the separation of small signals (invariant mass\\npeaks) over large smoothly falling backgrounds. Since then the Higgs\\ndecays and couplings to the heavy W and Z gauge bosons, as well as the\\nheavy third generation quarks (bottom and top) and tau leptons, have\\nbeen observed by both ATLAS and CMS, and are consistent with the\\npredictions of the SM at the current level of precision. With the\\nHiggs boson firmly established, attention has turned to measuring its\\nproperties.\\nThe next frontier is observing Higgs decays and measuring its\\ncouplings to fermions outside the third generation. The search for\\nHiggs decays to a pair of muons with opposite charge ($\\\\mu^+\\\\mu^-$)\\noffers the best chance to establish and measure the Higgs couplings to\\nthe second generation. This is a very challenging undertaking: the SM\\nbranching fraction is expected to be $\\\\sim$0.02\\\\signal has to be extracted over a huge irreducible background\\nproducing opposite sign muon pairs: Drell-Yan, top quark or W boson\\npairs production. The Higgs signal has a narrow dimuon invariant mass\\npeak near 125~GeV, only a few GeV wide, determined by the experimental\\nmuon momentum resolution. In contrast, the background events exhibit a\\nsmoothly falling mass spectrum in the search region from 110 to\\n160~GeV.\\nThe CMS collaboration developed a method to enhance the signal\\nextraction by using a BDT classifier, as implemented in the TMVA\\nclass~\\\\cite{Hocker:2007ht} of the ROOT analysis package~\\\\cite{root},\\naugmented with automated categorization for optimal event\\nclassification. The results of the 2016 analysis, using 35.9~fb$^{-1}$\\nof collision data, were published in~\\\\cite{Sirunyan:2018hbu}. Details\\nof how the analysis was optimized for maximum signal sensitivity,\\nutilizing multivariate and machine learning techniques, are provided\\nin~\\\\cite{Bourilkov1}. Events are divided into categories based on the\\ntransverse momentum ($p_T$) of the dimuon pair (which is higher for\\nthe main gluon-gluon fusion signal (ggF) relative to the main\\nDrell-Yan background), or the presence of a high-invariant-mass dijet\\npair, characteristic of vector boson fusion (VBF) signal\\nevents. Categories are sub-divided further based on the muon\\npseudorapidity ($\\\\eta$), as central muons have better $p_T$\\nresolution, resulting in a sharper signal mass peak.\\nThe training is based on one million simulated events for the various\\nchannels, fully reconstructed in the CMS detector. Fourteen kinematic\\nvariables characterizing the dimuon system are used, their\\ndistributions are very similar between the signal and background\\nevents, making the separation that much harder. The signal sample is\\nsplit into three independent sets: one for training, a second for\\ntesting, and a third completely independent - to avoid any bias - for\\nthe final measurement. The background samples are typically split in\\n75\\\\separation is computed, yielding a BDT score between -1 and 1, where\\nevents close to 1 are more signal-like, and events close to -1 are\\nmore background-like.\\nAs a last step the auto-categorizer procedure determines 15 event\\ncategories based on $|\\\\eta|$ and BDT scores. Performing separate\\nsignal-plus-background fits to the data in all of these categories and\\ncombining the results significantly increases the search sensitivity\\nrelative to a measurement of all candidate dimuon events together. The\\nnet result of applying machine learning techniques is a 23\\\\in sensitivity equivalent to 50\\\\\\nThe ATLAS collaboration has presented an updated\\nresult~\\\\cite{ATLAS-Hmm} using all the Run2 data: 139 fb$^{-1}$. The\\nobserved upper limit on the cross section times the branching ratio of\\nthe Higgs decay to a muon pair is 1.7 times the SM prediction, so the\\nLHC experiments are closing in on the observation of this channel, but\\nwill need more data.\\nThis analysis follows a somewhat similar approach, using 14 kinematic\\nvariables and 12 categories to optimize the separation of the signal\\nfrom the backgrounds. Data events from the sidebands and simulated\\nsignal events enter the BDT training procedure. The XGBoost (eXtreme\\nGradient Boosting)~\\\\cite{Chen:2016btl} package is used. First a BDT is\\ntrained in the category of events with two or more jets to disentangle\\nthe VBF signal from the background. Three VBF categories with\\ndifferent purities are defined based on this BDT score. Then the rest\\nof the events is divided with three BDTs providing ggF scores, and\\nsplit according to jet multiplicities with zero, one or two jets,\\ngiving nine additional categories. To maximize the sensitivity to the\\nHiggs to muons decays the boundaries are adjusted by BDT scores, and\\nin each of the twelve BDT categories a fit to the invariant mass\\nspectrum from 110-–160 GeV is performed to extract the signal.\\nThis analysis is able to achieve about 50\\\\sensitivity compared to the previous ATLAS result, with roughly equal\\nparts due to the increase in integrated luminosity or from refinements\\nin the analysis techniques, where machine learning plays a key role.\\nA substantially more difficult task is the search for Higgs decays to\\na pair of charm quarks from the second generation. The CMS\\ncollaboration has performed a direct search for this Higgs decay where\\nthe Higgs is produced in association with a W or Z boson, based on an\\nintegrated luminosity of 35.9 fb$^{−1}$ collected at the CERN LHC in\\n2016~\\\\cite{CMS-Hcc}.\\nTwo types of jet topologies are analyzed: ``resolved-jet'' , where\\nboth charm quark jets from the Higgs decay are observed, and the\\n``merged-jet'' topology, where the two jets from the charm quark can\\nonly be reconstructed as a single jet. In both topologies, novel tools\\nbased on advanced machine learning techniques are deployed.\\nFor the ``resolved-jet'' topology BDT with gradient\\nboost are trained to enhance the signal-background separation. Four\\ncategories having 0, 1 or 2 leptons from the associated W or Z decays\\n(the 2 lepton case subdivided depending on the p$_T$ of the vector\\nboson) and 25 input variables are used for training. For the\\n``merged-jet'' topology a novel algorithm based on advanced ML methods\\nis deployed to identify jet substructures in order to tag the highly\\nboosted W, Z, and Higgs decays, giving sizable gains.\\nThe use of an adversarial network~\\\\cite{Goodfellow:2014upx} helps to\\nlargely decorrelate the algorithm from the mass of a jet while\\npreserving most of its discriminating power. For example, for large\\njets with p$_T\\\\ >\\\\ $~200 GeV, misidentification rates for charm quark\\npairs of 1\\\\35\\\\9\\\\\\nThe results of the two topologies help to provide an upper limit on\\nthe branching ratio of Higgs decays to charm quarks. There is still a\\nlong way to reach the sensitivity needed to observe this Higgs decay\\nwith SM strength.\\nWhile in the early Higgs papers BDTs were the prefered ML approach,\\nnewer analyses deploy deep learning and NN. The CMS collaboration has\\nmeasured~\\\\cite{CMS-Htautau} the inclusive cross section for the\\nproduction and subsequent decay of Higgs bosons to tau lepton pairs\\nwith 77.4 fb$^{−1}$ of data collected in 2016 and 2017.\\nA multi-classification approach is applied for each final state and\\nyear of data-taking, eight independent tasks in total. For each of\\nthem a fully connected feed-forward NN is trained. The architecture\\nconsists of two hidden layers with 200 nodes each, and five or eight\\nnodes in the output layer, each representing an event class\\nprediction, depending on the final state. The total cross section as\\nwell as cross sections for individual production modes and kinematic\\nregimes are obtained. This is made possible by the power of\\nclassification using deep learning.\\nAnother recent example is the measurement of associated production of\\ntop quark-antiquark pairs and Higgs bosons (ttH), with Higgs decaying\\nto b quarks, by the CMS collaboration~\\\\cite{CMS-ttH}. The analysis is\\nbased on 41.5 fb$^{−1}$ collected in 2017, and combined with the 2016\\nanalysis reaches an observed (expected) significance of 3.9 (3.5)\\nstandard deviations above the background-only hypothesis, providing\\nthe first evidence for ttH production with subsequent H$\\\\rightarrow$bb\\ndecays. Multiple classifiers are deployed, like BDTs for the dilepton\\nchannel, or feedforward NN with three hidden layers of 100 nodes each,\\nas implemented in Keras~\\\\cite{Keras}, for the single lepton channel.\\nThe application of ML techniques for Higgs analyses at the LHC is not\\na one-way street. Data from the simulations of the Higgs decays to\\ntau-lepton pairs were released by ATLAS to the ML community and formed\\nthe basis for the HiggsML challenge~\\\\cite{HiggsML}. It ran from May\\nto September 2014 on the Kaggle platform~\\\\cite{HiggsMLKaggle}, was\\nextremely popular, attracted 785 teams with 1942 participants and\\ngenerated 35772 submissions and more than a thousand forum posts.\\nProbably more surprising then than now, first prize was won by Gabor\\nMelis, a software developer and consultant from Hungary, using\\nartificial NN. A special HEP meets ML award was provided to data\\nscience graduate students Tianqi Chen and Tong He for offering the\\nboosted decision trees tool XGBoost, used by many participants. By now\\nCERN provides an open data portal~\\\\cite{CERN-ODP} to the LHC\\nexperiments to encourage fruitful collaboration between high energy\\nphysicists and data scientists.\\n{\\\\it Great progress in computer vision has come from convolutional\\nneural networks (CNN), inspired by the animal visual cortex, where\\nindividual neurons process information only from parts of the visual\\nfield. This ``divide-and-conquer'' strategy simplifies the NN\\narchitecture and helps features like translational and rotational\\ninvariance, very desirable for image recognition. Typically the\\nfirst layers of a CNN are for convolution and pooling. In a\\nconvolution the shape of an input function is modified by another\\nfunction by taking an integral of the product of the two. The\\nconvolutional filtering helps e.g. in edge detection.\\nHand-engineering the filters is replaced by learning them from the\\nimages. Pooling layers combine the inputs from several neurons (the\\nsimplest being a} 2x2 {\\\\it cluster) into one output neuron, thus\\nreducing the dimensionality. This is usually followed by fully\\nconnected layers like in standard DNN for the final classification\\nstep.}\\nTraditionally a HEP analysis proceeds to reconstructing higher level\\nobjects like tracks and energy deposits in electromagnetic and hadron\\ncalorimeters from the raw detector data, and finally arriving at\\nparticle level objects. A promising new approach is to apply deep\\nlearning algorithms directly to low-level detector data. This is\\nexplored in what is called end-to-end event\\nclassification~\\\\cite{Andrews}. The study is based on 2012 CMS\\nSimulated Open Data for the decays of the Higgs boson to a pair of\\nphotons. The gluon-gluon fusion Higgs production is the signal, while\\nirreducible quark fusion to photon pairs and a photon plus jet faking\\na second photon events form the backgrounds in this simplified\\nstudy. The events are simulated taking into account the interactions\\nin the detector materials and the detailed CMS geometry.\\nThe low level detector data is converted into images of size 170x360\\nin pseudorapidity $\\\\eta$ and azimuthal angle $\\\\varphi$ for the CMS\\nbarrel, and two images of size 100x100 for the two CMS endcaps\\nextending to $|\\\\eta|\\\\ <\\\\ 2.3$. Each channel contains three layers\\ncorresponding to electromagnetic and hadron energy and track\\ntransverse momentum. This is the electromagnetic-centric\\nsegmentation. Alternatively, a hadron-centric segmentation with size\\n280x360 in $\\\\eta - \\\\varphi$ is used. Inspired by the recent progress\\nin computer vision, a CNN of the Residual Net-type\\n(ResNet-15)~\\\\cite{He} is used. The initial results show promise, with\\nsignal efficiency and background rejection on par with more\\ntraditional approaches.\\n\", \"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\", \"\\\\section{Supervised learning}\\nMachine learning (ML) has played a role in particle physics for decades. An emblematic use case is in ``$b$-tagging'': determining whether a given set of particles is associated with a primordial bottom quark. Bottom quarks are around four times heavier than a proton and have properties that help distinguish them from other particles. For example, they tend to travel around half a millimeter away from the collision point before decaying. Technically, the $b$ quark binds with other quarks into metastable hadrons, like the $B_d$ meson, which then decay into particles like muons and pions. One cannot directly measure the distance the particles travel, but by measuring things like the number of decay products, distances among charged tracks, whether there was a muon in the decay, etc., one can accumulate a number of highly correlated features that can be combined to estimate\\nthe probability that there was a $b$ quark involved. Traditionally, the various features might be fed into a shallow neural network or a boosted decision tree to determine a $b$-tagging probability.\\n$b$-tagging is characteristic of how ML has traditionally (and very successfully) been used in particle physics: physically motivated classifiers are first understood individually and then combined using a relatively simple multivariate technique.\\nOver the last several years, this paradigm has been replaced by what I like to call {\\\\it modern} machine learning. The modern approach is to feed raw, minimally-processed data, rather than high-level physically-motivated variables, into a deep neural network. The network is then free to find what it thinks is most valuable in the data. For example, with $b$ tagging, a modern machine learning approach is to put all the measured tracks into a recurrent neural network. The network is then trained using labeled simulated data to distinguish signal events ($b$ quarks) and background events (other quarks). This is in contrast to the traditional approach, where the tracks are connected with curves and distilled down to an impact parameter. While the traditional approach works very well, it might for example obtain a factor of 1000 rejection of background quarks while keeping 50\\\\\\nAs a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collected into bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, 100 or more protons may collide each time the bunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e., has quarks within each proton strike each other with enough energy to produce something of interest, like a Higgs boson (only one in a billion collisions produce a Higgs boson). When there is a direct hit, often called a primary collision, there are other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintegrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritus is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is) and azimuthal angle. Area subtraction recalibrates the event based on the amount energy deposited in some region of the detector where products from the primary collision are believed to be absent. Another method, used extensively by the CMS collaboration, is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so that one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction works only on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary collision point, one for charged particles from the secondary collisions points and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is adapted for a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top quark has a mass, the top jet usually has three subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms noticeably outperform traditional physics-motivated algorithms.\\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{ATLAStoptag}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to around 500,000 parameters for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were identified by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\", \" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'stochastic gradient descent', 'entity_type': 'analysis_technique', 'description': 'An optimization algorithm used in machine learning and neural networks to minimize the loss or cost function by iteratively updating model parameters based on the gradient of the loss function with respect to those parameters.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\"}},\n",
       "       {'entity_name': 'boosted decision trees', 'entity_type': 'analysis_technique', 'description': 'An ensemble learning technique that combines multiple decision trees to enhance classification accuracy. This method iteratively reweights training samples and is particularly effective in particle physics for distinguishing small signals from large backgrounds by creating a series of decision trees that improve prediction accuracy.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Classification and Event Selection}\\nThe difficulty in extracting small signals from the towering LHC\\nbackgrounds has helped to introduce ML techniques for classification\\npurposes. Classification algorithms are a type of supervised learning\\nwhere the outputs are restricted to a limited set of values, or\\nclasses like signals or backgrounds. The Higgs analyses are a prime\\nexample.\\nThe discovery of the Higgs boson in 2012 by the\\nCMS~\\\\cite{Chatrchyan:2012xdj} and ATLAS~\\\\cite{Aad:2012tfa}\\ncollaborations saw the first use of boosted decision trees in such a\\nhigh stakes search for the separation of small signals (invariant mass\\npeaks) over large smoothly falling backgrounds. Since then the Higgs\\ndecays and couplings to the heavy W and Z gauge bosons, as well as the\\nheavy third generation quarks (bottom and top) and tau leptons, have\\nbeen observed by both ATLAS and CMS, and are consistent with the\\npredictions of the SM at the current level of precision. With the\\nHiggs boson firmly established, attention has turned to measuring its\\nproperties.\\nThe next frontier is observing Higgs decays and measuring its\\ncouplings to fermions outside the third generation. The search for\\nHiggs decays to a pair of muons with opposite charge ($\\\\mu^+\\\\mu^-$)\\noffers the best chance to establish and measure the Higgs couplings to\\nthe second generation. This is a very challenging undertaking: the SM\\nbranching fraction is expected to be $\\\\sim$0.02\\\\signal has to be extracted over a huge irreducible background\\nproducing opposite sign muon pairs: Drell-Yan, top quark or W boson\\npairs production. The Higgs signal has a narrow dimuon invariant mass\\npeak near 125~GeV, only a few GeV wide, determined by the experimental\\nmuon momentum resolution. In contrast, the background events exhibit a\\nsmoothly falling mass spectrum in the search region from 110 to\\n160~GeV.\\nThe CMS collaboration developed a method to enhance the signal\\nextraction by using a BDT classifier, as implemented in the TMVA\\nclass~\\\\cite{Hocker:2007ht} of the ROOT analysis package~\\\\cite{root},\\naugmented with automated categorization for optimal event\\nclassification. The results of the 2016 analysis, using 35.9~fb$^{-1}$\\nof collision data, were published in~\\\\cite{Sirunyan:2018hbu}. Details\\nof how the analysis was optimized for maximum signal sensitivity,\\nutilizing multivariate and machine learning techniques, are provided\\nin~\\\\cite{Bourilkov1}. Events are divided into categories based on the\\ntransverse momentum ($p_T$) of the dimuon pair (which is higher for\\nthe main gluon-gluon fusion signal (ggF) relative to the main\\nDrell-Yan background), or the presence of a high-invariant-mass dijet\\npair, characteristic of vector boson fusion (VBF) signal\\nevents. Categories are sub-divided further based on the muon\\npseudorapidity ($\\\\eta$), as central muons have better $p_T$\\nresolution, resulting in a sharper signal mass peak.\\nThe training is based on one million simulated events for the various\\nchannels, fully reconstructed in the CMS detector. Fourteen kinematic\\nvariables characterizing the dimuon system are used, their\\ndistributions are very similar between the signal and background\\nevents, making the separation that much harder. The signal sample is\\nsplit into three independent sets: one for training, a second for\\ntesting, and a third completely independent - to avoid any bias - for\\nthe final measurement. The background samples are typically split in\\n75\\\\separation is computed, yielding a BDT score between -1 and 1, where\\nevents close to 1 are more signal-like, and events close to -1 are\\nmore background-like.\\nAs a last step the auto-categorizer procedure determines 15 event\\ncategories based on $|\\\\eta|$ and BDT scores. Performing separate\\nsignal-plus-background fits to the data in all of these categories and\\ncombining the results significantly increases the search sensitivity\\nrelative to a measurement of all candidate dimuon events together. The\\nnet result of applying machine learning techniques is a 23\\\\in sensitivity equivalent to 50\\\\\\nThe ATLAS collaboration has presented an updated\\nresult~\\\\cite{ATLAS-Hmm} using all the Run2 data: 139 fb$^{-1}$. The\\nobserved upper limit on the cross section times the branching ratio of\\nthe Higgs decay to a muon pair is 1.7 times the SM prediction, so the\\nLHC experiments are closing in on the observation of this channel, but\\nwill need more data.\\nThis analysis follows a somewhat similar approach, using 14 kinematic\\nvariables and 12 categories to optimize the separation of the signal\\nfrom the backgrounds. Data events from the sidebands and simulated\\nsignal events enter the BDT training procedure. The XGBoost (eXtreme\\nGradient Boosting)~\\\\cite{Chen:2016btl} package is used. First a BDT is\\ntrained in the category of events with two or more jets to disentangle\\nthe VBF signal from the background. Three VBF categories with\\ndifferent purities are defined based on this BDT score. Then the rest\\nof the events is divided with three BDTs providing ggF scores, and\\nsplit according to jet multiplicities with zero, one or two jets,\\ngiving nine additional categories. To maximize the sensitivity to the\\nHiggs to muons decays the boundaries are adjusted by BDT scores, and\\nin each of the twelve BDT categories a fit to the invariant mass\\nspectrum from 110-–160 GeV is performed to extract the signal.\\nThis analysis is able to achieve about 50\\\\sensitivity compared to the previous ATLAS result, with roughly equal\\nparts due to the increase in integrated luminosity or from refinements\\nin the analysis techniques, where machine learning plays a key role.\\nA substantially more difficult task is the search for Higgs decays to\\na pair of charm quarks from the second generation. The CMS\\ncollaboration has performed a direct search for this Higgs decay where\\nthe Higgs is produced in association with a W or Z boson, based on an\\nintegrated luminosity of 35.9 fb$^{−1}$ collected at the CERN LHC in\\n2016~\\\\cite{CMS-Hcc}.\\nTwo types of jet topologies are analyzed: ``resolved-jet'' , where\\nboth charm quark jets from the Higgs decay are observed, and the\\n``merged-jet'' topology, where the two jets from the charm quark can\\nonly be reconstructed as a single jet. In both topologies, novel tools\\nbased on advanced machine learning techniques are deployed.\\nFor the ``resolved-jet'' topology BDT with gradient\\nboost are trained to enhance the signal-background separation. Four\\ncategories having 0, 1 or 2 leptons from the associated W or Z decays\\n(the 2 lepton case subdivided depending on the p$_T$ of the vector\\nboson) and 25 input variables are used for training. For the\\n``merged-jet'' topology a novel algorithm based on advanced ML methods\\nis deployed to identify jet substructures in order to tag the highly\\nboosted W, Z, and Higgs decays, giving sizable gains.\\nThe use of an adversarial network~\\\\cite{Goodfellow:2014upx} helps to\\nlargely decorrelate the algorithm from the mass of a jet while\\npreserving most of its discriminating power. For example, for large\\njets with p$_T\\\\ >\\\\ $~200 GeV, misidentification rates for charm quark\\npairs of 1\\\\35\\\\9\\\\\\nThe results of the two topologies help to provide an upper limit on\\nthe branching ratio of Higgs decays to charm quarks. There is still a\\nlong way to reach the sensitivity needed to observe this Higgs decay\\nwith SM strength.\\nWhile in the early Higgs papers BDTs were the prefered ML approach,\\nnewer analyses deploy deep learning and NN. The CMS collaboration has\\nmeasured~\\\\cite{CMS-Htautau} the inclusive cross section for the\\nproduction and subsequent decay of Higgs bosons to tau lepton pairs\\nwith 77.4 fb$^{−1}$ of data collected in 2016 and 2017.\\nA multi-classification approach is applied for each final state and\\nyear of data-taking, eight independent tasks in total. For each of\\nthem a fully connected feed-forward NN is trained. The architecture\\nconsists of two hidden layers with 200 nodes each, and five or eight\\nnodes in the output layer, each representing an event class\\nprediction, depending on the final state. The total cross section as\\nwell as cross sections for individual production modes and kinematic\\nregimes are obtained. This is made possible by the power of\\nclassification using deep learning.\\nAnother recent example is the measurement of associated production of\\ntop quark-antiquark pairs and Higgs bosons (ttH), with Higgs decaying\\nto b quarks, by the CMS collaboration~\\\\cite{CMS-ttH}. The analysis is\\nbased on 41.5 fb$^{−1}$ collected in 2017, and combined with the 2016\\nanalysis reaches an observed (expected) significance of 3.9 (3.5)\\nstandard deviations above the background-only hypothesis, providing\\nthe first evidence for ttH production with subsequent H$\\\\rightarrow$bb\\ndecays. Multiple classifiers are deployed, like BDTs for the dilepton\\nchannel, or feedforward NN with three hidden layers of 100 nodes each,\\nas implemented in Keras~\\\\cite{Keras}, for the single lepton channel.\\nThe application of ML techniques for Higgs analyses at the LHC is not\\na one-way street. Data from the simulations of the Higgs decays to\\ntau-lepton pairs were released by ATLAS to the ML community and formed\\nthe basis for the HiggsML challenge~\\\\cite{HiggsML}. It ran from May\\nto September 2014 on the Kaggle platform~\\\\cite{HiggsMLKaggle}, was\\nextremely popular, attracted 785 teams with 1942 participants and\\ngenerated 35772 submissions and more than a thousand forum posts.\\nProbably more surprising then than now, first prize was won by Gabor\\nMelis, a software developer and consultant from Hungary, using\\nartificial NN. A special HEP meets ML award was provided to data\\nscience graduate students Tianqi Chen and Tong He for offering the\\nboosted decision trees tool XGBoost, used by many participants. By now\\nCERN provides an open data portal~\\\\cite{CERN-ODP} to the LHC\\nexperiments to encourage fruitful collaboration between high energy\\nphysicists and data scientists.\\n{\\\\it Great progress in computer vision has come from convolutional\\nneural networks (CNN), inspired by the animal visual cortex, where\\nindividual neurons process information only from parts of the visual\\nfield. This ``divide-and-conquer'' strategy simplifies the NN\\narchitecture and helps features like translational and rotational\\ninvariance, very desirable for image recognition. Typically the\\nfirst layers of a CNN are for convolution and pooling. In a\\nconvolution the shape of an input function is modified by another\\nfunction by taking an integral of the product of the two. The\\nconvolutional filtering helps e.g. in edge detection.\\nHand-engineering the filters is replaced by learning them from the\\nimages. Pooling layers combine the inputs from several neurons (the\\nsimplest being a} 2x2 {\\\\it cluster) into one output neuron, thus\\nreducing the dimensionality. This is usually followed by fully\\nconnected layers like in standard DNN for the final classification\\nstep.}\\nTraditionally a HEP analysis proceeds to reconstructing higher level\\nobjects like tracks and energy deposits in electromagnetic and hadron\\ncalorimeters from the raw detector data, and finally arriving at\\nparticle level objects. A promising new approach is to apply deep\\nlearning algorithms directly to low-level detector data. This is\\nexplored in what is called end-to-end event\\nclassification~\\\\cite{Andrews}. The study is based on 2012 CMS\\nSimulated Open Data for the decays of the Higgs boson to a pair of\\nphotons. The gluon-gluon fusion Higgs production is the signal, while\\nirreducible quark fusion to photon pairs and a photon plus jet faking\\na second photon events form the backgrounds in this simplified\\nstudy. The events are simulated taking into account the interactions\\nin the detector materials and the detailed CMS geometry.\\nThe low level detector data is converted into images of size 170x360\\nin pseudorapidity $\\\\eta$ and azimuthal angle $\\\\varphi$ for the CMS\\nbarrel, and two images of size 100x100 for the two CMS endcaps\\nextending to $|\\\\eta|\\\\ <\\\\ 2.3$. Each channel contains three layers\\ncorresponding to electromagnetic and hadron energy and track\\ntransverse momentum. This is the electromagnetic-centric\\nsegmentation. Alternatively, a hadron-centric segmentation with size\\n280x360 in $\\\\eta - \\\\varphi$ is used. Inspired by the recent progress\\nin computer vision, a CNN of the Residual Net-type\\n(ResNet-15)~\\\\cite{He} is used. The initial results show promise, with\\nsignal efficiency and background rejection on par with more\\ntraditional approaches.\\n\", \"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\"}},\n",
       "       {'entity_name': 'overfitting and overtraining', 'entity_type': 'statistics_concept', 'description': 'A statistical phenomenon where a model learns noise from the training data instead of the underlying distribution, resulting in poor generalization on unseen data. Both terms describe the same issue of models becoming too complex and tailored to the training data, which negatively impacts their performance on new, unseen datasets.', 'relevant_passages': {\"\\\\section{Hypothesis tests}\\nHypothesis testing addresses the question whether some observed data sample\\nis more compatible with one theory model or another alternative one.\\nThe terminology used in statistics may sometimes be not very natural for physics applications,\\nbut it has become popular among physicists as well as long as more statistical methods\\nare becoming part of common practice. In a test, usually two hypotheses are considered:\\n\\\\begin{itemize}\\n\\\\item $H_0$, the {\\\\it null hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains only background''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a pion''}.\\n\\\\item $H_1$, the {\\\\it alternative hypothesis}.\\n\\\\\\\\Example 1: {\\\\it ``a sample contains background + signal''}.\\n\\\\\\\\Example 2: {\\\\it ``a particle is a muon''}.\\n\\\\end{itemize}\\nA {\\\\it test statistic} is a variable computed from our data sample that discriminates between the two hypotheses\\n$H_0$ and $H_1$. Usually it is a `summary' of the information available in the data sample.\\nIn physics it's common to perform an event selection based on a discriminating variable $x$.\\nFor instance, we can take as signal sample all events whose value of $x$ is above a\\nthreshold, $x > x_{\\\\mathrm{cut}}$. $x$ is an example of {\\\\it test statistic} used to discriminate\\nbetween the two hypotheses, $H_1 =$~``signal'' and $H_2 =$~``background''.\\nThe following quantities are useful to give quantitative information about a test:\\n\\\\begin{itemize}\\n\\\\item $\\\\alpha$, the {\\\\it significance level}: probability to reject $H_0$ if $H_0$ is assumed to be true (type I error, or false negative).\\nIn physics $\\\\alpha$ is equal to one minus the selection efficiency.\\n\\\\item $\\\\beta$, the {\\\\it misidentification probability}, i.e.: probability to reject $H_1$ if $H_1$ is assumed to be true\\n(type II error, or false negative). $1 - \\\\beta$ is also called {\\\\it power of the test}.\\n\\\\item a $p$-value is the probability, assuming $H_0$ to be true, of getting a value of the test statistic as result\\nof our test at least as extreme as the observed test statistic.\\n\\\\end{itemize}\\nIn case of multiple discriminating variables, a selection of a signal against a background\\nmay be implemented in different ways. E.g.: applying a selection on each individual variable, or on a combination of\\nthose variables, or selecting an area of the multivariate space which is enriched in signal events.\\n\\\\subsection{The Neyman--Pearson lemma}\\nThe Neyman--Pearson lemma~\\\\cite{Neyman_Pearson} ensures that, for a fixed significance level\\n($\\\\alpha$) or equivalently a signal efficiency ($1 - \\\\alpha$),\\nthe selection that gives the lowest possible misidentification probability ($\\\\beta$) is based on a likelihood ratio:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x|H_1)}{L(x|H_0)} > k_\\\\alpha\\\\,,\\n\\\\end{equation}\\nwhere $L(x|H_0)$ and $L(x|H_1)$ are the values of the likelihood functions for the two\\nconsidered hypotheses. $k_\\\\alpha$ is a constant whose value depends on the fixed significance\\nlevel $\\\\alpha$.\\nThe likelihood function can't always be determined exactly.\\nIn cases where it's not possible to determine the exact likelihood function,\\nother discriminators can be used as test statistics.\\nNeural Networks, Boosted Decision Trees and other machine-learning algorithms\\nare examples of discriminators that may closely approximate the performances of the exact likelihood\\nratio, approaching the Neyman--Pearson optimal performances~\\\\cite{Roe2005577}.\\nIn general, algorithms that provide a test statistic for samples with multiple variables\\nare referred to as {\\\\it multivariate discriminators}.\\nSimple mathematical algorithms exist, as well as complex implementations based on extensive CPU computations.\\nIn general, the algorithms are `trained' using input samples whose nature is known ({\\\\it training samples}),\\ni.e.: where either $H_0$ or $H_1$ is know to be true.\\nThis is typically done using data samples simulated with computer algorithms (Monte Carlo)\\nor, when possible, with control samples obtained from data.\\nAmong the most common problems that arise with training of multivariate algorithms,\\nthe size of training samples is necessarily finite, hence the true distributions for the considered hypotheses can't be determined exactly form the training sample distribution. Moreover, the distribution assumed in the simulation of the input samples may not reproduce exactly the\\ntrue distribution of real data, for instance because of systematic errors that affect our simulation.\\n\\\\subsection{Projective likelihood ratio}\\nIn case of independent variables, the likelihood functions appearing in the numerator and\\ndenominator of Eq.~(\\\\ref{eq:neymanPearsonLemma}) can be factorized as product of\\none-dimensional PDF (Eq.~(\\\\ref{eq:indVar})). Even in the cases when variables are not\\nindependent, this can be taken as an approximate evaluation of the Neyman--Pearson\\nlikelihood ratio, so we can write:\\n\\\\begin{equation}\\n\\\\lambda(x) = \\\\frac{L(x_1,\\\\cdots,x_n|H_1)}{L(x_1,\\\\cdots,x_n|H_0)}\\n\\\\simeq\\n\\\\frac{\\\\prod_{i=1}^n f_i(x_i|H_1)}{\\\\prod_{i=1}^n f_i(x_i|H_0)}\\\\,.\\n\\\\end{equation}\\nThe approximation may be improved if\\na proper rotation is first applied to the input variables in order to\\neliminate their correlation. This approach is called {\\\\it principal component analysis}.\\n\\\\subsection{Fisher discriminant}\\nFisher~\\\\cite{Fisher_discriminant} introduced a discriminator based on a linear combination of input variables\\nthat maximizes the distance of the means of the two classes while minimizing the variance,\\nprojected along a direction $\\\\mathbf{w}$:\\n\\\\begin{equation}\\nJ(\\\\mathbf{w}) = \\\\frac{|\\\\mu_0-\\\\mu_1|^2}{\\\\sigma_0^2+\\\\sigma_1^2}\\n=\\\\frac{\\\\mathbf{w}^{\\\\mathrm{T}}\\\\cdot(\\\\mathbf{m}_0 - \\\\mathbf{m}_1)}\\n{\\\\mathbf{w}^{\\\\mathrm{T}}(\\\\mathbf{\\\\Sigma}_0 + \\\\mathbf{\\\\Sigma}_1)\\\\mathbf{w}}\\\\,.\\n\\\\end{equation}\\nThe selection is achieved by requiring $J(\\\\mathbf{w}) > J_{\\\\mathrm{cut}}$, which determines an hyperplane\\nperpendicular to $\\\\mathbf{w}$.\\nExamples of two different projections for a two-dimensional case is shown in Fig.~\\\\ref{fig:Fisher}.\\nThe problem of maximising $J(\\\\mathbf{w})$ over all possible directions $\\\\mathbf{w}$\\ncan be solved analytically using linear algebra.\\n\\\\subsection{Artificial Neural Network}\\nArtificial Neural Networks (ANN)\\nare computer implementations of simplified models of how neuron cells work.\\nThe schematic structure of an ANN is shown in Fig.~\\\\ref{fig:ANN}.\\nEach node in the network receives inputs from either the input variables\\n(input layer) or from the previous layer, and provides an output\\neither of the entire network (output layer) or which is used as input to the next layer.\\nWithin a node, inputs are combined linearly with proper weights\\nthat are different for each of the nodes. Each output is then\\ntransformed using a sigmoid function $\\\\varphi$:\\n\\\\begin{equation}\\ny^{(n)}(\\\\vec{x}) = \\\\varphi\\\\left(\\n\\\\sum_{j=1}^p w_{kj}^{(n)} x_j\\n\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\varphi$ is typically:\\n\\\\begin{equation}\\n\\\\varphi(\\\\nu) = \\\\frac{1}{1+e^{-\\\\lambda\\\\nu}}\\\\,,\\n\\\\end{equation}\\nso that the output values are bound between 0 and 1.\\nIn order to find the optimal set of network weights $w_{ij}^{(n)}$, a minimization\\nis performed on the {\\\\it loss function} defined as the following sum\\nover a training sample of size $N$:\\n\\\\begin{equation}\\nL(w) = \\\\sum_{i=1}^N(y_i^{\\\\mathrm{true}}-y(\\\\vec{x}_i))^2\\\\,,\\n\\\\end{equation}\\n$y_i^{\\\\mathrm{true}}$ being usually equal to 1 for signal ($H_1$) and 0 for background ($H_0$).\\nIteratively, weights are modified ({\\\\it back propagation}) for each training event (or each group\\nof training events) using the {\\\\it stochastic gradient descent} technique:\\n\\\\begin{equation}\\nw_{ij} \\\\rightarrow w_{ij} -\\\\eta\\\\frac{\\\\partial L(w)}{\\\\partial w_{ij}}\\\\,.\\n\\\\end{equation}\\nThe parameter $\\\\eta$ controls the learning rate of the network.\\nVariations of the training implementation exist.\\nThough it can be proven~\\\\cite{ANNproof} that, under some regularity conditions,\\nneural networks with a single hidden layer can approximate any analytical function\\nwith a sufficiently high number of neurons,\\nin practice this limit is hard to achieve.\\nNetworks with several hidden layers can better manage complex variables combinations,\\ne.g.: exploiting invariant mass distributions features using only four-vectors as input~\\\\cite{Baldi:2014kfa}.\\nThose complex implementation that were almost intractable in the past\\ncan now be better approached thanks to the availability of improved training algorithms\\nand more easily available CPU power.\\n\\\\subsection{Boosted Decision Trees}\\nA {\\\\it decision tree} is a sequence of simple cuts that are sequentially\\napplied on events in a data sample. Each cut splits the sample\\ninto nodes that may be further split by the application of subsequent cuts.\\nNodes where signal or background is largely dominant are classified as leafs.\\nAlternatively, the splitting may stop if too few events per node remain, or if the total number of nodes too high.\\nEach branch on the tree represents one sequence of cuts.\\nCuts can be optimized in order to achieve the best split level.\\nOne possible implementation is to maximize for each node the gain of Gini index after a splitting:\\n\\\\begin{equation}\\nG = P(1 - P)\\\\,,\\n\\\\end{equation}\\nwhere $P$ is the purity of the node (i.e.: the fraction of signal events).\\n$G$ is equal to zero for nodes containing only signal or background events.\\nAlternative metrics can be used (e.g.: the {\\\\it cross entropy},\\nequal to: $-(P\\\\ln P+(1-P)\\\\ln(1-P))$ ) in place of the Gini index.\\nAn optimized single decision tree does not usually provide optimal performances\\nor stability, hence multiple decision trees are usually combined.\\nEach tree is added iteratively after weights are applied to test events.\\n{\\\\it Boosting} is achieved by\\niteratively reweighting the events in the training sample according to the classifier\\noutput in the previous iteration. The {\\\\it boosted decision tree} (BDT) algorithm usually\\nproceeds as follows:\\n\\\\begin{itemize}\\n\\\\item Events are reweighted using the previous iteration's classifier result.\\n\\\\item A new tree is build and optimized using the reweighted events as training sample.\\n\\\\item A score is given to each tree.\\n\\\\item The final BDT classifier result is a weighted average over all trees:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\nw_iC^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\n\\\\end{itemize}\\nOne of the most popular algorithm is the {\\\\it adaptive boosting}~\\\\cite{AdaBoost}:\\nmisclassified events only are reweighted according to the fraction of classification\\nerror of the previous tree:\\n\\\\begin{equation}\\n\\\\frac{1-f}{f}\\\\,,f=\\\\frac{N_{\\\\mathrm{misclassified}}}{N_{\\\\mathrm{tot}}}\\\\,.\\n\\\\end{equation}\\nThe weights applied to each tree are also related to the misclassification fraction:\\n\\\\begin{equation}\\ny(\\\\vec{x}) = \\\\sum_{k=1}^{N_{\\\\mathrm{trees}}}\\\\ln\\\\left(\\\\frac{1-f^{(i)}}{f^{(i)}}\\\\right)C^{(i)}(\\\\vec{x})\\\\,.\\n\\\\end{equation}\\nThis algorithm enhances the weight of events misclassified on the previous iteration\\nin order to improve the performance on those events.\\nFurther variations and more algorithms are available.\\n\\\\subsection{Overtraining}\\nAlgorithms may learn too much from the training sample, exploiting features that are\\nonly due to random fluctuations.\\nIt may be important to check for overtraining comparing the discriminator's distributions\\nfor the training sample and for an independent {\\\\it test sample}:\\ncompatible distributions will be an indication that no overtraining occurred.\\n\", \"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\"}},\n",
       "       {'entity_name': 'cousins and highland method', 'entity_type': 'analysis_technique', 'description': 'A technique for integrating over nuisance parameters in likelihood functions to obtain results that do not depend on these parameters, often used in frequentist statistical analyses.', 'relevant_passages': {\"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Treatment of nuisance parameters}\\nNuisance parameters have been introduced in Sec.~\\\\ref{sec:extLikFun}.\\nUsually, signal extraction procedures (either parameter fits or upper limits determinations) determine,\\ntogether with parameters of interest, also nuisance parameters that model effects\\nnot strictly related to our final measurement, like\\nbackground yield and shape, detector resolution, etc.\\nNuisance parameters are also used to model sources of systematic\\nuncertainties.\\nOften, the true value of a nuisance parameter is not known, but we may have some\\nestimate from sources that are external to our problem.\\nIn those cases, we can refer to {\\\\it nominal values} of the nuisance parameter and\\ntheir uncertainty. Nominal values of nuisance parameters are random variables\\ndistributed according to some PDF that depend on their true value.\\nA Gaussian distribution is the simplest assumption for nominal values of nuisance parameters.\\nAnyway, this may give negative values corresponding to the\\nleftmost tail, which are not suitable for\\nnon-negative quantities like cross sections.\\nFor instance, we may have an estimate of some background yield $b$ given by:\\n\\\\begin{equation}\\nb = \\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $L_{\\\\mathrm{int}}$ is the estimate of the integrated luminosity (assumed to be known\\nwith negligible uncertainty), $\\\\sigma_b$ is the background cross section evaluated\\nfrom theory, and $\\\\beta$ is a nuisance parameter, whose nominal value is equal to one, representing the\\nuncertainty on the cross-section evaluation. If the uncertainty on $\\\\beta$ is large, one may\\nhave a negative value of $\\\\beta$ with non-negligible probability, hence an unphysical negative value of\\nthe background yield $b$.\\nA safer assumption in such cases is to take a log normal distribution for the uncertain non-negative\\nquantities:\\n\\\\begin{equation}\\nb = e^\\\\beta \\\\sigma_b L_{\\\\mathrm{int}}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\beta$ is again distributed according to a normal distribution with nominal value equal to zero,\\nin this case.\\nUnder the Bayesian approach, nuisance parameters don't require a special treatment.\\nIf we have a parameter of interest $\\\\mu$ and a nuisance parameter $\\\\theta$,\\na Bayesian posterior will be obtained as (Eq.~(\\\\ref{eq:BayesianInferenceSimple})):\\n\\\\begin{equation}\\nP(\\\\mu,\\\\theta|\\\\vec{x}) = \\\\frac{\\nL(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\pi(\\\\mu^\\\\prime,\\\\theta^\\\\prime)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta^\\\\prime\\n}\\\\,.\\n\\\\end{equation}\\n$P(\\\\mu|\\\\vec{x})$ can be obtained as marginal PDF of $\\\\mu$ by integrating $P(\\\\mu,\\\\theta|\\\\vec{x})$ over $\\\\theta$:\\n\\\\begin{equation}\\nP(\\\\mu|\\\\vec{x}) = \\\\int P(\\\\mu,\\\\theta|\\\\vec{x}),\\\\mathrm{d}\\\\theta =\\n\\\\frac{\\n\\\\int L(\\\\vec{x};\\\\mu,\\\\theta)\\\\pi(\\\\mu,\\\\theta)\\\\,\\\\mathrm{d}\\\\theta\\n}{\\n\\\\int L(\\\\vec{x};\\\\mu^\\\\prime,\\\\theta)\\\\pi(\\\\mu^\\\\prime,\\\\theta)\\\\,\\\\mathrm{d}\\\\mu^\\\\prime\\\\,\\\\mathrm{d}\\\\theta\\n}\\\\,.\\n\\\\end{equation}\\nIn the frequentist approach, one possibility is to introduce in the likelihood\\nfunction a model for a data sample that can constrain the nuisance parameter.\\nIdeally, we may have a control sample $\\\\vec{y}$, complementary to the main\\ndata sample $\\\\vec{x}$, that only depends on the nuisance parameter, and\\nwe can write a global likelihood function as:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_y(\\\\vec{y};\\\\theta)\\\\,.\\n\\\\end{equation}\\nUsing control regions to constrain nuisance parameters is usually a good method\\nto reduce systematic uncertainties. Anyway, it may not always be\\nfeasible and in many cases we may just have information abut the nominal\\nvalue $\\\\theta^{\\\\mathrm{nom}}$ of $\\\\theta$ and its distribution obtained from a complementary measurement:\\n\\\\begin{equation}\\nL(\\\\vec{x},\\\\vec{y};\\\\mu,\\\\theta) = L_x(\\\\vec{x};\\\\mu, \\\\theta) L_\\\\theta(\\\\theta^{\\\\mathrm{nom}};\\\\theta)\\\\,.\\n\\\\end{equation}\\n$L_\\\\theta$ may be a Gaussian or log normal distribution in the easiest cases.\\nIn order to achieve a likelihood function that does not depend on\\nnuisance parameters, for many measurements at LEP or Tevatron a method\\nproposed by Cousins and Highland was adopted~\\\\cite{Cousins_Highlands}\\nwhich consists of integrating the likelihood function over the nuisance\\nparameters, similarly to what is done in the Bayesian approach (Eq.~(\\\\ref{eq:BayesianNuisance})).\\nFor this reason, this method was also called hybrid.\\nAnyway the Cousins--Highland does not guarantee to provide exact coverage,\\nand was often used as pragmatic solution in the frequentist context.\\n\"}},\n",
       "       {'entity_name': 'asimov dataset', 'entity_type': 'statistics_concept', 'description': 'A theoretical dataset used in particle physics that represents the expected values of all observables and the expected number of events under a specific hypothesis. It is utilized to evaluate estimators for parameters in statistical analyses and to facilitate the determination of parameter space for new physics searches.', 'relevant_passages': {\"\\\\section{Experimental sensitivity using the $\\\\mathcal{Q}$ estimator}\\nThe experimental sensitivity to detect a new physics signal depends on multiple factors, including the accelerator's integrated luminosity, data quality, detector triggers, simulations, and accurate estimation of events associated with known physics (background)~\\\\cite{barlow2002systematic}. Given the vast parameter space and variety of theories, there arises a need to identify a specific search region to efficiently focus experimental efforts towards generating new discoveries~\\\\cite{casadei2011statistical}.\\nPhenomenology in high-energy physics (HEP) defines this search region as the signal region, determined by the expected number of new physics events for certain observables, such as invariant mass, transverse mass, etc. A common strategy to identify this region involves maximizing the statistical significance of observing \\\\( n = b + \\\\mu s \\\\), with \\\\( \\\\mu = 1 \\\\) events consistent with the new theory, assuming the background-only hypothesis (\\\\( H_{0} \\\\)) is true. This expected number of events is referred to as Asimov data~\\\\cite{lista2016practical,cowan2011asymptotic}, and it is used to determine the parameter space window of the theory measurable in the experiment with a specific luminosity. The statistical estimator \\\\( \\\\mathcal{Q}(\\\\mu) \\\\), with expected value \\\\( n = s + b \\\\), is given by:\\n\\\\begin{eqnarray}\\n\\\\mathcal{Q}(\\\\mu) & = & 2( \\\\mu s - nLn(1 + \\\\frac{\\\\mu s}{b}) ) {} \\\\nonumber \\\\\\\\\\n\\\\mathcal{Q}(1) & = & 2( s - (s+b)Ln(1 + \\\\frac{s}{b}) ). {}\\n\\\\end{eqnarray}\\nFrom this value, \\\\( \\\\mathcal{Q}_{\\\\text{obs}}(1) \\\\) is calculated, which allows estimating the significance in the context of a distribution corresponding to background-only hypothesis:\\n\\\\begin{equation}\\n\\\\alpha(s) = p_{0} = \\\\int_{-\\\\infty}^{\\\\mathcal{Q}_{\\\\text{obs}}(1)} f(\\\\mathcal{Q}/0) d\\\\mathcal{Q}.\\n\\\\end{equation}\\nThe optimal signal region for the experimental search is determined by finding the expected number of new physics events that maximizes the statistical significance. Thus:\\n\\\\begin{equation}\\ns_{best} = \\\\max_{s} \\\\alpha(s).\\n\\\\end{equation}\\nThis value of \\\\( \\\\alpha \\\\) is converted into units of standard deviations from a \\\\( \\\\mathcal{N}(0,1) \\\\) distribution. For now, this estimate assumes that the number of background events is well-determined, with no associated statistical or systematic error. Systematic effects on the background can modify both the signal region and the upper limits of theoretical predictions, and must be taken into account in phenomenological studies. Figure~[\\\\ref{fig:12}] illustrates the distribution \\\\( f(\\\\mathcal{Q},0) \\\\) with \\\\( Q_{obs}(1) \\\\), which is used to estimate the significance of the new physics model with \\\\( s=10 \\\\) and \\\\( b=100 \\\\) background events. The value of \\\\( p_{0} \\\\) is \\\\( \\\\alpha = 0.1705 \\\\), which corresponds to \\\\( Z_{0} = 0.952 \\\\) standard deviations. This result can also be approximated using the estimator~\\\\cite{florez2016probing,cms2022portrait,atlas2022detailed}:\\n\\\\begin{equation}\\nZ_{0} = \\\\frac{s}{\\\\sqrt{s+b}} \\\\approx 0.953.\\n\\\\end{equation}\\nThis estimator is considered an approximate estimation of the signal significance and is valid when \\\\( s \\\\ll b \\\\) (Appendix~\\\\ref{sec:AppendixB}). It is worth noting that this estimator has been widely used in optimizing search regions beyond the Standard Model in the context of phenomenology and experimental analysis~\\\\cite{florez2016probing,allahverdi2016distinguishing,cms2012observation,atlas2012observation}.\\n\", \"\\\\section{Discoveries and upper limits}\\n\\\\subsection{Variations on test statistics}\\nA number of test statistics is proposed in Ref.~\\\\cite{asymptotic} that better\\nserve various purposes. Below the main ones are reported:\\n\\\\begin{itemize}\\n\\\\item {\\\\bf Test statistic for discovery:}\\n\\\\begin{equation}\\nq_0 = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(0), &\\\\hat{\\\\mu}\\\\ge 0\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} < 0\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIn case of a negative estimate of $\\\\mu$ ($\\\\hat{\\\\mu}<0$), the test statistic is set to zero in order to\\nconsider only positive $\\\\hat{\\\\mu}$ as evidence against the background-only hypothesis.\\nWithin an asymptotic approximation, the significance is given by: $Z\\\\simeq\\\\sqrt{q_0}$.\\n\\\\item {\\\\bf Test statistic for upper limit:}\\n\\\\begin{equation}\\nq_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\lambda(\\\\mu), &\\\\hat{\\\\mu}\\\\le \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} > \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nIf the $\\\\hat{\\\\mu}$ estimate is larger than the assumed value for $\\\\mu$, an upward fluctuation occurred.\\nIn those cases, $\\\\mu$ is not excluded by setting the test statistic to zero.\\n\\\\item {\\\\bf Test statistic for Higgs boson search:}\\n\\\\begin{equation}\\n\\\\tilde q_\\\\mu = \\\\left\\\\{\\n\\\\begin{array}{ll}\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|0,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(0))}, & \\\\hat{\\\\mu} < 0\\\\,,\\\\\\\\\\n-2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\hat{\\\\vec{\\\\theta}}}(\\\\mu))}\\n{L(\\\\vec{x}|\\\\mu,\\\\hat{\\\\vec{\\\\theta}}(\\\\mu))}, & 0\\\\le \\\\hat{\\\\mu} < \\\\mu\\\\,,\\\\\\\\\\n0, & \\\\hat{\\\\mu} \\\\ge \\\\mu\\\\,.\\n\\\\end{array}\\n\\\\right.\\n\\\\end{equation}\\nThis test statistics both protects against unphysical cases with $\\\\mu <0$\\nand, as the test statistic for upper limits, protects upper limits\\nin cases of an upward $\\\\hat{\\\\mu}$ fluctuation.\\n\\\\end{itemize}\\nA number of measurements performed at LEP and Tevatron used a\\ntest statistic based on the ratio of the likelihood function evaluated under\\nthe signal plus background hypothesis and under the background only hypothesis,\\ninspired by the Neyman--Pearson lemma:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|s+b)}{L(\\\\vec{x}|b)}\\\\,.\\n\\\\end{equation}\\nIn many LEP and Tevatron analyses, nuisance parameters were treated using the hybrid Cousins--Hyghland approach.\\nAlternatively, one could use a formalism similar to the profile likelihood, \\nsetting $\\\\mu=0$ in the denominator and $\\\\mu=1$ in the numerator, and minimizing\\nthe likelihood functions with respect to the nuisance parameters:\\n\\\\begin{equation}\\nq = -2\\\\ln\\\\frac{L(\\\\vec{x}|\\\\mu=1, \\\\hat{\\\\hat{\\\\theta}}(1))}{L(\\\\vec{x}|\\\\mu=0,\\\\hat{\\\\hat{\\\\theta}}(0))}\\\\,.\\n\\\\end{equation}\\nFor all the mentioned test statistics, asymptotic approximations exist and\\nare reported in Ref.~\\\\cite{asymptotic}. Those are based either on Wilks' theorem\\nor on Wald's approximations~\\\\cite{Wald}. If a value $\\\\mu$ is tested, and \\nthe data are supposed to be distributed according to another value of the signal strength $\\\\mu^\\\\prime$,\\nthe following approximation holds, asymptotically:\\n\\\\begin{equation}\\n-2\\\\ln\\\\lambda(\\\\mu) = \\\\frac{(\\\\mu-\\\\hat{\\\\mu})^2}{\\\\sigma^2} + {\\\\cal O}\\\\left(\\\\frac{1}{\\\\sqrt{N}}\\\\right)\\\\,,\\n\\\\end{equation}\\nwhere $\\\\hat{\\\\mu}$ is distributed according to a Gaussian with average $\\\\mu^\\\\prime$ and\\nstandard deviation $\\\\sigma$. The covariance matrix for the nuisance parameters is\\ngiven, in the asymptotic approximation, by:\\n\\\\begin{equation}\\nV_{ij}^{-1} = \\\\left.\\\\left<\\\\frac{\\\\partial^2\\\\ln L}{\\\\partial\\\\theta_i\\\\partial\\\\theta_j}\\\\right>\\\\right|_{\\\\mu=\\\\mu^\\\\prime}\\\\,,\\n\\\\end{equation}\\nwhere $\\\\mu^\\\\prime$ is assumed as value for the signal strength.\\nIn some cases, asymptotic approximations (Eq.~(\\\\ref{eq:waldTestStat})) can be written in terms of an\\n{\\\\it Asimov dataset}~\\\\cite{Asimov}:\\n\\\\begin{displayquote}\\n{\\\\it We define the Asimov data set such that when one uses it to evaluate the estimators\\nfor all parameters, one obtains the true parameter values}~\\\\cite{asymptotic}.\\n\\\\end{displayquote}\\nIn practice, an Asimov dataset is a single ``representative'' dataset obtained by replacing all\\nobservable (random) varibles with their expecteted value. In particular,\\nall yields in the data sample (e.g.: in a binned case) are replaced with their expected values, that may be non integer values.\\nThe median significance for different cases of test statistics can be computed in this\\nway without need of producing extensive sets of toy Monte Carlo. The implementation\\nof those asymptotic formulate is available in the {\\\\sc RooStats} library, released\\nas part an optional component {\\\\sc Root}~\\\\cite{Root}.\\n\"}},\n",
       "       {'entity_name': 'lookelsewhere effect', 'entity_type': 'statistics_concept', 'description': 'A statistical phenomenon in particle physics where the significance of an observed signal is inflated due to testing across multiple regions or parameters, increasing the chance of finding a seemingly significant result by random fluctuation.', 'relevant_passages': {\"\\\\section{Discoveries and upper limits}\\n\\\\subsection{The look-elsewhere effect}\\nWhen searching for a signal peak on top of a background that is smoothly distributed over a wide range,\\none can either know the position of the peak or not.\\nOne example in which the peak position is known is the\\nsearch for a rare decay of a known particle, like $\\\\mathrm{B}_{\\\\mathrm{s}}\\\\rightarrow\\\\mu^+\\\\mu^-$.\\nA case when the position was not know was the search for the Higgs boson, whose mass is not\\nprediceted by theory.\\nIn a case like the decay of a particle of known mass, it's easy to compute the peak significance:\\nfrom the distribution of the test statistic $f(q)$ computed assuming $\\\\mu=0$ (background only),\\ngiven the observed value of the test statistic $q^{\\\\mathrm{obs}}$, a $p$-value can be determined and then\\ntranslated into a significance level:\\n\\\\begin{equation}\\np = \\\\int_{q^{\\\\mathrm{obs}}}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q,\\\\quad Z = \\\\Phi^{-1}(1-p)\\\\,.\\n\\\\end{equation}\\nIn case, instead, the search is performed without knowing the position of the peak,\\nEq.~(\\\\ref{eq:localPval}) gives only a {\\\\it local $p$-value}, which means it reflects\\nthe probability that a background fluctuation {\\\\it at a given mass value $m$} gives a value\\nof the test statistic greater than the observed one:\\n\\\\begin{equation}\\np(m) = \\\\int_{q^{\\\\mathrm{obs}}(m)}^{+\\\\infty} f(q|\\\\mu=0)\\\\,\\\\mathrm{d}q\\\\,.\\n\\\\end{equation}\\nThe {\\\\it global $p$-value}, instead, should quantify the probability that a background\\nfluctuation {\\\\it at any mass value} gives a value of the test statistic greater than the\\nobserved one.\\nThe chance that an overfluctuation occurs for {\\\\it at least} one mass value increases with\\nthe size of the search range, and the magnitude of the effect depends on the resolution.\\nOne possibility to evaluate a global $p$-value is to let also $m$ fluctuate in the test statistic:\\n\\\\begin{equation}\\n\\\\hat{q} = -2\\\\ln \\\\frac{L(\\\\mu=0)}{L(\\\\hat{\\\\mu};\\\\hat{m})}\\\\,.\\n\\\\end{equation}\\nNote that in the numerator $L$ doesn't depend on $m$ for $\\\\mu=0$. This is a case\\nwhere Wilks' theorem doesn't apply, and no simple asymptotic approximations exist.\\nThe global $p$-value can be computed, in principle, as follows:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = \\\\int_{\\\\hat{q}^{\\\\mathrm{obs}}}^{+\\\\infty}f(\\\\hat{q}|\\\\mu=0)\\\\,\\\\mathrm{d}\\\\hat{q}_0\\\\,.\\n\\\\end{equation}\\nThe effect in practice can be evaluated with brute-force toy Monte Carlo:\\n\\\\begin{itemize}\\n\\\\item Produce a large number of pseudoexperiments simulating background-only samples.\\n\\\\item Find the maximum $\\\\hat{q}$ of the test statistic $q$ in the entire search range.\\n\\\\item Determine the distribution of $\\\\hat{q}$.\\n\\\\item Compute the global $p$-value as probability to have a value of $\\\\hat{q}$ greater than the observed one.\\n\\\\end{itemize}\\nThis procedure usually requires very large toy Monte Carlo samples in order to treat a discovery case:\\na $p$-value close to $3\\\\times 10^{−7}$ ($5\\\\sigma$ level) requires\\na sample significantly larger than $\\\\sim 10^7$ entries in order to\\ndetermine the $p$-value with small uncertainty.\\nAn asymptotic approximation for the global $p$-value is given by\\nthe following inequation~\\\\cite{lee_trial}~\\\\footnote{\\nIn case of a test statistic for discovery $q_0$ (Eq.~(\\\\ref{eq:tsfd})), the term $P(\\\\chi^2 > u)$\\nin Eq.~(\\\\ref{eq:leeasin}) achieves an extra factor 1/2, which is usually not be present\\nfor other test statistics.\\n}:\\n\\\\begin{equation}\\np^{\\\\mathrm{glob}} = P(\\\\hat{q}>u ) \\\\le \\\\left<N_u\\\\right> +\\nP(\\\\chi^2>u)\\\\,,\\n\\\\end{equation}\\nwhere $P(\\\\chi^2>u)$ is a standard $\\\\chi^2$ probability and\\n$\\\\left<N_u\\\\right>$ is the average number of {\\\\it upcrossings} of\\nthe test statistic, i.e.: the average number of times that\\nthe curve $q(m)$ crosses a given horizontal line at a level $u$ with a positive derivative,\\nas illustrated in Fig.~\\\\ref{fig:lee}.\\nThe number of upcrossings may be very small for some values of $u$,\\nbut an approximate scaling law exists and allows to perform the computation\\nat a more convenient level $u_0$:\\n\\\\begin{equation}\\n\\\\left<N_u\\\\right> = \\\\left<N_{u_0}\\\\right> e^{-{(u-u_0)}/{2}}\\\\,.\\n\\\\end{equation}\\nSo, $ \\\\left<N_{u_0}\\\\right>$ can be more conveniently evaluated using\\na reasonable number of toy Monte Carlo generations, then it can be extrapolated following\\nthe exponential scaling law.\\nNumerical comparisons of this approach with the full toy Monte Carlo\\nshow that good agreement is achieved for sufficiently\\nlarge number of observations.\\nIn case more parameters are estimated from data, e.g.: when searching for\\na new resonance whose mass and width are both unknown, the look-elsewere\\neffect can be addressed with an extension of the approach described above,\\nas detailed in Ref.~\\\\cite{leeND}.\\n\\\\addcontentsline{toc}{chapter}{Bibliography}\\n\\\\bibliographystyle{ieeetr}\\n\\\\bibliography{lista}\\n\\\\end{documen\", \"\\\\section{Making a discovery}\\nWe now turn from setting limits, to say what you did not see,\\nto the more exciting prospect of making a discovery.\\nRemembering hypothesis testing, in claiming a discovery you have to show that your data can't be explained without it.\\nThis is \\nquantified by the $p-$value: the probability of getting a result this extreme (or worse) under the null hypothesis/Standard Model. \\n(This is {\\\\it not} `The probability that the Standard Model is correct', but it seems impossible for journalists\\nto understand the difference.)\\nSome journals (particularly in psychology) refuse to publish papers giving $p-$values.\\nIf you do lots of studies, some will have low $p-$values (5\\\\The danger is that these get published, but the unsuccessful ones are binned.\\nIs $p$ like the significance $\\\\alpha$? Yes and no. The formula is the same, but $\\\\alpha$ is a property of the test, computed before you see the data.\\n$p$ is a property of the data. \\n\\\\subsection{Sigma language} \\nThe probability ($p-$value) is often\\ntranslated into Gaussian-like language: the probability of a result more than 3$\\\\sigma$ from the mean is 0.27\\\\ whether one takes the 1-tailed or 2-tailed option. Both are used.)\\nIn reporting a result with a significance of `so many $\\\\sigma$' there is no actual \\n$\\\\sigma$ involved: it is just a translation to give a better feel for the size of the probability.\\nBy convention, 3 sigma, $p= 0.0013$ is reported as `Evidence for' whereas a full \\n5 sigma\\\\\\\\ $p=0.0000003$ is required for `discovery of'.\\n\\\\subsection{The look-elsewhere effect}\\nYou may think that the requirement for 5 $\\\\sigma$ is excessively cautious.\\nIts justification comes from history---too many 3- and 4- sigma `signals' have gone away when more data was taken.\\nThis is partly explained by the `look-elsewhere effect'. How many peaks can you see in the\\ndata in Fig.~\\\\ref{fig:LEE}?\\nThe answer is that there are none. The data is in fact purely random and flat. But the human eye is very good at seeing features.\\nWith 100 bins, a $p-$value below 1\\\\ This can be factored in, to some extent, using pseudo-experiments, but this does\\nnot allow for the sheer number of plots being produced by \\nhard-working physicists looking for something. Hence the need for caution.\\nThis is not just ancient history. ATLAS and CMS recently observed a signal in the $\\\\gamma \\\\gamma$ mass around 750~GeV, with a significance of\\n$3.9 \\\\sigma$ (ATLAS) and $3.4 \\\\sigma$ (CMS), which went away when more data was taken.\\n\\\\subsection{Blind analysis}\\nIt is said\\\\footnote{This story is certainly not historically accurate, but it's still a good story (\\\\textit{quoteinvestigator.com}: \\\\url{https://quoteinvestigator.com/2014/06/22/chip-away/}).} that when Michaelangelo was asked how he created his masterpiece sculpture `David' \\nhe replied\\n`It was easy---all I did was get a block of marble and chip away everything that didn't look like David'.\\nSuch creativity may be good for sculpture, but it's bad for physics. \\nIf you take your data and devise cuts to remove all the events that don't look like the signal you want to see, then whatever is left \\nat the end will look like that signal. \\nMany/most analyses are now done `blind'. \\nCuts are devised using Monte Carlo and/or non-signal data.\\nYou only `open the box' once the cuts are fixed. Most collaborations have a formal procedure for doing this.\\nThis may seem a tedious imposition, but we have learnt the hard way that it avoids embarrassing mistakes.\\n\"}},\n",
       "       {'entity_name': 'goodness of fit', 'entity_type': 'statistics_concept', 'description': \"A statistical method used to evaluate how well a theoretical model describes observed data by comparing the model's predictions with actual measurements. This assessment often involves various metrics, such as the chi-squared statistic and likelihood functions, to quantify the agreement between the model and the data.\", 'relevant_passages': {\"\\\\section{What is Statistics?}\\nStatistics is used to provide quantitative results that give summaries of available data. In High Energy Physics,\\nthere are several different types of statistical activities that are used:\\n\\\\begin{itemize}\\n\\\\item{Parameter Determination:\\\\\\\\\\nWe analyse the data in order to extract the best value(s) of one or more parameters in a model. This could be,\\nfor example, the gradient and intercept of a straight line fit to the data; or the mass of the Higgs boson, as deduced using its decay products. In all cases, as well as obtaining the best values of the parameter(s), their uncertainties and possible correlations must be specified. }\\n\\\\item{Goodness of Fit: \\\\\\\\\\nWe are comparing a single theory with the data, in order to see if they are compatible. If the theory contains free parameters, their best values need to be used to check the Goodness of Fit. If the quality of the fit is unsatisfactory, the best values of the parameters are probably meaningless. }\\n\\\\item{Hypothesis Testing: \\\\\\\\ \\nHere we are comparing the data with two different theories, to see which provides a better description. For example, we may be very interested in knowing whether a model involving the production of a supersymmetric particle is better than one without it. }\\n\\\\item{Decision Making: \\\\\\\\\\nAs the result of the information we have available, we want to decide what further action to take. For example, we may have some evidence that our data shows hints of an exciting discovery, and need to decide whether we should collect more data. This was the situation faced by the CERN management in 2000, when there were perhaps hints of a Higgs boson in data collected at the LEP Collider.\\nSuch decisions usually require a `cost function' for the various possible outcomes, as well as assessments of their relative probabilities. In the example just quoted, numerical values were needed for the cost of missing an important discovery if the experiment was not continued; and on the other hand of running the LEP Collider for another year and for delaying the \\nstart of building the Large Hadron Collider.\\nDecision Making is not considered further in these lectures. }\\n\\\\end{itemize}\\n\", \"\\\\section{Least squares: Basic idea}\\nAs a specific example, we will consider fitting a straight line $y = a + bx$ to some data, which consist of a series on $n$ data\\npoints, each of which specifies $(x_i, y_i \\\\pm \\\\sigma_i)$ i.e. at precisely known $x_i$, the $y$ co-ordinate is measured\\nwith an uncertainty $\\\\sigma_i$. The $\\\\sigma_i$ are assumed to be uncorrelated. The more general case could involve \\n\\\\begin{itemize}\\n\\\\item{a more complicated functional form than linear;}\\n\\\\item{multidimensional $x$ and/or $y$;}\\n\\\\item{correlations among the $\\\\sigma_i$; and}\\n\\\\item{uncertainties on the $x_i$ values.}\\n\\\\end{itemize} \\nIn Particle Physics, we often deal with a histogram of some physical quantity $x$ (e.g. mass, angle, \\ntransverse momentum, etc.), in which case $y$ is simply the number of counts for that $x$ bin. Another possiblity\\nis that $y$ and $x$ are both physical quantities e.g. we have a two-dimensional plot showing the recession velocities \\nof galaxies as a function their distance. \\nThere are two statistical issues: Are our data consistent with the theory i.e. a straight line? And what are \\nthe best estimates of the parameters, the intercept and the gradient? The former is a Goodness of Fit \\nissue, while the latter is Parameter Determination. The Goodness of Fit is more fundamental, in that\\nif the data are not consistent with the hypothesis, the parameter values are meaningless. However, we will first\\nconsider Parameter Detemination, since checking the quality of the fit requires us to use the best straight\\nline. \\nThe data statistic used for both questions is $S$, the weighted sum of squared discrepancies\\\\footnote{Many people \\nrefer to this as $\\\\chi^2$. I prefer S, because otherwise a discussion about whether or not $\\\\chi^2$ follows the\\nmathematical $\\\\chi^2$ distribution sounds confusing.}\\n\\\\begin{equation}\\nS = \\\\Sigma (y_i^{th} - y_i^{obs})^2/\\\\sigma_i^2 = \\\\Sigma (a + bx_i - y_i^{obs})^2 / \\\\sigma_i^2\\n\\\\end{equation}\\nwhere $y_i^{th} = a + bx_i$ is the predicted value of $y$ at $x_i$, and $y_i^{obs}$ is the observed value. In the \\nexpression for $S$, we regard the data $(x_i, y_i \\\\pm \\\\sigma_i)$ as being fixed, and the parameters $a$\\nand $b$ as being variable.\\nIf for specific values of $a$ and $b$ the predicted values of $y$ and the corresponding observed ones are all close \\n(as measured in terms of the \\nuncertainties $\\\\sigma$), then $S$ will be `small', while significant discrepancies result in large $S$. Thus, according\\nto the least squares method, the best values of the parameters are those that minimise $S$, and the width of the $S$\\ndistribution determines their uncertainties. For a good fit, the value of $S_{min}$ should be `small'. A more quantative\\ndiscussion of `small' appears below. \\nTo determine the best values of $a$ and $b$, we need to set the first derivatives of $S$ with respect to $a$ and $b$ both\\nequal to zero. This leads to two simultaneous linear equations for $a$ and $b$ \\\\footnote{The derivatives are linear in the \\nparameters, because the functional form is linear in them. This would also be true for more complicated situations \\nsuch as a higher order polynomial (Yes, with respect to the coefficients, a $10^{th}$ order polynomial is linear),\\na series of inverse powers, Fourier series, etc.} \\nwhich are readily solved, to yield\\n\\\\begin{equation}\\n\\\\begin{split}\\na&=\\\\frac{<x^2><y> - <xy><x>}{<x^2> - <x>^2} \\\\\\\\\\nb&=\\\\frac {<xy> - <x> <y>}{<x^2> - <x>^2} \\n\\\\end{split}\\n\\\\end{equation} \\nwhere $<f>\\\\ = \\\\Sigma (f_i/\\\\sigma_i^2) / \\\\Sigma (1/\\\\sigma_i^2)$ i.e it is the weighted average of the quantity inside the\\nbrackets. If the positions of the data points are such that $<x>\\\\ = 0$, then $a=\\\\ <y>$, i.e. the height of the\\nbest fit line at the weighted centre of gravity of the data points is just the weighted average of the $y$ values. \\nIt is also essential to calculate the uncertainties $\\\\sigma_a$ and $\\\\sigma_b$ on the parameters and their correlation\\ncoefficient $\\\\rho = cov/(\\\\sigma_x \\\\sigma_y)$, where $cov$ is their covariance. The elements of the inverse\\ncovariance matrix $M$ are given by\\n\\\\begin{equation}\\n\\\\begin{split}\\nM_{aa} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a^2} = \\\\Sigma (1/\\\\sigma_i^2) \\\\\\\\\\nM_{ab} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial a \\\\ \\\\partial b} = \\\\Sigma(x_i/\\\\sigma_i^2) \\\\\\\\\\nM_{bb} &=\\\\frac{1}{2} \\\\frac{\\\\partial^2S}{\\\\partial b^2} = \\\\Sigma(x_i^2/\\\\sigma_i^2) \\\\\\\\\\n\\\\end{split}\\n\\\\end{equation}\\nThe covariance matrix is obtained by inverting $M$. Since the covariance is proportional to $-< x >$, if the \\ndata are centred around $x = 0$, the uncertainties on $a$ and $b$ will be uncorrelated. That is \\none reason why track parameters are usually specified at the centre of the track, rather than at its starting point.\\n\\\\subsection{Correlated uncertainties on data}\\nSo far we have considered that the uncertainties on the data are uncorrelated, but this is not always the case; correlations can \\narise from some common systematic. Then instead of the first equation of (\\\\ref{eqn:S}), we use\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (y_i^{th} - y_i^{obs})E_{ij} (y_j^{th} - y_j^{obs})\\n\\\\end{equation}\\nwhere the double summation is over $i$ and $j$, and $E$ is the inverse covariance matrix\\\\footnote{We use the symbol $E$ for the inverse covariance matrix of the measured variables $y$, and $M$ for that of the output parameters (e.g. $a$ and $b$\\nfor the straight line fit).}\\nfor the uncertainties on the \\n$y_i$. For the special case of uncorrelated uncertainties, the only non-zero elements of $E$ are the diagonal ones \\n$E_{ii} = 1/\\\\sigma_i^2$ and then \\neqn. (\\\\ref{correlated_S}) reduces to (\\\\ref{eqn:S}).\\nThis new equation for $S$ can then be minimised to give the best values of the parameters, and $S_{min}$ can be used in a Goodness of Fit test. As before, if $y^{th}$ is linear in the parameters, their best estimates can be obtained by solving simultaneous linear equations, without the need for a minimisation programme.\\n\", '\\\\section{Introduction to Lecture 2}\\nThis lecture deals with two different methods for determining parameters, least squares and likelihood, \\nwhen a functional form is fitted\\nto our data. A simple example would be straight line fitting, where the parameters are the intercept and gradient\\nof the line. However the methods are much more general than this. Also there are other\\nmethods of extracting parameters; these include the more fundamental Bayesian and Frequentist methods,\\nwhich are dealt with in Lecture 3 . \\nThe least squares method also provides a measure of Goodness of Fit for the agreement between the theory with the \\nbest values of the parameters, and the data; this is dealt with in section \\\\ref{GofF}. The likelihood technique plays \\nan important role in the Bayes approach, and likelihood ratios are relevant for choosing between two hypotheses;\\nthis is covered in Lecture 4. \\n', \"\\\\section{Goodness of fit}\\nYou have the best fit model to your data---but is it good enough? The upper plot in Fig.~\\\\ref{fig:badfit}\\nshows the best straight line through a set of points which are clearly not well described by a straight line. How can one quantify this?\\nYou construct some measure of agreement---call it $t$---between the model and the data.\\nConvention: $t\\\\geq 0$, $t=0$ is perfect agreement. Worse agreement implies larger $t$.\\nThe null hypothesis $H_0$ is that the model did indeed produce this data.\\nYou calculate the\\n$p-$value: the probability under $H_0$ of getting a $t$ this bad, or worse. This is shown schematically in the lower plot.\\nUsually this can be done using known algebra---if not one can use simulation (a so-called `Toy Monte Carlo').\\n\\\\subsection{\\\\texorpdfstring\\n{The $\\\\chi^2$ distribution}\\n{The chi distribution}}\\nThe overwhelmingly most used such measure of agreement is the quantity $\\\\chi^2$\\n\\\\begin{equation}\\n\\\\chi^2 = \\\\sum_1^N \\\\left({y_i-f(x_i) \\\\over \\\\sigma_i}\\\\right)^2\\n\\\\quad.\\n\\\\end{equation}\\nIn words: the total of the squared differences between prediction and data, scaled by the expected error. \\nObviously each term will be about 1, so $\\\\left<\\\\chi^2\\\\right> \\\\approx N$,\\nand this turns out to be exact.\\nThe distribution for $\\\\chi^2$ is given by\\n\\\\begin{equation}\\nP(\\\\chi^2;N)={1 \\\\over 2^{N/2} \\\\Gamma(N/2)} \\\\chi^{N-2} e^{-\\\\chi^2/2} \\n\\\\end{equation}\\nshown in Fig.~\\\\ref{fig:chisq1}, \\nthough this is in fact not much used: one is usually interested in the $p-$value,\\nthe probability (under the null hypothesis) of getting a value of $\\\\chi^2$ as large as, or larger than, the one observed. This can be found in ROOT with {\\\\tt TMath::Prob(chisquared,ndf)},\\nand\\nin R from {\\\\tt 1-pchisq(chisquared,ndf)}.\\nThus for example with \\n$N=10,\\\\chi^2=15$ then $p=0.13$. This is probably OK. \\nBut for\\n$N=10,\\\\chi^2=20$ then $p=0.03$, which is probably not OK.\\nIf the model has parameters which have been adjusted to fit the data, this \\nclearly reduces $\\\\chi^2$. It is a very useful fact that \\nthe result also follows a $\\\\chi^2$ distribution for $NDF=N_{data}-N_{parameters}$\\nwhere $NDF$ is called the `number of degrees of freedom'.\\nIf your $\\\\chi^2$ is suspiciously big, there are 4 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your model is wrong,\\n\\\\item Your data are wrong,\\n\\\\item Your errors are too small, or\\n\\\\item You are unlucky.\\n\\\\end{enumerate}\\nIf your $\\\\chi^2$ is suspiciously small there are 2 possible reasons:\\n\\\\begin{enumerate}\\n\\\\item Your errors are too big, or\\n\\\\item You are lucky.\\n\\\\end{enumerate}\\n\\\\subsection{Wilks' theorem}\\nThe Likelihood on its own tells you {\\\\it nothing}.\\nEven if you include all the constant factors normally omitted in maximisation.\\nThis may seem counter-intuitive, but it is inescapably true.\\nThere is a theorem due to \\nWilks which is frequently invoked and appears to link likelihood and $\\\\chi^2$,\\nbut it does so only in very specific circumstances. \\nGiven two nested models, for large $N$\\nthe improvement in $ \\\\ln L$ is distributed like $\\\\chi^2$ in $- 2\\\\Delta \\\\ln L$, with $NDF$ the number of extra parameters.\\nSo suppose you have some data with many $(x,y)$ values and two models, Model 1 being linear and Model 2 quadratic.\\nYou maximise the likelihood using Model 1 and then using Model 2: the Likelihood increases as more parameters are available ($NDF=1$). If this increase is significantly \\nmore than $N$ that justifies using Model 2 rather than Model 1. \\nSo it may tell you whether or not the extra term in a quadratic gives a meaningful improvement, but not \\nwhether the final quadratic (or linear) model is a good one.\\nEven this has an important exception. it does \\nnot apply if Model 2 contains a parameter which is meaningless under Model 1. \\nThis is a surprisingly common occurrence. Model 1 may be background, Model 2 background plus a Breit-Wigner with adjustable mass, width and normalization ($NDF=3$).\\nThe mass and the width are meaningless under Model 1 so Wilks' theorem does not apply and the improvement in likelihood cannot be translated into a $\\\\chi^2$ for testing.\\n\\\\subsection{Toy Monte Carlos and likelihood for goodness of fit}\\nAlthough the likelihood contains no information about the goodness of fit of the model,\\nan obvious way to get such information is to run many simulations of the model, plot the spread of fitted likelihoods and use it to get the $p-$value.\\nThis may be obvious, but it is wrong~\\\\cite{Heinrich}.\\nConsider a test case observing decay times where the model is \\na simple exponential $P(t)={ 1 \\\\over \\\\tau}e^{-t/\\\\tau}$, with $\\\\tau$ an adjustable parameter.\\nThen\\nyou get the \\nLog Likelihood $\\\\sum (-t_i/\\\\tau - \\\\ln \\\\tau)=-N(\\\\overline t /\\\\tau + \\\\ln \\\\tau)$\\nand maximum likelihood gives $\\\\hat t = \\\\overline t = {1 \\\\over N} \\\\sum_i t_i$,\\nso\\n$\\\\ln L(\\\\hat t;x)= - N(1 + \\\\ln \\\\overline t)$ . This holds\\nwhatever the original sample $\\\\{t_i\\\\}$ looks like:\\nany distribution with the same $\\\\overline t$ has the same likelihood, after fitting.\\n\", '\\\\section{Worked example: Lifetime determination}\\nHere we consider an experiment which has resulted in $N$ observed decay times $t_i$ of a particle\\nwhose lifetime $\\\\tau$ we want to determine. The probability density for observing a decay at time $t$ \\nis \\n\\\\begin{equation}\\np(t;\\\\tau) = (1/\\\\tau) \\\\ e^{-t/\\\\tau}\\n\\\\end{equation} \\nNote the essential normalisation factor $1/\\\\tau$; without this the likelihood method does not work.\\nIt should be realised that realistic situations are more complicated than this. For example, we ignore\\nthe possibility of backgrounds, time resolution which smears the expected values of $t$, acceptance or \\nefficiency effects which vary with $t$, etc., but this enables us to estimate $\\\\tau$ and its uncertainty\\n$\\\\sigma_{\\\\tau}$ analytically. In real practical cases, it is almost always necessary to calculate the \\nlikelihood as a function of $\\\\tau$ numerically. \\nFrom equation \\\\ref{exp} we calculate the log-likelihood as\\n\\\\begin{equation}\\n\\\\ln L(\\\\tau) = \\\\ln[\\\\Pi\\\\ (1/\\\\tau) e^{-t_i/\\\\tau}] \\\\ \\\\ = \\\\ \\\\ \\\\Sigma (-\\\\ln \\\\tau - t_i/\\\\tau)\\n\\\\end{equation}\\nDifferentiating $\\\\ln L(\\\\tau)$ with respect to $\\\\tau$ and setting the derivative to zero then yields\\n\\\\begin{equation}\\n\\\\tau = \\\\Sigma t_i/N\\n\\\\end{equation}\\nThis equation has an appealing feature, as it can be read as ``The mean lifetime is equal to the mean lifetime\",\\nwhich sounds as if it must be true. However, what it really says is not quite so trivial: ``Our\\nbest estimate of the lifetime parameter $\\\\tau$ is equal to the mean of the $N$ observed decay times in our \\nexperiment.\"\\nWe next calculate $\\\\sigma_{\\\\tau}$ from the second derivative of $\\\\ln L$, and obtain \\n\\\\begin{equation}\\n\\\\sigma_{\\\\tau} = \\\\tau/\\\\sqrt N\\n\\\\end{equation}\\nThis exhibits a common feature that the uncertainty of our parameter estimate decreases as $1/\\\\sqrt N$ as we \\ncollect more and more data. However, a potential problem arises from the fact that our estimated uncertainty\\nis proportional to our estimate of the parameter. This is relevant if we are trying to combine different experimental\\nresults on the lifetime of a particle. For combining procedures which weight each result by $1/\\\\sigma^2$, a \\nmeasurement where the fluctuations in the observed times result in a low estimate of $\\\\tau$\\nwill tend to be over-weighted (compare the section on `Combining Experiments\\' in Lecture 1), \\nand so the weighted average would be biassed \\ndownwards. This shows that it is better to combine different experiments at the data level, rather than \\nsimply trying to use their results.\\nOne final point to note about our simplified example is that the likelihood $L(\\\\tau)$ depends on the \\nobservations only via the {\\\\bf sum} of the times $\\\\Sigma t_i$ i.e. their {\\\\bf distribution} is \\nirrelevant. Thus the likelihood distributions for two experiments having the same number of events and the \\nsame sum of observed decay times, but with one having the decay times consistent with an exponential \\ndistribution and the other having something completely different (e.g. all decays occur at the same time), \\nwould have identical likelihood\\nfunctions. This provides an example of the fact that the unbinned likelihood function does not in general provide\\nuseful information on Goodness of Fit. \\n'}},\n",
       "       {'entity_name': 'uncertainty in measurements and parameters', 'entity_type': 'statistics_concept', 'description': 'A quantification of the doubt regarding the value of measurements and parameter estimates, often expressed as standard deviations or confidence intervals. This concept reflects the precision of both measurements and parameter estimates, and is derived from statistical methods such as the likelihood function, playing a crucial role in the interpretation of results in particle physics.', 'relevant_passages': {'\\\\section{Worked example: Lifetime determination}\\nHere we consider an experiment which has resulted in $N$ observed decay times $t_i$ of a particle\\nwhose lifetime $\\\\tau$ we want to determine. The probability density for observing a decay at time $t$ \\nis \\n\\\\begin{equation}\\np(t;\\\\tau) = (1/\\\\tau) \\\\ e^{-t/\\\\tau}\\n\\\\end{equation} \\nNote the essential normalisation factor $1/\\\\tau$; without this the likelihood method does not work.\\nIt should be realised that realistic situations are more complicated than this. For example, we ignore\\nthe possibility of backgrounds, time resolution which smears the expected values of $t$, acceptance or \\nefficiency effects which vary with $t$, etc., but this enables us to estimate $\\\\tau$ and its uncertainty\\n$\\\\sigma_{\\\\tau}$ analytically. In real practical cases, it is almost always necessary to calculate the \\nlikelihood as a function of $\\\\tau$ numerically. \\nFrom equation \\\\ref{exp} we calculate the log-likelihood as\\n\\\\begin{equation}\\n\\\\ln L(\\\\tau) = \\\\ln[\\\\Pi\\\\ (1/\\\\tau) e^{-t_i/\\\\tau}] \\\\ \\\\ = \\\\ \\\\ \\\\Sigma (-\\\\ln \\\\tau - t_i/\\\\tau)\\n\\\\end{equation}\\nDifferentiating $\\\\ln L(\\\\tau)$ with respect to $\\\\tau$ and setting the derivative to zero then yields\\n\\\\begin{equation}\\n\\\\tau = \\\\Sigma t_i/N\\n\\\\end{equation}\\nThis equation has an appealing feature, as it can be read as ``The mean lifetime is equal to the mean lifetime\",\\nwhich sounds as if it must be true. However, what it really says is not quite so trivial: ``Our\\nbest estimate of the lifetime parameter $\\\\tau$ is equal to the mean of the $N$ observed decay times in our \\nexperiment.\"\\nWe next calculate $\\\\sigma_{\\\\tau}$ from the second derivative of $\\\\ln L$, and obtain \\n\\\\begin{equation}\\n\\\\sigma_{\\\\tau} = \\\\tau/\\\\sqrt N\\n\\\\end{equation}\\nThis exhibits a common feature that the uncertainty of our parameter estimate decreases as $1/\\\\sqrt N$ as we \\ncollect more and more data. However, a potential problem arises from the fact that our estimated uncertainty\\nis proportional to our estimate of the parameter. This is relevant if we are trying to combine different experimental\\nresults on the lifetime of a particle. For combining procedures which weight each result by $1/\\\\sigma^2$, a \\nmeasurement where the fluctuations in the observed times result in a low estimate of $\\\\tau$\\nwill tend to be over-weighted (compare the section on `Combining Experiments\\' in Lecture 1), \\nand so the weighted average would be biassed \\ndownwards. This shows that it is better to combine different experiments at the data level, rather than \\nsimply trying to use their results.\\nOne final point to note about our simplified example is that the likelihood $L(\\\\tau)$ depends on the \\nobservations only via the {\\\\bf sum} of the times $\\\\Sigma t_i$ i.e. their {\\\\bf distribution} is \\nirrelevant. Thus the likelihood distributions for two experiments having the same number of events and the \\nsame sum of observed decay times, but with one having the decay times consistent with an exponential \\ndistribution and the other having something completely different (e.g. all decays occur at the same time), \\nwould have identical likelihood\\nfunctions. This provides an example of the fact that the unbinned likelihood function does not in general provide\\nuseful information on Goodness of Fit. \\n', \"\\\\section{Likelihood}\\nThe likelihood function is very widely used in many statistics applications. In this \\nSection, we consider it just for Parameter Determination. An important feature of the \\nlikelihood approach is that it can be used with {\\\\bf unbinned} data, and\\nhence can be applied in situations where there are not enough individual observations\\nto construct a histogram for the $\\\\chi^2$ approach. \\nWe start by assuming that we wish to fit our data $x$, using a model $f(x;\\\\mu)$ \\nwhich has one or more free parameters $\\\\mu$, whose value(s) we need to determine. \\nThe function $f$ is known as the `probability distribution' ($pdf$) and \\nspecifies the probability (or probability density, for the data having continuous as\\nopposed to discrete values) for obtaining different values of the data, when the parameter(s)\\nare specified. Without this \\nit is impossible to apply the likelihood (or many other) approaches. \\nFor example $x$ could be observations of a variable of interest within some \\nrange, and $f$ could be\\nany function such as a straight line, with gradient and intercept as parameters.\\nBut we will start with an angular distribution\\n\\\\begin{equation}\\ny(\\\\cos\\\\theta;\\\\beta) = \\\\frac{d\\\\ p}{d\\\\cos\\\\theta} = N(1+\\\\beta \\\\cos^2\\\\theta)\\n\\\\end{equation}\\nHere $\\\\theta$ is the angle at which a particle is observed, $dp/d\\\\cos\\\\theta$ is the $pdf$\\nspecifying the probability density for observing a decay at any $\\\\cos\\\\theta$, $\\\\beta$ is\\nthe parameter we want to determine, and $N$ is the crucial nomalisation factor \\nwhich ensures that the probability of observing a given decay at any $\\\\cos\\\\theta$\\nin the whole range from $-1$ to $+1$ is unity. In this case $N = 1/(2(1+\\\\beta/3))$. \\nThe data consists of $N$ decays, with their individual observations $\\\\cos\\\\theta_i$.\\nAssuming temporarily that the value of the parameter $\\\\beta$ is specified,\\nthe probability density $y_1$ of observing the first decay at $\\\\cos\\\\theta_1$ is\\n\\\\begin{equation}\\ny_1 = N (1+\\\\beta \\\\cos^2\\\\theta_1) = 0.5 (1+\\\\beta \\\\cos^2\\\\theta_1)/(1 + \\\\beta/3),\\n\\\\end{equation}\\nand similarly for the rest of the $N$ observations. Since the individual observations\\nare independent, the overall probability $P(\\\\beta)$ of observing the complete data set\\nof $N$ events is given by the product of the individual probabilities\\n\\\\begin{equation}\\nP(\\\\beta) = \\\\Pi y_i = \\\\Pi \\\\ 0.5 (1+\\\\beta \\\\cos^2\\\\theta_i)/(1 + \\\\beta/3) \\n\\\\end{equation}\\nWe imagine that this is computed for all values of the parameter $\\\\beta$; \\nthen this is known as the likelihood function ${\\\\it L}(\\\\beta)$.\\nThe likelihood method then takes as the estimate of $\\\\beta$ that value which \\nmaximises the likelihood. That is, it is the value which maximises (with respect to \\n$\\\\beta$) the probability density of observing the given data set. Conversely \\nwe rule out values of $\\\\beta$ for which ${\\\\it L}(\\\\beta)$ is very small. The\\nuncertainty on $\\\\beta$ is related to the width of the ${\\\\it L}(\\\\beta)$ \\ndistribution (see later). \\nIt is often convenient to consider the logarithm of the likelihood\\n\\\\begin{equation} \\n{\\\\it l} = \\\\ln{\\\\it L} = \\\\Sigma \\\\ln y_i\\n\\\\end{equation}\\nOne reason for this is that, for a large number of observations \\nsome fraction could have small $y_i$. Then the likelihood, involving the product of the\\n$y_i$, could be very small and may underflow the computer's range for real numbers.\\nIn contrast, {\\\\it l} involves a sum rather than a product, and $\\\\ln y_i$ rather than \\n$y_i$, and so produces a gentler number.\\n\\\\subsection{Likelihood and $pdf$}\\nThe procedure for constructing the likelihood is first to write down the $pdf$, and then to insert into that \\nexpression the observed data values in order to evaluate their product, which is the likelihood. Thus both \\nthe $pdf$ and the likelihood involve the data $x$ and the parameter(s) $\\\\mu$. The difference is that the $pdf$ is a function of $x$ for fixed values of $\\\\mu$, while the likelihood is a function of $\\\\mu$ given the fixed observed \\ndata $x_{obs}$. \\nThus for a Poisson distribution, the probability of observing $n$ events when the rate $\\\\mu$ is specified is \\n\\\\begin{equation}\\nP(n;\\\\mu) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $n$, while the likelihood is\\n\\\\begin{equation}\\nL(\\\\mu;n) = e^{-\\\\mu} \\\\mu^n /n!\\n\\\\end{equation}\\nand is a function of $\\\\mu$ for the fixed observed number $n$.\\n\\\\subsection{Intuitive example: Location and width of peak}\\nWe consider a \\nsituation where we are studying a resonant state which would result in a bump in the mass distribution of its decay particles.\\nWe assume that the bump can be parametrised as a simple Breit-Wigner\\n\\\\begin{equation}\\ny(m;M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nwhere $y$ is the probability density of obtaining a mass $m$ if the location and width the state are $M_0$ and $\\\\Gamma$,\\nthe parameters we want to determine. It is essential that $y$ is normalised, i.e. its integral over all physical values of \\n$m$ is unity; hence the normalisation factor of $\\\\Gamma/(2\\\\pi)$. The data consists of $n$ observations of $m$, as shown in fig. \\\\ref{fig:L_for_Resonance}.\\nAssume for the moment that we know $M_0$ and $\\\\Gamma$. Then the probability density for observing the $i^{th}$\\nevent with mass $m_i$ is\\n\\\\begin{equation}\\ny_i(M_0,\\\\Gamma) = \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nSince the events are independent, the probability density for observing the whole data sample is\\n\\\\begin{equation}\\ny_{all}(M_0,\\\\Gamma) =\\\\Pi \\\\ \\\\frac{\\\\Gamma/(2\\\\pi)}{(m_i-M_0)^2 + (\\\\Gamma/2)^2}\\n\\\\end{equation}\\nand this is known as the likelihood $L(M_0,\\\\Gamma)$. Then the best values for the parameters are taken as\\nthe combination that maximises the probability density for the whole data sample i.e. $L(M_0,\\\\Gamma)$. \\nParameter values for which $L$ is very small compared to its maximum value are rejected, and the uncertainties \\non the parameters are related to the width of the distribution of $L$; we will be more specific later.\\nThe curve in\\nfig. \\\\ref{fig:L_for_Resonance}(left) shows the expected probability distribution for fixed parameter values. The way $L$ is calculated involves\\nmultiplying the heights of the curve at all the observed $m_i$ values. If we now consider varying $M_0$, this moves the curve bodily to the left or right without changing its shape or normalisation. So to determine the best value of $M_0$, we need to find where to locate the curve so that the product of the heights is a maximum; it is plausibe that the peak will be located where the majority of events are to be found.\\nNow we will consider how the optimum value of $\\\\Gamma$ is obtained. A small $\\\\Gamma$ results in a narrow curve, so the masses in the tail will make an even smaller contribution to the product in eqn. \\\\ref{product}, and hence reduce the likelihood. But a large $\\\\Gamma$ is not good, because not only is the width larger, but because of the normalisation condition, the peak height is reduced, and so the observations in the peak region make a smaller contribution to the likelihood. The optimal \\n$\\\\Gamma$ involves a trade-off between these two effects.\\nOf course, in finding the optimal of values of the two parameters, in general it is necessary to find the maximum of the \\nlikelihood as a function of the two parameters, rather than maximising with respect to just one, and then with respect to the other and then stopping (see section \\\\ref{More_variables}).\\n\\\\subsection{Uncertainty on parameter}\\nWith a large amount of data, the likelihood as a function of a parameter $\\\\mu$ is \\noften approximately Gaussian. In that case, ${\\\\it l}$ is an upturned parabola. Then\\nthe following definitions of $\\\\sigma_\\\\mu$, the uncertainty on $\\\\mu_{best}$, \\nyield identical answers:\\n\\\\begin{itemize}\\n\\\\item{The RMS of the likelihood distribution.}\\n\\\\item{[$-\\\\frac{d^2 {\\\\it l}}{d \\\\mu^2}]^{-1/2}$. If you remember that \\nthe second derivative of the log likelihood function is involved because it \\ncontrols the width of the ${\\\\it l}$ distribution, a mneumonic helps \\nyou remember the formula for $\\\\sigma_\\\\mu$: Since $\\\\sigma_\\\\mu$ must have the \\nsame units as $\\\\mu$, the second derivative must appear to the power $-1/2$. But because the\\nlog of the likelihood has a maximum, the second derivative is negative, so the minus \\nsign is necessary before we take the square root.}\\n\\\\item{It is the distance in $\\\\mu$ from the maximum in order to decrease ${\\\\it l}$ by half a unit\\nfrom its maximum value. i.e.\\n\\\\begin{equation}\\n{\\\\it l} (\\\\mu_{best} + \\\\sigma_{\\\\mu}) = {\\\\it l}_{max} - 0.5 \\n\\\\end{equation}\\n}\\n\\\\end{itemize} \\nIn situations where the likelihood is not Gaussian in shape, these three definitions no longer agree.\\nThe third one is most commonly used in that case. Now the upper and lower ends of the intervals can \\nbe asymmetric with respect to the central value. It is a mistake to believe that this method \\nprovides intervals which have a $68\\\\parameter\\\\footnote{Unfortunately, this incorrect statement occurs in my book\\\\cite{LL_book}. It is \\ncorrected in a separate update\\\\cite{LL_book_update}.}.\\nSymmetric uncertainties are easier to work with than asymmetric ones. It is thus sometimes better to quote the \\nuncertainty on a function of the first variable you think of. For example, for a charged particle in a magnetic field,\\nthe reciprocal of the momentum has a nearly symmetric uncertainty. Especially for high\\nmomentum tracks, the upper uncertainty on the momentum can be much larger than the lower one \\ne.g. $1.0\\\\ ^{+1.5}_{-0.4}$ TeV.\\n\\\\subsection{Coverage}\\nAn important feature of any statistical method for estimating a range for some parameter $\\\\mu$ at a \\nspecified confidence level $\\\\alpha$ is its coverage $C$. If the procedure is applied many times, \\nthese ranges will vary because of statistical fluctuations in the observed data. Then $C$ is defined as\\nthe fraction of ranges which contain the true value $\\\\mu_{true}$; it can vary with $\\\\mu_{true}$. \\nIt is very\\nimportant to realise that coverage is a property of the {\\\\bf statistical procedure} and does not apply\\nto your particular measurement. An ideal plot of coverage as a function of $\\\\mu$ would have $C$ constant \\nat its nominal value $\\\\alpha$. For a Poisson counting experiment, figure \\\\ref{fig:PoissonCoverage} shows $C$ as a \\nfunction of the Poisson parameter $\\\\mu$, when the observed number of counts $n$ is used to determine a range \\nfor $\\\\mu$ via the change in log-likelihood being 0.5. The coverage is far from constant at small $\\\\mu$.\\nIf $C$ is smaller than $\\\\alpha$, this is known as undercoverage. Certainly frequentists would regard this \\nas unfortunate; it means that people reading an article containing parameters determined this way are \\nlikely to place more than justified reliance on the quoted range. Methods using the Neyman construction \\nto determine parameter ranges by construction do not have undercoverage. \\nCoverage involves a statement about $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$. This is to be interpreted as a\\nprobability statement about how often the ranges $\\\\mu_l$ to $\\\\mu_u$ contain the (unknown but constant) true\\nvalue $\\\\mu_{true}$. This is a frequentist statement; Bayesians do not want to consider the ensemble of possible\\nresults if the measurement procedure were to be repeated. Thus Bayesians would regard the statement\\nabout $Prob[\\\\mu_l \\\\leq \\\\mu_{true} \\\\leq \\\\mu_u]$ as describing what fraction of their estimated \\nposterior probability density for $\\\\mu_{true}$ would be \\nbetween the fixed values $\\\\mu_l$ and $\\\\mu_u$, derived from their actual measurement.\\n\\\\subsection{More than one parameter}\\nFor the case of just one parameter $\\\\mu$, the likelihood best estimate $\\\\hat{\\\\mu}$ is given \\nby the value of $\\\\mu$ which maximises $L$. Its uncertainty $\\\\sigma_\\\\mu$ is determined either from \\n\\\\begin{equation}\\n1/\\\\sigma_\\\\mu^2 = -d^2\\\\ln L/d\\\\mu^2 ;\\n\\\\end{equation} \\nof by finding how far $\\\\hat{\\\\mu}$ would have to be changed in order to reduce $\\\\ln L$ by 0.5.\\nWhen we have two or more parameters $\\\\beta_i$ the rule for finding the best estimates $\\\\hat{\\\\beta_i}$\\nis still to maximise $L$.\\nFor the uncertainties and their correlations, the generalisation of equation \\\\ref{error} is to construct\\nthe inverse covariance matrix ${\\\\bf M}$, whose elements are given by\\n\\\\begin{equation}\\nM_{ij} = -\\\\frac{\\\\partial^2 \\\\ln L} {\\\\partial \\\\beta_i\\\\ \\\\partial \\\\beta_j} \\n\\\\end{equation} \\nThen the inverse of $\\\\bf{M}$ is the covariance matrix, whose diagonal elements are the variances of $\\\\beta_i$,\\nand whose off-diagonal ones are the covariances.\\nAlternatively (and more common in practice), the uncertainty on a specific $\\\\beta_j$ can be obtained \\nby using the profile likelihood $L_{prof}(\\\\beta_j)$.\\nThis is the likelihood as a function of the specific $\\\\beta_j$, where for each value of $\\\\beta_j,\\\\ L$ has been remaximised\\nwith respect to all the other $\\\\beta$. Then $L_{prof}(\\\\beta_j)$ is used with the `reduce $\\\\ln L_{prof}$ = 0.5' rule\\nto obtain the uncertainty on $\\\\beta_j$. This is equivalent to determining the contour in $\\\\beta$-space where \\n$\\\\ln L = \\\\ln L_{max} - 0.5$, and finding the values $\\\\beta_{j,1}$ and $\\\\beta_{j,2}$ on the contour which are \\nfurthest from $\\\\hat{\\\\beta_j.}$ Then the (probably asymmetric) upper and lower uncertainties on $\\\\beta_j$ are \\ngiven by $\\\\beta_{j,2}- \\\\hat{\\\\beta_j}$ and $\\\\hat{\\\\beta_j} - \\\\beta_{j,1}$ respectively.\\nBecause these are likelihood methods of obtaining the intervals, these estimates of uncertainities provide only\\n{\\\\bf nominal} regions of 68\\\\the region within \\nthe contour described in the previous paragraph for the multidimensional $\\\\beta$ space will have less than 68\\\\overage. To achieve that, the $`0.5'$ in the rule for how much $\\\\ln L$ has to be reduced from its maximum \\nmust be replaced by a larger number, whose value depends on the dimensionality of $\\\\beta$.\\n\", \"\\\\section{Combining experiments} \\nSometimes different experiments will measure the same physical quantity. It is then reasonable to ask what is our\\nbest information available when these experiments are combined. It is a general rule that it is better to use the\\n{\\\\bf DATA} for the experiments and then perform a combined analysis, rather than simply combine the {\\\\bf RESULTS}.\\nHowever, combining the results is a simpler procedure, and access to the original data is not always possible. \\nFor a series of unbiassed, uncorrelated measurements $x_i$ of the same physical quantity,\\nthe combined value $\\\\hat{x} \\\\pm \\\\hat{\\\\sigma}$ is given by weighting each measurement by $w_i$,\\nwhich is proportional to the inverse of the square of its uncertainty i.e. \\n\\\\begin{equation}\\n\\\\hat{x} = \\\\Sigma w_i x_i, \\\\ \\\\ \\\\ \\\\ w_i =(1/\\\\sigma_i^2)/\\\\Sigma (1/\\\\sigma_j^2) \\\\end{equation}\\nwith the uncertainty $\\\\hat{\\\\sigma}$ on the combined value being given by\\n\\\\begin{equation}\\n1/\\\\hat{\\\\sigma}^2 = \\\\Sigma 1/\\\\sigma_i^2\\n\\\\end{equation}\\nThis ensures that the uncertainty on the combination is at least as small as the \\nsmallest uncertainty of the individual measurements. It should be remembered that the combined uncertainty takes no \\naccount of whether or not the individual measurements are consistent with each other.\\nIn an informal sense, $1/\\\\sigma_i^2$ is the information content of a measurement. Then each $x_i$ is weighted\\nproportionally to its information content. Also the equation for\\n$\\\\hat{\\\\sigma^2}$ says that the information content of the combination is the sum of the information contents\\nof the individual measurements.\\nAn example demonstrates that care is needed in applying the formulae. Consider counting the number of high\\nenergy cosmic rays being recorded by a large counter system for two consecutive one-week periods, with the number of counts being\\n$100 \\\\pm 10$ and $1 \\\\pm 1$ \\\\footnote{It is vital to be aware that it is a crime (punishable by a forcible transfer\\nto doing a doctorate on Astrology) to combine such discrepant measurements. It seems likely that someone turned off\\nthe detector between the two runs; or there was a large background in the first measurement which was eliminated\\nfor the second; etc. The only reason for my using such discrepant numbers is to produce a dramatically stupid\\nresult. The effect would have been present with measurements like $100 \\\\pm 10$ and $81 \\\\pm 9$.}.\\n(See section \\\\ref{Poisson} for the choice of uncertainties). Unthinking application of the formulae\\nfor the combined result give the ridiculous $2 \\\\pm 1$. What has gone wrong?\\nThe answer is that we are supposed to use the {\\\\bf true} accuracies of the individual measurements to assign the weights. \\nHere we have used the {\\\\bf estimated} accuracies. Because the estimated uncertainty \\ndepends on the estimated rate, a downward fluctuation in the measurement results in an underestimated uncertainty,\\nan overestimated weight, and a downward bias in the combination. In our example, the combination should assume \\nthat the true rate was the same in the two measurements which used the same detector and which \\nlasted the same time as each other, and hence their\\ntrue accuracies are (unknown but) equal. So the two measurements should each be given a weight of 0.5, which \\nyields the sensible combined result of $50.5 \\\\pm 5$ counts. \\n\\\\subsection{\\\\bf BLUE}\\nA method of combining correlated results is the `{\\\\bf B}est {\\\\bf L}inear {\\\\bf U}nbiassed {\\\\bf E}stimate' ({\\\\bf BLUE}). \\nWe look for the best linear unbiassed combination\\n\\\\begin{equation}\\nx_{BLUE} = \\\\Sigma w_i x_i,\\n\\\\end{equation} \\nwhere the weights are chosen to give the smallest uncertainty $\\\\sigma_{BLUE}$ on \\n$x_{BLUE}$. Also for the combination to be unbiassed, the weights must add up to unity.\\nThey are thus determined by minimising $\\\\Sigma\\\\Sigma w_i w_j E^{-1}_{ij}$, subject to the constraint\\n$\\\\Sigma w_i = 1$; here $E$ is the covariance matrix for the correlated measurements. \\nThe $BLUE$ procedure just described is equivalent to the $\\\\chi^2$ approach for checking whether\\na correlated set of measurements are consistent with a common value. The advantage of $BLUE$ is that \\nit provides the weights for each measurement in the combination. It thus enables us to calculate the contribution \\nof various sources of uncertainty in the individual measurements to the uncertainty on the combined result.\\n\\\\subsection{Why weighted averaging can be better than simple averaging}\\nConsider a remote island whose inhabitants are very conservative, and no-one leaves or arrives \\nexcept for some anthropologists who wish to determine the number of married people there.\\nBecause the islanders are very traditional, it is necessary to send two teams of anthropologists,\\none consisting of males to interview the men, and the other of females for the women. There are too\\nmany islanders to interview them all, so each team interviews a sample and then extrapolates. \\nThe first team estimates the number of married men as $10,000 \\\\pm 300$. The second, who \\nunfortunately have less funding and so can interview only a smaller sample, have a \\nlarger statistical uncertainty; they estimate $9,000 \\\\pm 900$ married women. Then how many \\nmarried people are there on the island? \\nThe simple approach is to add the numbers of married men and women, to give $19,000 \\\\pm 950$\\nmarried people. But if we use some theoretical input, maybe we can improve the accuracy of \\nour estimate. So if we assume that the islanders are monogamous, the numbers of married men and \\nwomen should be equal, as they are both estimates of the number of married couples. The weighted \\naverage is $9,900 \\\\pm 285$ married couples and hence $19,800 \\\\pm 570$ married people.\\nThe contrast in these results is not so much the difference in the estimates, but that \\nincorporating the assumption of monogamy and hence using the weighted average gives a smaller \\nuncertainty on the answer. Of course, if our assumption is incorrect, this answer will be biassed.\\nA Particle Physics example incorporating the same idea of theoretical input reducing the \\nuncertainty of a measurement can be found in the `Kinematic Fitting' section of Lecture 2.\\n\"}},\n",
       "       {'entity_name': 'chisquared method', 'entity_type': 'analysis_technique', 'description': 'A statistical method used for parameter estimation and goodness of fit testing, which compares the observed data with the expected data under a specific model.', 'relevant_passages': {'\\\\section{Poisson distribution}\\nThe Poisson distribution (see Fig.~\\\\ref{fig:Poissons}) applies to situations where we are counting a series of observations which are occuring randomly and independently during a fixed time interval $t$, where the underlying rate $r$ is constant. The observed number $n$ will fluctuate when the experiment is repeated, and can in principle take any integer value from zero to infinity. The Poisson probabilty of observing $n$ decays is given by\\n\\\\begin{equation}\\nP_n = e^{-rt} (rt)^n/n!\\n\\\\end{equation}\\nIt applies to the number of decays observed from a large number $N$ of radioactive nuclei, when the observation time $t$ is small compared to the lifetime $\\\\tau$. It will not apply if $t$ is much larger than $\\\\tau$, or if the detection system has a dead time, so that after observing a decay the detector cannot observe another decay for a period $T_{dead}$.\\nAnother example is the number of counts in any specific bin of a histogram when the data is accumulated over a fixed time. \\nThe average number of observations is given by \\n\\\\begin{equation}\\n<n> = \\\\Sigma n P_n = rt\\n\\\\end{equation}\\nIf we write the expected number as $\\\\mu$, the Poisson probability becomes\\n\\\\begin{equation}\\nP_n = e^{-\\\\mu} \\\\mu^n/n!\\n\\\\end{equation}\\nIt is also relatively easy to show that the variance \\n\\\\begin{equation}\\n\\\\sigma^2 = \\\\Sigma (n - \\\\mu)^2 P_n = \\\\mu\\n\\\\end{equation}\\nThis leads to the well-known $n \\\\pm \\\\sqrt n$ approximation for the value of the Poisson parameter when we have $n$ counts. This approximation is, however, particularly bad when there are zero observed events; then $0\\\\pm0$ incorrectly suggests that the Poisson parameter can be only zero.\\nPoisson probabilities can be regarded as the limit of Binomial ones as the number of trials $N$ tends to infinity and the Binomial probability of success $p$ tends to zero, but the product $Np$ remains constant at $\\\\mu$. \\nWhen the Poisson mean becomes large, the distribution of observed counts approximates to a Gaussian (although the Gaussian is a continuous distribution extending down to $-\\\\infty$, while a Poisson observable can only take on non-negative integral values). This approximation is useful for the $\\\\chi^2$ method for parameter estimation and goodness of fit (see Lecture 2).\\n\\\\subsection{Relation of Poisson and Binomial Distributions}\\nAn interesting example of the relationship between the Poisson and Binomial distributions is exhibited by the following example.\\nImagine that the number of people attending a series of lectures is Poisson distributed with a constant \\nmean $\\\\nu$, and that the fraction of them who are male is $p$. Then the overall probability $P$ of \\nhaving N people of whom $M$ are male and $F = N - M$ are female is given by the product of the Poisson \\nprobability $P_{pois}$ for $N$ and the binomial probability $P_{bin}$ for $M$ of the $N$ people being male. i.e.\\n\\\\begin{equation}\\nP = P_{pois} P_{bin} = \\\\frac{e^{-\\\\nu} \\\\nu^N}{N!} \\\\times \\\\frac{N!}{M!F!} p^M (1-p)^F\\n\\\\end{equation} \\nThis can be rearranged as\\n\\\\begin{equation}\\nP = \\\\frac{e^{-\\\\nu p} (\\\\nu p)^M}{M!} \\\\times \\\\frac{e^{-\\\\nu (1-p)} (\\\\nu(1-p))^F}{F!}\\n\\\\end{equation} \\nThis is the product of two Poissons, one with Poisson parameter $\\\\nu p$, the expected number of males, and the other with parameter $\\\\nu(1-p)$, the expected number of females. Thus with a Poisson-varying total number of observations, divided into two categories (here male and female), we can regard this as Poissonian in the total number and Binomial in the separate categories, or as two independent Poissons, one for each category. Other situations to which this applies could be radioactive nuclei, with decays detected in the forward or backward hemispheres; cosmic ray showers, initiated by protons or by heavier nuclei; patients arriving at a hospital emergency centre, who survive or who die; etc.\\n\\\\subsection{For your thought}\\nThe first few Poisson probabilities $P(n;\\\\mu)$ are\\n\\\\begin{equation} \\n\\\\begin{split}\\nP(0) = e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ P(1) = \\\\mu e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ P(2) = (\\\\mu^2/2!)\\\\ e^{-\\\\mu}, \\\\ \\\\ \\\\ \\\\ \\\\ etc.\\n\\\\end{split}\\n\\\\end{equation}\\nThus for small $\\\\mu$, $P(1)$ and $P(2)$ are approximately $\\\\mu$ and $\\\\mu^2/2$ respectively. But if the probability\\nof one rare event happening is $\\\\mu$, why is the probability for 2 independent rare events not equal to $\\\\mu^2$?\\n'}},\n",
       "       {'entity_name': 'tail area', 'entity_type': 'statistics_concept', 'description': 'The area under the probability density function of a distribution that lies beyond a certain threshold, used to assess the extremity of a measurement in relation to a theoretical prediction.', 'relevant_passages': {\"\\\\section{Gaussian distribution}\\nThe Gaussian or normal distribution (shown in Fig.~\\\\ref{fig:Gaussians}) is of widespread usage in data analysis. Under suitable conditions, in\\na repeated series of measurements $x$ with accuracy $\\\\sigma$ when the true value of the quantity\\nis $\\\\mu$, the distribution of $x$ is given by a Gaussian\\\\footnote{However, it is often the case \\nthat such a distribution has heavier tails than the Gaussian.}. A mathematical motivation is given by the \\nCentral Limit Theorem, which states that the sum of\\na large number of variables with (almost) any distributions is approximately Gaussian. \\nFor the Gaussian, the probability density $y(x)$ of an observation $x$ is given by\\n\\\\begin{equation}\\ny(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi} \\\\sigma} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\n\\\\end{equation}\\nwhere the parameters $\\\\mu$ and $\\\\sigma$ are respectively the centre and width of the distribution.\\nThe factor $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$ is required to normalise the area under the curve, so that $y(x)$\\ncan be directly interpreted as a probability density.\\nThere are several properties of $\\\\sigma$:\\n\\\\begin{itemize}\\n\\\\item{The mean value of $x$ is $\\\\mu$, and the standard deviation of its distribution is $\\\\sigma$. Since the usual symbol for \\nstandard deviation is $\\\\sigma$, this leads to the formula $\\\\sigma = \\\\sigma$ (which is not so trivial as it seems, since the \\ntwo $\\\\sigma$s have different meanings). This explains the curious factor of 2 in the denominator of the exponential,\\nsince without it, the two types of $\\\\sigma$ would not be equal.}\\n\\\\item{The value of $y$ at the $\\\\mu \\\\pm \\\\sigma$ is equal to the peak height multiplied by $e^{-0.5}$ = 0.61.\\nIf we are prepared to overlook the difference between 0.61 and 0.5, $\\\\sigma$ is the half-width of the distribution at\\n`half' the peak height.}\\n\\\\item{The fractional area in the range $x = \\\\mu - \\\\sigma$ to $\\\\mu + \\\\sigma$ is 0.68. Thus for a series of unbiassed, independent Gaussian \\ndistributed measurements}, about 2/3 are expected to lie within $\\\\sigma$ of the true value.\\n\\\\item{The peak height of $y$ at $x=\\\\mu$ is $1/(\\\\sqrt{2\\\\pi} \\\\sigma)$. It is reasonable that this is proportional to \\n$1/\\\\sigma$ as the width is proportional to $\\\\sigma$, so $\\\\sigma$ cancels out in the product\\nof the height and width, as is \\nrequired for a distribution normalised to unity.}\\n\\\\end{itemize}\\nFor deciding whether an experimental measurement is consistent with a theory, more useful than the Gaussian distribution \\nitself is its tail area beyond $r$, a number of standard deviations from the central value (see Fig.~\\\\ref{fig:Gauss_tail}). \\nThis gives the probability of obtaining a result as extreme as ours or more so as a consequence of statistical fluctuations, \\nassuming that the theory is correct (and that our measurement is unbiassed, it is Gaussian distributed, etc.). If this \\nprobability is small, the measurement and the theory may be inconsistent. \\nFigure~\\\\ref{fig:Gauss_tail} has two different vertical scales, the left one for the probability of a fluctuation in a specific \\ndirection, and the right side for a fluctuation in either direction. Which to use depends on the particular\\nsituation. For example if we were performing a neutrino oscillation disappearance experiment, we would be looking for \\na reduction in the number of events as compared with the no-oscillation scenario, and hence would be interested in \\njust the single-sided tail. In contrast searching for any deviation from the Standard Model expectation, maybe the two-sided tails would be more relevant. \\n\"}},\n",
       "       {'entity_name': 'goodness of fit test', 'entity_type': 'analysis_technique', 'description': 'A statistical test used to determine how well a theoretical model fits a set of observed data, often using metrics such as the chi-squared statistic.', 'relevant_passages': {'\\\\section{Least squares for Goodness of Fit}\\n\\\\subsection{The chi-squared distribution}\\nIt turns out that, if we repeated our experiment a large number of times, and certain conditions are satisfied, \\nthen $S_{min}$ will follow a $\\\\chi^2$ distribution with $\\\\nu = n - p$ degrees of freedom, where $n$ is the \\nnumber of data points, $p$ is the number of free parameters in the fit, and $S_{min}$ is the value of $S$ \\nfor the best values of the free parameters. For example, a straight line with free intercept and gradient fitted \\nto 12 data points would have $\\\\nu = 10$.\\nThe conditions for this to be true include:\\n\\\\begin{itemize}\\n\\\\item{the theory is correct:}\\n\\\\item{the data are unbiassed and asymptotic;}\\n\\\\item{the $y_i$ are Gaussian distributed about their true values;}\\n\\\\item{the estimates for $\\\\sigma_i$ are correct;$\\\\ \\\\ \\\\ \\\\ \\\\ $ etc. }\\n\\\\end{itemize}\\nUseful properties to know about the mathematical $\\\\chi^2$ distribution are that their mean is $\\\\nu$ and their variance is\\n$2\\\\nu$. Thus if a global fit to a lot of data has $S_{min}$ = 2200 and there are 2000 degrees of freedom, we can \\nimmediately estimate that this is equivalent to a fluctuation of 3.2$\\\\sigma$.\\nMore useful than plots of $\\\\chi^2$ distributions are those of the fractional tail area beyond a particular value \\nof $\\\\chi^2$ (see figs. \\\\ref{fig:chi_squared} and \\\\ref{fig:chi_sq_tail} respectively).\\nThe $\\\\chi^2$ goodness of fit test consists of\\n\\\\begin{itemize}\\n\\\\item{For the given theoretical form, find the best values of its free parameters, and hence $S_{min}$;}\\n\\\\item{Determine $\\\\nu$ from $n$ and $p$; and}\\n\\\\item{Use $S_{min}$ and $\\\\nu$ to obtain the tail probability $p$ \\\\footnote{If the conditions for $S_{min}$ to follow a \\n$\\\\chi^2$ distribution are satisfied, this simply involves using the tail probability of a $\\\\chi^2$ distribution. \\nIn other cases, it may be necessary to use Monte Carlo simulation to obtain the distribution of $S_{min}$;\\nthis could be tedious. }.} \\n\\\\end{itemize}\\nThen $p$ is the probability that, if the theory is correct, by random fluctuations we would have obtained a value of $S_{min}$ at least as large as the observed one. If this probability is smaller than some pre-defined level $\\\\alpha$, we reject the hypothesis that the model provides a good description of the data.\\n\\\\subsection{When $\\\\nu \\\\ne n - p$}\\nIf we add an extra parameter into our theoretical description, even if it is not really needed, we expect the value of $S_{min}$ to decrease slightly. (This contrasts with including a parameter which is really relevant, which can result in a dramatic reduction in $S_{min}$.) In determining $p$-values, this is allowed for by the reduction of $\\\\nu$. On average,\\na parameter which is not needed reduces $S_{min}$ by 1. But consider the following examples.\\n\\\\subsubsection{Small oscillatory term}\\nImaging we are fitting a histogram of a variable $\\\\phi$ by a distribution of the form\\n\\\\begin{equation}\\n\\\\frac{dy}{d\\\\phi} = N[ 1 + 10^{-6} cos(\\\\phi -\\\\phi_0)],\\n\\\\end{equation}\\nwhere the two parameters are the normaisation $N$ and the phase $\\\\phi_0$. Because of the factor $10^{-6}$ in front\\nof the cosine term, $\\\\phi_0$ will have a miniscule effect on the prediction, and so including this as a parameter has negligible effect on $S_{min}$; $\\\\phi_0$ is effectively not a free parameter.\\n\\\\subsubsection{Neutrino oscillations} \\nFor a scenario of two oscillating neutrino flavours, the probability $P$ of a neutrino of energy $E$ to remain the same flavour after\\na flight length $L$ is\\n\\\\begin{equation}\\nP = 1 - A sin^2(\\\\delta m^2 L/E)\\n\\\\end{equation}\\nwhere the two parameters are $\\\\delta m^2$, the difference in the mass-squareds of the two neutrino flavours, and \\n$A = sin^2 2\\\\theta$ with $\\\\theta$ being the mixing angle. However, since for small angles $\\\\alpha,\\\\ sin\\\\alpha\\\\approx \\\\alpha$, for small $\\\\delta m^2L/E$ the probability $P$ of eqn \\\\ref{neutrino} is approximately $1 - A(\\\\delta m^2 L/E)^2$. Thus the two parameters occur only as the product $A (\\\\delta m^2)^2$, and cannot be determined separately. Thus in that regime we have effectively just a single parameter.\\n\\\\vspace{0.2in}\\nIn both the above examples, an enormous amount of data would enable us to distinguish the small effects produced by the second \\nparameter; hence the requirement for asymptotic conditions. \\n\\\\subsection{Errors of First and Second Kind}\\nIn deciding in a Goodness of Fit test whether or not to reject the null hypothesis $H_0$ (e.g. that the data points lie on a \\nstraight line), there are two sorts of mistake we might make:\\n\\\\begin{itemize}\\n\\\\item{Error of the First Kind. This is when we reject $H_0$ when it is in fact true. The fraction of cases in which this happens \\nshould equal $\\\\alpha$, the cut on the $p$-value. }\\n\\\\item{Error of the Second Kind. This is when we do not reject $H_0$, even though some other hypothesis is true. The rate at which this happens depends on how similar $H_0$ and the alternative hypothesis are, the relative frequencies of the two hypotheses being true, etc.}\\n\\\\end{itemize}\\nAs $\\\\alpha$ increases the rates of Errors of the First and Second kinds go up and down respectively. \\nThese Errors correspond to a loss of efficiency and to an increase of contamination respectively.\\n\\\\subsection{Other Goodness of Fit tests}\\nThe $\\\\chi^2$ method is by no means the only one for testing Goodness of Fit.\\nIndeed whole books have been written on the subject\\\\cite{DAgostino}. Here\\nwe mention just one other, the Kolmogorov-Smirnov method (K-S), which has the \\nadvantage of working with individual observations. It thus can be used with \\nfewer observations than are required for the binned histograms in the $\\\\chi^2$\\napproach. \\nA cumulative plot is produced of the fraction of events as a function of the variable\\nof interest $x$. An example is shown in Fig.~\\\\ref{fig:K_S}. \\nThis shows the fraction of data events with $x$ smaller than any particular \\nvalue. It is thus a stepped plot, with the fraction going from zero at the extreme left, \\nto unity on the right hand side. Also on the plot is a curve showing the expected cumulative\\nfraction for some theory. The K-S method makes use of the largest (as a function of $x$) vertical \\ndiscrepancy $d$ between the data plot and the theoretical curve. Assuming the theory is true\\nand given the number of observations $N$, the probability $p_{KS}$ of obtaining $d$ at least as large \\nas the observed value can be calculated. The beauty of the K-S method is that this probability\\nis independent of the details of the theory. As in the $\\\\chi^2$ approach, the K-S probability \\ngives a numerical way of checking the compatibility of theory and data. If $p_{KS}$ is small, we \\nare likely to reject the theory as being a good description of the data.\\nSome features of the K-S method are:\\n\\\\begin{itemize}\\n\\\\item{The main advantage is that it can use a small number of observations.}\\n\\\\item{The calculation of the K-S probability depends on there being no adjustable parameters in the theory.\\nIf there are, it will be necessary for you to determine the expected distribution for $d$, presumably \\nby Monte Carlo simulation.}\\n\\\\item{It does not extend naturally to data of more than one dimension, because of there being no unique\\nway of producing an ordering in several dimensions.}\\n\\\\item{It is not very sensitive to deviations in the tails of distributions, which is where searches for\\nnew physics are often concentrated e.g. high mass or transverse momentum. Fortunately variants of K-S exist, \\nwhich put more emphasis on discrepancies in the tails.}\\n\\\\item{Instead of comparing a data cumulative distribution with a theoretical curve, it can alternatively be\\ncompared with another distribution. This can be from a simulation of a theory, or with another data set. The\\nlatter could be to check that two data sets are compatible. \\nThe calculation of the K-S probability now requires the maximum discrepancy $d$, and the numbers of events \\n$N_1$ and $N_2$ in each of the two distributions being compared.}\\n\\\\end{itemize}\\n'}},\n",
       "       {'entity_name': 'kolmogorovsmirnov method', 'entity_type': 'analysis_technique', 'description': 'A non-parametric test used to compare a sample distribution with a reference probability distribution or to compare two sample distributions, assessing the goodness of fit based on the maximum discrepancy between empirical and theoretical cumulative distributions.', 'relevant_passages': {'\\\\section{Least squares for Goodness of Fit}\\n\\\\subsection{The chi-squared distribution}\\nIt turns out that, if we repeated our experiment a large number of times, and certain conditions are satisfied, \\nthen $S_{min}$ will follow a $\\\\chi^2$ distribution with $\\\\nu = n - p$ degrees of freedom, where $n$ is the \\nnumber of data points, $p$ is the number of free parameters in the fit, and $S_{min}$ is the value of $S$ \\nfor the best values of the free parameters. For example, a straight line with free intercept and gradient fitted \\nto 12 data points would have $\\\\nu = 10$.\\nThe conditions for this to be true include:\\n\\\\begin{itemize}\\n\\\\item{the theory is correct:}\\n\\\\item{the data are unbiassed and asymptotic;}\\n\\\\item{the $y_i$ are Gaussian distributed about their true values;}\\n\\\\item{the estimates for $\\\\sigma_i$ are correct;$\\\\ \\\\ \\\\ \\\\ \\\\ $ etc. }\\n\\\\end{itemize}\\nUseful properties to know about the mathematical $\\\\chi^2$ distribution are that their mean is $\\\\nu$ and their variance is\\n$2\\\\nu$. Thus if a global fit to a lot of data has $S_{min}$ = 2200 and there are 2000 degrees of freedom, we can \\nimmediately estimate that this is equivalent to a fluctuation of 3.2$\\\\sigma$.\\nMore useful than plots of $\\\\chi^2$ distributions are those of the fractional tail area beyond a particular value \\nof $\\\\chi^2$ (see figs. \\\\ref{fig:chi_squared} and \\\\ref{fig:chi_sq_tail} respectively).\\nThe $\\\\chi^2$ goodness of fit test consists of\\n\\\\begin{itemize}\\n\\\\item{For the given theoretical form, find the best values of its free parameters, and hence $S_{min}$;}\\n\\\\item{Determine $\\\\nu$ from $n$ and $p$; and}\\n\\\\item{Use $S_{min}$ and $\\\\nu$ to obtain the tail probability $p$ \\\\footnote{If the conditions for $S_{min}$ to follow a \\n$\\\\chi^2$ distribution are satisfied, this simply involves using the tail probability of a $\\\\chi^2$ distribution. \\nIn other cases, it may be necessary to use Monte Carlo simulation to obtain the distribution of $S_{min}$;\\nthis could be tedious. }.} \\n\\\\end{itemize}\\nThen $p$ is the probability that, if the theory is correct, by random fluctuations we would have obtained a value of $S_{min}$ at least as large as the observed one. If this probability is smaller than some pre-defined level $\\\\alpha$, we reject the hypothesis that the model provides a good description of the data.\\n\\\\subsection{When $\\\\nu \\\\ne n - p$}\\nIf we add an extra parameter into our theoretical description, even if it is not really needed, we expect the value of $S_{min}$ to decrease slightly. (This contrasts with including a parameter which is really relevant, which can result in a dramatic reduction in $S_{min}$.) In determining $p$-values, this is allowed for by the reduction of $\\\\nu$. On average,\\na parameter which is not needed reduces $S_{min}$ by 1. But consider the following examples.\\n\\\\subsubsection{Small oscillatory term}\\nImaging we are fitting a histogram of a variable $\\\\phi$ by a distribution of the form\\n\\\\begin{equation}\\n\\\\frac{dy}{d\\\\phi} = N[ 1 + 10^{-6} cos(\\\\phi -\\\\phi_0)],\\n\\\\end{equation}\\nwhere the two parameters are the normaisation $N$ and the phase $\\\\phi_0$. Because of the factor $10^{-6}$ in front\\nof the cosine term, $\\\\phi_0$ will have a miniscule effect on the prediction, and so including this as a parameter has negligible effect on $S_{min}$; $\\\\phi_0$ is effectively not a free parameter.\\n\\\\subsubsection{Neutrino oscillations} \\nFor a scenario of two oscillating neutrino flavours, the probability $P$ of a neutrino of energy $E$ to remain the same flavour after\\na flight length $L$ is\\n\\\\begin{equation}\\nP = 1 - A sin^2(\\\\delta m^2 L/E)\\n\\\\end{equation}\\nwhere the two parameters are $\\\\delta m^2$, the difference in the mass-squareds of the two neutrino flavours, and \\n$A = sin^2 2\\\\theta$ with $\\\\theta$ being the mixing angle. However, since for small angles $\\\\alpha,\\\\ sin\\\\alpha\\\\approx \\\\alpha$, for small $\\\\delta m^2L/E$ the probability $P$ of eqn \\\\ref{neutrino} is approximately $1 - A(\\\\delta m^2 L/E)^2$. Thus the two parameters occur only as the product $A (\\\\delta m^2)^2$, and cannot be determined separately. Thus in that regime we have effectively just a single parameter.\\n\\\\vspace{0.2in}\\nIn both the above examples, an enormous amount of data would enable us to distinguish the small effects produced by the second \\nparameter; hence the requirement for asymptotic conditions. \\n\\\\subsection{Errors of First and Second Kind}\\nIn deciding in a Goodness of Fit test whether or not to reject the null hypothesis $H_0$ (e.g. that the data points lie on a \\nstraight line), there are two sorts of mistake we might make:\\n\\\\begin{itemize}\\n\\\\item{Error of the First Kind. This is when we reject $H_0$ when it is in fact true. The fraction of cases in which this happens \\nshould equal $\\\\alpha$, the cut on the $p$-value. }\\n\\\\item{Error of the Second Kind. This is when we do not reject $H_0$, even though some other hypothesis is true. The rate at which this happens depends on how similar $H_0$ and the alternative hypothesis are, the relative frequencies of the two hypotheses being true, etc.}\\n\\\\end{itemize}\\nAs $\\\\alpha$ increases the rates of Errors of the First and Second kinds go up and down respectively. \\nThese Errors correspond to a loss of efficiency and to an increase of contamination respectively.\\n\\\\subsection{Other Goodness of Fit tests}\\nThe $\\\\chi^2$ method is by no means the only one for testing Goodness of Fit.\\nIndeed whole books have been written on the subject\\\\cite{DAgostino}. Here\\nwe mention just one other, the Kolmogorov-Smirnov method (K-S), which has the \\nadvantage of working with individual observations. It thus can be used with \\nfewer observations than are required for the binned histograms in the $\\\\chi^2$\\napproach. \\nA cumulative plot is produced of the fraction of events as a function of the variable\\nof interest $x$. An example is shown in Fig.~\\\\ref{fig:K_S}. \\nThis shows the fraction of data events with $x$ smaller than any particular \\nvalue. It is thus a stepped plot, with the fraction going from zero at the extreme left, \\nto unity on the right hand side. Also on the plot is a curve showing the expected cumulative\\nfraction for some theory. The K-S method makes use of the largest (as a function of $x$) vertical \\ndiscrepancy $d$ between the data plot and the theoretical curve. Assuming the theory is true\\nand given the number of observations $N$, the probability $p_{KS}$ of obtaining $d$ at least as large \\nas the observed value can be calculated. The beauty of the K-S method is that this probability\\nis independent of the details of the theory. As in the $\\\\chi^2$ approach, the K-S probability \\ngives a numerical way of checking the compatibility of theory and data. If $p_{KS}$ is small, we \\nare likely to reject the theory as being a good description of the data.\\nSome features of the K-S method are:\\n\\\\begin{itemize}\\n\\\\item{The main advantage is that it can use a small number of observations.}\\n\\\\item{The calculation of the K-S probability depends on there being no adjustable parameters in the theory.\\nIf there are, it will be necessary for you to determine the expected distribution for $d$, presumably \\nby Monte Carlo simulation.}\\n\\\\item{It does not extend naturally to data of more than one dimension, because of there being no unique\\nway of producing an ordering in several dimensions.}\\n\\\\item{It is not very sensitive to deviations in the tails of distributions, which is where searches for\\nnew physics are often concentrated e.g. high mass or transverse momentum. Fortunately variants of K-S exist, \\nwhich put more emphasis on discrepancies in the tails.}\\n\\\\item{Instead of comparing a data cumulative distribution with a theoretical curve, it can alternatively be\\ncompared with another distribution. This can be from a simulation of a theory, or with another data set. The\\nlatter could be to check that two data sets are compatible. \\nThe calculation of the K-S probability now requires the maximum discrepancy $d$, and the numbers of events \\n$N_1$ and $N_2$ in each of the two distributions being compared.}\\n\\\\end{itemize}\\n'}},\n",
       "       {'entity_name': 'kinematic fitting and simultaneous fit', 'entity_type': 'analysis_technique', 'description': 'Statistical techniques used in particle physics to optimize the estimation of particle momenta and directions in high-energy interactions. Kinematic fitting focuses on configurations that conserve energy and momentum to minimize discrepancies between measured and fitted values, while simultaneous fitting extracts signal components by concurrently fitting both signal and background probability distributions.', 'relevant_passages': {\"\\\\section{Kinematic Fitting}\\nEarlier we had the example of estimating the number of married people on \\nan island, and saw that introducing theoretical information could improve\\nthe accuracy of our answer. Here we use the same idea in the context of\\nestimating the momenta and directions of objects produced in a high energy\\ninteraction. The theory we use is that energy and momentum are conserved\\nbetween the inital state collison and the observed objects in the reaction.\\nThe reaction can be either at a collider or with a stationary target. We \\ndenote it by $a + b \\\\rightarrow c + d + e$, but the number of final state \\nobjects can be arbitrary. We assume for the time being the energy and \\nmomenta of all the objects are measured\\\\footnote{For objects like charged \\nparticles whose momenta are determined from their trajectories in a \\nmagnetic field, the energy is determined from the momentum by using the \\nrelevant particle mass.}.\\nThe technique is to consider all possible configurations of the particles'\\nkinematic variables that conserve momentum and energy, and to choose that \\nconfiguration that is closest to the measured variables. The degree of\\ncloseness is defined by the weighted sum of squares of the discrepancies $S$,\\ntaking the uncertainties and correlations into account. If the uncertainties \\non the kinematic quantities $m_i$ were uncorrelated, \\n\\\\begin{equation}\\nS = \\\\Sigma (f_i - m_i)^2/\\\\sigma_i^2\\n\\\\end{equation}\\nwhere the summation is over the 4 components for all the objects in the \\nreaction, $m_i$ are the measured values and $f_i$ are the corresponding \\nfitting quantities. Because of correlations, however, this becomes\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (f_i - m_i) E_{ij} (f_j - m_j) \\n\\\\end{equation}\\nwhere there is now a double summation over the components, and $E_{ij}$ is the \\n$(i,j)^{th}$ component of the inverse covariance matrix for the measured \\nquantities\\\\footnote{The main correlations are among the 4 components of a single\\nobject, rather than between different objects.}. \\nThe procedure then consists in varying $f$ in order to minimise $S$, subject to\\nthe energy and momentum constraints. This usually involves Lagrange Multipliers.\\nThe result of this procedure is to produce a set of fitted values of all the \\nkinematic quantities, which will have smaller uncertainties than the measured ones.\\nThis is an example of incorporating theory to improve the results. Thus if the objects \\nare jets, their directions are usually quite well determined, but their energies less so. \\nThe fitting procedure enables the accurately determined jet directions to help \\nreduce the uncertainties on the jet energies.\\nThe fitting procedure \\nalso provides $S_{min}$, which is a measure of how well the best $f_i$ agree with the $m_i$.\\nIn the case described, the distribution of $S_{min}$ is approximately $\\\\chi^2$ \\nwith 4 degrees of freedom (because of the 4 constraints).\\nIf $S_{min}$ is too large, then our assumed hypothesis for the reaction may be \\nincorrect; for example, there might have been an extra object produced in the \\ncollision that was undetected (e.g. a neutrino, or a charged particle which passed \\nthrough an uninstrumented region of our detector).\\nSince we have 4 constraint equations, we can also allow for up to 4 missing kinematic\\nquantities. Examples include an undetected neutrino in the final state (3 unmeasured \\nmomentum components), a wide-band neutrino beam of known direction (1 missing variable),\\netc. With $m$ missing variables in an interaction involving a single vertex, \\n$S_{min}$ should have a $\\\\chi^2$ distribution with $4-m$ degrees of freedom.\\nKinematic fitting can be extended to more complicated event topologies including production\\nand decay vertices, reactions involving particles of well known mass which decay \\npromptly (e.g. $\\\\psi \\\\rightarrow \\\\mu^+ \\\\mu^-$), etc. \\n\\\\subsection{Example of a simplified kinematic fit}\\nConsider a non-relativistic elastic scattering of two equal mass objects, for example a slow \\nanti-proton hitting a stationary proton. \\nFor simplicity, the measured angles $\\\\theta_1^m \\\\pm \\\\sigma$ and $\\\\theta_2^m \\\\pm \\\\sigma$ that \\nthe outgoing particles make with the direction\\nof travel of the incident anti-proton are assumed to have the same uncorrelated \\nuncertainties $\\\\sigma$. As a result of energy and momentum conservation, the angles must\\nsatisfy the constraint\\n\\\\begin{equation}\\n\\\\theta_1^t + \\\\theta_2^t = \\\\pi/2\\n\\\\end{equation}\\nwhere the superscipt $t$ denotes the true value. There are 3 further constraints \\nbut for simplicity we shall ignore them.\\nTo find our best estimates of $\\\\theta_1^t$ and $\\\\theta_2^t$, we must minimise\\n\\\\begin{equation}\\nS = (\\\\theta_1^t - \\\\theta_1^m)^2/\\\\sigma^2 + (\\\\theta_2^t - \\\\theta_2^m)^2/\\\\sigma^2\\n\\\\end{equation}\\nsubject to the constraint \\\\ref{constraint}. By using Lagrange Multipliers or by \\neliminating $\\\\theta_2^t$ and then minimising $S$, this yields\\n\\\\begin{equation}\\n\\\\begin{split}\\n\\\\theta_1^t &= \\\\theta_1^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m) \\\\\\\\ \\n\\\\theta_2^t &= \\\\theta_2^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m)\\n\\\\end{split}\\n\\\\end{equation}\\nThat is, the best estimate of each true value is obtained by adding to the corresponding measured value\\nhalf the amount by which the measured values fail to satisfy the constraint \\n\\\\ref{constraint}. \\nThe uncertainties on the fitted estimates of the angles are easily obtained by \\npropagation of the uncertainties $\\\\sigma$ on the measured angles vias eqns. \\n\\\\ref{fitted}, and are both equal to $\\\\sigma/\\\\sqrt 2$. \\nWe thus have an example \\nof the promised outcome that kinematic fitting improves the accuracy of our \\nmeasurements. The factor of $\\\\sqrt 2$ improvement can easily be understood in that \\nwe have two independent estimates of $\\\\theta_1^t$, the first being the \\noriginal measurement $\\\\theta_1^m$, and the other coming from the measurement \\n$\\\\theta_2^m$ via the constraint \\\\ref{constraint}. However, even with uncorrelated uncertainties\\non the measured angles, the fitted ones would be anti-correlated.\\n\", '\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\nSearching for physics beyond the Standard Model is one of the most important aspects of the physics program at the Large Hadron Collider (LHC). Since the start of proton-proton collisions at the LHC in 2011, the ATLAS~\\\\cite{ATLAS} and CMS~\\\\cite{CMS} Collaborations have derived stringent bounds on a range of new physics signatures, pushing the allowed mass range for many postulated new particles far into the TeV scale. While it is possible that these particles have yet to be observed because they are too heavy to be produced at the LHC, or have to small cross section to be detected with the current data size, it could also be that new particles are kinematically accessible and produced at observable rates, but our current methods of detection prevent their discovery.\\nSearches for new physics processes at particle colliders are usually performed as \\\\textit{blind searches}. Such searches proceed by defining a region of interest in the parameter space, using simulated data of the signal and the Standard Model background processes in order to enhance the data purity. The data is only looked at in the very end where it is tested for the presence of signal through a simultaneous fit of the signal and background probability distributions, hoping to extract a non-zero signal component.\\nHundreds of such searches have been performed for hundreds of different potential new particles, but thus far none have been discovered. Despite this, there are still regions of the data that have not yet been probed for the presence of a signal. This has led to an increased interest in more \\\\textit{model-agnostic} search strategies. Model-independent searches is nothing new in high energy particle physics, and strategies relying less on a signal hypothesis have been devised and utilized~\\\\cite{D0:2000vuh,H1:2008aak,H1:2004rlm,Cranmer:823591,CDF:2007iou,CDF:2007ykt,CDF:2008voc,CMS-PAS-EXO-14-016,CMS-PAS-EXO-10-021,CMS-PAS-EXO-19-008,CMS:2020zjg,ATLAS:2018zdn,ATLAS-CONF-2014-006,ATLAS-CONF-2012-107,ATLAS:2020iwa}.\\nThese mainly take advantage of Monte Carlo simulation, and use this to compare distributions in the observed data to simulation across several observables and many histogram bins. The drawback of this methodology is that one needs to rely on accurate simulation, and also that, due to the vast size of the parameter space being searched, an observation that appears statistically significant could potentially be the result of a statistical fluctuation.\\nIn the following, we discuss machine learning techniques which mitigate some of these challenges and have the potential to improve and extend model-independent searches.\\n'}},\n",
       "       {'entity_name': 'lagrange multipliers', 'entity_type': 'analysis_technique', 'description': 'A mathematical method used in optimization problems to find the local maxima and minima of a function subject to equality constraints, commonly applied in kinematic fitting to minimize discrepancies while adhering to conservation laws.', 'relevant_passages': {\"\\\\section{Kinematic Fitting}\\nEarlier we had the example of estimating the number of married people on \\nan island, and saw that introducing theoretical information could improve\\nthe accuracy of our answer. Here we use the same idea in the context of\\nestimating the momenta and directions of objects produced in a high energy\\ninteraction. The theory we use is that energy and momentum are conserved\\nbetween the inital state collison and the observed objects in the reaction.\\nThe reaction can be either at a collider or with a stationary target. We \\ndenote it by $a + b \\\\rightarrow c + d + e$, but the number of final state \\nobjects can be arbitrary. We assume for the time being the energy and \\nmomenta of all the objects are measured\\\\footnote{For objects like charged \\nparticles whose momenta are determined from their trajectories in a \\nmagnetic field, the energy is determined from the momentum by using the \\nrelevant particle mass.}.\\nThe technique is to consider all possible configurations of the particles'\\nkinematic variables that conserve momentum and energy, and to choose that \\nconfiguration that is closest to the measured variables. The degree of\\ncloseness is defined by the weighted sum of squares of the discrepancies $S$,\\ntaking the uncertainties and correlations into account. If the uncertainties \\non the kinematic quantities $m_i$ were uncorrelated, \\n\\\\begin{equation}\\nS = \\\\Sigma (f_i - m_i)^2/\\\\sigma_i^2\\n\\\\end{equation}\\nwhere the summation is over the 4 components for all the objects in the \\nreaction, $m_i$ are the measured values and $f_i$ are the corresponding \\nfitting quantities. Because of correlations, however, this becomes\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (f_i - m_i) E_{ij} (f_j - m_j) \\n\\\\end{equation}\\nwhere there is now a double summation over the components, and $E_{ij}$ is the \\n$(i,j)^{th}$ component of the inverse covariance matrix for the measured \\nquantities\\\\footnote{The main correlations are among the 4 components of a single\\nobject, rather than between different objects.}. \\nThe procedure then consists in varying $f$ in order to minimise $S$, subject to\\nthe energy and momentum constraints. This usually involves Lagrange Multipliers.\\nThe result of this procedure is to produce a set of fitted values of all the \\nkinematic quantities, which will have smaller uncertainties than the measured ones.\\nThis is an example of incorporating theory to improve the results. Thus if the objects \\nare jets, their directions are usually quite well determined, but their energies less so. \\nThe fitting procedure enables the accurately determined jet directions to help \\nreduce the uncertainties on the jet energies.\\nThe fitting procedure \\nalso provides $S_{min}$, which is a measure of how well the best $f_i$ agree with the $m_i$.\\nIn the case described, the distribution of $S_{min}$ is approximately $\\\\chi^2$ \\nwith 4 degrees of freedom (because of the 4 constraints).\\nIf $S_{min}$ is too large, then our assumed hypothesis for the reaction may be \\nincorrect; for example, there might have been an extra object produced in the \\ncollision that was undetected (e.g. a neutrino, or a charged particle which passed \\nthrough an uninstrumented region of our detector).\\nSince we have 4 constraint equations, we can also allow for up to 4 missing kinematic\\nquantities. Examples include an undetected neutrino in the final state (3 unmeasured \\nmomentum components), a wide-band neutrino beam of known direction (1 missing variable),\\netc. With $m$ missing variables in an interaction involving a single vertex, \\n$S_{min}$ should have a $\\\\chi^2$ distribution with $4-m$ degrees of freedom.\\nKinematic fitting can be extended to more complicated event topologies including production\\nand decay vertices, reactions involving particles of well known mass which decay \\npromptly (e.g. $\\\\psi \\\\rightarrow \\\\mu^+ \\\\mu^-$), etc. \\n\\\\subsection{Example of a simplified kinematic fit}\\nConsider a non-relativistic elastic scattering of two equal mass objects, for example a slow \\nanti-proton hitting a stationary proton. \\nFor simplicity, the measured angles $\\\\theta_1^m \\\\pm \\\\sigma$ and $\\\\theta_2^m \\\\pm \\\\sigma$ that \\nthe outgoing particles make with the direction\\nof travel of the incident anti-proton are assumed to have the same uncorrelated \\nuncertainties $\\\\sigma$. As a result of energy and momentum conservation, the angles must\\nsatisfy the constraint\\n\\\\begin{equation}\\n\\\\theta_1^t + \\\\theta_2^t = \\\\pi/2\\n\\\\end{equation}\\nwhere the superscipt $t$ denotes the true value. There are 3 further constraints \\nbut for simplicity we shall ignore them.\\nTo find our best estimates of $\\\\theta_1^t$ and $\\\\theta_2^t$, we must minimise\\n\\\\begin{equation}\\nS = (\\\\theta_1^t - \\\\theta_1^m)^2/\\\\sigma^2 + (\\\\theta_2^t - \\\\theta_2^m)^2/\\\\sigma^2\\n\\\\end{equation}\\nsubject to the constraint \\\\ref{constraint}. By using Lagrange Multipliers or by \\neliminating $\\\\theta_2^t$ and then minimising $S$, this yields\\n\\\\begin{equation}\\n\\\\begin{split}\\n\\\\theta_1^t &= \\\\theta_1^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m) \\\\\\\\ \\n\\\\theta_2^t &= \\\\theta_2^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m)\\n\\\\end{split}\\n\\\\end{equation}\\nThat is, the best estimate of each true value is obtained by adding to the corresponding measured value\\nhalf the amount by which the measured values fail to satisfy the constraint \\n\\\\ref{constraint}. \\nThe uncertainties on the fitted estimates of the angles are easily obtained by \\npropagation of the uncertainties $\\\\sigma$ on the measured angles vias eqns. \\n\\\\ref{fitted}, and are both equal to $\\\\sigma/\\\\sqrt 2$. \\nWe thus have an example \\nof the promised outcome that kinematic fitting improves the accuracy of our \\nmeasurements. The factor of $\\\\sqrt 2$ improvement can easily be understood in that \\nwe have two independent estimates of $\\\\theta_1^t$, the first being the \\noriginal measurement $\\\\theta_1^m$, and the other coming from the measurement \\n$\\\\theta_2^m$ via the constraint \\\\ref{constraint}. However, even with uncorrelated uncertainties\\non the measured angles, the fitted ones would be anti-correlated.\\n\"}},\n",
       "       {'entity_name': 'degrees of freedom', 'entity_type': 'statistics_concept', 'description': 'A concept in statistics that refers to the number of independent values or quantities that can vary in an analysis without violating any constraints, crucial for determining the distribution of test statistics like chi-squared.', 'relevant_passages': {\"\\\\section{Kinematic Fitting}\\nEarlier we had the example of estimating the number of married people on \\nan island, and saw that introducing theoretical information could improve\\nthe accuracy of our answer. Here we use the same idea in the context of\\nestimating the momenta and directions of objects produced in a high energy\\ninteraction. The theory we use is that energy and momentum are conserved\\nbetween the inital state collison and the observed objects in the reaction.\\nThe reaction can be either at a collider or with a stationary target. We \\ndenote it by $a + b \\\\rightarrow c + d + e$, but the number of final state \\nobjects can be arbitrary. We assume for the time being the energy and \\nmomenta of all the objects are measured\\\\footnote{For objects like charged \\nparticles whose momenta are determined from their trajectories in a \\nmagnetic field, the energy is determined from the momentum by using the \\nrelevant particle mass.}.\\nThe technique is to consider all possible configurations of the particles'\\nkinematic variables that conserve momentum and energy, and to choose that \\nconfiguration that is closest to the measured variables. The degree of\\ncloseness is defined by the weighted sum of squares of the discrepancies $S$,\\ntaking the uncertainties and correlations into account. If the uncertainties \\non the kinematic quantities $m_i$ were uncorrelated, \\n\\\\begin{equation}\\nS = \\\\Sigma (f_i - m_i)^2/\\\\sigma_i^2\\n\\\\end{equation}\\nwhere the summation is over the 4 components for all the objects in the \\nreaction, $m_i$ are the measured values and $f_i$ are the corresponding \\nfitting quantities. Because of correlations, however, this becomes\\n\\\\begin{equation}\\nS = \\\\Sigma\\\\Sigma (f_i - m_i) E_{ij} (f_j - m_j) \\n\\\\end{equation}\\nwhere there is now a double summation over the components, and $E_{ij}$ is the \\n$(i,j)^{th}$ component of the inverse covariance matrix for the measured \\nquantities\\\\footnote{The main correlations are among the 4 components of a single\\nobject, rather than between different objects.}. \\nThe procedure then consists in varying $f$ in order to minimise $S$, subject to\\nthe energy and momentum constraints. This usually involves Lagrange Multipliers.\\nThe result of this procedure is to produce a set of fitted values of all the \\nkinematic quantities, which will have smaller uncertainties than the measured ones.\\nThis is an example of incorporating theory to improve the results. Thus if the objects \\nare jets, their directions are usually quite well determined, but their energies less so. \\nThe fitting procedure enables the accurately determined jet directions to help \\nreduce the uncertainties on the jet energies.\\nThe fitting procedure \\nalso provides $S_{min}$, which is a measure of how well the best $f_i$ agree with the $m_i$.\\nIn the case described, the distribution of $S_{min}$ is approximately $\\\\chi^2$ \\nwith 4 degrees of freedom (because of the 4 constraints).\\nIf $S_{min}$ is too large, then our assumed hypothesis for the reaction may be \\nincorrect; for example, there might have been an extra object produced in the \\ncollision that was undetected (e.g. a neutrino, or a charged particle which passed \\nthrough an uninstrumented region of our detector).\\nSince we have 4 constraint equations, we can also allow for up to 4 missing kinematic\\nquantities. Examples include an undetected neutrino in the final state (3 unmeasured \\nmomentum components), a wide-band neutrino beam of known direction (1 missing variable),\\netc. With $m$ missing variables in an interaction involving a single vertex, \\n$S_{min}$ should have a $\\\\chi^2$ distribution with $4-m$ degrees of freedom.\\nKinematic fitting can be extended to more complicated event topologies including production\\nand decay vertices, reactions involving particles of well known mass which decay \\npromptly (e.g. $\\\\psi \\\\rightarrow \\\\mu^+ \\\\mu^-$), etc. \\n\\\\subsection{Example of a simplified kinematic fit}\\nConsider a non-relativistic elastic scattering of two equal mass objects, for example a slow \\nanti-proton hitting a stationary proton. \\nFor simplicity, the measured angles $\\\\theta_1^m \\\\pm \\\\sigma$ and $\\\\theta_2^m \\\\pm \\\\sigma$ that \\nthe outgoing particles make with the direction\\nof travel of the incident anti-proton are assumed to have the same uncorrelated \\nuncertainties $\\\\sigma$. As a result of energy and momentum conservation, the angles must\\nsatisfy the constraint\\n\\\\begin{equation}\\n\\\\theta_1^t + \\\\theta_2^t = \\\\pi/2\\n\\\\end{equation}\\nwhere the superscipt $t$ denotes the true value. There are 3 further constraints \\nbut for simplicity we shall ignore them.\\nTo find our best estimates of $\\\\theta_1^t$ and $\\\\theta_2^t$, we must minimise\\n\\\\begin{equation}\\nS = (\\\\theta_1^t - \\\\theta_1^m)^2/\\\\sigma^2 + (\\\\theta_2^t - \\\\theta_2^m)^2/\\\\sigma^2\\n\\\\end{equation}\\nsubject to the constraint \\\\ref{constraint}. By using Lagrange Multipliers or by \\neliminating $\\\\theta_2^t$ and then minimising $S$, this yields\\n\\\\begin{equation}\\n\\\\begin{split}\\n\\\\theta_1^t &= \\\\theta_1^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m) \\\\\\\\ \\n\\\\theta_2^t &= \\\\theta_2^m + 0.5*(\\\\pi/2 - \\\\theta_1^m - \\\\theta_2^m)\\n\\\\end{split}\\n\\\\end{equation}\\nThat is, the best estimate of each true value is obtained by adding to the corresponding measured value\\nhalf the amount by which the measured values fail to satisfy the constraint \\n\\\\ref{constraint}. \\nThe uncertainties on the fitted estimates of the angles are easily obtained by \\npropagation of the uncertainties $\\\\sigma$ on the measured angles vias eqns. \\n\\\\ref{fitted}, and are both equal to $\\\\sigma/\\\\sqrt 2$. \\nWe thus have an example \\nof the promised outcome that kinematic fitting improves the accuracy of our \\nmeasurements. The factor of $\\\\sqrt 2$ improvement can easily be understood in that \\nwe have two independent estimates of $\\\\theta_1^t$, the first being the \\noriginal measurement $\\\\theta_1^m$, and the other coming from the measurement \\n$\\\\theta_2^m$ via the constraint \\\\ref{constraint}. However, even with uncorrelated uncertainties\\non the measured angles, the fitted ones would be anti-correlated.\\n\"}},\n",
       "       {'entity_name': 'mean lifetime', 'entity_type': 'statistics_concept', 'description': 'The average time a particle is expected to exist before decaying, calculated as the mean of observed decay times in an experiment.', 'relevant_passages': {'\\\\section{Worked example: Lifetime determination}\\nHere we consider an experiment which has resulted in $N$ observed decay times $t_i$ of a particle\\nwhose lifetime $\\\\tau$ we want to determine. The probability density for observing a decay at time $t$ \\nis \\n\\\\begin{equation}\\np(t;\\\\tau) = (1/\\\\tau) \\\\ e^{-t/\\\\tau}\\n\\\\end{equation} \\nNote the essential normalisation factor $1/\\\\tau$; without this the likelihood method does not work.\\nIt should be realised that realistic situations are more complicated than this. For example, we ignore\\nthe possibility of backgrounds, time resolution which smears the expected values of $t$, acceptance or \\nefficiency effects which vary with $t$, etc., but this enables us to estimate $\\\\tau$ and its uncertainty\\n$\\\\sigma_{\\\\tau}$ analytically. In real practical cases, it is almost always necessary to calculate the \\nlikelihood as a function of $\\\\tau$ numerically. \\nFrom equation \\\\ref{exp} we calculate the log-likelihood as\\n\\\\begin{equation}\\n\\\\ln L(\\\\tau) = \\\\ln[\\\\Pi\\\\ (1/\\\\tau) e^{-t_i/\\\\tau}] \\\\ \\\\ = \\\\ \\\\ \\\\Sigma (-\\\\ln \\\\tau - t_i/\\\\tau)\\n\\\\end{equation}\\nDifferentiating $\\\\ln L(\\\\tau)$ with respect to $\\\\tau$ and setting the derivative to zero then yields\\n\\\\begin{equation}\\n\\\\tau = \\\\Sigma t_i/N\\n\\\\end{equation}\\nThis equation has an appealing feature, as it can be read as ``The mean lifetime is equal to the mean lifetime\",\\nwhich sounds as if it must be true. However, what it really says is not quite so trivial: ``Our\\nbest estimate of the lifetime parameter $\\\\tau$ is equal to the mean of the $N$ observed decay times in our \\nexperiment.\"\\nWe next calculate $\\\\sigma_{\\\\tau}$ from the second derivative of $\\\\ln L$, and obtain \\n\\\\begin{equation}\\n\\\\sigma_{\\\\tau} = \\\\tau/\\\\sqrt N\\n\\\\end{equation}\\nThis exhibits a common feature that the uncertainty of our parameter estimate decreases as $1/\\\\sqrt N$ as we \\ncollect more and more data. However, a potential problem arises from the fact that our estimated uncertainty\\nis proportional to our estimate of the parameter. This is relevant if we are trying to combine different experimental\\nresults on the lifetime of a particle. For combining procedures which weight each result by $1/\\\\sigma^2$, a \\nmeasurement where the fluctuations in the observed times result in a low estimate of $\\\\tau$\\nwill tend to be over-weighted (compare the section on `Combining Experiments\\' in Lecture 1), \\nand so the weighted average would be biassed \\ndownwards. This shows that it is better to combine different experiments at the data level, rather than \\nsimply trying to use their results.\\nOne final point to note about our simplified example is that the likelihood $L(\\\\tau)$ depends on the \\nobservations only via the {\\\\bf sum} of the times $\\\\Sigma t_i$ i.e. their {\\\\bf distribution} is \\nirrelevant. Thus the likelihood distributions for two experiments having the same number of events and the \\nsame sum of observed decay times, but with one having the decay times consistent with an exponential \\ndistribution and the other having something completely different (e.g. all decays occur at the same time), \\nwould have identical likelihood\\nfunctions. This provides an example of the fact that the unbinned likelihood function does not in general provide\\nuseful information on Goodness of Fit. \\n'}},\n",
       "       {'entity_name': 'jeffreys prior', 'entity_type': 'analysis_technique', 'description': 'A technique for selecting a prior distribution that is invariant under reparameterization, based on the Fisher information, which aims to provide an objective basis for Bayesian analysis.', 'relevant_passages': {\"\\\\section{Probability}\\n\\\\subsection {Bayesian probability}\\nThe Bayesian definition of probability is that $P_A$ represents your belief in $A$.\\n1 represents certainty, 0 represents total disbelief.\\nIntermediate values can be calibrated by asking whether you would prefer to bet on $A$, or on a white ball being drawn from an urn containing a mix of white and black balls. \\nThis avoids the limitations of frequentist probability---coins, dice, kaons, rain tomorrow, existence of supersymmetry (SUSY) can all have probabilities assigned to them.\\nThe drawback is that your value for $P_A$ may be different from mine, or anyone else's. It is\\nalso called subjective probability.\\nBayesian probability makes great use of Bayes' theorem, in the form\\n\\\\begin{equation}P(Theory|Data)= {P(Data|Theory) \\\\over P(Data) } \\\\times P(Theory) \\\\quad.\\n\\\\end{equation}\\n$P(Theory)$ is called the {\\\\em prior}: your initial belief in $Theory$. $P(Data|Theory)$ is the {\\\\em Likelihood}:\\nthe probability of getting $Data$ if $Theory$ is true. $P(Theory|Data)$ is the {\\\\em Posterior}: your belief in $Theory$ \\nin the light of a particular $Data$ being observed.\\nSo this all works very sensibly. If the data observed is predicted by the theory, your belief in that theory is boosted,\\nthough this is moderated by the probabilty that the data could have arisen anyway. Conversely, if data is observed which \\nis disfavoured by the theory, your belief in that theory is weakened.\\nThe process can be chained. \\nThe posterior from a first experiment can be taken as the prior for a second experiment, and so on. \\nWhen you write out the factors you find that the order doesn't matter. \\n\\\\subsubsection{Prior distributions}\\nOften, though, the theory being considered is not totally defined: it may contain a parameter (or several parameters)\\nsuch as a mass, coupling constant, or decay rate. Generically we will call this $a$, with the proviso that it may\\nbe multidimensional. \\nThe prior is now not a single number $P(Theory)$ \\nbut a probability distribution $P_0(a)$. \\n$\\\\int_{a_1}^{a_2} P_0(a)\\\\, da $ is your prior belief that $a$ lies between $a_1$ and $a_2$.\\n$\\\\int_{-\\\\infty}^{\\\\infty} P_0(a)\\\\, da$ is your original $P(Theory)$. This is generally taken as 1, which is valid provided the possibility that the theory that is false is matched by some value of $a$---for example if the coupling constant for a hypothetical particle is zero, that accommodates any belief that it might not exist. Bayes' theorem then runs:\\n\\\\begin{equation} P_1(a;x) \\\\propto L(a;x) P_0(a)\\n\\\\quad.\\n\\\\end{equation}\\nIf the range of $a$ is infinite, $P_0(a)$ may be vanishingly small (this is called an `improper prior'). \\nHowever this is not a problem. Suppose, for example, that all we know about $a$ is that it is non-negative, and we\\nare genuinely equally open to its having any value. We write $P_0(a)$ as $C$, so $\\\\int_{a_1}^{a_2} P_0(a)\\\\, da =C(a_2-a_1)$.\\nThis probability is vanishingly small: if you were offered the choice of a bet on $a$ lying within the range $[a_1,a_2]$\\nor of drawing a white ball from an urn containing 1 white ball and $N$ black balls, you would choose the latter, however large $N$ was. However it is not zero: if the urn contained $N$ black balls, but no white ball, your betting choice would change. After a measurement you have\\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) C da'} C$, and the factors of $C$ can be cancelled (which, and this is the point, you could {\\\\em not} do if $C$ were exactly zero) giving \\n$P_1(a;x)={L(a;x) \\\\over \\\\int L(a';x) da'} $ or,\\n$\\nP_1(a;x) \\\\propto L(a;x) $,\\nand you can then just normalize $P_1(a)$ to 1.\\nFigure~\\\\ref{fig:bayes1} shows Eq.~\\\\ref{eq:bayes} at work. Suppose $a$ is known to lie between 0 and 6, and\\nthe prior distribution is taken as flat, as shown in the left hand plot. A measurement of $a$ gives a result \\n$4.4 \\\\pm 1.0$~, as shown in the central plot. The product of the two gives (after normalization) the posterior, as shown in the right hand plot.\\n\\\\subsubsection{Likelihood}\\nThe likelihood---the number $P(Data|Theory)$---is now generalised to the function $L(a,x)$, where $x$ is the observed value of the data. Again, $x$ may be multidimensional, but in what follows it is not misleading to ignore that.\\nThis can be confusing. For example, anticipating Section~\\\\ref{sec:poisson}, the probability of getting $x$ counts from a Poisson process with mean $a$ is\\n\\\\begin{equation}\\nP(x,a)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWe also write\\n\\\\begin{equation}\\nL(a,x)=e^{-a} {a^x \\\\over x!}\\n\\\\quad.\\n\\\\end{equation}\\nWhat's the difference? Technically there is none. These are identical joint functions of two variables ($x$ and $a$)\\nto which we have just happened to have given different names. Pragmatically we regard Eq.~\\\\ref{eq:lone}\\nas describing the probability of getting various different $x$ from some fixed $a$, whereas Eq.~\\\\ref{eq:ltwo}\\ndescribes the likelihood for various different $a$ from some given $x$. \\nBut be careful with the term `likelihood'. If $P(x_1,a)>P(x_2,a)$ then $x_1$ is more probable (whatever you mean by that) than\\n$x_2$. If $L(a_1,x)>L(a_2,x)$ it does not mean that $a_1$ is more likely (however you define that) than $a_2$.\\n\\\\subsubsection{Shortcomings of Bayesian probability}\\nThe big problem with Bayesian probability is that it is subjective.\\nYour $P_0(a)$ and my $P_0(a)$ may be different---so how can we compare results?\\nScience does, after all, take pride in being objective: it handles real facts, not opinions.\\nIf you present a Bayesian result from your search for the $X$ particle this embodies\\nthe actual experiment and your irrational prior prejudices. I am interested in your experiment but not\\nin your irrational prior prejudices---I have my own---and it is unhelpful if you combine the two.\\nBayesians sometimes ask about the right prior they should use. \\nThis is the wrong question. The prior is what you believe, and only you know that.\\nThere is an argument made for taking the prior as uniform. This is sometimes\\ncalled the \\n`Principle of ignorance' and justified as being impartial. But this is misleading, even dishonest. \\nIf $P_0(a)$ is taken as constant, favouring no particular value, then it is not constant for $a^2$ or $\\\\sqrt a$ or $\\\\ln a$,\\nwhich are equally valid parameters. \\nIt is true that with lots of data, $P_1(a)$ decouples from $P_0(a)$.\\nThe final result depends only on the measurements.\\nBut this is not the case with little data---and that's the situation we're usually in---when doing statistics properly matters.\\nAs an example, suppose you make a Gaussian measurement (anticipating slightly Section~\\\\ref{sec:measurement}).\\nYou consider a prior flat in $a$ and a prior flat in $\\\\ln a$. This latter is quite sensible---it says you expect a \\nresult between 0.1 and 0.2 as being equally likely as a result between 1 and 2, or 10 and 20.\\nThe posteriors are shown in Fig.~\\\\ref{fig:differentpriors}.\\nFor an `accurate' result of $3\\\\pm 0.5$ the posteriors are very close. For an `intermediate' result\\nof $4.0 \\\\pm 1.0$ there is an appreciable difference in the peak value and the shape. For a `poor'\\nmeasurement of $5.0 \\\\pm 2.0$ the posteriors are {\\\\em very} different.\\nSo you should never just quote results from a single prior. \\nTry several forms of prior and examine the spread of results. If they are pretty much the same\\nyou are vindicated. This is called\\n`robustness under choice of prior' and it is standard practice for statisticians. If they are different\\nthen the data are telling you about the limitations of your results.\\n\\\\subsubsection {Jeffreys' prior}\\nJeffreys~\\\\cite{Jeffreys} suggested a technique now known as the Jeffreys' or {\\\\em objective prior}: that\\nyou should \\nchoose a prior flat in a transformed variable $a'$ for which the Fisher information, ${\\\\cal I} =-\\\\left< {\\\\partial^2 L(x;a)\\n\\\\over \\\\partial a^2}\\\\right> $ is constant. \\nThe Fisher information (which is important in maximum likelihood estimation, as described in Section~\\\\ref{sec:ML})\\nis a measure of how much a measurement tells you about the parameter: a large ${\\\\cal I}$ has a likelihood function with a sharp peak and will tell you (by some measure) a lot about $a$; a small ${\\\\cal I}$ has a featureless likelihood function\\nwhich will not be useful. Jeffrey's principle is that the prior should not favour or disfavour particular values of the parameter.\\nIt is equivalently---and more conveniently---used as taking a prior in the original $a$ which is proportional to\\n$\\\\sqrt{\\\\cal I}$.\\nIt has not been universally adopted for various reasons. Some practitioners like to be able to include their own\\nprior belief into the analysis. It also makes the prior dependent on the experiment (in the form of the likelihood function). \\nThus if ATLAS and CMS searched for the same new $X$ particle they would use different priors for $P_0(M_X)$, \\nwhich is (to some people) absurd.\\nSo it is not universal---but when you are selecting a bunch of priors to test robustness---the Jefferys' prior \\nis a strong contender for inclusion.\\n\"}},\n",
       "       {'entity_name': 'mean', 'entity_type': 'statistics_concept', 'description': 'A measure of central tendency, often denoted by μ, representing the average value of a dataset, calculated as the sum of all values divided by the number of values.', 'relevant_passages': {\"\\\\section{Probability distributions and their properties} \\nWe have to make a simple distinction between two sorts of data: \\\\emph{integer} data and \\\\emph{real-number} data\\\\footnote{Other branches of science have to include a third, \\\\emph{categorical} data, but we will ignore that.}.\\nThe first covers results which are of their nature whole numbers: the numbers of kaons produced in \\na collision, or the number of entries falling into some bin of a histogram.\\nGenerically let's call such numbers $r$. They have probabilities $P(r)$ which are dimensionless.\\nThe second covers results whose values are real (or floating-point) numbers. There are lots of these:\\nenergies, angles, invariant masses $\\\\dots$\\nGenerically let's call such numbers $x$, and they have probability density functions $P(x)$\\nwhich have \\ndimensions of $[x]^{-1}$, so $\\\\int_{x_1}^{x_2} P(x) dx$ or $P(x)\\\\, dx$ are probabilities.\\nYou will also sometimes meet the cumulative distribution $C(x)=\\\\int_{-\\\\infty}^x P(x') \\\\, dx'$.\\n\\\\subsection{Expectation values}\\nFrom $P(r)$ or $P(x)$ one can form the expectation value\\n\\\\begin{equation}\\n\\\\langle f \\\\rangle =\\\\sum_r f(r) P(r) \\\\qquad {\\\\rm or} \\\\qquad \\\\langle f \\\\rangle = \\\\int f(x) P(x) \\\\, dx \\n\\\\quad,\\n\\\\end{equation}\\nwhere the sum or integral is taken as appropriate.\\nSome authors write this as\\n$E(f)$, but I personally prefer the angle-bracket notation. You may think it looks too much like quantum mechanics,\\nbut in fact it's quantum mechanics which looks like statistics: an expression like $\\\\langle \\\\psi | \\\\hat Q | \\\\psi \\\\rangle$\\nis the average value of an operator $\\\\hat Q$ in some state $\\\\psi$, where `average value' has exactly the same \\nmeaning and significance. \\n\\\\subsubsection{Mean and standard deviation} \\nIn particular the {\\\\em mean}, often written $\\\\mu$, is given by \\n$ \\\\langle r \\\\rangle = \\\\sum_r r P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle x \\\\rangle =\\\\int x P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent Similarly one can write \\nhigher {\\\\em moments} \\n$\\\\mu_k = \\\\langle r^k \\\\rangle = \\\\sum_r r^k P(r)\\\\qquad {\\\\rm or } \\\\qquad \\\\langle x^k \\\\rangle =\\\\int x^k P(x) \\\\, dx \\\\quad,$ \\n\\\\noindent and {\\\\em central moments} \\n$\\\\mu'_k = \\\\langle (r-\\\\mu)^k \\\\rangle = \\\\sum_r (r-\\\\mu)^k P(r) \\\\qquad {\\\\rm or } \\\\qquad \\\\langle (x-\\\\mu)^k \\\\rangle =\\\\int (x-\\\\mu)^k P(x) \\\\, dx \\\\quad.$ \\n\\\\noindent The second central moment is known as the \\n{\\\\em variance} \\n$\\\\mu'_2=V= \\\\sum_r (r-\\\\mu)^2 P(r) = \\\\langle r^2 \\\\rangle - \\\\langle r \\\\rangle ^2$\\n\\\\qquad \\nor \\\\qquad $ \\\\int (x-\\\\mu)^2 P(x) \\\\, dx = \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2$\\n\\\\noindent It is easy to show that $\\\\langle (x-\\\\mu)^2 \\\\rangle =\\\\langle x^2 \\\\rangle -\\\\mu^2$. The {\\\\em standard deviation} is just the square root of the variance $\\\\sigma=\\\\sqrt{V}$.\\nStatisticians usually use variance, perhaps because formulae come out simpler. Physicists usually use standard deviation,\\nperhaps because it has the same dimensions as the variable being studied, and can be drawn as an error bar on a plot. \\nYou may also meet {\\\\em skew}, which is $\\\\gamma=\\\\langle (x-\\\\mu)^3 \\\\rangle /\\\\sigma^3$ and {\\\\em kurtosis}, $h=\\\\langle (x-\\\\mu)^4 \\\\rangle /\\\\sigma^4 -3$.\\nDefinitions vary, so be careful. Skew is a dimensionless measure of the asymmetry of a distribution. Kurtosis is\\n(thanks to that rather arbitrary looking 3 in the definition) \\nzero for a Gaussian distribution (see Section~\\\\ref{sec:measurement}): positive kurtosis indicates a\\nnarrow core with a wide tail, negative kurtosis indicates the tails are reduced. \\n\\\\subsubsection{Covariance and correlation}\\nIf your data are \\n2-dimensional pairs $(x,y)$,\\nthen besides forming $\\\\langle x \\\\rangle, \\\\langle y \\\\rangle, \\\\sigma_x$ etc., you can also form the \\n{\\\\em Covariance}\\n${\\\\rm Cov}(x,y)=\\\\langle (x-\\\\mu_x)(y-\\\\mu_y) \\\\rangle = \\\\langle xy \\\\rangle - \\\\langle x \\\\rangle \\\\langle y \\\\rangle \\\\quad.$\\nExamples are shown in Fig.~\\\\ref{fig:covariance}. If there is a tendency for positive fluctuations in $x$ to be associated with positive fluctuations in $y$ (and therefore negative with negative) then\\nthe product $(x_i-\\\\overline x)(y_i-\\\\overline y)$ tends to be positive and the covariance is greater than 0. A negative covariance, as in the 3rd plot, happens if a positive fluctuation in one variable is associated with a negative fluctuation in the other.\\nIf the variables are independent then a positive variation in $x$ is equally likely to be associated with a positive or a negative variation in $y$ and the covariance is zero, as in the first plot. However the converse is not always the case, there can be two-dimensional distributions where the covariance is zero, but the two variables are not independent, as is shown in the fourth plot.\\nCovariance is useful, but it has dimensions. Often one uses the \\n{\\\\em correlation}, which is just\\n\\\\begin{equation}\\n\\\\rho={{\\\\rm Cov}(x,y)\\\\over \\\\sigma_x \\\\sigma_y}\\n\\\\quad.\\n\\\\end{equation}\\nIt is easy to show that\\n$\\\\rho$ lies between 1 (complete correlation) and -1 (complete anticorrelation). \\n$\\\\rho=0$ if $x$ and $y$ are independent.\\nIf there are more than two variables---the alphabet runs out so let's call them \\n$(x_1,x_2,x_3\\\\dots x_n)$---\\nthen these generalise to the \\n{\\\\em covariance matrix}\\n${\\\\bf V}_{ij}=\\\\langle x_i x_j \\\\rangle - \\\\langle x_i \\\\rangle \\\\langle x_j \\\\rangle$ \\n\\\\noindent and the {\\\\em \\ncorrelation matrix}\\n${\\\\bf \\\\rho}_{ij}={{\\\\bf V}_{ij} \\\\over \\\\sigma_i \\\\sigma_j} \\\\quad.$\\n\\\\noindent The diagonal of $\\\\bf V$ is $\\\\sigma_i^2$. \\nThe\\ndiagonal of $\\\\bf \\\\rho$ is 1.\\n\\\\subsection{Binomial, Poisson and Gaussian}\\nWe now move from considering the general properties of distributions to considering three specific ones.\\nThese are the ones you will most commonly meet for the distribution of the original data\\n(as opposed to quantities constructed from it). Actually the first, the binomial, is not nearly as common as the second, the Poisson; and the third, the Gaussian, is overwhelmingly more common. However it is \\nuseful to consider all three as concepts are built up from the simplest to the more sophisticated.\\n\\\\subsubsection {The binomial distribution }\\nThe binomial distribution is easy to understand as it basically describes the familiar\\\\footnote{Except, as it happens, in Vietnam, where coins have been completely replaced by banknotes.} tossing of coins. \\nIt describes the number $r$ of successes in $N$ trials, each with probability $p$ of success.\\n$r$ is discrete so the process is described by a probability distribution\\n\\\\begin{equation}\\nP(r;p,N)={N! \\\\over r! (N-r)!} p^r q^{N-r} \\n\\\\quad,\\n\\\\end{equation}\\nwhere $ q \\\\equiv 1-p$.\\nSome examples are shown in Fig.~\\\\ref{fig:binom}.\\nThe distribution has \\nmean $\\\\mu=Np$, variance $V=Npq$, and standard deviation $\\\\sigma=\\\\sqrt{Npq}$.\\n\\\\subsubsection {The Poisson distribution}\\nThe Poisson distribution also describes the probability of some discrete number $r$,\\nbut rather than a fixed number of `trials' it considers a random rate $\\\\lambda$: \\n\\\\begin{equation}\\nP(r;\\\\lambda)=e^{-\\\\lambda}{\\\\lambda^r \\\\ \\\\over r!}\\n\\\\quad.\\n\\\\end{equation}\\nIt is linked to the binomial---the Poisson is the \\nlimit of the binomial---as $N\\\\to \\\\infty$, $p \\\\to 0$ with $np=\\\\lambda=constant$. Figure~\\\\ref{fig:poisson} shows various examples. It has mean $\\\\mu=\\\\lambda$, variance $V=\\\\lambda$, and standard deviation $\\\\sigma=\\\\sqrt{\\\\lambda}=\\\\sqrt{\\\\mu}$.\\nThe clicks of a Geiger counter are the standard illustration of a Poisson process.\\nYou will meet it a lot as it applies to event counts---on their own or in histogram bins.\\nTo help you think about the Poisson, here is a simple question (which\\ndescribes a situation \\nI have seen in practice, more than once, from people who ought to know better).\\n\\\\\\n\\\\hrule\\n\\\\\\nYou need to know the efficiency of your PID system for positrons.\\nYou find 1000 data events where 2 tracks have a combined mass of 3.1~GeV ($J/\\\\psi)$ and the negative track is \\nidentified as an $e^-$ (`Tag-and-probe' technique).\\nIn 900 events the $e^+$ is also identified. In 100 events it is not. The efficiency is 90\\\\\\nWhat about the error?\\nColleague A says $\\\\sqrt{900}=30$ so efficiency is $90.0 \\\\pm 3.0 $\\\\\\ncolleague B says $\\\\sqrt{100}=10$ so efficiency is $90.0 \\\\pm 1.0 $\\\\\\nWhich is right?\\n\\\\\\n\\\\hrule\\n\\\\\\nPlease think about this before turning the page...\\n\\\\vfill\\\\eject\\n{Neither---both are wrong}. This is binomial not Poisson: $p=0.9, N=1000$.\\n\\\\noindent The error is $\\\\sqrt{Npq}=\\\\sqrt{1000 \\\\times 0.9 \\\\times 0.1}$ (or $\\\\sqrt{1000 \\\\times 0.1 \\\\times 0.9}$) =$\\\\sqrt{90} = 9.49$ so the efficiency is $90.0 \\\\pm 0.9$ \\\\\\n\\\\subsubsection{The Gaussian distribution}\\nThis is by far the most important statistical distribution.\\nThe probability density function (PDF) for a variable $x$ is given by the formula\\n\\\\begin{equation} \\nP(x;\\\\mu,\\\\sigma)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-{(x-\\\\mu)^2 \\\\over 2 \\\\sigma^2}}\\n\\\\quad.\\n\\\\end{equation}\\nPictorially this is shown in Fig.~\\\\ref{fig:gauss}.\\nThis is sometimes called the `bell curve', though in fact a real bell does not have flared edges like that.\\nThere is (in contrast to the Poisson and binomial) \\nonly one \\nGaussian curve, as $\\\\mu$ and $\\\\sigma$ are just location and scale parameters.\\nThe mean is $\\\\mu$ and the standard deviation is $\\\\sigma$. The \\nSkew is zero, as it is symmetric, and the kurtosis is zero by construction.\\nIn statistics, and most disciplines, this is known as the {\\\\em normal distribution}. Only in physics is it known as `The Gaussian'---perhaps because the word `normal' already has so many meanings. \\nThe reason for the importance of the Gaussian is the {\\\\em central limit theorem} (CLT) that states:\\nif the variable $X$ is the sum of $N$ variables $x_1,x_2\\\\dots x_N$ then:\\n\\\\begin{enumerate}\\n\\\\item Means add: $ \\\\langle X \\\\rangle = \\\\langle x_1 \\\\rangle + \\\\langle x_2 \\\\rangle + \\\\dots \\\\langle x_N \\\\rangle$, \\n\\\\item Variances add: $V_X=V_1+V_2 +\\\\dots V_N$,\\n\\\\item If the variables $x_i$ are independent and identically distributed (i.i.d.) then $P(X)$ tends to a Gaussian for large $N$.\\n\\\\end{enumerate}\\n(1) is obvious, (2) is pretty obvious, and means that standard deviations add in quadrature, and that the standard deviation of an average falls like $1\\\\over \\\\sqrt N$, (3) applies whatever the form of the original $P(x)$.\\nBefore proving this, it is helpful to see a demonstration to convince yourself that the implausible assertion in (3)\\nactually does happen.\\nTake a uniform distribution from 0 to 1, as shown in the top left subplot of Fig.~\\\\ref{fig:CLT}. It is flat. Add two such numbers and the distribution is triangular, between 0 and 2, as shown in the top right.\\nWith 3 numbers, at the bottom left, it gets curved. With 10 numbers, at the bottom right, it looks pretty Gaussian. The proof follows. \\n\\\\begin{proof}\\nFirst, introduce the characteristic function $\\\\langle e^{i k x} \\\\rangle = \\\\int e^{i k x } P(x) \\\\, dx = \\\\tilde P(k)$.\\nThis can usefully be thought of as an expectation value and as a Fourier transform, FT.\\nExpand the exponential as a series\\n$\\\\langle e^{i k x} \\\\rangle = \\\\langle 1+ikx+{(ikx)^2 \\\\over 2!}+{(ikx)^3 \\\\over 3!}\\\\dots \\\\rangle = 1 + ik \\\\langle x \\\\rangle +(ik)^2{\\\\langle x^2 \\\\rangle \\\\over 2!} + (ik^3) {\\\\langle x^3 \\\\rangle \\\\over 3!} \\\\dots$. \\nTake the logarithm and use the expansion $\\\\ln(1+z)=z-{z^2 \\\\over 2 } + {z^3 \\\\over 3} \\\\dots$\\nThis gives a power series in $(ik)$, where the coefficient ${\\\\kappa_r \\\\over r!}$ of $(ik)^r$ is made up of expectation values of $x$ of total power $r$\\n$\\\\kappa_1= \\\\langle x \\\\rangle, \\\\kappa_2= \\\\langle x^2 \\\\rangle - \\\\langle x \\\\rangle ^2 =, \\\\kappa_3= \\\\langle x^3 \\\\rangle -3 \\\\langle x^2 \\\\rangle \\\\langle x \\\\rangle +2 \\\\langle x \\\\rangle^3 \\\\dots$ \\nThese are called the semi-invariant cumulants of Thi\\\\`ele . Under a change of scale $\\\\alpha$, $\\\\kappa_r \\\\to \\\\alpha^r \\\\kappa_r$. Under a change in location only $\\\\kappa_1$ changes.\\nIf $X$ is the sum of i.i.d. random variables, $x_1+x_2+x_3...$, then $P(X)$ is the convolution of $P(x)$ with itself $N$ times.\\nThe FT of a convolution is the product of the individual FTs,\\nthe logarithm of a product is the sum of the logarithms,\\nso $P(X)$ has cumulants $K_r=N \\\\kappa_r$.\\nTo make graphs commensurate, you need to scale the $X$ axis by the\\nstandard deviation, which grows like $\\\\sqrt{N}$. The cumulants of the scaled graph are $K'_r = N^{1-r/2} \\\\kappa_r$. \\nAs $N \\\\to \\\\infty$, these vanish for $r>2$, leaving a quadratic.\\nIf the log is a quadratic, the exponential is a Gaussian. So $\\\\tilde P(X)$ is Gaussian.\\nAnd finally, the inverse FT of a Gaussian is also a Gaussian.\\n\\\\end{proof} \\nEven if the distributions are not identical, the CLT tends to apply, unless one (or two) dominates.\\nMost `errors' fit this, being compounded of many different sources.\\n\"}},\n",
       "       {'entity_name': 'receiver operating characteristic roc plot', 'entity_type': 'analysis_technique', 'description': 'A graphical representation used to evaluate the performance of a binary classifier system by plotting the true positive rate against the false positive rate at various threshold settings.', 'relevant_passages': {\"\\\\section{Hypothesis testing}\\n`Hypothesis testing' is another piece of statistical technical jargon. \\nIt just means `making choices'---in a logical way---on the basis of statistical information. \\n\\\\begin{itemize}\\n\\\\item\\nIs some track a pion or a kaon?\\n\\\\item Is this event signal or background?\\n\\\\item Is the detector performance degrading with time?\\n\\\\item Do the data agree with the Standard Model prediction or not?\\n\\\\end{itemize}\\nTo establish some terms: you have a {\\\\it hypothesis} (the track is a pion, the event is signal,\\nthe detector is stable, the Standard Model is fine $\\\\dots$). and an alternative hypothesis (kaon, background, changing, new physics needed $\\\\dots$) Your hypothesis is usually {\\\\it simple} i.e. completely specified, \\nbut the alternative is often {\\\\it composite} containing a parameter (for example, the detector decay rate) which may have any non-zero value. \\n\\\\subsection{Type I and type II errors}\\nAs an example, let's use the signal/background decision. Do you accept or reject the event (perhaps in the trigger, perhaps in your offline analysis)? To make things easy we consider the case where both hypotheses are simple, i.e. completely defined.\\nSuppose you measure some parameter $x$ which is related to what you are trying to measure.\\nIt may well be the output from a neural network or other machine learning (ML) systems. \\nThe expected distributions for $x$ under the hypothesis and the alternative, $S$ and $B$ respectively, are shown in Fig.~\\\\ref{fig:hyp}. \\nYou impose a cut as shown---you have to put one somewhere---accepting events above $x=x_{cut}$ and rejecting those below.\\nThis means losing \\na\\nfraction $\\\\alpha$ of signal. This is called a {\\\\em type I error} and $\\\\alpha$ is known as the {\\\\em significance}.\\nYou admit a fraction $\\\\beta$ of background. This is called a {\\\\em type II error} and $1-\\\\beta$ is the power.\\nYou would like to know the best place to put the cut. This graph cannot tell you! \\nThe strategy for the cut depends on three things---hypothesis testing only covers one of them.\\nThe second is the \\nprior signal to noise ratio.\\nThese plots are normalized to 1. The red curve is (probably) MUCH bigger.\\nA value of $\\\\beta$ of, say, 0.01 looks nice and small---only one in a hundred background events get through.\\nBut if your background is 10,000 times bigger than your signal (and it often is) you are still swamped.\\nThe third is the cost of making mistakes, which will be different for the two types of error.\\nYou have a trade-off between efficiency and purity: what are they worth?\\nIn a typical analysis, a type II error is more serious than a type I: losing a signal event is regrettable, but it happens. \\nIncluding background events in your selected pure sample can give a very misleading result. \\nBy contrast, \\nin medical decisions, type I errors are much worse than type II. Telling healthy patients they are sick leads to worry and perhaps further tests, but telling sick patients they are healthy means they don't get the treatment they need.\\n\\\\subsection {The Neymann-Pearson lemma}\\nIn Fig.~\\\\ref{fig:hyp} the strategy is plain---you choose $x_{cut}$ and evaluate $\\\\alpha$ and $\\\\beta$.\\nBut\\nsuppose the $S$ and $B$ curves are more complicated, as in Fig.~\\\\ref{fig:hyp1}? Or that $x$ is multidimensional?\\nNeymann and Pearson say: your acceptance region just includes regions of greatest $S(x) \\\\over B(x)$ (the ratio of likelihoods).\\nFor a given $\\\\alpha$, this gives the smallest $\\\\beta$ (`Most powerful at a given significance')\\nThe proof is simple: having done this, if you then move a small region from `accept' to `reject' it has to be replaced by an equivalent region, to balance $\\\\alpha$, which (by construction) \\nbrings more background, increasing $\\\\beta$.\\nHowever complicated, such a problem reduces to a single monotonic variable $S \\\\over B$, and you cut on that. \\n\\\\subsection{Efficiency, purity, and ROC plots}\\nROC plots are often used to show the efficacy of different selection variables.\\nYou scan over the cut value (in $x$, for Fig.~\\\\ref{fig:hyp} or in $S/B$ for a case like Fig.~\\\\ref{fig:hyp1}\\nand plot the fraction of background accepted ($\\\\beta$) against fraction of signal retained ($1-\\\\alpha$),\\nas shown in Fig.~\\\\ref{fig:ROC}. \\nFor a very loose cut all data is accepted, corresponding to a point at the top right. As the cut is tightened both signal and background fractions fall, so the point moves to the left and down, though hopefully the background loss is greater than the signal loss, so it moves more to the left than it does downwards. As the cut is increased the line moves towards the bottom left, the limit of a very tight cut where all data is rejected.\\nA diagonal line corresponds to no discrimination---the $S$ and $B$ curves are identical.\\nThe further the actual line bulges away from that diagonal, the better. \\nWhere you should put your cut depends, as pointed out earlier, also on the prior signal/background ratio and the relative costs of errors. The ROC plots do not tell you that, but they can be useful in comparing the performance of different\\ndiscriminators.\\nThe name `ROC' stands for \\n`receiver operating characteristic', for reasons that are lost in history. Actually it is good to use this meaningless acronym, otherwise they get called `efficiency-purity plots' even though they definitely do not show the purity (they cannot, as that depends on the overall signal/background ratio). Be careful, as the phrases\\n`background efficiency', `contamination', and `purity' are used ambiguously in the literature.\\n\\\\subsection{The null hypothesis}\\nAn analysis is often (but not always) investigating whether an effect is present, motivated by\\nthe hope that the results will show that it is: \\n\\\\begin{itemize}\\n\\\\item Eating broccoli makes you smart.\\n\\\\item Facebook advertising increases sales.\\n\\\\item A new drug increases patient survival rates.\\n\\\\item The data show Beyond-the-Standard-Model physics.\\n\\\\end{itemize}\\nTo reach such a conclusion you have to use your best efforts to try, and to fail, to prove the opposite: the {\\\\em Null Hypothesis} $H_0$.\\n\\\\begin{itemize}\\n\\\\item Broccoli lovers have the same or small IQ than broccoli loathers.\\n\\\\item Sales are independent of the Facebook advertising budget.\\n\\\\item The survival rates for the new treatment is the same.\\n\\\\item The Standard Model (functions or Monte-Carlo) describe the data.\\n\\\\end{itemize}\\nIf the null hypothesis is not tenable, you've proved---or at least, supported---your point. \\nThe reason for calling $\\\\alpha$ the `significance' is now clear. It is the probability that the null hypothesis will be wrongly rejected, and you'll claim an effect where there isn't any.\\nThere is a minefield of difficulties. Correlation is not causation. If broccoli eaters are more intelligent, \\nperhaps that's because it's intelligent to eat green vegetables, not that vegetables make you intelligent. \\nOne has to consider that if similar experiments are done, self-censorship will influence which results get published. \\nThis is further discussed in Section~\\\\ref{sec:discovery}.\\nThis account is perhaps unconventional in introducing the null hypothesis at such a late stage. Most treatments\\nbring it in right at the start of the description of hypothesis testing, because they assume that all decisions are of this type.\\n\\\\def \\\\xbar {\\\\overline x}\\n\\\\def \\\\xsqbar {\\\\overline {x^2}}\\n\"}},\n",
       "       {'entity_name': 'minimum variance bound', 'entity_type': 'statistics_concept', 'description': 'A theoretical lower bound on the variance of unbiased estimators, indicating that the variance cannot be lower than a certain value determined by the Fisher information.', 'relevant_passages': {\"\\\\section{Estimation}\\nWhat statisticians call `estimation',\\nphysicists would generally call `measurement'.\\nSuppose you\\nknow the probability (density) function $P(x;a)$ \\nand you \\ntake a set of data $\\\\{x_i\\\\}$. What is the best value for $a$? (Sometimes one wants to estimate a property (e.g. the mean) rather than a parameter, but \\nthis is relatively uncommon, and the methodology is the same.) \\n$x_i$ may be single values, or pairs, or higher-dimensional.\\nThe unknown\\n$a$ may be a single parameter or several. If it has more than one component, these are sometimes split into `parameters of interest' and `nuisance parameters'.\\nThe {\\\\em estimator} is defined very broadly:\\nan estimator $\\\\hat a(x_1\\\\dots x_N)$ is a function of the data that gives a value for the parameter $a$. There is no `correct' estimator, but some are better than others. A perfect estimator would be:\\n\\\\begin{itemize}\\n\\\\item\\nConsistent. $\\\\hat a(x_1 \\\\dots x_N) \\\\to a$ as $ N \\\\to \\\\infty $,\\n\\\\item\\nUnbiased: $\\\\langle \\\\hat a \\\\rangle = a $,\\n\\\\item\\nEfficient: $\\\\langle (\\\\hat a - a)^2 \\\\rangle$ is as small as possible,\\n\\\\item \\nInvariant: $\\\\hat f(a) = f(\\\\hat a)$.\\n\\\\end{itemize}\\nNo estimator is perfect---these 4 goals are incompatible. In particular the second and the fourth; if\\nan estimator $\\\\hat a$ is unbiased for $a$ then\\n$\\\\sqrt{\\\\hat a}$ is not an unbiased estimator of $\\\\sqrt a$.\\n\\\\subsection{Bias}\\nSuppose we estimate the mean by taking the obvious\\\\footnote{Note the difference between $\\\\langle x \\\\rangle$ which is an average over a PDF and $\\\\overline x$ \\nwhich denotes the average over a particular sample: both are called `the mean $x$'.} $\\\\hat \\\\mu = \\\\xbar$\\n$\\\\left< \\\\hat \\\\mu \\\\right> = \\\\left< {1 \\\\over N } \\\\sum x_i \\\\right> = {1 \\\\over N } \\\\sum \\\\mu = \\\\mu$. \\nSo there is no bias. This expectation value of this estimator of $\\\\mu$ is just $\\\\mu$ itself. By contrast suppose\\nwe estimate the variance by the apparently obvious\\n$\\\\hat V = \\\\xsqbar-\\\\xbar^2$.\\nThen $\\\\left< \\\\hat V \\\\right> = \\\\left< \\\\xsqbar \\\\right> - \\\\left< \\\\xbar^2 \\\\right>$.\\nThe first term is just $\\\\left< x^2 \\\\right>$. To make sense of the second term, note that $\\\\left< x \\\\right> = \\\\left< \\\\xbar \\\\right>$ and add and subtract $\\\\left< x \\\\right>^2$ to get\\n$\\\\left< \\\\hat V \\\\right> = \\\\left< x^2 \\\\right> - \\\\left< x \\\\right>^2 - (\\\\left< \\\\xbar^2 \\\\right>- \\\\left< \\\\xbar \\\\right>^2)$\\n$\\\\left< \\\\hat V \\\\right> =V(x)-V(\\\\xbar)=V-{V \\\\over N}={N-1 \\\\over N} V$.\\nSo the estimator is biased! $\\\\hat V$ will, on average, give too small a value.\\nThis bias, like any known bias, can be corrected for. \\nUsing $\\\\hat V = {N \\\\over N-1} (\\\\xsqbar-\\\\xbar^2)$ corrects the bias. The familiar estimator for\\nthe standard deviation follows:\\n$\\\\hat \\\\sigma=\\\\sqrt{\\\\sum_i (x_i-\\\\xbar)^2 \\\\over N-1}$. \\n(Of course this gives a biased estimate of $\\\\sigma$. But $V$ is generally more important in this context.)\\n\\\\subsection {Efficiency}\\nSomewhat surprisingly, there is a limit to the efficiency of an estimator: the\\n{\\\\em minimum variance bound} (MVB),\\nalso known as the {\\\\em Cramér-Rao bound}.\\nFor any unbiased estimator $\\\\hat a(x)$, the variance is bounded \\n\\\\begin{equation}V(\\\\hat a)\\\\geq\\n-{1 \\\\over \\\\left< {d^2 \\\\ln L \\\\over da^2}\\\\right>}\\n={1 \\\\over \\\\left<\\n\\\\left({d \\\\ln L \\\\over da }\\\\right) ^2\\n\\\\right>}\\n\\\\quad.\\n\\\\end{equation}\\n$L$ is the likelihood (as introduced in Section~\\\\ref{sec:likelihood}) of a sample of independent measurements, i.e. the\\nprobability for the whole data sample for a particular value of $a$.\\nIt is just the product of the individual probabilities:\\n$L(a;x_1,x_2,...x_N)=P(x_1;a)P(x_2;a)...P(x_N;a)$.\\nWe will write $L(a;x_1,x_2,...x_N)$ as $L(a;x)$ for simplicity.\\n\\\\begin{proof}{Proof of the MVB}\\nUnitarity requires $\\\\int P(x;a)\\\\, dx = \\\\int L(a;x) \\\\, dx =1$\\nDifferentiate wrt $a$: \\\\qquad \\\\begin{equation}\\n0=\\\\int {dL \\\\over da} \\\\, dx = \\\\int L {d \\\\ln L \\\\over da} \\\\, dx = \\\\left< {d \\\\ln L \\\\over da} \\\\right>\\n\\\\end{equation}\\nIf $\\\\hat a$ is unbiased \\n$\\\\left< \\\\hat a\\\\right> = \\\\int \\\\hat a(x) P(x;a) \\\\, dx = \\\\int \\\\hat a(x) L(a;x) \\\\, dx =a$\\nDifferentiate wrt $a$: \\\\qquad $1=\\\\int \\\\hat a(x) {dL \\\\over da} \\\\, dx = \\\\int \\\\hat a L {d \\\\ln L \\\\over da} \\\\, dx $\\nSubtract Eq.~\\\\ref{eq:one} multiplied by $a$, and get $\\\\int (\\\\hat a - a){d \\\\ln L \\\\over da} L dx =1$\\nInvoke the Schwarz inequality $\\\\int u^2 \\\\, dx \\\\int v^2 \\\\, dx \\\\geq \\\\left( \\\\int u v \\\\, dx \\\\right)^2 $ with $u\\\\equiv (\\\\hat a - a) \\\\sqrt L, v\\\\equiv {d \\\\ln L \\\\over da} \\\\sqrt L$\\nHence $\\\\int (\\\\hat a - a)^2 L \\\\, dx \\\\int \\\\left( {d \\\\ln L \\\\over da}\\\\right)^2 L \\\\, dx \\\\geq 1$\\n\\\\begin{equation} \\n\\\\left< (\\\\hat a - a)^2 \\\\right> \\\\geq 1/\\\\left<\\\\left( {d ln L \\\\over da }\\\\right)^2 \\\\right>\\n\\\\end{equation}\\n\\\\end{proof}\\nDifferentiating Eq.~\\\\ref{eq:one} again gives\\n${ d \\\\over da} \\\\int L { d \\\\ln L \\\\over da} \\\\, dx = \\\\int {d L \\\\over da} \\\\, {d \\\\ln L \\\\over da} \\\\, dx + \\\\int L {d^2 \\\\ln A \\\\over da^2} \\\\, dx\\n=\\n\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>+\\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>=0$,\\nhence \\n$\\\\left< \\\\left( {d \\\\ln L \\\\over da} \\\\right)^2\\\\right>= - \\\\left<{d^2 \\\\ln L \\\\over da^2}\\\\right>$.\\nThis is the {\\\\em Fisher information} referred to in Section~\\\\ref{sec:Jeffreys}. Note how it is intrinsically positive.\\n\\\\subsection{Maximum likelihood estimation}\\nThe {\\\\em maximum likelihood} (ML) estimator just does what it says: $a$ is adjusted to maximise the\\nlikelihood of the sample\\n(for practical reasons one actually maximises the log likelihood, which is a sum rather than a product).\\n\\\\begin{equation}\\n{\\\\rm Maximise } \\\\ln L = \\\\sum_i \\\\ln {P(x_i;a)}\\n\\\\quad,\\n\\\\end{equation}\\n\\\\begin{equation}\\n\\\\left. {d \\\\ln L \\\\over d a } \\\\right|_{\\\\hat a}=0\\n\\\\quad.\\n\\\\end{equation}\\nThe \\nML estimator is very commonly used. It is not only simple and intuitive, it has lots of nice properties.\\n\\\\begin{itemize}\\n\\\\item\\nIt is consistent.\\n\\\\item\\nIt is biased, but bias falls like $1/N$.\\n\\\\item\\nIt is efficient for the large $N$.\\n\\\\item\\nIt is invariant---doesn't matter if you reparametrize $a$. \\n\\\\end{itemize}\\nA particular maximisation problem may be solved in 3 ways, depending on the complexity\\n\\\\begin{enumerate}\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} algebraically,\\n\\\\item Solve Eq.~\\\\ref{eq:dlogL} numerically, and\\n\\\\item Solve Eq.~\\\\ref{eq:logL} numerically.\\n\\\\end{enumerate}\\n\\\\subsection{Least squares}\\n{\\\\em Least squares estimation} follows from maximum likelihood estimation.\\nIf you have \\nGaussian measurements of $y$ taken at various $x$ values, with measurement error $\\\\sigma$, and a prediction $y=f(x;a)$\\nthen the Gaussian probability\\n\\\\centerline{$P(y;x,a)={1 \\\\over \\\\sigma \\\\sqrt{2 \\\\pi}} e^{-(y-f(x,a))^2/2 \\\\sigma^2}$}\\ngives the log likelihood\\n\\\\centerline{$\\\\ln L = - \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over 2 \\\\sigma_i^2} + {\\\\rm constants}$.}\\nTo maximise $\\\\ln L$, you minimise $\\\\chi^2 = \\\\sum { \\\\left(y_i - f(x_i;a)\\\\right)^2 \\\\over \\\\sigma_i^2} $, hence the name `least squares'.\\nDifferentiating gives the {\\\\em normal equations}:\\n$\\\\sum { \\\\left(y_i - f(x_i;a)\\\\right) \\\\over \\\\sigma_i^2}f'(x_i;a) =0$.\\nIf $f(x;a)$ is linear in $a$ then these can be solved exactly. Otherwise an iterative method has to be used.\\n\\\\subsection{Straight line fits}\\nAs a particular instance of least squares estimation, suppose the function is $y=mx+c$, and assume all $\\\\sigma_i$ are the same (the extension to the general case is straightforward).\\nThe normal equations are then $\\\\sum (y_i - m x_i -c) x_i = 0$ and $\\\\sum (y_i-m x_i - c ) =0$\\\\ , for which the solution, shown in Fig.~\\\\ref{fig:slfit}, is\\n\\\\noindent $m={\\\\overline{xy} - \\\\overline x \\\\ , \\\\overline y \\\\over \\\\xsqbar - \\\\xbar^2}$\\\\ , $c=\\\\overline y - m \\\\xbar$ \\\\ .\\nStatisticians call this {\\\\em regression}. Actually there is a subtle difference, as shown in Fig.~\\\\ref{fig:regression}.\\nThe straight line fit considers well-defined $x$ values and $y$ values with measurement errors---if it were not for those\\nerrors then presumably the values would line up perfectly, with no scatter. The scatter in regression is not caused by measurement errors, but by the fact that the variables are linked only loosely. \\nThe history of regression started with Galton, who measured the heights of fathers and their (adult) sons.\\nTall parents tend to have tall children so there is a correlation. Because the height of a son depends\\nnot just on his paternal genes but on many factors (maternal genes, diet, childhood illnesses $\\\\dots$), the points\\ndo not line up exactly---and using a high accuracy laser interferometer to do the measurements, rather than a simple\\nruler, would not change anything. \\nGalton, incidentally, used this to show that although \\ntall fathers tend to have tall sons, they are not that tall. An outstandingly tall father will have (on average) quite tall children, and only tallish grandchildren. He called this \\n`Regression towards mediocrity', hence the name.\\nIt is also true that tall sons tend to have tall fathers---but not that tall---and only tallish grandfathers. Regress works in both directions!\\nThus for regression there is always an ambiguity as to whether to plot $x$ against $y$ or $y$ against $x$.\\nFor a straight line fit as we usually meet them this does not arise: one variable is precisely specified and we call that one $x$, and the one with measurement errors is $y$. \\n\\\\subsection{Fitting histograms}\\nWhen fitting a histogram the error is given by Poisson statistics for the number of events in each bin.\\nThere are \\n4 methods of approaching this problem---in order of increasing accuracy and decreasing speed. It is assumed that the bin width $W$ is narrow, so that $f(x_i,a)=\\\\int_{x_i}^{x_i+W} P(x,a)\\\\, dx$ can be approximated by \\n$f_i(x_i;a)=P(x_i;a) \\\\times W$. $W$ is almost always the same for all bins,\\nbut the rare cases of variable bin width can easily be included.\\n\\\\begin{enumerate}\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2 \\\\over n_i}$. This is the simplest but clearly breaks if $n_i=0$.\\n\\\\item Minimise $\\\\chi^2 = \\\\sum_i {(n_i-f_i)^2\\\\over f_i}$ . Minimising the Pearson $\\\\chi^2$ (which {\\\\em is}\\nvalid here) avoids the division-by-zero problem. It assumes that the Poisson distribution can be approximated by a Gaussian.\\n\\\\item Maximise $\\\\ln L = \\\\sum \\\\ln(e^{-f_i} f_i^{n_i} / n_i!) \\\\sim \\\\sum n_i \\\\ln f_i - f_i$. This, known as {\\\\em binned maximum likelihood}, remedies that assumption.\\n\\\\item Ignore bins and maximise the total likelihood. Sums run over $N_{events}$ not $N_{bins}$. So if you have large data samples this is much slower. You have to use it for sparse data, but of course in such cases the sample is small and the\\ntime penalty is irrelevant.\\n\\\\end{enumerate}\\nWhich method to use is something you have to decide on a case by case basis. \\nIf you have bins with zero entries then the first method is ruled out\\n(and removing such bins from the fit introduces bias so this should not be done).\\nOtherwise, in my experience, the improvement in adopting a more complicated method tends to be small.\\n\"}},\n",
       "       {'entity_name': 'combination of errors', 'entity_type': 'analysis_technique', 'description': 'A technique used to combine individual measurement errors to obtain the error on a derived quantity, often expressed in terms of variances and covariances.', 'relevant_passages': {\"\\\\section{Errors}\\n\\\\subsection {Combining errors}\\nHaving obtained---by whatever means---errors $\\\\sigma_x, \\\\sigma_y...$ \\nhow does one combine them to get errors on derived quantities $f(x,y...), g(x,y,...)$?\\nSuppose $f=Ax+By+C$, with $A,B$ and $C$ constant.\\nThen it is easy to show that \\n\\\\begin{align}\\nV_f&= \\\\notag\\n\\\\left< (f - \\\\left< f\\\\right>)^2\\\\right>\\\\\\\\&= \\\\notag\\n\\\\left< (Ax+By+C - \\\\left< Ax+By+C\\\\right>)^2\\\\right> \\\\\\\\\\n&= \\\\notag\\nA^2(\\\\left<x^2\\\\right> - \\\\left<x\\\\right>^2)\\n+B^2(\\\\left<y^2\\\\right> - \\\\left<y\\\\right>^2)\\n+2AB(\\\\left<xy\\\\right> - \\\\left<x\\\\right>\\\\left< y \\\\right>)\\\\\\\\\\n&=A^2 V_x + B^2 V_y + 2AB\\\\, {\\\\rm Cov}_{xy}\\n\\\\quad.\\n\\\\end{align}\\nIf $f$ is not a simple linear function of $x$ and $y$ then one can use a first order Taylor expansion to\\napproximate it about a central value $f_0(x_0,y_0)$\\n\\\\begin{equation}\\nf(x,y)\\\\approx f_0 \\n+ \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) (x-x_0)\\n+ \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) (y-y_0)\\n\\\\end{equation}\\n\\\\noindent and application of Eq.~\\\\ref{eq:COE0} gives\\n\\\\begin{equation}\\nV_f=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 V_x+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 V_y+\\n2 \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) {\\\\rm Cov}_{xy}\\n\\\\end{equation}\\n\\\\noindent writing the more familiar $\\\\sigma^2$ \\ninstead of $V$ this is equivalent to\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n+ 2 \\\\rho \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) \\\\sigma_x \\\\sigma_y\\n\\\\quad.\\n\\\\end{equation}\\nIf $x$ and $y$ are independent, which is often but not always the case, this reduces to what is often known as the\\n`combination of errors' formula\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n\\\\quad.\\n\\\\end{equation}\\nExtension to more than two variables is trivial: an extra squared term is added for each and\\nan extra covariance term for each of the variables (if any) with which it is correlated.\\nThis can be expressed in language as {\\\\it errors add in quadrature}. This is a friendly fact, as\\nthe result is smaller than you would get from arithmetic addition. If this puzzles you, it may be helpful to think \\nof this as allowing for the possibility that a positive fluctuation in one variable may be cancelled by a negative fluctuation in\\nthe other. \\nThere are a couple of special cases we need to consider. \\nIf $f$ is a simple product, $f=Axy$, then Eq.~\\\\ref{eq:COE2} gives\\n$$\\\\sigma_f^2=(Ay)^2 \\\\sigma_x^2+ (Ax)^2 \\\\sigma_y^2 \\\\ ,$$\\nwhich, dividing by $f^2$, can be written as\\n\\\\begin{equation}\\n\\\\left({ \\\\sigma_f \\\\over f }\\\\right)^2 =\\\\left( {\\\\sigma_x \\\\over x }\\\\right)^2 +\\n\\\\left({ \\\\sigma_y \\\\over y }\\\\right)^2.\\n\\\\end{equation}\\nFurthermore this also applies if $f$ is a simple quotient, \\n$f=Ax/y$ or $Ay/x$ or even $A/(xy)$.\\nThis is very elegant, but it should not be overemphasised. Equation~\\\\ref{eq:COE3}\\nis not fundamental: it only applies in certain cases (products or quotients). Equation~\\\\ref{eq:COE2} is\\nthe fundamental one, and Eq.~\\\\ref{eq:COE3} is just a special case of it.\\nFor example: if you measure the radius of a cylinder as $r=123 \\\\pm 2$ mm and the height as $h=456 \\\\pm 3$ mm\\nthen the volume $\\\\pi r^2 h$ is $\\\\pi \\\\times 123^2 \\\\times 456 = 21673295 \\\\ {\\\\rm mm}^3$ \\nwith error $\\\\sqrt{(2 \\\\pi r h)^2 \\\\times \\\\sigma_r^2 + ( \\\\pi r^2 )^2 \\\\times \\\\sigma_h^2}=719101$,\\nso one could write it as $v=(216.73 \\\\pm 0.72) \\\\times 10^5\\\\ {\\\\rm mm}^3$.\\nThe surface area $2 \\\\pi r^2 + 2\\\\pi r h$ is $ 2 \\\\pi \\\\times 123^2 + 2 \\\\pi \\\\times 123 \\\\times 456 = 447470\\\\ {\\\\rm mm}^2$\\nwith error $\\\\sqrt{(4\\\\pi r + 2 \\\\pi h)^2 \\\\sigma_r^2 + (2 \\\\pi r)^2 \\\\sigma_h^2 }= 9121 \\\\ {\\\\rm mm}^2 $---so one could write the result \\nas $a=(447.5 \\\\pm 9.1) \\\\times 10^3 \\\\ {\\\\rm mm}^2$.\\nA full error analysis has to include the treatment of the covariance terms---if only to show that they can be ignored.\\nWhy should the $x$ and $y$ in Eq.~\\\\ref{eq:COE1} be correlated? \\nFor direct measurements very often (but not always) they will not be.\\nHowever the interpretation of results is generally a multistage process. \\nFrom raw numbers of events one computes branching ratios (or cross sections...), from which one computes matrix elements (or particle masses...). Many quantities of interest to theorists are expressed as ratios of experimental numbers. \\nAnd in this interpretation there is plenty of scope for correlations to creep into the analysis.\\nFor example, an experiment might measure a cross section $\\\\sigma(pp \\\\to X) $ from a number of observed events $N$ in the decay channel $X \\\\to \\\\mu^+\\\\mu^-$. One would \\nuse a formula\\n$$\\\\sigma={N \\\\over B \\\\eta {\\\\cal L}} \\\\ ,$$\\nwhere $\\\\eta$ is the efficiency for detecting and reconstructing \\nan event, $B$ is the branching ratio for $X \\\\to \\\\mu^+\\\\mu^-$, and ${\\\\cal L}$ is the integrated luminosity.\\nThese will all have errors, and the above prescription can be applied.\\nHowever it might also use the $X \\\\to e^+e^-$ channel and then use\\n$$\\\\sigma'={N' \\\\over B' \\\\eta' {\\\\cal L}} \\\\ .$$\\nNow $\\\\sigma$ and $\\\\sigma'$ are clearly correlated; even though $N$ and $N'$ are \\nindependent, the same ${\\\\cal L}$ appears in both. If the estimate of ${\\\\cal L}$ is on the high side, that will push both $\\\\sigma$ and $\\\\sigma'$ downwards, and vice versa. \\nOn the other hand, if a second experiment did the same measurement it would have its own $N$, $\\\\eta$ and ${\\\\cal L}$, but would be correlated with the first\\nthrough using the same branching ratio (taken, presumably, from the Particle Data Group).\\nTo calculate correlations between results we need the equivalent of Eq.~\\\\ref{eq:COE0}\\n\\\\begin{align}\\n{\\\\rm Cov}_{fg} &= \\\\notag \\\\left< (f-\\\\langle f \\\\rangle )(g-\\\\langle g \\\\rangle ) \\\\right> \\\\\\\\\\n&=\\\\left({\\\\partial f \\\\over \\\\partial x} \\\\right) \\\\left( { \\\\partial g \\\\over \\\\partial x} \\\\right) \\\\sigma_x^2\\n\\\\quad,\\n\\\\end{align} \\nThis can all be combined in the general formula which encapsulates all of the ones above\\n\\\\begin{equation}\\n{\\\\bf V_f} = {\\\\bf G V_x \\\\tilde G}\\n\\\\quad,\\n\\\\end{equation}\\nwhere ${\\\\bf V_x}$ is the covariance matrix of the primary quantities (often, as pointed out earlier, this is diagonal),\\n${\\\\bf V_f}$ is the covariance matrix of secondary quantities, and \\n\\\\begin{equation}\\nG_{ij}={\\\\partial f_i \\\\over \\\\partial x_j}\\n\\\\quad.\\n\\\\end{equation}\\nThe {\\\\bf G} matrix is rectangular but need not be square. \\nThere may be more---or fewer---derived quantities than primary quantities.\\nThe matrix algebra of ${\\\\bf G}$ and its transpose ${\\\\bf \\\\tilde G}$ \\nensures that the numbers of rows and columns match for Eq.~\\\\ref{eq:COE4}.\\nTo show how this works, we go back to our earlier example of a cylinder.\\n$v$ and $a$ are correlated: if $r$ or $h$ fluctuate upwards (or downwards), that makes both volume and area larger\\n(or smaller). The matrix ${\\\\bf G}$ is\\n\\\\begin{equation}\\n{\\\\bf G}=\\\\left( \\n\\\\begin{matrix}\\n2 \\\\pi r h & \\\\pi r^2 \\\\\\\\\\n2 \\\\pi (2 r + h) &\\n2 \\\\pi r\\n\\\\end{matrix}\\n\\\\right)\\n=\\\\left( \\n\\\\begin{matrix}\\n352411 & 47529 \\\\\\\\\\n4411 & 773\\n\\\\end{matrix}\\n\\\\right)\\n\\\\quad,\\n\\\\end{equation} \\n\\\\noindent the variance matrix $V_x$ is\\n\\\\begin{equation*}\\n{\\\\bf V_x}=\\\\left( \\n\\\\begin{matrix}\\n4 & 0\\\\\\\\\\n0& 9\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\n\\\\noindent and Eq.~\\\\ref{eq:COE4} gives\\n\\\\begin{equation*}\\n{\\\\bf V_f}=\\\\left( \\n\\\\begin{matrix}\\n517.1 \\\\times 10^9 & 6.548 \\\\times 10^9\\\\\\\\\\n6.548 \\\\times 10^9 & 83.20 \\\\times 10^6\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\nfrom which one obtains, as before,\\n$\\\\sigma_v= 719101, \\\\sigma_a=9121$ but also $\\\\rho=0.9983$.\\nThis can be used to provide a useful example of why correlation matters. Suppose\\nyou want to know the volume to surface ratio, $z=v/a$, of this cylinder. \\nDivision gives $z=21673295/447470=48.4352$ mm.\\nIf we just use Eq.~\\\\ref{eq:COE2} for the error, this gives $\\\\sigma_z=1.89$ mm. \\nIncluding the correlation term, as in Eq.~\\\\ref{eq:COE2a}, reduces this to\\n$0.62$ mm---three times smaller. It makes a big difference.\\nWe can also check that this is correct, because the ration ${v \\\\over a}$ can be written as\\n$\\\\pi r^2 h \\\\over 2 \\\\pi r^2 + 2 \\\\pi r h$, and applying the uncorrelated errors of the original $r$ and $h$ to this also gives\\nan error of $0.62$ mm.\\nAs a second, hopefully helpful, example we consider a simple straight line fit, $y=mx+c$.\\nAssuming that all the $N$ $y$ values are measured with the same error $\\\\sigma$,\\nleast squares estimation gives the well known results\\n\\\\begin{equation}\\nm={\\\\overline{xy} - \\\\overline x \\\\, \\\\overline y \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\qquad\\nc={\\\\overline{y}\\\\, \\\\overline{x^2} - \\\\overline {xy} \\\\, \\\\overline x \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\quad.\\n\\\\end{equation}\\nFor simplicity we write $D=1/(\\\\overline{x^2}-\\\\overline x^2)$. The differentials are\\n\\\\begin{equation*}\\n{\\\\partial m \\\\over \\\\partial y_i}={D \\\\over N} (x_i-\\\\overline x) \\\\qquad\\n{\\\\partial c \\\\over \\\\partial y_i}={D \\\\over N} (\\\\overline{x^2}- x_i\\\\overline x)\\n\\\\quad,\\n\\\\end{equation*}\\n\\\\noindent from which, remembering that the $y$ values are uncorrelated,\\n\\\\begin{align*}\\nV_m=\\\\sigma^2\\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)^2=\\\\sigma^2 {D \\\\over N}\\n\\\\\\\\\\nV_c=\\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (\\\\overline{x^2}- x_i\\\\overline x)^2 =\\\\sigma^2 \\\\overline {x^2} {D \\\\over N}\\\\\\\\\\n{\\\\rm Cov}_{mc}= \\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)(\\\\overline{x^2}-x_i \\\\overline x)=-\\\\sigma^2 \\\\overline x {D \\\\over N}\\n\\\\end{align*}\\n\\\\noindent from which the correlation between $m$ and $c$ is just $\\\\rho=- \\\\overline x / \\\\sqrt{\\\\overline{x^2}}$.\\nThis makes sense. Imagine you're fitting a straight line through a set of points with a range of positive $x$ values (so $\\\\overline x$ is positive). If the rightmost point happened to be a bit higher, that would push the slope $m$ up and the intercept $c$ down. Likewise if the leftmost point happened to be too high that would push the slope down and the intercept up. There is a negative correlation between the two fitted quantities.\\nDoes it matter? Sometimes. Not if you're just interested in the slope---or the constant. But suppose you intend to use them to find the expected value of $y$ at some extrapolated $x$. Equation~\\\\ref{eq:COE2a} gives\\n\\\\begin{equation*}\\ny=m x + c \\\\pm \\\\sqrt {x^2 \\\\sigma_m^2 + \\\\sigma_c^2 + 2 x \\\\rho \\\\sigma_m \\\\sigma_c}\\n\\\\end{equation*}\\nand if, for a typical case where $\\\\overline x$ is positive so $\\\\rho$ is negative, you leave out the correlation term you will overestimate your error.\\nThis is an educational example because this correlation can be avoided. Shifting to a co-ordinate system in which\\n$\\\\overline x$ is zero ensures that the quantities are uncorrelated. This is \\nequivalent to rewriting the well-known $y=mx+c$ formula as $y=m(x-\\\\overline x)+c'$, where\\n$m$ is the same as before and $c'=c+m \\\\overline x$. $m$ and $c'$ are now uncorrelated, and \\nerror calculations involving them become a lot simpler. \\n\"}},\n",
       "       {'entity_name': 'taylor expansion', 'entity_type': 'analysis_technique', 'description': 'A mathematical method used to approximate a function near a central value by using its derivatives, allowing for the estimation of errors in derived quantities.', 'relevant_passages': {\"\\\\section{Errors}\\n\\\\subsection {Combining errors}\\nHaving obtained---by whatever means---errors $\\\\sigma_x, \\\\sigma_y...$ \\nhow does one combine them to get errors on derived quantities $f(x,y...), g(x,y,...)$?\\nSuppose $f=Ax+By+C$, with $A,B$ and $C$ constant.\\nThen it is easy to show that \\n\\\\begin{align}\\nV_f&= \\\\notag\\n\\\\left< (f - \\\\left< f\\\\right>)^2\\\\right>\\\\\\\\&= \\\\notag\\n\\\\left< (Ax+By+C - \\\\left< Ax+By+C\\\\right>)^2\\\\right> \\\\\\\\\\n&= \\\\notag\\nA^2(\\\\left<x^2\\\\right> - \\\\left<x\\\\right>^2)\\n+B^2(\\\\left<y^2\\\\right> - \\\\left<y\\\\right>^2)\\n+2AB(\\\\left<xy\\\\right> - \\\\left<x\\\\right>\\\\left< y \\\\right>)\\\\\\\\\\n&=A^2 V_x + B^2 V_y + 2AB\\\\, {\\\\rm Cov}_{xy}\\n\\\\quad.\\n\\\\end{align}\\nIf $f$ is not a simple linear function of $x$ and $y$ then one can use a first order Taylor expansion to\\napproximate it about a central value $f_0(x_0,y_0)$\\n\\\\begin{equation}\\nf(x,y)\\\\approx f_0 \\n+ \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) (x-x_0)\\n+ \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) (y-y_0)\\n\\\\end{equation}\\n\\\\noindent and application of Eq.~\\\\ref{eq:COE0} gives\\n\\\\begin{equation}\\nV_f=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 V_x+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 V_y+\\n2 \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) {\\\\rm Cov}_{xy}\\n\\\\end{equation}\\n\\\\noindent writing the more familiar $\\\\sigma^2$ \\ninstead of $V$ this is equivalent to\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n+ 2 \\\\rho \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) \\\\sigma_x \\\\sigma_y\\n\\\\quad.\\n\\\\end{equation}\\nIf $x$ and $y$ are independent, which is often but not always the case, this reduces to what is often known as the\\n`combination of errors' formula\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n\\\\quad.\\n\\\\end{equation}\\nExtension to more than two variables is trivial: an extra squared term is added for each and\\nan extra covariance term for each of the variables (if any) with which it is correlated.\\nThis can be expressed in language as {\\\\it errors add in quadrature}. This is a friendly fact, as\\nthe result is smaller than you would get from arithmetic addition. If this puzzles you, it may be helpful to think \\nof this as allowing for the possibility that a positive fluctuation in one variable may be cancelled by a negative fluctuation in\\nthe other. \\nThere are a couple of special cases we need to consider. \\nIf $f$ is a simple product, $f=Axy$, then Eq.~\\\\ref{eq:COE2} gives\\n$$\\\\sigma_f^2=(Ay)^2 \\\\sigma_x^2+ (Ax)^2 \\\\sigma_y^2 \\\\ ,$$\\nwhich, dividing by $f^2$, can be written as\\n\\\\begin{equation}\\n\\\\left({ \\\\sigma_f \\\\over f }\\\\right)^2 =\\\\left( {\\\\sigma_x \\\\over x }\\\\right)^2 +\\n\\\\left({ \\\\sigma_y \\\\over y }\\\\right)^2.\\n\\\\end{equation}\\nFurthermore this also applies if $f$ is a simple quotient, \\n$f=Ax/y$ or $Ay/x$ or even $A/(xy)$.\\nThis is very elegant, but it should not be overemphasised. Equation~\\\\ref{eq:COE3}\\nis not fundamental: it only applies in certain cases (products or quotients). Equation~\\\\ref{eq:COE2} is\\nthe fundamental one, and Eq.~\\\\ref{eq:COE3} is just a special case of it.\\nFor example: if you measure the radius of a cylinder as $r=123 \\\\pm 2$ mm and the height as $h=456 \\\\pm 3$ mm\\nthen the volume $\\\\pi r^2 h$ is $\\\\pi \\\\times 123^2 \\\\times 456 = 21673295 \\\\ {\\\\rm mm}^3$ \\nwith error $\\\\sqrt{(2 \\\\pi r h)^2 \\\\times \\\\sigma_r^2 + ( \\\\pi r^2 )^2 \\\\times \\\\sigma_h^2}=719101$,\\nso one could write it as $v=(216.73 \\\\pm 0.72) \\\\times 10^5\\\\ {\\\\rm mm}^3$.\\nThe surface area $2 \\\\pi r^2 + 2\\\\pi r h$ is $ 2 \\\\pi \\\\times 123^2 + 2 \\\\pi \\\\times 123 \\\\times 456 = 447470\\\\ {\\\\rm mm}^2$\\nwith error $\\\\sqrt{(4\\\\pi r + 2 \\\\pi h)^2 \\\\sigma_r^2 + (2 \\\\pi r)^2 \\\\sigma_h^2 }= 9121 \\\\ {\\\\rm mm}^2 $---so one could write the result \\nas $a=(447.5 \\\\pm 9.1) \\\\times 10^3 \\\\ {\\\\rm mm}^2$.\\nA full error analysis has to include the treatment of the covariance terms---if only to show that they can be ignored.\\nWhy should the $x$ and $y$ in Eq.~\\\\ref{eq:COE1} be correlated? \\nFor direct measurements very often (but not always) they will not be.\\nHowever the interpretation of results is generally a multistage process. \\nFrom raw numbers of events one computes branching ratios (or cross sections...), from which one computes matrix elements (or particle masses...). Many quantities of interest to theorists are expressed as ratios of experimental numbers. \\nAnd in this interpretation there is plenty of scope for correlations to creep into the analysis.\\nFor example, an experiment might measure a cross section $\\\\sigma(pp \\\\to X) $ from a number of observed events $N$ in the decay channel $X \\\\to \\\\mu^+\\\\mu^-$. One would \\nuse a formula\\n$$\\\\sigma={N \\\\over B \\\\eta {\\\\cal L}} \\\\ ,$$\\nwhere $\\\\eta$ is the efficiency for detecting and reconstructing \\nan event, $B$ is the branching ratio for $X \\\\to \\\\mu^+\\\\mu^-$, and ${\\\\cal L}$ is the integrated luminosity.\\nThese will all have errors, and the above prescription can be applied.\\nHowever it might also use the $X \\\\to e^+e^-$ channel and then use\\n$$\\\\sigma'={N' \\\\over B' \\\\eta' {\\\\cal L}} \\\\ .$$\\nNow $\\\\sigma$ and $\\\\sigma'$ are clearly correlated; even though $N$ and $N'$ are \\nindependent, the same ${\\\\cal L}$ appears in both. If the estimate of ${\\\\cal L}$ is on the high side, that will push both $\\\\sigma$ and $\\\\sigma'$ downwards, and vice versa. \\nOn the other hand, if a second experiment did the same measurement it would have its own $N$, $\\\\eta$ and ${\\\\cal L}$, but would be correlated with the first\\nthrough using the same branching ratio (taken, presumably, from the Particle Data Group).\\nTo calculate correlations between results we need the equivalent of Eq.~\\\\ref{eq:COE0}\\n\\\\begin{align}\\n{\\\\rm Cov}_{fg} &= \\\\notag \\\\left< (f-\\\\langle f \\\\rangle )(g-\\\\langle g \\\\rangle ) \\\\right> \\\\\\\\\\n&=\\\\left({\\\\partial f \\\\over \\\\partial x} \\\\right) \\\\left( { \\\\partial g \\\\over \\\\partial x} \\\\right) \\\\sigma_x^2\\n\\\\quad,\\n\\\\end{align} \\nThis can all be combined in the general formula which encapsulates all of the ones above\\n\\\\begin{equation}\\n{\\\\bf V_f} = {\\\\bf G V_x \\\\tilde G}\\n\\\\quad,\\n\\\\end{equation}\\nwhere ${\\\\bf V_x}$ is the covariance matrix of the primary quantities (often, as pointed out earlier, this is diagonal),\\n${\\\\bf V_f}$ is the covariance matrix of secondary quantities, and \\n\\\\begin{equation}\\nG_{ij}={\\\\partial f_i \\\\over \\\\partial x_j}\\n\\\\quad.\\n\\\\end{equation}\\nThe {\\\\bf G} matrix is rectangular but need not be square. \\nThere may be more---or fewer---derived quantities than primary quantities.\\nThe matrix algebra of ${\\\\bf G}$ and its transpose ${\\\\bf \\\\tilde G}$ \\nensures that the numbers of rows and columns match for Eq.~\\\\ref{eq:COE4}.\\nTo show how this works, we go back to our earlier example of a cylinder.\\n$v$ and $a$ are correlated: if $r$ or $h$ fluctuate upwards (or downwards), that makes both volume and area larger\\n(or smaller). The matrix ${\\\\bf G}$ is\\n\\\\begin{equation}\\n{\\\\bf G}=\\\\left( \\n\\\\begin{matrix}\\n2 \\\\pi r h & \\\\pi r^2 \\\\\\\\\\n2 \\\\pi (2 r + h) &\\n2 \\\\pi r\\n\\\\end{matrix}\\n\\\\right)\\n=\\\\left( \\n\\\\begin{matrix}\\n352411 & 47529 \\\\\\\\\\n4411 & 773\\n\\\\end{matrix}\\n\\\\right)\\n\\\\quad,\\n\\\\end{equation} \\n\\\\noindent the variance matrix $V_x$ is\\n\\\\begin{equation*}\\n{\\\\bf V_x}=\\\\left( \\n\\\\begin{matrix}\\n4 & 0\\\\\\\\\\n0& 9\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\n\\\\noindent and Eq.~\\\\ref{eq:COE4} gives\\n\\\\begin{equation*}\\n{\\\\bf V_f}=\\\\left( \\n\\\\begin{matrix}\\n517.1 \\\\times 10^9 & 6.548 \\\\times 10^9\\\\\\\\\\n6.548 \\\\times 10^9 & 83.20 \\\\times 10^6\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\nfrom which one obtains, as before,\\n$\\\\sigma_v= 719101, \\\\sigma_a=9121$ but also $\\\\rho=0.9983$.\\nThis can be used to provide a useful example of why correlation matters. Suppose\\nyou want to know the volume to surface ratio, $z=v/a$, of this cylinder. \\nDivision gives $z=21673295/447470=48.4352$ mm.\\nIf we just use Eq.~\\\\ref{eq:COE2} for the error, this gives $\\\\sigma_z=1.89$ mm. \\nIncluding the correlation term, as in Eq.~\\\\ref{eq:COE2a}, reduces this to\\n$0.62$ mm---three times smaller. It makes a big difference.\\nWe can also check that this is correct, because the ration ${v \\\\over a}$ can be written as\\n$\\\\pi r^2 h \\\\over 2 \\\\pi r^2 + 2 \\\\pi r h$, and applying the uncorrelated errors of the original $r$ and $h$ to this also gives\\nan error of $0.62$ mm.\\nAs a second, hopefully helpful, example we consider a simple straight line fit, $y=mx+c$.\\nAssuming that all the $N$ $y$ values are measured with the same error $\\\\sigma$,\\nleast squares estimation gives the well known results\\n\\\\begin{equation}\\nm={\\\\overline{xy} - \\\\overline x \\\\, \\\\overline y \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\qquad\\nc={\\\\overline{y}\\\\, \\\\overline{x^2} - \\\\overline {xy} \\\\, \\\\overline x \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\quad.\\n\\\\end{equation}\\nFor simplicity we write $D=1/(\\\\overline{x^2}-\\\\overline x^2)$. The differentials are\\n\\\\begin{equation*}\\n{\\\\partial m \\\\over \\\\partial y_i}={D \\\\over N} (x_i-\\\\overline x) \\\\qquad\\n{\\\\partial c \\\\over \\\\partial y_i}={D \\\\over N} (\\\\overline{x^2}- x_i\\\\overline x)\\n\\\\quad,\\n\\\\end{equation*}\\n\\\\noindent from which, remembering that the $y$ values are uncorrelated,\\n\\\\begin{align*}\\nV_m=\\\\sigma^2\\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)^2=\\\\sigma^2 {D \\\\over N}\\n\\\\\\\\\\nV_c=\\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (\\\\overline{x^2}- x_i\\\\overline x)^2 =\\\\sigma^2 \\\\overline {x^2} {D \\\\over N}\\\\\\\\\\n{\\\\rm Cov}_{mc}= \\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)(\\\\overline{x^2}-x_i \\\\overline x)=-\\\\sigma^2 \\\\overline x {D \\\\over N}\\n\\\\end{align*}\\n\\\\noindent from which the correlation between $m$ and $c$ is just $\\\\rho=- \\\\overline x / \\\\sqrt{\\\\overline{x^2}}$.\\nThis makes sense. Imagine you're fitting a straight line through a set of points with a range of positive $x$ values (so $\\\\overline x$ is positive). If the rightmost point happened to be a bit higher, that would push the slope $m$ up and the intercept $c$ down. Likewise if the leftmost point happened to be too high that would push the slope down and the intercept up. There is a negative correlation between the two fitted quantities.\\nDoes it matter? Sometimes. Not if you're just interested in the slope---or the constant. But suppose you intend to use them to find the expected value of $y$ at some extrapolated $x$. Equation~\\\\ref{eq:COE2a} gives\\n\\\\begin{equation*}\\ny=m x + c \\\\pm \\\\sqrt {x^2 \\\\sigma_m^2 + \\\\sigma_c^2 + 2 x \\\\rho \\\\sigma_m \\\\sigma_c}\\n\\\\end{equation*}\\nand if, for a typical case where $\\\\overline x$ is positive so $\\\\rho$ is negative, you leave out the correlation term you will overestimate your error.\\nThis is an educational example because this correlation can be avoided. Shifting to a co-ordinate system in which\\n$\\\\overline x$ is zero ensures that the quantities are uncorrelated. This is \\nequivalent to rewriting the well-known $y=mx+c$ formula as $y=m(x-\\\\overline x)+c'$, where\\n$m$ is the same as before and $c'=c+m \\\\overline x$. $m$ and $c'$ are now uncorrelated, and \\nerror calculations involving them become a lot simpler. \\n\"}},\n",
       "       {'entity_name': 'errors add in quadrature', 'entity_type': 'statistics_concept', 'description': 'A principle stating that when combining independent errors, the total error is obtained by taking the square root of the sum of the squares of the individual errors.', 'relevant_passages': {\"\\\\section{Errors}\\n\\\\subsection {Combining errors}\\nHaving obtained---by whatever means---errors $\\\\sigma_x, \\\\sigma_y...$ \\nhow does one combine them to get errors on derived quantities $f(x,y...), g(x,y,...)$?\\nSuppose $f=Ax+By+C$, with $A,B$ and $C$ constant.\\nThen it is easy to show that \\n\\\\begin{align}\\nV_f&= \\\\notag\\n\\\\left< (f - \\\\left< f\\\\right>)^2\\\\right>\\\\\\\\&= \\\\notag\\n\\\\left< (Ax+By+C - \\\\left< Ax+By+C\\\\right>)^2\\\\right> \\\\\\\\\\n&= \\\\notag\\nA^2(\\\\left<x^2\\\\right> - \\\\left<x\\\\right>^2)\\n+B^2(\\\\left<y^2\\\\right> - \\\\left<y\\\\right>^2)\\n+2AB(\\\\left<xy\\\\right> - \\\\left<x\\\\right>\\\\left< y \\\\right>)\\\\\\\\\\n&=A^2 V_x + B^2 V_y + 2AB\\\\, {\\\\rm Cov}_{xy}\\n\\\\quad.\\n\\\\end{align}\\nIf $f$ is not a simple linear function of $x$ and $y$ then one can use a first order Taylor expansion to\\napproximate it about a central value $f_0(x_0,y_0)$\\n\\\\begin{equation}\\nf(x,y)\\\\approx f_0 \\n+ \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) (x-x_0)\\n+ \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) (y-y_0)\\n\\\\end{equation}\\n\\\\noindent and application of Eq.~\\\\ref{eq:COE0} gives\\n\\\\begin{equation}\\nV_f=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 V_x+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 V_y+\\n2 \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) {\\\\rm Cov}_{xy}\\n\\\\end{equation}\\n\\\\noindent writing the more familiar $\\\\sigma^2$ \\ninstead of $V$ this is equivalent to\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n+ 2 \\\\rho \\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right) \\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right) \\\\sigma_x \\\\sigma_y\\n\\\\quad.\\n\\\\end{equation}\\nIf $x$ and $y$ are independent, which is often but not always the case, this reduces to what is often known as the\\n`combination of errors' formula\\n\\\\begin{equation}\\n\\\\sigma_f^2=\\n\\\\left( {\\\\partial f \\\\over \\\\partial x}\\\\right)^2 \\\\sigma_x^2+\\n\\\\left( {\\\\partial f \\\\over \\\\partial y}\\\\right)^2 \\\\sigma_y^2 \\n\\\\quad.\\n\\\\end{equation}\\nExtension to more than two variables is trivial: an extra squared term is added for each and\\nan extra covariance term for each of the variables (if any) with which it is correlated.\\nThis can be expressed in language as {\\\\it errors add in quadrature}. This is a friendly fact, as\\nthe result is smaller than you would get from arithmetic addition. If this puzzles you, it may be helpful to think \\nof this as allowing for the possibility that a positive fluctuation in one variable may be cancelled by a negative fluctuation in\\nthe other. \\nThere are a couple of special cases we need to consider. \\nIf $f$ is a simple product, $f=Axy$, then Eq.~\\\\ref{eq:COE2} gives\\n$$\\\\sigma_f^2=(Ay)^2 \\\\sigma_x^2+ (Ax)^2 \\\\sigma_y^2 \\\\ ,$$\\nwhich, dividing by $f^2$, can be written as\\n\\\\begin{equation}\\n\\\\left({ \\\\sigma_f \\\\over f }\\\\right)^2 =\\\\left( {\\\\sigma_x \\\\over x }\\\\right)^2 +\\n\\\\left({ \\\\sigma_y \\\\over y }\\\\right)^2.\\n\\\\end{equation}\\nFurthermore this also applies if $f$ is a simple quotient, \\n$f=Ax/y$ or $Ay/x$ or even $A/(xy)$.\\nThis is very elegant, but it should not be overemphasised. Equation~\\\\ref{eq:COE3}\\nis not fundamental: it only applies in certain cases (products or quotients). Equation~\\\\ref{eq:COE2} is\\nthe fundamental one, and Eq.~\\\\ref{eq:COE3} is just a special case of it.\\nFor example: if you measure the radius of a cylinder as $r=123 \\\\pm 2$ mm and the height as $h=456 \\\\pm 3$ mm\\nthen the volume $\\\\pi r^2 h$ is $\\\\pi \\\\times 123^2 \\\\times 456 = 21673295 \\\\ {\\\\rm mm}^3$ \\nwith error $\\\\sqrt{(2 \\\\pi r h)^2 \\\\times \\\\sigma_r^2 + ( \\\\pi r^2 )^2 \\\\times \\\\sigma_h^2}=719101$,\\nso one could write it as $v=(216.73 \\\\pm 0.72) \\\\times 10^5\\\\ {\\\\rm mm}^3$.\\nThe surface area $2 \\\\pi r^2 + 2\\\\pi r h$ is $ 2 \\\\pi \\\\times 123^2 + 2 \\\\pi \\\\times 123 \\\\times 456 = 447470\\\\ {\\\\rm mm}^2$\\nwith error $\\\\sqrt{(4\\\\pi r + 2 \\\\pi h)^2 \\\\sigma_r^2 + (2 \\\\pi r)^2 \\\\sigma_h^2 }= 9121 \\\\ {\\\\rm mm}^2 $---so one could write the result \\nas $a=(447.5 \\\\pm 9.1) \\\\times 10^3 \\\\ {\\\\rm mm}^2$.\\nA full error analysis has to include the treatment of the covariance terms---if only to show that they can be ignored.\\nWhy should the $x$ and $y$ in Eq.~\\\\ref{eq:COE1} be correlated? \\nFor direct measurements very often (but not always) they will not be.\\nHowever the interpretation of results is generally a multistage process. \\nFrom raw numbers of events one computes branching ratios (or cross sections...), from which one computes matrix elements (or particle masses...). Many quantities of interest to theorists are expressed as ratios of experimental numbers. \\nAnd in this interpretation there is plenty of scope for correlations to creep into the analysis.\\nFor example, an experiment might measure a cross section $\\\\sigma(pp \\\\to X) $ from a number of observed events $N$ in the decay channel $X \\\\to \\\\mu^+\\\\mu^-$. One would \\nuse a formula\\n$$\\\\sigma={N \\\\over B \\\\eta {\\\\cal L}} \\\\ ,$$\\nwhere $\\\\eta$ is the efficiency for detecting and reconstructing \\nan event, $B$ is the branching ratio for $X \\\\to \\\\mu^+\\\\mu^-$, and ${\\\\cal L}$ is the integrated luminosity.\\nThese will all have errors, and the above prescription can be applied.\\nHowever it might also use the $X \\\\to e^+e^-$ channel and then use\\n$$\\\\sigma'={N' \\\\over B' \\\\eta' {\\\\cal L}} \\\\ .$$\\nNow $\\\\sigma$ and $\\\\sigma'$ are clearly correlated; even though $N$ and $N'$ are \\nindependent, the same ${\\\\cal L}$ appears in both. If the estimate of ${\\\\cal L}$ is on the high side, that will push both $\\\\sigma$ and $\\\\sigma'$ downwards, and vice versa. \\nOn the other hand, if a second experiment did the same measurement it would have its own $N$, $\\\\eta$ and ${\\\\cal L}$, but would be correlated with the first\\nthrough using the same branching ratio (taken, presumably, from the Particle Data Group).\\nTo calculate correlations between results we need the equivalent of Eq.~\\\\ref{eq:COE0}\\n\\\\begin{align}\\n{\\\\rm Cov}_{fg} &= \\\\notag \\\\left< (f-\\\\langle f \\\\rangle )(g-\\\\langle g \\\\rangle ) \\\\right> \\\\\\\\\\n&=\\\\left({\\\\partial f \\\\over \\\\partial x} \\\\right) \\\\left( { \\\\partial g \\\\over \\\\partial x} \\\\right) \\\\sigma_x^2\\n\\\\quad,\\n\\\\end{align} \\nThis can all be combined in the general formula which encapsulates all of the ones above\\n\\\\begin{equation}\\n{\\\\bf V_f} = {\\\\bf G V_x \\\\tilde G}\\n\\\\quad,\\n\\\\end{equation}\\nwhere ${\\\\bf V_x}$ is the covariance matrix of the primary quantities (often, as pointed out earlier, this is diagonal),\\n${\\\\bf V_f}$ is the covariance matrix of secondary quantities, and \\n\\\\begin{equation}\\nG_{ij}={\\\\partial f_i \\\\over \\\\partial x_j}\\n\\\\quad.\\n\\\\end{equation}\\nThe {\\\\bf G} matrix is rectangular but need not be square. \\nThere may be more---or fewer---derived quantities than primary quantities.\\nThe matrix algebra of ${\\\\bf G}$ and its transpose ${\\\\bf \\\\tilde G}$ \\nensures that the numbers of rows and columns match for Eq.~\\\\ref{eq:COE4}.\\nTo show how this works, we go back to our earlier example of a cylinder.\\n$v$ and $a$ are correlated: if $r$ or $h$ fluctuate upwards (or downwards), that makes both volume and area larger\\n(or smaller). The matrix ${\\\\bf G}$ is\\n\\\\begin{equation}\\n{\\\\bf G}=\\\\left( \\n\\\\begin{matrix}\\n2 \\\\pi r h & \\\\pi r^2 \\\\\\\\\\n2 \\\\pi (2 r + h) &\\n2 \\\\pi r\\n\\\\end{matrix}\\n\\\\right)\\n=\\\\left( \\n\\\\begin{matrix}\\n352411 & 47529 \\\\\\\\\\n4411 & 773\\n\\\\end{matrix}\\n\\\\right)\\n\\\\quad,\\n\\\\end{equation} \\n\\\\noindent the variance matrix $V_x$ is\\n\\\\begin{equation*}\\n{\\\\bf V_x}=\\\\left( \\n\\\\begin{matrix}\\n4 & 0\\\\\\\\\\n0& 9\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\n\\\\noindent and Eq.~\\\\ref{eq:COE4} gives\\n\\\\begin{equation*}\\n{\\\\bf V_f}=\\\\left( \\n\\\\begin{matrix}\\n517.1 \\\\times 10^9 & 6.548 \\\\times 10^9\\\\\\\\\\n6.548 \\\\times 10^9 & 83.20 \\\\times 10^6\\n\\\\end{matrix}\\n\\\\right)\\n\\\\end{equation*} \\nfrom which one obtains, as before,\\n$\\\\sigma_v= 719101, \\\\sigma_a=9121$ but also $\\\\rho=0.9983$.\\nThis can be used to provide a useful example of why correlation matters. Suppose\\nyou want to know the volume to surface ratio, $z=v/a$, of this cylinder. \\nDivision gives $z=21673295/447470=48.4352$ mm.\\nIf we just use Eq.~\\\\ref{eq:COE2} for the error, this gives $\\\\sigma_z=1.89$ mm. \\nIncluding the correlation term, as in Eq.~\\\\ref{eq:COE2a}, reduces this to\\n$0.62$ mm---three times smaller. It makes a big difference.\\nWe can also check that this is correct, because the ration ${v \\\\over a}$ can be written as\\n$\\\\pi r^2 h \\\\over 2 \\\\pi r^2 + 2 \\\\pi r h$, and applying the uncorrelated errors of the original $r$ and $h$ to this also gives\\nan error of $0.62$ mm.\\nAs a second, hopefully helpful, example we consider a simple straight line fit, $y=mx+c$.\\nAssuming that all the $N$ $y$ values are measured with the same error $\\\\sigma$,\\nleast squares estimation gives the well known results\\n\\\\begin{equation}\\nm={\\\\overline{xy} - \\\\overline x \\\\, \\\\overline y \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\qquad\\nc={\\\\overline{y}\\\\, \\\\overline{x^2} - \\\\overline {xy} \\\\, \\\\overline x \\\\over \\\\overline{x^2}-{\\\\overline x}^2}\\n\\\\quad.\\n\\\\end{equation}\\nFor simplicity we write $D=1/(\\\\overline{x^2}-\\\\overline x^2)$. The differentials are\\n\\\\begin{equation*}\\n{\\\\partial m \\\\over \\\\partial y_i}={D \\\\over N} (x_i-\\\\overline x) \\\\qquad\\n{\\\\partial c \\\\over \\\\partial y_i}={D \\\\over N} (\\\\overline{x^2}- x_i\\\\overline x)\\n\\\\quad,\\n\\\\end{equation*}\\n\\\\noindent from which, remembering that the $y$ values are uncorrelated,\\n\\\\begin{align*}\\nV_m=\\\\sigma^2\\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)^2=\\\\sigma^2 {D \\\\over N}\\n\\\\\\\\\\nV_c=\\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (\\\\overline{x^2}- x_i\\\\overline x)^2 =\\\\sigma^2 \\\\overline {x^2} {D \\\\over N}\\\\\\\\\\n{\\\\rm Cov}_{mc}= \\\\sigma^2 \\\\left({D \\\\over N}\\\\right)^2 \\\\sum (x_i-\\\\overline x)(\\\\overline{x^2}-x_i \\\\overline x)=-\\\\sigma^2 \\\\overline x {D \\\\over N}\\n\\\\end{align*}\\n\\\\noindent from which the correlation between $m$ and $c$ is just $\\\\rho=- \\\\overline x / \\\\sqrt{\\\\overline{x^2}}$.\\nThis makes sense. Imagine you're fitting a straight line through a set of points with a range of positive $x$ values (so $\\\\overline x$ is positive). If the rightmost point happened to be a bit higher, that would push the slope $m$ up and the intercept $c$ down. Likewise if the leftmost point happened to be too high that would push the slope down and the intercept up. There is a negative correlation between the two fitted quantities.\\nDoes it matter? Sometimes. Not if you're just interested in the slope---or the constant. But suppose you intend to use them to find the expected value of $y$ at some extrapolated $x$. Equation~\\\\ref{eq:COE2a} gives\\n\\\\begin{equation*}\\ny=m x + c \\\\pm \\\\sqrt {x^2 \\\\sigma_m^2 + \\\\sigma_c^2 + 2 x \\\\rho \\\\sigma_m \\\\sigma_c}\\n\\\\end{equation*}\\nand if, for a typical case where $\\\\overline x$ is positive so $\\\\rho$ is negative, you leave out the correlation term you will overestimate your error.\\nThis is an educational example because this correlation can be avoided. Shifting to a co-ordinate system in which\\n$\\\\overline x$ is zero ensures that the quantities are uncorrelated. This is \\nequivalent to rewriting the well-known $y=mx+c$ formula as $y=m(x-\\\\overline x)+c'$, where\\n$m$ is the same as before and $c'=c+m \\\\overline x$. $m$ and $c'$ are now uncorrelated, and \\nerror calculations involving them become a lot simpler. \\n\"}},\n",
       "       {'entity_name': 'confidence belt', 'entity_type': 'analysis_technique', 'description': \"A graphical technique used to represent confidence intervals for a parameter of interest across a range of measurements, allowing for visual assessment of the parameter's plausible values.\", 'relevant_passages': {\"\\\\section{Upper limits}\\n\\\\subsection{Confidence belts}\\nWe have shown that a simple Gaussian measurement is basically a statement about \\nconfidence regions. \\n$x=100\\\\pm 10 $ implies that [90,110] is the 68\\\\ \\nWe want to extend this to less simple scenarios. As a first step, we consider a proportional Gaussian.\\nSuppose we measure $x=100$ from Gaussian measurement with $\\\\sigma= 0.1 x$ (a 10\\\\If the true value is 90 the error is $\\\\sigma=9$ so $x=100$ is more than one standard deviation, whereas if the true value is 110 then $\\\\sigma=11$ and it is less than one standard deviation. 90 and 110 are not equidistant from 100.\\nThis is done with a technique called a confidence belt. The key point is that they are\\nare constructed horizontally and read vertically, using the following procedure (as shown in Fig.~\\\\ref{fig:cbelt}). Suppose that $a$ is the parameter of interest and $x$ is the measurement.\\n\\\\begin{enumerate}\\n\\\\item For each $a$, construct desired\\nconfidence interval \\n(here 68\\\\\\\\item The result $(x,a)$ lies inside the \\nbelt (the red lines), with 68\\\\\\\\item Measure $x$.\\n\\\\item The result $(x,a)$ lies inside the \\nbelt, with 68\\\\ \\\\item Read off the belt limits $a_+$ and $a_-$ at that $x$: in this case they are 111.1, 90.9. \\nSo we can report that $a$ lies in [90.9,111.1] with 68\\\\ \\\\item Other choices for the confidence level value and for the strategy are available.\\n\\\\end{enumerate}\\nThis can be extended to the case of a Poisson distribution, Fig.~\\\\ref{fig:confpois}. \\nThe only difference is that the horizontal axis is discrete as the number observed, $x$, is integer.\\nIn constructing the belt (horizontally) there will not in general be $x$ values available to give $\\\\sum_{x_-}^{x_+}=CL$ and we call, again, on the `at least' in the definition and allow it to be $\\\\sum_{x_-}^{x_+}\\\\ge CL$.\\nThus for a central 90\\\\ for which \\n$\\\\sum_{x=0}^{x_{lo}-1} e^{-a}{a^x \\\\over x!} \\\\leq 0.05$\\nand \\n$\\\\sum_{x=x_{hi}+1}^{\\\\infty} e^{-a}{a^x \\\\over x!} \\\\leq 0.05$.\\nFor the second sum it is easier to calculate\\n$\\\\sum_{x=0}^{x_{hi}} e^{-a}{a^x \\\\over x!} \\\\geq 0.95$\\\\ .\\nWhatever the value of $a$, the probability of the result falling in the belt is 90\\\\ \\n\"}},\n",
       "       {'entity_name': 'blind analysis techniques', 'entity_type': 'analysis_technique', 'description': 'A set of methodologies in particle physics designed to conduct data analysis without prior knowledge of the results, thereby minimizing bias. This includes strategies such as devising cuts using Monte Carlo simulations or non-signal data, and employing blind searches where data is not examined until the analysis is complete, ensuring an unbiased evaluation of potential signals.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\nSearching for physics beyond the Standard Model is one of the most important aspects of the physics program at the Large Hadron Collider (LHC). Since the start of proton-proton collisions at the LHC in 2011, the ATLAS~\\\\cite{ATLAS} and CMS~\\\\cite{CMS} Collaborations have derived stringent bounds on a range of new physics signatures, pushing the allowed mass range for many postulated new particles far into the TeV scale. While it is possible that these particles have yet to be observed because they are too heavy to be produced at the LHC, or have to small cross section to be detected with the current data size, it could also be that new particles are kinematically accessible and produced at observable rates, but our current methods of detection prevent their discovery.\\nSearches for new physics processes at particle colliders are usually performed as \\\\textit{blind searches}. Such searches proceed by defining a region of interest in the parameter space, using simulated data of the signal and the Standard Model background processes in order to enhance the data purity. The data is only looked at in the very end where it is tested for the presence of signal through a simultaneous fit of the signal and background probability distributions, hoping to extract a non-zero signal component.\\nHundreds of such searches have been performed for hundreds of different potential new particles, but thus far none have been discovered. Despite this, there are still regions of the data that have not yet been probed for the presence of a signal. This has led to an increased interest in more \\\\textit{model-agnostic} search strategies. Model-independent searches is nothing new in high energy particle physics, and strategies relying less on a signal hypothesis have been devised and utilized~\\\\cite{D0:2000vuh,H1:2008aak,H1:2004rlm,Cranmer:823591,CDF:2007iou,CDF:2007ykt,CDF:2008voc,CMS-PAS-EXO-14-016,CMS-PAS-EXO-10-021,CMS-PAS-EXO-19-008,CMS:2020zjg,ATLAS:2018zdn,ATLAS-CONF-2014-006,ATLAS-CONF-2012-107,ATLAS:2020iwa}.\\nThese mainly take advantage of Monte Carlo simulation, and use this to compare distributions in the observed data to simulation across several observables and many histogram bins. The drawback of this methodology is that one needs to rely on accurate simulation, and also that, due to the vast size of the parameter space being searched, an observation that appears statistically significant could potentially be the result of a statistical fluctuation.\\nIn the following, we discuss machine learning techniques which mitigate some of these challenges and have the potential to improve and extend model-independent searches.\\n', \"\\\\section{Making a discovery}\\nWe now turn from setting limits, to say what you did not see,\\nto the more exciting prospect of making a discovery.\\nRemembering hypothesis testing, in claiming a discovery you have to show that your data can't be explained without it.\\nThis is \\nquantified by the $p-$value: the probability of getting a result this extreme (or worse) under the null hypothesis/Standard Model. \\n(This is {\\\\it not} `The probability that the Standard Model is correct', but it seems impossible for journalists\\nto understand the difference.)\\nSome journals (particularly in psychology) refuse to publish papers giving $p-$values.\\nIf you do lots of studies, some will have low $p-$values (5\\\\The danger is that these get published, but the unsuccessful ones are binned.\\nIs $p$ like the significance $\\\\alpha$? Yes and no. The formula is the same, but $\\\\alpha$ is a property of the test, computed before you see the data.\\n$p$ is a property of the data. \\n\\\\subsection{Sigma language} \\nThe probability ($p-$value) is often\\ntranslated into Gaussian-like language: the probability of a result more than 3$\\\\sigma$ from the mean is 0.27\\\\ whether one takes the 1-tailed or 2-tailed option. Both are used.)\\nIn reporting a result with a significance of `so many $\\\\sigma$' there is no actual \\n$\\\\sigma$ involved: it is just a translation to give a better feel for the size of the probability.\\nBy convention, 3 sigma, $p= 0.0013$ is reported as `Evidence for' whereas a full \\n5 sigma\\\\\\\\ $p=0.0000003$ is required for `discovery of'.\\n\\\\subsection{The look-elsewhere effect}\\nYou may think that the requirement for 5 $\\\\sigma$ is excessively cautious.\\nIts justification comes from history---too many 3- and 4- sigma `signals' have gone away when more data was taken.\\nThis is partly explained by the `look-elsewhere effect'. How many peaks can you see in the\\ndata in Fig.~\\\\ref{fig:LEE}?\\nThe answer is that there are none. The data is in fact purely random and flat. But the human eye is very good at seeing features.\\nWith 100 bins, a $p-$value below 1\\\\ This can be factored in, to some extent, using pseudo-experiments, but this does\\nnot allow for the sheer number of plots being produced by \\nhard-working physicists looking for something. Hence the need for caution.\\nThis is not just ancient history. ATLAS and CMS recently observed a signal in the $\\\\gamma \\\\gamma$ mass around 750~GeV, with a significance of\\n$3.9 \\\\sigma$ (ATLAS) and $3.4 \\\\sigma$ (CMS), which went away when more data was taken.\\n\\\\subsection{Blind analysis}\\nIt is said\\\\footnote{This story is certainly not historically accurate, but it's still a good story (\\\\textit{quoteinvestigator.com}: \\\\url{https://quoteinvestigator.com/2014/06/22/chip-away/}).} that when Michaelangelo was asked how he created his masterpiece sculpture `David' \\nhe replied\\n`It was easy---all I did was get a block of marble and chip away everything that didn't look like David'.\\nSuch creativity may be good for sculpture, but it's bad for physics. \\nIf you take your data and devise cuts to remove all the events that don't look like the signal you want to see, then whatever is left \\nat the end will look like that signal. \\nMany/most analyses are now done `blind'. \\nCuts are devised using Monte Carlo and/or non-signal data.\\nYou only `open the box' once the cuts are fixed. Most collaborations have a formal procedure for doing this.\\nThis may seem a tedious imposition, but we have learnt the hard way that it avoids embarrassing mistakes.\\n\"}},\n",
       "       {'entity_name': 'crossvalidation', 'entity_type': 'analysis_technique', 'description': 'A statistical method used to estimate the predictive performance and skill of machine learning models by partitioning the data into training and validation subsets. This technique ensures that different portions of the data are utilized for training and testing, providing a robust assessment of model performance.', 'relevant_passages': {\"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\", \"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\"}},\n",
       "       {'entity_name': 'cost function', 'entity_type': 'statistics_concept', 'description': 'A mathematical function that measures the difference between the predicted outcomes of a model and the actual outcomes, which is minimized during the training of machine learning models.', 'relevant_passages': {\"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\"}},\n",
       "       {'entity_name': 'hyperparameter tuning', 'entity_type': 'analysis_technique', 'description': \"The process of optimizing the hyperparameters of a machine learning model, which are set before the learning process begins and can significantly affect the model's performance.\", 'relevant_passages': {\"\\\\section{Introduction}\\nThe twenty-first century has brought widespread advances in the\\nnatural and social sciences by making them data-intensive. The\\nrise in computing power and networking has allowed to amass ever\\nexpanding collections of data in the petabyte and even exabyte\\nrange~\\\\footnote{\\nFor pioneering developments in 2001-5 see e.g. the International\\nVirtual-Data Grid Laboratory for Data Intensive Science (iVDGL),\\ncombining the efforts of the Laser Interferometer Gravitationalwave\\nObservatory (LIGO), the ATLAS and CMS detectors at LHC at CERN and the\\nSloan Digital Sky Survey (SDSS)~\\\\cite{iVDGL}.}. The progress in\\nsocial media and e-commerce has only added to the flood. This in turn\\nhas accelerated the development of novel techniques needed to analyze\\nthe data and extract useful and timely information from it. The field\\nof data science was born.\\nThe traditional way to analyze, or generate simulated, data is to\\nfirst develop algorithms based on domain knowledge, then implement\\nthem in software, and use the resulting programs to analyze or\\ngenerate data. This process is labor intensive, and analyzing complex\\ndatasets with many input variables becomes increasingly difficult and\\nsometimes intractable. Artificial intelligence (AI) and the subfield\\nof machine learning (ML) attack these problems in a different way:\\ninstead of humans developing highly specialized algorithms, computers\\nlearn from data how to analyze complex data and produce the desired\\nresults. There is no need to explicitly program the computers.\\nInstead, ML algorithms use (often large amounts of) data to build\\nmodels with relatively small human intervention. These models can then\\nbe applied to predict the behavior of new, previously unseen data, to\\ndetect anomalies or to generate simulated data. While early work\\nstretches back more than fifty years, progress was slow for long\\nperiods of time. Advances in academic research paired with the needs\\nof large companies like Google, IBM, Amazon, Facebook and Netflix,\\njust to name a few, are producing a fundamental paradigm shift,\\nespecially with the recent successes of deep learning (for an\\nexcellent introduction to the topic, see e.g.~\\\\cite{DL}).\\nUsing mostly traditional analysis methods, physics has advanced\\nrapidly, establishing the Standard Model (SM) of particle physics, and\\nmore recently its cosmological homologue, $\\\\Lambda$CDM. The coming\\nyears will bring unprecedented amounts of data and complexity at the\\nLarge Hadron Collider (LHC), accelerating protons at CERN, as well as\\nat the intensity frontier and elsewhere. Extracting the underlying\\nphysics in the same way becomes more and more challenging, or simply\\nimpossible in a timely manner. That explains the recent spark of\\ninterest in ML (for excellent recent reviews and plans for the future,\\nsee e.g.~\\\\cite{Radovic:2018dip,Albertsson:2018maf,Carleo:2019ptp}).\\nThe physical sciences are in a unique position. While in many other\\nfields there are less firm theoretical foundations or models,\\nphysicists have well established methods to predict and to compare the\\nresults of experiments to theoretical calculations, as the many\\nsuccesses of the SM attest. This means that physics motivated ML\\nmethods can be developed and applied, accelerating the learning\\nprocess and making it more efficient and precise. At the same time the\\nbreath-taking advances in data science and computing technology will\\nhelp to address the coming challenges in particle physics.\\nThis review is not meant to be all-encompassing. Rather, some\\ncutting-edge applications at the energy and intensity frontiers of\\nparticle physics are selected to illustrate the many amazing ways in\\nwhich ML is applied, and to highlight both the successes and the\\nchallenges. The review is organized as follows: after an introduction\\nto ML, the applications in experimental high energy physics (HEP) are\\nreviewed in section 2, and in phenomenological and theoretical HEP in\\nsection 3. Open issues and challenges are discussed in section 4,\\nfollowed by a more general overview of how ML works or can be improved\\nin section 5, and an outlook in section 6.\\n\\\\subsection{Machine Learning Basics and Vocabulary}.\\nWith the increasing complexity of events in high energy physics,\\nthe importance of multivariate analysis for LHC has been recognized\\nbefore the start of data taking. The main motivation was to go beyond\\nthe traditional methods for event selection by applying series of\\ncuts on individual variables, and be able to use correlations and more\\nintricate patterns in the multidimensional data. A\\nworkshop~\\\\cite{caltechmva} at Caltech in 2008 was dedicated to the\\ntopic; ML techniques were practically not on the radar. What a sea\\nchange ten years later.\\nMachine learning algorithms, which are general in nature and not\\ntask-specific, are geared towards improving the measurable performance\\non some given task by training on more and more data.\\nThe data are split in training, validation and test subsets. The first\\ntwo are often combined together, as in cross-validation, where a\\ndifferent chunk of the data is used at each training step to estimate\\nthe predictive power of a model. The ultimate measure of the model\\ngeneralization ability is how it will perform on unseen test data,\\nwhich can include real or future data. To avoid the danger of\\noverfitting, in ML approximate solutions are preferred: the goal is to\\nlearn the essential features of the data, not all the quirks and\\nfluctuations of the training sample; this way models will generalize\\nbetter. Instead of an exact, ``ideal'', a ``good enough'' solution is\\nfavored, even when several runs on the same data, due to random\\neffects, generate several similar, but not identical models. In ML\\ncourses often Ockham's razor, named for the fourteenth century\\nFranciscan friar, is cited as a helpful path to generalizibility:\\n``More things should not be used than are necessary.'' Based on our\\nknowledge about physics, we can be less restrictive. As Albert\\nEinstein famously said: ``Everything should be made as simple as\\npossible, but not simpler.'' Good ML models find a balance between the\\ntwo. Once a model is trained, it can be applied on new data, the so\\ncalled inference. Usually this step is much less computationally\\nintensive, providing sizable speed-ups in processing data.\\nEarly ML applications in HEP often used decision trees: a tree like\\nmodel for decisions, starting at the root, climbing up the branches\\nand reaching the leaves, where each leaf represents a decision. For\\nclassification problems, each leaf represents our decision assigning a\\ndata item to a class (binary or multiclass problems). In HEP, the most\\nwidely used are boosted decision trees (BDT), which convert ``weak''\\nto ``strong'' learners.\\nArtificial neural networks (ANN or just NN) try to imitate in a\\nsimplified way biological brains. The neurons and synapses are\\nreplaced with connected layers of nodes (units, or sometimes even\\nsimply neurons) and edges. A node takes inputs from its connections as\\nreal numbers (a weighted sum of the connected outputs from the\\nprevious layer), and performs a non-linear transformation to form its\\noutput. Typical activation functions for this are: $sigmoid$\\n(logistic) and $tanh$ where the output is limited below $|1|$ for any\\ninput values, and the rectified linear unit $ReLU$ ($max(0,x)$ or the\\npositive part of the argument). NN have an input, an output, and one\\nor multiple (``deep learning''- DL) hidden layers. Deep NN are denoted\\nas DNN.\\nThe learning can be supervised based on pairs of inputs with known\\noutputs for training, or unsupervised, for example density estimation,\\nclustering or compression. A cost or loss function measuring the\\n``distance'' between the current and the desired outcomes is minimized\\nto train the model. Classical optimization aims to minimize the cost\\nfunction on the available (training) data, while in ML the goal is to\\ngeneralize, or minimize the cost best, on the unseen (test) data. At\\neach step the weights for all the edges can be adjusted by\\nbackpropagation based on the differentiation chain rule to reduce the\\ncost function by small amounts. This is the stochastic gradient\\ndescent (SGD). The associated learning rate is similar to the\\n$\\\\epsilon$ introduced by Cauchy~\\\\cite{Cauchy} to formalize calculus in\\nthe nineteenth century.\\nMany familiar terms have their equivalents in ML jargon: variables are\\ncalled features, iterations become epochs, labels often are called\\ntargets. To speed up convergence, minimizations are carried over data\\nbatches of limited size, and the weights adjusted, instead of\\ntraditional global solutions in one go, which are much slower.\\nMultilayer architectures can be trained by backpropagation and\\nSGD. The fears from local minima, unwanted e.g. in HEP fit\\napplications, have largely dissipated. For complex phase spaces there\\nare many saddle points which give very similar values of the cost\\nfunction, i.e similar models~\\\\cite{DL}. Instead of SGD, a popular\\noptimizer is Adam~\\\\cite{Kingma:2014vow}, which adjusts the learning\\nrates per parameters and based on recent history.\\nWhile the values of edge weights are learned during training, the so\\ncalled hyperparameters, like learning rate, model architecture\\n(e.g. number of hidden layers and nodes per layer), activation\\nfunctions, or batch size, are set before one run of the learning cycle\\nbegins. Depending on the data patterns to be learned or abstracted,\\ndifferent values of the hyperparameters will be needed for the same ML\\ntool. The hyperparameter tuning necessitates several, often many\\nlearning runs. Here is where human intervention and data scientist\\nskills are key.\\nTo keep this ML overview concise, more details about specific ML\\ntechniques will be provided throughout the text.\\n\"}},\n",
       "       {'entity_name': 'bdt score', 'entity_type': 'statistics_concept', 'description': 'A score generated by boosted decision trees that indicates the likelihood of an event being signal-like or background-like, ranging from -1 to 1.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Classification and Event Selection}\\nThe difficulty in extracting small signals from the towering LHC\\nbackgrounds has helped to introduce ML techniques for classification\\npurposes. Classification algorithms are a type of supervised learning\\nwhere the outputs are restricted to a limited set of values, or\\nclasses like signals or backgrounds. The Higgs analyses are a prime\\nexample.\\nThe discovery of the Higgs boson in 2012 by the\\nCMS~\\\\cite{Chatrchyan:2012xdj} and ATLAS~\\\\cite{Aad:2012tfa}\\ncollaborations saw the first use of boosted decision trees in such a\\nhigh stakes search for the separation of small signals (invariant mass\\npeaks) over large smoothly falling backgrounds. Since then the Higgs\\ndecays and couplings to the heavy W and Z gauge bosons, as well as the\\nheavy third generation quarks (bottom and top) and tau leptons, have\\nbeen observed by both ATLAS and CMS, and are consistent with the\\npredictions of the SM at the current level of precision. With the\\nHiggs boson firmly established, attention has turned to measuring its\\nproperties.\\nThe next frontier is observing Higgs decays and measuring its\\ncouplings to fermions outside the third generation. The search for\\nHiggs decays to a pair of muons with opposite charge ($\\\\mu^+\\\\mu^-$)\\noffers the best chance to establish and measure the Higgs couplings to\\nthe second generation. This is a very challenging undertaking: the SM\\nbranching fraction is expected to be $\\\\sim$0.02\\\\signal has to be extracted over a huge irreducible background\\nproducing opposite sign muon pairs: Drell-Yan, top quark or W boson\\npairs production. The Higgs signal has a narrow dimuon invariant mass\\npeak near 125~GeV, only a few GeV wide, determined by the experimental\\nmuon momentum resolution. In contrast, the background events exhibit a\\nsmoothly falling mass spectrum in the search region from 110 to\\n160~GeV.\\nThe CMS collaboration developed a method to enhance the signal\\nextraction by using a BDT classifier, as implemented in the TMVA\\nclass~\\\\cite{Hocker:2007ht} of the ROOT analysis package~\\\\cite{root},\\naugmented with automated categorization for optimal event\\nclassification. The results of the 2016 analysis, using 35.9~fb$^{-1}$\\nof collision data, were published in~\\\\cite{Sirunyan:2018hbu}. Details\\nof how the analysis was optimized for maximum signal sensitivity,\\nutilizing multivariate and machine learning techniques, are provided\\nin~\\\\cite{Bourilkov1}. Events are divided into categories based on the\\ntransverse momentum ($p_T$) of the dimuon pair (which is higher for\\nthe main gluon-gluon fusion signal (ggF) relative to the main\\nDrell-Yan background), or the presence of a high-invariant-mass dijet\\npair, characteristic of vector boson fusion (VBF) signal\\nevents. Categories are sub-divided further based on the muon\\npseudorapidity ($\\\\eta$), as central muons have better $p_T$\\nresolution, resulting in a sharper signal mass peak.\\nThe training is based on one million simulated events for the various\\nchannels, fully reconstructed in the CMS detector. Fourteen kinematic\\nvariables characterizing the dimuon system are used, their\\ndistributions are very similar between the signal and background\\nevents, making the separation that much harder. The signal sample is\\nsplit into three independent sets: one for training, a second for\\ntesting, and a third completely independent - to avoid any bias - for\\nthe final measurement. The background samples are typically split in\\n75\\\\separation is computed, yielding a BDT score between -1 and 1, where\\nevents close to 1 are more signal-like, and events close to -1 are\\nmore background-like.\\nAs a last step the auto-categorizer procedure determines 15 event\\ncategories based on $|\\\\eta|$ and BDT scores. Performing separate\\nsignal-plus-background fits to the data in all of these categories and\\ncombining the results significantly increases the search sensitivity\\nrelative to a measurement of all candidate dimuon events together. The\\nnet result of applying machine learning techniques is a 23\\\\in sensitivity equivalent to 50\\\\\\nThe ATLAS collaboration has presented an updated\\nresult~\\\\cite{ATLAS-Hmm} using all the Run2 data: 139 fb$^{-1}$. The\\nobserved upper limit on the cross section times the branching ratio of\\nthe Higgs decay to a muon pair is 1.7 times the SM prediction, so the\\nLHC experiments are closing in on the observation of this channel, but\\nwill need more data.\\nThis analysis follows a somewhat similar approach, using 14 kinematic\\nvariables and 12 categories to optimize the separation of the signal\\nfrom the backgrounds. Data events from the sidebands and simulated\\nsignal events enter the BDT training procedure. The XGBoost (eXtreme\\nGradient Boosting)~\\\\cite{Chen:2016btl} package is used. First a BDT is\\ntrained in the category of events with two or more jets to disentangle\\nthe VBF signal from the background. Three VBF categories with\\ndifferent purities are defined based on this BDT score. Then the rest\\nof the events is divided with three BDTs providing ggF scores, and\\nsplit according to jet multiplicities with zero, one or two jets,\\ngiving nine additional categories. To maximize the sensitivity to the\\nHiggs to muons decays the boundaries are adjusted by BDT scores, and\\nin each of the twelve BDT categories a fit to the invariant mass\\nspectrum from 110-–160 GeV is performed to extract the signal.\\nThis analysis is able to achieve about 50\\\\sensitivity compared to the previous ATLAS result, with roughly equal\\nparts due to the increase in integrated luminosity or from refinements\\nin the analysis techniques, where machine learning plays a key role.\\nA substantially more difficult task is the search for Higgs decays to\\na pair of charm quarks from the second generation. The CMS\\ncollaboration has performed a direct search for this Higgs decay where\\nthe Higgs is produced in association with a W or Z boson, based on an\\nintegrated luminosity of 35.9 fb$^{−1}$ collected at the CERN LHC in\\n2016~\\\\cite{CMS-Hcc}.\\nTwo types of jet topologies are analyzed: ``resolved-jet'' , where\\nboth charm quark jets from the Higgs decay are observed, and the\\n``merged-jet'' topology, where the two jets from the charm quark can\\nonly be reconstructed as a single jet. In both topologies, novel tools\\nbased on advanced machine learning techniques are deployed.\\nFor the ``resolved-jet'' topology BDT with gradient\\nboost are trained to enhance the signal-background separation. Four\\ncategories having 0, 1 or 2 leptons from the associated W or Z decays\\n(the 2 lepton case subdivided depending on the p$_T$ of the vector\\nboson) and 25 input variables are used for training. For the\\n``merged-jet'' topology a novel algorithm based on advanced ML methods\\nis deployed to identify jet substructures in order to tag the highly\\nboosted W, Z, and Higgs decays, giving sizable gains.\\nThe use of an adversarial network~\\\\cite{Goodfellow:2014upx} helps to\\nlargely decorrelate the algorithm from the mass of a jet while\\npreserving most of its discriminating power. For example, for large\\njets with p$_T\\\\ >\\\\ $~200 GeV, misidentification rates for charm quark\\npairs of 1\\\\35\\\\9\\\\\\nThe results of the two topologies help to provide an upper limit on\\nthe branching ratio of Higgs decays to charm quarks. There is still a\\nlong way to reach the sensitivity needed to observe this Higgs decay\\nwith SM strength.\\nWhile in the early Higgs papers BDTs were the prefered ML approach,\\nnewer analyses deploy deep learning and NN. The CMS collaboration has\\nmeasured~\\\\cite{CMS-Htautau} the inclusive cross section for the\\nproduction and subsequent decay of Higgs bosons to tau lepton pairs\\nwith 77.4 fb$^{−1}$ of data collected in 2016 and 2017.\\nA multi-classification approach is applied for each final state and\\nyear of data-taking, eight independent tasks in total. For each of\\nthem a fully connected feed-forward NN is trained. The architecture\\nconsists of two hidden layers with 200 nodes each, and five or eight\\nnodes in the output layer, each representing an event class\\nprediction, depending on the final state. The total cross section as\\nwell as cross sections for individual production modes and kinematic\\nregimes are obtained. This is made possible by the power of\\nclassification using deep learning.\\nAnother recent example is the measurement of associated production of\\ntop quark-antiquark pairs and Higgs bosons (ttH), with Higgs decaying\\nto b quarks, by the CMS collaboration~\\\\cite{CMS-ttH}. The analysis is\\nbased on 41.5 fb$^{−1}$ collected in 2017, and combined with the 2016\\nanalysis reaches an observed (expected) significance of 3.9 (3.5)\\nstandard deviations above the background-only hypothesis, providing\\nthe first evidence for ttH production with subsequent H$\\\\rightarrow$bb\\ndecays. Multiple classifiers are deployed, like BDTs for the dilepton\\nchannel, or feedforward NN with three hidden layers of 100 nodes each,\\nas implemented in Keras~\\\\cite{Keras}, for the single lepton channel.\\nThe application of ML techniques for Higgs analyses at the LHC is not\\na one-way street. Data from the simulations of the Higgs decays to\\ntau-lepton pairs were released by ATLAS to the ML community and formed\\nthe basis for the HiggsML challenge~\\\\cite{HiggsML}. It ran from May\\nto September 2014 on the Kaggle platform~\\\\cite{HiggsMLKaggle}, was\\nextremely popular, attracted 785 teams with 1942 participants and\\ngenerated 35772 submissions and more than a thousand forum posts.\\nProbably more surprising then than now, first prize was won by Gabor\\nMelis, a software developer and consultant from Hungary, using\\nartificial NN. A special HEP meets ML award was provided to data\\nscience graduate students Tianqi Chen and Tong He for offering the\\nboosted decision trees tool XGBoost, used by many participants. By now\\nCERN provides an open data portal~\\\\cite{CERN-ODP} to the LHC\\nexperiments to encourage fruitful collaboration between high energy\\nphysicists and data scientists.\\n{\\\\it Great progress in computer vision has come from convolutional\\nneural networks (CNN), inspired by the animal visual cortex, where\\nindividual neurons process information only from parts of the visual\\nfield. This ``divide-and-conquer'' strategy simplifies the NN\\narchitecture and helps features like translational and rotational\\ninvariance, very desirable for image recognition. Typically the\\nfirst layers of a CNN are for convolution and pooling. In a\\nconvolution the shape of an input function is modified by another\\nfunction by taking an integral of the product of the two. The\\nconvolutional filtering helps e.g. in edge detection.\\nHand-engineering the filters is replaced by learning them from the\\nimages. Pooling layers combine the inputs from several neurons (the\\nsimplest being a} 2x2 {\\\\it cluster) into one output neuron, thus\\nreducing the dimensionality. This is usually followed by fully\\nconnected layers like in standard DNN for the final classification\\nstep.}\\nTraditionally a HEP analysis proceeds to reconstructing higher level\\nobjects like tracks and energy deposits in electromagnetic and hadron\\ncalorimeters from the raw detector data, and finally arriving at\\nparticle level objects. A promising new approach is to apply deep\\nlearning algorithms directly to low-level detector data. This is\\nexplored in what is called end-to-end event\\nclassification~\\\\cite{Andrews}. The study is based on 2012 CMS\\nSimulated Open Data for the decays of the Higgs boson to a pair of\\nphotons. The gluon-gluon fusion Higgs production is the signal, while\\nirreducible quark fusion to photon pairs and a photon plus jet faking\\na second photon events form the backgrounds in this simplified\\nstudy. The events are simulated taking into account the interactions\\nin the detector materials and the detailed CMS geometry.\\nThe low level detector data is converted into images of size 170x360\\nin pseudorapidity $\\\\eta$ and azimuthal angle $\\\\varphi$ for the CMS\\nbarrel, and two images of size 100x100 for the two CMS endcaps\\nextending to $|\\\\eta|\\\\ <\\\\ 2.3$. Each channel contains three layers\\ncorresponding to electromagnetic and hadron energy and track\\ntransverse momentum. This is the electromagnetic-centric\\nsegmentation. Alternatively, a hadron-centric segmentation with size\\n280x360 in $\\\\eta - \\\\varphi$ is used. Inspired by the recent progress\\nin computer vision, a CNN of the Residual Net-type\\n(ResNet-15)~\\\\cite{He} is used. The initial results show promise, with\\nsignal efficiency and background rejection on par with more\\ntraditional approaches.\\n\"}},\n",
       "       {'entity_name': 'xgboost', 'entity_type': 'analysis_technique', 'description': 'An efficient and scalable implementation of gradient boosting framework, widely used in machine learning for classification and regression tasks, particularly in high-energy physics analyses.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Classification and Event Selection}\\nThe difficulty in extracting small signals from the towering LHC\\nbackgrounds has helped to introduce ML techniques for classification\\npurposes. Classification algorithms are a type of supervised learning\\nwhere the outputs are restricted to a limited set of values, or\\nclasses like signals or backgrounds. The Higgs analyses are a prime\\nexample.\\nThe discovery of the Higgs boson in 2012 by the\\nCMS~\\\\cite{Chatrchyan:2012xdj} and ATLAS~\\\\cite{Aad:2012tfa}\\ncollaborations saw the first use of boosted decision trees in such a\\nhigh stakes search for the separation of small signals (invariant mass\\npeaks) over large smoothly falling backgrounds. Since then the Higgs\\ndecays and couplings to the heavy W and Z gauge bosons, as well as the\\nheavy third generation quarks (bottom and top) and tau leptons, have\\nbeen observed by both ATLAS and CMS, and are consistent with the\\npredictions of the SM at the current level of precision. With the\\nHiggs boson firmly established, attention has turned to measuring its\\nproperties.\\nThe next frontier is observing Higgs decays and measuring its\\ncouplings to fermions outside the third generation. The search for\\nHiggs decays to a pair of muons with opposite charge ($\\\\mu^+\\\\mu^-$)\\noffers the best chance to establish and measure the Higgs couplings to\\nthe second generation. This is a very challenging undertaking: the SM\\nbranching fraction is expected to be $\\\\sim$0.02\\\\signal has to be extracted over a huge irreducible background\\nproducing opposite sign muon pairs: Drell-Yan, top quark or W boson\\npairs production. The Higgs signal has a narrow dimuon invariant mass\\npeak near 125~GeV, only a few GeV wide, determined by the experimental\\nmuon momentum resolution. In contrast, the background events exhibit a\\nsmoothly falling mass spectrum in the search region from 110 to\\n160~GeV.\\nThe CMS collaboration developed a method to enhance the signal\\nextraction by using a BDT classifier, as implemented in the TMVA\\nclass~\\\\cite{Hocker:2007ht} of the ROOT analysis package~\\\\cite{root},\\naugmented with automated categorization for optimal event\\nclassification. The results of the 2016 analysis, using 35.9~fb$^{-1}$\\nof collision data, were published in~\\\\cite{Sirunyan:2018hbu}. Details\\nof how the analysis was optimized for maximum signal sensitivity,\\nutilizing multivariate and machine learning techniques, are provided\\nin~\\\\cite{Bourilkov1}. Events are divided into categories based on the\\ntransverse momentum ($p_T$) of the dimuon pair (which is higher for\\nthe main gluon-gluon fusion signal (ggF) relative to the main\\nDrell-Yan background), or the presence of a high-invariant-mass dijet\\npair, characteristic of vector boson fusion (VBF) signal\\nevents. Categories are sub-divided further based on the muon\\npseudorapidity ($\\\\eta$), as central muons have better $p_T$\\nresolution, resulting in a sharper signal mass peak.\\nThe training is based on one million simulated events for the various\\nchannels, fully reconstructed in the CMS detector. Fourteen kinematic\\nvariables characterizing the dimuon system are used, their\\ndistributions are very similar between the signal and background\\nevents, making the separation that much harder. The signal sample is\\nsplit into three independent sets: one for training, a second for\\ntesting, and a third completely independent - to avoid any bias - for\\nthe final measurement. The background samples are typically split in\\n75\\\\separation is computed, yielding a BDT score between -1 and 1, where\\nevents close to 1 are more signal-like, and events close to -1 are\\nmore background-like.\\nAs a last step the auto-categorizer procedure determines 15 event\\ncategories based on $|\\\\eta|$ and BDT scores. Performing separate\\nsignal-plus-background fits to the data in all of these categories and\\ncombining the results significantly increases the search sensitivity\\nrelative to a measurement of all candidate dimuon events together. The\\nnet result of applying machine learning techniques is a 23\\\\in sensitivity equivalent to 50\\\\\\nThe ATLAS collaboration has presented an updated\\nresult~\\\\cite{ATLAS-Hmm} using all the Run2 data: 139 fb$^{-1}$. The\\nobserved upper limit on the cross section times the branching ratio of\\nthe Higgs decay to a muon pair is 1.7 times the SM prediction, so the\\nLHC experiments are closing in on the observation of this channel, but\\nwill need more data.\\nThis analysis follows a somewhat similar approach, using 14 kinematic\\nvariables and 12 categories to optimize the separation of the signal\\nfrom the backgrounds. Data events from the sidebands and simulated\\nsignal events enter the BDT training procedure. The XGBoost (eXtreme\\nGradient Boosting)~\\\\cite{Chen:2016btl} package is used. First a BDT is\\ntrained in the category of events with two or more jets to disentangle\\nthe VBF signal from the background. Three VBF categories with\\ndifferent purities are defined based on this BDT score. Then the rest\\nof the events is divided with three BDTs providing ggF scores, and\\nsplit according to jet multiplicities with zero, one or two jets,\\ngiving nine additional categories. To maximize the sensitivity to the\\nHiggs to muons decays the boundaries are adjusted by BDT scores, and\\nin each of the twelve BDT categories a fit to the invariant mass\\nspectrum from 110-–160 GeV is performed to extract the signal.\\nThis analysis is able to achieve about 50\\\\sensitivity compared to the previous ATLAS result, with roughly equal\\nparts due to the increase in integrated luminosity or from refinements\\nin the analysis techniques, where machine learning plays a key role.\\nA substantially more difficult task is the search for Higgs decays to\\na pair of charm quarks from the second generation. The CMS\\ncollaboration has performed a direct search for this Higgs decay where\\nthe Higgs is produced in association with a W or Z boson, based on an\\nintegrated luminosity of 35.9 fb$^{−1}$ collected at the CERN LHC in\\n2016~\\\\cite{CMS-Hcc}.\\nTwo types of jet topologies are analyzed: ``resolved-jet'' , where\\nboth charm quark jets from the Higgs decay are observed, and the\\n``merged-jet'' topology, where the two jets from the charm quark can\\nonly be reconstructed as a single jet. In both topologies, novel tools\\nbased on advanced machine learning techniques are deployed.\\nFor the ``resolved-jet'' topology BDT with gradient\\nboost are trained to enhance the signal-background separation. Four\\ncategories having 0, 1 or 2 leptons from the associated W or Z decays\\n(the 2 lepton case subdivided depending on the p$_T$ of the vector\\nboson) and 25 input variables are used for training. For the\\n``merged-jet'' topology a novel algorithm based on advanced ML methods\\nis deployed to identify jet substructures in order to tag the highly\\nboosted W, Z, and Higgs decays, giving sizable gains.\\nThe use of an adversarial network~\\\\cite{Goodfellow:2014upx} helps to\\nlargely decorrelate the algorithm from the mass of a jet while\\npreserving most of its discriminating power. For example, for large\\njets with p$_T\\\\ >\\\\ $~200 GeV, misidentification rates for charm quark\\npairs of 1\\\\35\\\\9\\\\\\nThe results of the two topologies help to provide an upper limit on\\nthe branching ratio of Higgs decays to charm quarks. There is still a\\nlong way to reach the sensitivity needed to observe this Higgs decay\\nwith SM strength.\\nWhile in the early Higgs papers BDTs were the prefered ML approach,\\nnewer analyses deploy deep learning and NN. The CMS collaboration has\\nmeasured~\\\\cite{CMS-Htautau} the inclusive cross section for the\\nproduction and subsequent decay of Higgs bosons to tau lepton pairs\\nwith 77.4 fb$^{−1}$ of data collected in 2016 and 2017.\\nA multi-classification approach is applied for each final state and\\nyear of data-taking, eight independent tasks in total. For each of\\nthem a fully connected feed-forward NN is trained. The architecture\\nconsists of two hidden layers with 200 nodes each, and five or eight\\nnodes in the output layer, each representing an event class\\nprediction, depending on the final state. The total cross section as\\nwell as cross sections for individual production modes and kinematic\\nregimes are obtained. This is made possible by the power of\\nclassification using deep learning.\\nAnother recent example is the measurement of associated production of\\ntop quark-antiquark pairs and Higgs bosons (ttH), with Higgs decaying\\nto b quarks, by the CMS collaboration~\\\\cite{CMS-ttH}. The analysis is\\nbased on 41.5 fb$^{−1}$ collected in 2017, and combined with the 2016\\nanalysis reaches an observed (expected) significance of 3.9 (3.5)\\nstandard deviations above the background-only hypothesis, providing\\nthe first evidence for ttH production with subsequent H$\\\\rightarrow$bb\\ndecays. Multiple classifiers are deployed, like BDTs for the dilepton\\nchannel, or feedforward NN with three hidden layers of 100 nodes each,\\nas implemented in Keras~\\\\cite{Keras}, for the single lepton channel.\\nThe application of ML techniques for Higgs analyses at the LHC is not\\na one-way street. Data from the simulations of the Higgs decays to\\ntau-lepton pairs were released by ATLAS to the ML community and formed\\nthe basis for the HiggsML challenge~\\\\cite{HiggsML}. It ran from May\\nto September 2014 on the Kaggle platform~\\\\cite{HiggsMLKaggle}, was\\nextremely popular, attracted 785 teams with 1942 participants and\\ngenerated 35772 submissions and more than a thousand forum posts.\\nProbably more surprising then than now, first prize was won by Gabor\\nMelis, a software developer and consultant from Hungary, using\\nartificial NN. A special HEP meets ML award was provided to data\\nscience graduate students Tianqi Chen and Tong He for offering the\\nboosted decision trees tool XGBoost, used by many participants. By now\\nCERN provides an open data portal~\\\\cite{CERN-ODP} to the LHC\\nexperiments to encourage fruitful collaboration between high energy\\nphysicists and data scientists.\\n{\\\\it Great progress in computer vision has come from convolutional\\nneural networks (CNN), inspired by the animal visual cortex, where\\nindividual neurons process information only from parts of the visual\\nfield. This ``divide-and-conquer'' strategy simplifies the NN\\narchitecture and helps features like translational and rotational\\ninvariance, very desirable for image recognition. Typically the\\nfirst layers of a CNN are for convolution and pooling. In a\\nconvolution the shape of an input function is modified by another\\nfunction by taking an integral of the product of the two. The\\nconvolutional filtering helps e.g. in edge detection.\\nHand-engineering the filters is replaced by learning them from the\\nimages. Pooling layers combine the inputs from several neurons (the\\nsimplest being a} 2x2 {\\\\it cluster) into one output neuron, thus\\nreducing the dimensionality. This is usually followed by fully\\nconnected layers like in standard DNN for the final classification\\nstep.}\\nTraditionally a HEP analysis proceeds to reconstructing higher level\\nobjects like tracks and energy deposits in electromagnetic and hadron\\ncalorimeters from the raw detector data, and finally arriving at\\nparticle level objects. A promising new approach is to apply deep\\nlearning algorithms directly to low-level detector data. This is\\nexplored in what is called end-to-end event\\nclassification~\\\\cite{Andrews}. The study is based on 2012 CMS\\nSimulated Open Data for the decays of the Higgs boson to a pair of\\nphotons. The gluon-gluon fusion Higgs production is the signal, while\\nirreducible quark fusion to photon pairs and a photon plus jet faking\\na second photon events form the backgrounds in this simplified\\nstudy. The events are simulated taking into account the interactions\\nin the detector materials and the detailed CMS geometry.\\nThe low level detector data is converted into images of size 170x360\\nin pseudorapidity $\\\\eta$ and azimuthal angle $\\\\varphi$ for the CMS\\nbarrel, and two images of size 100x100 for the two CMS endcaps\\nextending to $|\\\\eta|\\\\ <\\\\ 2.3$. Each channel contains three layers\\ncorresponding to electromagnetic and hadron energy and track\\ntransverse momentum. This is the electromagnetic-centric\\nsegmentation. Alternatively, a hadron-centric segmentation with size\\n280x360 in $\\\\eta - \\\\varphi$ is used. Inspired by the recent progress\\nin computer vision, a CNN of the Residual Net-type\\n(ResNet-15)~\\\\cite{He} is used. The initial results show promise, with\\nsignal efficiency and background rejection on par with more\\ntraditional approaches.\\n\"}},\n",
       "       {'entity_name': 'endtoend event classification', 'entity_type': 'analysis_technique', 'description': 'A novel approach in particle physics where deep learning algorithms are applied directly to low-level detector data for classifying events, bypassing traditional reconstruction steps.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Classification and Event Selection}\\nThe difficulty in extracting small signals from the towering LHC\\nbackgrounds has helped to introduce ML techniques for classification\\npurposes. Classification algorithms are a type of supervised learning\\nwhere the outputs are restricted to a limited set of values, or\\nclasses like signals or backgrounds. The Higgs analyses are a prime\\nexample.\\nThe discovery of the Higgs boson in 2012 by the\\nCMS~\\\\cite{Chatrchyan:2012xdj} and ATLAS~\\\\cite{Aad:2012tfa}\\ncollaborations saw the first use of boosted decision trees in such a\\nhigh stakes search for the separation of small signals (invariant mass\\npeaks) over large smoothly falling backgrounds. Since then the Higgs\\ndecays and couplings to the heavy W and Z gauge bosons, as well as the\\nheavy third generation quarks (bottom and top) and tau leptons, have\\nbeen observed by both ATLAS and CMS, and are consistent with the\\npredictions of the SM at the current level of precision. With the\\nHiggs boson firmly established, attention has turned to measuring its\\nproperties.\\nThe next frontier is observing Higgs decays and measuring its\\ncouplings to fermions outside the third generation. The search for\\nHiggs decays to a pair of muons with opposite charge ($\\\\mu^+\\\\mu^-$)\\noffers the best chance to establish and measure the Higgs couplings to\\nthe second generation. This is a very challenging undertaking: the SM\\nbranching fraction is expected to be $\\\\sim$0.02\\\\signal has to be extracted over a huge irreducible background\\nproducing opposite sign muon pairs: Drell-Yan, top quark or W boson\\npairs production. The Higgs signal has a narrow dimuon invariant mass\\npeak near 125~GeV, only a few GeV wide, determined by the experimental\\nmuon momentum resolution. In contrast, the background events exhibit a\\nsmoothly falling mass spectrum in the search region from 110 to\\n160~GeV.\\nThe CMS collaboration developed a method to enhance the signal\\nextraction by using a BDT classifier, as implemented in the TMVA\\nclass~\\\\cite{Hocker:2007ht} of the ROOT analysis package~\\\\cite{root},\\naugmented with automated categorization for optimal event\\nclassification. The results of the 2016 analysis, using 35.9~fb$^{-1}$\\nof collision data, were published in~\\\\cite{Sirunyan:2018hbu}. Details\\nof how the analysis was optimized for maximum signal sensitivity,\\nutilizing multivariate and machine learning techniques, are provided\\nin~\\\\cite{Bourilkov1}. Events are divided into categories based on the\\ntransverse momentum ($p_T$) of the dimuon pair (which is higher for\\nthe main gluon-gluon fusion signal (ggF) relative to the main\\nDrell-Yan background), or the presence of a high-invariant-mass dijet\\npair, characteristic of vector boson fusion (VBF) signal\\nevents. Categories are sub-divided further based on the muon\\npseudorapidity ($\\\\eta$), as central muons have better $p_T$\\nresolution, resulting in a sharper signal mass peak.\\nThe training is based on one million simulated events for the various\\nchannels, fully reconstructed in the CMS detector. Fourteen kinematic\\nvariables characterizing the dimuon system are used, their\\ndistributions are very similar between the signal and background\\nevents, making the separation that much harder. The signal sample is\\nsplit into three independent sets: one for training, a second for\\ntesting, and a third completely independent - to avoid any bias - for\\nthe final measurement. The background samples are typically split in\\n75\\\\separation is computed, yielding a BDT score between -1 and 1, where\\nevents close to 1 are more signal-like, and events close to -1 are\\nmore background-like.\\nAs a last step the auto-categorizer procedure determines 15 event\\ncategories based on $|\\\\eta|$ and BDT scores. Performing separate\\nsignal-plus-background fits to the data in all of these categories and\\ncombining the results significantly increases the search sensitivity\\nrelative to a measurement of all candidate dimuon events together. The\\nnet result of applying machine learning techniques is a 23\\\\in sensitivity equivalent to 50\\\\\\nThe ATLAS collaboration has presented an updated\\nresult~\\\\cite{ATLAS-Hmm} using all the Run2 data: 139 fb$^{-1}$. The\\nobserved upper limit on the cross section times the branching ratio of\\nthe Higgs decay to a muon pair is 1.7 times the SM prediction, so the\\nLHC experiments are closing in on the observation of this channel, but\\nwill need more data.\\nThis analysis follows a somewhat similar approach, using 14 kinematic\\nvariables and 12 categories to optimize the separation of the signal\\nfrom the backgrounds. Data events from the sidebands and simulated\\nsignal events enter the BDT training procedure. The XGBoost (eXtreme\\nGradient Boosting)~\\\\cite{Chen:2016btl} package is used. First a BDT is\\ntrained in the category of events with two or more jets to disentangle\\nthe VBF signal from the background. Three VBF categories with\\ndifferent purities are defined based on this BDT score. Then the rest\\nof the events is divided with three BDTs providing ggF scores, and\\nsplit according to jet multiplicities with zero, one or two jets,\\ngiving nine additional categories. To maximize the sensitivity to the\\nHiggs to muons decays the boundaries are adjusted by BDT scores, and\\nin each of the twelve BDT categories a fit to the invariant mass\\nspectrum from 110-–160 GeV is performed to extract the signal.\\nThis analysis is able to achieve about 50\\\\sensitivity compared to the previous ATLAS result, with roughly equal\\nparts due to the increase in integrated luminosity or from refinements\\nin the analysis techniques, where machine learning plays a key role.\\nA substantially more difficult task is the search for Higgs decays to\\na pair of charm quarks from the second generation. The CMS\\ncollaboration has performed a direct search for this Higgs decay where\\nthe Higgs is produced in association with a W or Z boson, based on an\\nintegrated luminosity of 35.9 fb$^{−1}$ collected at the CERN LHC in\\n2016~\\\\cite{CMS-Hcc}.\\nTwo types of jet topologies are analyzed: ``resolved-jet'' , where\\nboth charm quark jets from the Higgs decay are observed, and the\\n``merged-jet'' topology, where the two jets from the charm quark can\\nonly be reconstructed as a single jet. In both topologies, novel tools\\nbased on advanced machine learning techniques are deployed.\\nFor the ``resolved-jet'' topology BDT with gradient\\nboost are trained to enhance the signal-background separation. Four\\ncategories having 0, 1 or 2 leptons from the associated W or Z decays\\n(the 2 lepton case subdivided depending on the p$_T$ of the vector\\nboson) and 25 input variables are used for training. For the\\n``merged-jet'' topology a novel algorithm based on advanced ML methods\\nis deployed to identify jet substructures in order to tag the highly\\nboosted W, Z, and Higgs decays, giving sizable gains.\\nThe use of an adversarial network~\\\\cite{Goodfellow:2014upx} helps to\\nlargely decorrelate the algorithm from the mass of a jet while\\npreserving most of its discriminating power. For example, for large\\njets with p$_T\\\\ >\\\\ $~200 GeV, misidentification rates for charm quark\\npairs of 1\\\\35\\\\9\\\\\\nThe results of the two topologies help to provide an upper limit on\\nthe branching ratio of Higgs decays to charm quarks. There is still a\\nlong way to reach the sensitivity needed to observe this Higgs decay\\nwith SM strength.\\nWhile in the early Higgs papers BDTs were the prefered ML approach,\\nnewer analyses deploy deep learning and NN. The CMS collaboration has\\nmeasured~\\\\cite{CMS-Htautau} the inclusive cross section for the\\nproduction and subsequent decay of Higgs bosons to tau lepton pairs\\nwith 77.4 fb$^{−1}$ of data collected in 2016 and 2017.\\nA multi-classification approach is applied for each final state and\\nyear of data-taking, eight independent tasks in total. For each of\\nthem a fully connected feed-forward NN is trained. The architecture\\nconsists of two hidden layers with 200 nodes each, and five or eight\\nnodes in the output layer, each representing an event class\\nprediction, depending on the final state. The total cross section as\\nwell as cross sections for individual production modes and kinematic\\nregimes are obtained. This is made possible by the power of\\nclassification using deep learning.\\nAnother recent example is the measurement of associated production of\\ntop quark-antiquark pairs and Higgs bosons (ttH), with Higgs decaying\\nto b quarks, by the CMS collaboration~\\\\cite{CMS-ttH}. The analysis is\\nbased on 41.5 fb$^{−1}$ collected in 2017, and combined with the 2016\\nanalysis reaches an observed (expected) significance of 3.9 (3.5)\\nstandard deviations above the background-only hypothesis, providing\\nthe first evidence for ttH production with subsequent H$\\\\rightarrow$bb\\ndecays. Multiple classifiers are deployed, like BDTs for the dilepton\\nchannel, or feedforward NN with three hidden layers of 100 nodes each,\\nas implemented in Keras~\\\\cite{Keras}, for the single lepton channel.\\nThe application of ML techniques for Higgs analyses at the LHC is not\\na one-way street. Data from the simulations of the Higgs decays to\\ntau-lepton pairs were released by ATLAS to the ML community and formed\\nthe basis for the HiggsML challenge~\\\\cite{HiggsML}. It ran from May\\nto September 2014 on the Kaggle platform~\\\\cite{HiggsMLKaggle}, was\\nextremely popular, attracted 785 teams with 1942 participants and\\ngenerated 35772 submissions and more than a thousand forum posts.\\nProbably more surprising then than now, first prize was won by Gabor\\nMelis, a software developer and consultant from Hungary, using\\nartificial NN. A special HEP meets ML award was provided to data\\nscience graduate students Tianqi Chen and Tong He for offering the\\nboosted decision trees tool XGBoost, used by many participants. By now\\nCERN provides an open data portal~\\\\cite{CERN-ODP} to the LHC\\nexperiments to encourage fruitful collaboration between high energy\\nphysicists and data scientists.\\n{\\\\it Great progress in computer vision has come from convolutional\\nneural networks (CNN), inspired by the animal visual cortex, where\\nindividual neurons process information only from parts of the visual\\nfield. This ``divide-and-conquer'' strategy simplifies the NN\\narchitecture and helps features like translational and rotational\\ninvariance, very desirable for image recognition. Typically the\\nfirst layers of a CNN are for convolution and pooling. In a\\nconvolution the shape of an input function is modified by another\\nfunction by taking an integral of the product of the two. The\\nconvolutional filtering helps e.g. in edge detection.\\nHand-engineering the filters is replaced by learning them from the\\nimages. Pooling layers combine the inputs from several neurons (the\\nsimplest being a} 2x2 {\\\\it cluster) into one output neuron, thus\\nreducing the dimensionality. This is usually followed by fully\\nconnected layers like in standard DNN for the final classification\\nstep.}\\nTraditionally a HEP analysis proceeds to reconstructing higher level\\nobjects like tracks and energy deposits in electromagnetic and hadron\\ncalorimeters from the raw detector data, and finally arriving at\\nparticle level objects. A promising new approach is to apply deep\\nlearning algorithms directly to low-level detector data. This is\\nexplored in what is called end-to-end event\\nclassification~\\\\cite{Andrews}. The study is based on 2012 CMS\\nSimulated Open Data for the decays of the Higgs boson to a pair of\\nphotons. The gluon-gluon fusion Higgs production is the signal, while\\nirreducible quark fusion to photon pairs and a photon plus jet faking\\na second photon events form the backgrounds in this simplified\\nstudy. The events are simulated taking into account the interactions\\nin the detector materials and the detailed CMS geometry.\\nThe low level detector data is converted into images of size 170x360\\nin pseudorapidity $\\\\eta$ and azimuthal angle $\\\\varphi$ for the CMS\\nbarrel, and two images of size 100x100 for the two CMS endcaps\\nextending to $|\\\\eta|\\\\ <\\\\ 2.3$. Each channel contains three layers\\ncorresponding to electromagnetic and hadron energy and track\\ntransverse momentum. This is the electromagnetic-centric\\nsegmentation. Alternatively, a hadron-centric segmentation with size\\n280x360 in $\\\\eta - \\\\varphi$ is used. Inspired by the recent progress\\nin computer vision, a CNN of the Residual Net-type\\n(ResNet-15)~\\\\cite{He} is used. The initial results show promise, with\\nsignal efficiency and background rejection on par with more\\ntraditional approaches.\\n\"}},\n",
       "       {'entity_name': 'kalman filters', 'entity_type': 'analysis_technique', 'description': 'A statistical algorithm used for estimating the state of a dynamic system from a series of incomplete and noisy measurements, commonly applied in track reconstruction in particle physics.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Reconstruction}\\nRegression algorithms are another type of supervised learning,\\nproviding continuous outputs which can have any numerical value within\\na range. They can be deployed for reconstruction purposes in HEP\\ne.g. when we want to make precise determinations of continuous\\nquantities like hit positions, track momenta or jet energies.\\nAt the intensity frontier advanced detectors collect record amounts of\\nluminosity at what would be considered ``medium'' energies by today's\\nstandards. One example is the Beijing Electron Positron Collider\\n(BEPCII) running at center of mass energies 2.0--4.6 GeV. The BESIII\\nexperiment has collected record size data samples in this\\n$\\\\tau$--charm region. Advanced ML techniques have been applied for\\nmany tasks~\\\\cite{BESIII}. One of them is cluster reconstruction for\\nthe cylindrical triple-GEM inner tracker, part of the 2019 upgrade to\\nthe aging inner drift chamber. The goal is to measure the drift\\ncathode layer position of ionizing particles from the readouts of the\\nanode strips, which is the first reconstruction step. Two methods are\\navailable: weighted by electric charge average position of the anode\\nstrips (Q~method), or time measurement using the drift gap as kind of\\nmicro time projection chamber (T~method). The two methods can be\\ncombined to improve the position resolution, but this combination is\\nmade difficult by the correlations to the incident angle. Here ML\\ntechniques come to the rescue: a XGBoost regressor is developed to\\nreconstruct the initial particle positions from the Q and T\\nreadouts. Substantial improvements over the charge centroid method are\\nreported.\\nML techniques are entering in full force the ``sister'' field of\\nparticle astrophysics. One development in the field of very high\\nenergy gamma-ray astronomy is the Cherenkov Telescope Array (CTA)\\nwhich will ultimately consists of 19 telescopes in the Northern and 99\\ntelescopes in the Southern hemisphere to cover the full sky. A\\ncolossal amount of data in the multi-petabyte range per year is\\nexpected. The telescope arrays are operated as a single instrument to\\nobserve extensive air showers originating from gammas or charged\\nparticles, and aim to separate them and measure basic characteristics\\nas energy, direction and impact point of the original particle. An\\nexploratory regression study~\\\\cite{GammaLearn} in this direction uses\\nCNN with the hope to extract more information directly from the raw\\ndata and outperform traditional approaches based on human-selected\\nfeatures.\\nThe main difficulty is that conventional CNNs are developed to process\\nrectangular images with regular pixel grids. The telescope outputs\\nhere have hexagonal pixels forming hexagonal images. One, not very\\nsatisfying approach is resampling, converting the image to a standard\\none, potentially losing some information about the neighbors. This\\nanalysis takes the more difficult route of reimplementing the\\nconvolutional and pooling operations of CNNs by building matrices of\\nneighbor indices, rearranging the data accordingly and then applying\\nthe general methods for convolution, or for pooling with different\\nfunctions depending on the task ({\\\\it softmax, average, max}).\\nThe next difficulty is to combine images from several telescopes to\\nobtain stereoscopic information. Traditional DL methods\\nonly deal with single images, sequentially in time. This is solved by\\nadding a convolution block for each telescope in the array, and\\nfeeding them all to the dense fully connected part of the network. The\\nexploratory study with four telescopes shows promise in the\\nmeasurements of energy, direction and impact point for incoming\\nparticles; additional work is needed to outperform traditional methods\\nand solve technical details before applying the developed algorithm on\\nreal data.\\nTracking detectors form the core of most collider experiments, and\\nsuccessful track reconstruction is mission critical for achieving\\ntheir goals. Reconstructing tracks is a combinatorial problem,\\ni.e. finding the measurements (hits) belonging to individual particles\\nentering the detectors from an often huge set of possible\\ncombinations. With the transition to the High-Luminosity LHC (HL-LHC)\\nthe complexity of this task will increase substantially. Traditional\\napproaches like track following (inside-out or outside-in) and Kalman\\nfilters do not scale favorably to very high hit densities, and are\\ntypically custom implemented for each experiment with large amount of\\nhuman efforts.\\nThe TrackML~\\\\cite{TrackML} project has the ambition to stimulate new\\napproaches and the development of new algorithms by exposing data from\\na virtual, but realistic HL-LHC tracking detector to data science and\\ncomputer experts outside of the HEP community. Production of\\ntop-antitop quark pairs is selected for the signal events, which are\\nthen merged with 200 soft interactions (pile-up events). Fast\\nsimulation is used to generate hits in the tracker from charged\\ntracks. The magnetic field is inhomogeneous, energy loss, hadronic\\ninteractions and multiple scattering are parameterized. The silicon\\ntracker consists of three parts: innermost pixel detector, followed by\\ntwo layers of short and long silicon strips providing hermetic\\ncoverage up to $|\\\\eta|\\\\ <\\\\ 3$. For each collision, about ten thousand\\ncharged particles, originating approximately from the center of the\\ndetector, produce about ten precise hits per track in three\\ndimensions.\\nThe challenge, running on the Kaggle platform~\\\\cite{TrackMLKaggle} and\\non Codalab in 2018--2019, is split in two phases: accuracy and\\nthroughput. The first phase is scored by a specially developed metric,\\nwhich puts high priority on efficiency of finding real hits belonging\\nto a particle and low fake rates. At least 50\\\\originate from the same simulated truth particle, with hits on the\\ninnermost layers, key for good vertex resolution, and on the outermost\\nlayers, key for long lever arms and thus for good momentum resolution,\\ngetting highest weights in the overall score. A random solution will\\nget a score of zero and a perfect reconstruction of all events in the\\ntest dataset, consisting of 125 simulated events, will get a score of\\none.\\nThe challenge attracted more than 650 participants. In the accuracy\\nphase the participants provide their reconstruction of the test\\ndataset to Kaggle where it is scored. In the throughput phase the\\nparticipants provide their algorithms and software, and it is run in a\\nconsistent environment (Docker containers on two i686 processor cores\\nand 4GB of memory) to measure both accuracy and runtime, which will be\\nvery important to handle the enormous datasets expected from the\\nHL-LHC.\\nWinners~\\\\cite{TrackMLWin} of the accuracy phase are teams (with\\nscores): top-quarks(0.92219), outrunner(0.90400) and Sergey\\nGorbunov(0.89416). While training can consume lots of computer\\nresources, where machine learning really shines is the speed of\\nreconstruction once the algorithms are trained. Winners of the\\nthroughput phase are teams sgorbuno (Sergei Gorbunov), fastrack\\n(Dmitry Emelyanov) and cloudkitchen (Marcel Kunze), who were able to\\ncombine high accuracy scores with speeds well below ten seconds per\\nevent, and even below one second for the first two.\\nThe TrackML challenge shows that ML techniques like representation\\nlearning, combinatorial optimization, clustering and even time series\\nprediction can be applied to tracking. The best solutions offer a\\nsynergy between model-based and data-based approaches, combining the\\nbest of both worlds: physical track models and machine learning, with\\nsensible trade-offs between complexity and performance.\\n\"}},\n",
       "       {'entity_name': 'combinatorial optimization', 'entity_type': 'analysis_technique', 'description': 'A mathematical approach used to find an optimal object from a finite set of objects, often employed in particle tracking to determine the best association of hits to particles.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Reconstruction}\\nRegression algorithms are another type of supervised learning,\\nproviding continuous outputs which can have any numerical value within\\na range. They can be deployed for reconstruction purposes in HEP\\ne.g. when we want to make precise determinations of continuous\\nquantities like hit positions, track momenta or jet energies.\\nAt the intensity frontier advanced detectors collect record amounts of\\nluminosity at what would be considered ``medium'' energies by today's\\nstandards. One example is the Beijing Electron Positron Collider\\n(BEPCII) running at center of mass energies 2.0--4.6 GeV. The BESIII\\nexperiment has collected record size data samples in this\\n$\\\\tau$--charm region. Advanced ML techniques have been applied for\\nmany tasks~\\\\cite{BESIII}. One of them is cluster reconstruction for\\nthe cylindrical triple-GEM inner tracker, part of the 2019 upgrade to\\nthe aging inner drift chamber. The goal is to measure the drift\\ncathode layer position of ionizing particles from the readouts of the\\nanode strips, which is the first reconstruction step. Two methods are\\navailable: weighted by electric charge average position of the anode\\nstrips (Q~method), or time measurement using the drift gap as kind of\\nmicro time projection chamber (T~method). The two methods can be\\ncombined to improve the position resolution, but this combination is\\nmade difficult by the correlations to the incident angle. Here ML\\ntechniques come to the rescue: a XGBoost regressor is developed to\\nreconstruct the initial particle positions from the Q and T\\nreadouts. Substantial improvements over the charge centroid method are\\nreported.\\nML techniques are entering in full force the ``sister'' field of\\nparticle astrophysics. One development in the field of very high\\nenergy gamma-ray astronomy is the Cherenkov Telescope Array (CTA)\\nwhich will ultimately consists of 19 telescopes in the Northern and 99\\ntelescopes in the Southern hemisphere to cover the full sky. A\\ncolossal amount of data in the multi-petabyte range per year is\\nexpected. The telescope arrays are operated as a single instrument to\\nobserve extensive air showers originating from gammas or charged\\nparticles, and aim to separate them and measure basic characteristics\\nas energy, direction and impact point of the original particle. An\\nexploratory regression study~\\\\cite{GammaLearn} in this direction uses\\nCNN with the hope to extract more information directly from the raw\\ndata and outperform traditional approaches based on human-selected\\nfeatures.\\nThe main difficulty is that conventional CNNs are developed to process\\nrectangular images with regular pixel grids. The telescope outputs\\nhere have hexagonal pixels forming hexagonal images. One, not very\\nsatisfying approach is resampling, converting the image to a standard\\none, potentially losing some information about the neighbors. This\\nanalysis takes the more difficult route of reimplementing the\\nconvolutional and pooling operations of CNNs by building matrices of\\nneighbor indices, rearranging the data accordingly and then applying\\nthe general methods for convolution, or for pooling with different\\nfunctions depending on the task ({\\\\it softmax, average, max}).\\nThe next difficulty is to combine images from several telescopes to\\nobtain stereoscopic information. Traditional DL methods\\nonly deal with single images, sequentially in time. This is solved by\\nadding a convolution block for each telescope in the array, and\\nfeeding them all to the dense fully connected part of the network. The\\nexploratory study with four telescopes shows promise in the\\nmeasurements of energy, direction and impact point for incoming\\nparticles; additional work is needed to outperform traditional methods\\nand solve technical details before applying the developed algorithm on\\nreal data.\\nTracking detectors form the core of most collider experiments, and\\nsuccessful track reconstruction is mission critical for achieving\\ntheir goals. Reconstructing tracks is a combinatorial problem,\\ni.e. finding the measurements (hits) belonging to individual particles\\nentering the detectors from an often huge set of possible\\ncombinations. With the transition to the High-Luminosity LHC (HL-LHC)\\nthe complexity of this task will increase substantially. Traditional\\napproaches like track following (inside-out or outside-in) and Kalman\\nfilters do not scale favorably to very high hit densities, and are\\ntypically custom implemented for each experiment with large amount of\\nhuman efforts.\\nThe TrackML~\\\\cite{TrackML} project has the ambition to stimulate new\\napproaches and the development of new algorithms by exposing data from\\na virtual, but realistic HL-LHC tracking detector to data science and\\ncomputer experts outside of the HEP community. Production of\\ntop-antitop quark pairs is selected for the signal events, which are\\nthen merged with 200 soft interactions (pile-up events). Fast\\nsimulation is used to generate hits in the tracker from charged\\ntracks. The magnetic field is inhomogeneous, energy loss, hadronic\\ninteractions and multiple scattering are parameterized. The silicon\\ntracker consists of three parts: innermost pixel detector, followed by\\ntwo layers of short and long silicon strips providing hermetic\\ncoverage up to $|\\\\eta|\\\\ <\\\\ 3$. For each collision, about ten thousand\\ncharged particles, originating approximately from the center of the\\ndetector, produce about ten precise hits per track in three\\ndimensions.\\nThe challenge, running on the Kaggle platform~\\\\cite{TrackMLKaggle} and\\non Codalab in 2018--2019, is split in two phases: accuracy and\\nthroughput. The first phase is scored by a specially developed metric,\\nwhich puts high priority on efficiency of finding real hits belonging\\nto a particle and low fake rates. At least 50\\\\originate from the same simulated truth particle, with hits on the\\ninnermost layers, key for good vertex resolution, and on the outermost\\nlayers, key for long lever arms and thus for good momentum resolution,\\ngetting highest weights in the overall score. A random solution will\\nget a score of zero and a perfect reconstruction of all events in the\\ntest dataset, consisting of 125 simulated events, will get a score of\\none.\\nThe challenge attracted more than 650 participants. In the accuracy\\nphase the participants provide their reconstruction of the test\\ndataset to Kaggle where it is scored. In the throughput phase the\\nparticipants provide their algorithms and software, and it is run in a\\nconsistent environment (Docker containers on two i686 processor cores\\nand 4GB of memory) to measure both accuracy and runtime, which will be\\nvery important to handle the enormous datasets expected from the\\nHL-LHC.\\nWinners~\\\\cite{TrackMLWin} of the accuracy phase are teams (with\\nscores): top-quarks(0.92219), outrunner(0.90400) and Sergey\\nGorbunov(0.89416). While training can consume lots of computer\\nresources, where machine learning really shines is the speed of\\nreconstruction once the algorithms are trained. Winners of the\\nthroughput phase are teams sgorbuno (Sergei Gorbunov), fastrack\\n(Dmitry Emelyanov) and cloudkitchen (Marcel Kunze), who were able to\\ncombine high accuracy scores with speeds well below ten seconds per\\nevent, and even below one second for the first two.\\nThe TrackML challenge shows that ML techniques like representation\\nlearning, combinatorial optimization, clustering and even time series\\nprediction can be applied to tracking. The best solutions offer a\\nsynergy between model-based and data-based approaches, combining the\\nbest of both worlds: physical track models and machine learning, with\\nsensible trade-offs between complexity and performance.\\n\"}},\n",
       "       {'entity_name': 'global particle identification pid', 'entity_type': 'analysis_technique', 'description': 'A technique used in particle physics to classify and identify different types of particles based on their characteristics and behaviors, often employing machine learning methods.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\"}},\n",
       "       {'entity_name': 'probnn', 'entity_type': 'analysis_technique', 'description': 'A baseline particle identification approach that utilizes a one-layer shallow artificial neural network to classify particles by separating one type from the rest.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\"}},\n",
       "       {'entity_name': 'catboost', 'entity_type': 'analysis_technique', 'description': 'A machine learning algorithm that employs gradient boosting over oblivious decision trees for classification tasks, particularly in particle identification.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\"}},\n",
       "       {'entity_name': 'flavor tagging', 'entity_type': 'analysis_technique', 'description': 'A technique used to determine the flavor of B mesons in particle physics experiments, often utilizing machine learning methods to improve accuracy.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\"}},\n",
       "       {'entity_name': 'long shortterm memory lstm', 'entity_type': 'analysis_technique', 'description': 'A specialized type of recurrent neural network that includes feedback connections, enabling it to retain information over longer time intervals, useful in analyzing sequential data.', 'relevant_passages': {\"\\\\section{Machine Learning in Experimental HEP}\\n\\\\subsection{Particle Identification}\\nParticle and jet identification are examples where machine based\\nclassification methods are rapidly replacing the traditional HEP\\napproaches.\\nThe LHCb experiment at the LHC specializes in the physics of beauty\\nquarks. Identifying the types of long lived charged particles in the\\ntracker, ring-imaging Cherenkov detectors, electromagnetic and hadron\\ncalorimeters and the muon chambers is key. Global particle\\nidentification (PID) based on machine learning techniques is\\ndeveloped~\\\\cite{LHCbPID}. The charged particle classes are: electron,\\nmuon, pion, kaon, proton and ghost track (fakes created by the\\ntracking algorithm).\\nThe baseline PID approach, ProbNN, is based on six binary\\n(one-vs-rest) one-layer shallow artificial NN, implemented in the TMVA\\nlibrary. Each network separates one particle type from the rest. The\\nDeepNN with three hidden layers of 300, 300 and 400 neurons is based\\non Keras, and works in multiclassification mode to separate the six\\nparticle types in one go. CatBoost consists of six ``gradient boosting\\nover oblivious decision trees classifiers'', working in one-vs-rest\\nmode. Sixty observables from the LHCb detectors are available for PID;\\nDeepNN and CatBoost use all of them, while ProbNN uses different\\nsubsets for each PID hypotheses, based on physics reasons. The\\nclassifiers are trained on one million simulated events for each\\ncharged particle type.\\nThe performance is verified on real data using kinematically\\nidentified decays to known particles like $J/\\\\psi \\\\rightarrow\\ne^+e^-(\\\\mu^+\\\\mu^-)$, $\\\\Lambda \\\\rightarrow p\\\\pi^-$, $D^0 \\\\rightarrow\\nK^-\\\\pi^+$. The separation quality of the different classifiers is\\ncompared for six signal-background pairs: e-vs-$\\\\pi$, e-vs-K,\\n$\\\\mu$-vs-$\\\\pi$, K-vs-$\\\\pi$, p-vs-$\\\\pi$ and p-vs-K. Different\\nclassifiers score best for different pairs, with CatBoost and DeepNN,\\nby using all observables, outperforming ProbNN on most counts. The\\nproton-kaon separation is the most difficult, as both leave similar\\ntraces in all detector systems. Here using all the available\\ninformation provides a clear advantage.\\nThe Belle II experiment is operating at $\\\\Upsilon(4S)$ center-of-mass\\nenergy of 10.58 GeV at the SuperKEKB energy-asymmetric\\nelectron-positron B factory with record design luminosity of\\n8$\\\\cdot$10$^{35}$cm$^{-2}$s$^{-1}$, a factor of forty increase. This\\nwill expand the intensity frontier, with the size of the Belle II\\ndataset expected to be fifty times bigger than the one collected by\\nBelle. For the study of CP violation and flavor mixing in neutral B\\nmeson decays, the copious decays\\n$\\\\Upsilon(4S)\\\\rightarrow B^0\\\\bar{B}^0$\\nare used. One of the $B$ mesons is fully reconstructed (signal side,\\nincluding all products of this decay), and the flavor of the second\\n(containing a b quark or antiquark) has to be determined (tag side,\\nthe rest of the particles). This is called flavor tagging.\\nTo ensure the success of the physics program, improved flavor taggers\\nusing machine learning are developed~\\\\cite{BelleIIflavor} to cope with\\nthe ultra high luminosity and increased beam backgrounds. A\\ncategory-based tagger uses fast BDTs. A $B^0$($\\\\bar{B}^0$) meson\\ncontains a positively charged $\\\\bar{b}$ (negatively charged $b$)\\nquark, which can decay e.g. to a positive (negative) lepton. Using\\nmultivariate analysis, thirteen specific categories are identified,\\nwhere the flavor signatures of the measured decay particles are\\ncorrelated with the $B$ meson flavor. Each category contains one or\\ntwo particles: e, $\\\\mu$, lepton (e or $\\\\mu$), K, $\\\\pi$, $\\\\Lambda\\n\\\\rightarrow p\\\\pi^-$, called targets. PID variables from the various\\nsubdetectors and kinematic variables (simple like momenta and impact\\nparameters, and global like the recoil mass) are used to identify the\\ntargets among all the target side particles. In a first step\\nindividual tag-side tracks are found, using 108 unique inputs. Each\\nparticle candidate receives 13 weights in [0,1] for the probability of\\nbeing the target for a category. The candidate with the highest weight\\nfor a category is selected as the target. The second step combines the\\noutputs from the thirteen categories, again using multivariate\\nmethods. This improves the performance, as the $B^0_{tag}$ decay can\\nproduce more than one flavor-specific signature, so more than one\\ncategory will contribute.\\nThe performance of the category-based flavor tagger is evaluated on\\nsimulated Belle II, and simulated and real Belle events. For the\\nsimulated events $B^0_{sig}$ decays to $J/\\\\psi K^0_s \\\\rightarrow\\n\\\\mu^+\\\\mu^-\\\\pi^+\\\\pi^-$, while $B^0_{tag}$ has all possible decays. The\\nsizes of the testing and training samples are 1.3 and 2.6 (1 and 2)\\nmillion events for Belle II (Belle). Interestingly, the training\\nsample has to be generated {\\\\it without} CP violation to avoid the\\nalgorithm ``learning'' CP asymmetries on the tag side. The effective\\ntagger efficiency on simulated events is $\\\\sim$37\\\\improvement over the Belle result. Larger training data samples give\\nno further improvement. As an alternative, a deep-learning flavor\\ntagger, based on a multi-layer perceptron (MLP) with eight hidden\\nlayers and 140 input variables is under development. It tries to learn\\nthe correlations between the tag-side tracks and the $B^0_{tag}$\\nflavor using the full information without any preselection of decay\\nproducts. The first results are encouraging: while there is no\\nimprovement for Belle, the Belle II results indicate progress. The\\ncomplexity of the MLP tagger requires huge training samples: the best\\nresults so far use 55 million events for training, and the tendency is\\nto still improve with larger datasets. This computation takes about 48\\nhours with acceleration on a graphical GTX970 GPU, while the same\\ntraining consumes about five hours on a single CPU for the\\ncategory-based flavor tagger.\\nIn many HEP measurements, identification of jet flavors is a key\\ncomponent. Traditionally this is done exploiting the characteristic\\nfeatures of heavy flavor charm or beauty hadrons, decaying at some\\ndistance from the primary interaction point. This produces displaced\\ntracks and secondary vertices (SV), and often leptons from the sizable\\nleptonic and semi-leptonic branching ratios. Additional difficulties\\narise from the embedding of the decay products within jets resulting\\nfrom the parton shower. At collider energies these jets can often be\\nhighly boosted and collimated.\\nIn the CMS collaboration the jet flavor classifier\\nDeepCSV~\\\\cite{Sirunyan:2017ezt} (Combined Secondary Vertex) was\\ndeveloped. It uses a dense NN of five layers with 100 nodes each with\\n{\\\\it ReLU} activation, and an output layer with {\\\\it softmax}\\nactivation to separate four classes: b, bb (two merging B hadrons in\\nthe jet), c and light (both quarks and gluons). The model is\\nimplemented in Keras with a TensorFlow~\\\\cite{TensorFlow}\\nbackend. Sixty-eight input features enter the NN: 8 for each of the\\nsix most displaced tracks, 8 for the most displaced SV, and 12 global\\nvariables. Missing features are represented as zeros. Pile-up tracks,\\nfakes and nuclear interaction vertices are rejected in\\nadvance. Notably, cMVAv2, the best previous tagger using additional\\nlepton information, was outperformed by DeepCSV.\\nThe success of deep learning in the jet arena sparked interest for\\nmore complex models in CMS~\\\\cite{CMSJetFlavor}, based on CNN. These\\nnetworks have been used e.g. for classification of highly boosted\\njets, where the internal jet energy distribution is a major focus. The\\nDeepJet algorithm for flavor identification applies CNN not on images,\\nbut on single particles. No preselection is needed. The input\\nvariables are 16 for up to 25 displacement sorted tracks, 8 for up to\\n25 neutral candidates, 12 for up to 4 SV, and 15 global for a total of\\nup to 663 inputs. Passing through a set of convolutional layers, these\\nproduces 8, 4, and 8 features for each input track, neutral candidate\\nor SV. The network automatically ``engineers'' and selects the\\nrelevant features. This way the large number of input variables can be\\nhandled efficiently by a ``divide-and-conquer'' strategy. The network\\narchitecture is shown in Fig.~\\\\ref{DeepJet}.\\n{\\\\it In recurrent neural networks (RNN) the connections between the\\nnodes form a directed graph along a temporal sequence. The graphs\\ncan be cyclic or acyclic. Sequences of inputs can be processed by\\nthe same units (with same weights), giving the RNN a\\n``memory''. Besides helping with speech recognition, this\\narchitecture can process inputs of variable sizes, e.g. a changing\\nnumber of tracks and jets per event. Long short-term memory (LSTM)\\nis a special case of RNN with feedback connections, giving it a\\ngated memory (state) for retaining information over longer and\\nbetter controlled time intervals.}\\nThree independent RNN continue the processing, producing compact\\nsummaries of dimensionality 150, 50 and 50 for the candidate types\\ntrack, neutral or SV. These outputs are combined with the global\\nvariables to enter a dense NN of 1 layer with 200 nodes and 5 layers\\nof 100 nodes each. A final output layer separates six jet classes: one\\nB hadron, two B hadrons, leptonic B hadron decay (three b type jets),\\ncharm, light quark (uds) and gluon. The last layer has a {\\\\it softmax}\\nactivation, all the other {\\\\it ReLU} activation. DeepJet shows sizable\\nimprovements compared to DeepCSV, for example in $t\\\\bar{t}$ events at\\nhigh jet \\\\mbox{p$_T\\\\ >\\\\ $~90 GeV} for b jet efficiency of 90\\\\number of false positives is suppressed from 10 to 3\\\\time, the light quark versus gluon discrimination is on par with\\ndedicated RNN binary classifiers, and slightly better than CNN using\\njet images.\\n\"}},\n",
       "       {'entity_name': 'kullbackleibler divergence', 'entity_type': 'statistics_concept', 'description': 'A measure of how one probability distribution diverges from a second, expected probability distribution. It is commonly used in statistical tests to compare distributions and in variational inference to quantify the dissimilarity between a trial approximation and the prior distribution.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Outlier detection}\\nThe above methods focus on detecting new physics as overdensities in very specific regions of the kinematic phase space; this paradigm is similar to a traditional bump hunt, often performed in HEP searches for novel particles. However, new physics signatures are equally likely to manifest themselves as unexpected events in the tail of distributions. This type of events may be identified using out-of-distribution detection algorithms. The prime example of such an algorithm is the auto-encoder\\\\cite{lecun1987phd, ballard1987modular, hinton1993autoencoders}, which is especially popular in high-energy physics applications\\\\cite{Radovic:2018dip, albertsson2019machine, Jawahar:2021vyu, tsan2021particle, Finke_2021, Laguarta:2023evo, Vaslin:2023lig, Anzalone:2023ugq, Bohm:2023ihd}.\\n\\\\subsubsection{Self-supervised methods}\\nSelf-supervised learning\\\\cite{balestriero2023cookbook} is a form of unsupervised learning where the data provides the supervision. In general, a part of the data is initially withheld from the model and the task of the network is to reproduce this data. Consequently, the network learns a meaningful representation of the data to solve this problem. The self-supervised learning workflow usually involves two stages: first, generating a set of supervisory signals from the input data; and second, employing these signals for a supervised~task. Self-supervised learning can be seen as a hybrid approach that lies somewhere between unsupervised and supervised learning. In high-energy physics, the most used type of self-supervised model is by far the auto-encoder.\\nThe standard Auto-Encoder (AE) model consists of two neural networks: the encoder and the decoder. The encoder maps the input data to a \\\\textit{latent space} of a lower dimensionality. For example, a particle that is represented by 64 features (transverse momentum, azimuthal angle, etc.) is reduced to a 16 feature representation. In contrast, the objective of the decoder is to reconstruct the input features from the latent space features. The ultimate goal of the AE training is to minimise the difference between the input and reconstructed data. This difference can be quantified by employing various loss functions. The Mean Squared Error (MSE) loss function is the most basic example of quantifying the input-output discrepancy:\\n\\\\begin{equation}\\nL_\\\\mathrm{MSE} = (x - f(z,\\\\theta))^2 \\n\\\\end{equation}\\nwhere $x$ is the input data, $z$ is the latent space data, and $\\\\theta$ are the weights of the decoder. This reconstruction loss is propagated through both the decoder and the encoder. Thus, the latent space and the reconstructed data evolve simultaneously.\\nThe extent to which the auto-encoder latent space follows a statistical distribution is referred to as the latent space \\\\textit{regularity}. The latent space of the standard auto-encoder does not follow any particular distribution. The regularity of the standard AE depends on the input features, the dimension of the latent space, and the encoder architecture. Thus, the encoder will shape the latent space such that it facilitates the reconstruction task, thus minimising the MSE loss from \\\\autoref{eq:vanillaloss}. In contrast, the Variational Auto-encoder (VAE)\\\\cite{kingma2014autoencoding} is an extension of the conventional auto-encoder described above, which models the latent representation to approximate a given probability distribution. This is typically a Gaussian distribution, described by a mean and a variance; however, many alternatives exist\\\\cite{joo2019dirichlet, patrini2019sinkhorn, Cerri_2019, Dillon_2021, Cheng_2023}, and the choice of latent space distribution ultimately depends on the task. \\nThe main idea of variational inference is to deﬁne a parametrised family of distributions and to search within it for the best approximation of the chosen prior distribution. The ``best approximation\\'\\' is defined as the element of the aforementioned family of distributions that minimises a pre-deﬁned function that measures the dissimilarity between the trial approximation and the prior. The function that is most commonly employed for this task is the Kullback-Leibler\\\\cite{Joyce2011} (KL) divergence, defined as\\n\\\\begin{equation}\\n\\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma}) = -\\\\frac{1}{2}\\\\sum_i \\\\left ( \\\\log(\\\\sigma_i^2) - \\\\sigma_i^2 -\\\\mu_i^2 +1 \\\\right)~,\\n\\\\end{equation}\\nfor the specific case of comparing a parametrised Gaussian distribution $\\\\mathrm{N}(\\\\vec{\\\\mu},\\\\vec{\\\\sigma})$ with $\\\\mathrm{N}(1, 0)$. A broader discussion on the KL divergence is found in Ref.\\\\,\\\\citen{paisley2012variational}. Note that the KL divergence is a somewhat unstable dissimilarity metric. Hence, more robust alternatives exist, such as the Wasserstein distance, which led to the creation of an AE architecture with the same name\\\\cite{tolstikhin2019wasserstein}. Variations on the Wasserstein AE have also been applied in a high-energy physics context\\\\cite{Komiske_2019, Komiske_2020}.\\nThe VAE loss consists of two components: the reconstruction loss, conventionally the MSE, and the KL divergence term. The latter encourages the VAE to produce a latent space that follows a well-defined prior distribution, regularising the model. Thus, the VAE loss can be written schematically as\\n\\\\begin{equation}\\n{\\\\cal L} = (1-\\\\beta) \\\\mathrm{MSE}(\\\\mathrm{Output}, \\\\mathrm{Input}) + \\\\beta \\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma})~,\\n\\\\end{equation}\\nwhere MSE labels the reconstruction loss, $\\\\mathrm{D_\\\\mathrm{{KL}}}$ is the KL regularization term, and $\\\\beta\\\\in[0, 1]$ is a hyperparameter that balances the effect of the two loss components.\\nThe weakly supervised methods from the previous sections aim to learn the likelihood ratio and thus can identify anomalies. In contrast, self-supervised models only learn the probability density of the background. Hence, an event may be labeled as anomalous if its probability to be associated with the learned latent distribution is very low. Additionally, the learned distribution exists in a lower dimensional embedded space. This stops the model from memorizing the input and is a form of lossy compression. Therefore, the model is generally capable of reconstructing events it is frequently exposed to during its training, but it fails at reconstructing events that are rare in the training set. The difference between the input data and its reconstructed counterpart may then be used to define an anomaly score: a high MSE is expected for anomalous data and a low MSE is expected for typical events. An illustration of this paradigm is shown in Figure~\\\\ref{fig:ae}. There exist several studies in HEP where AEs and VAEs are used for detecting new physics as outliers in the data~\\\\cite{Farina:2018fyg, Heimel:2018mkt,Blance:2019ibf,Hajer:2018kqm,Roy:2019jae,Cheng_2023}. For~example, this type of workflow was used to search for new physics in the two-body invariant mass spectrum of two jets or a jet and a lepton with the ATLAS Experiment in Ref.~\\\\citen{ATLAS:2023ixc}. Therein, a selection on an auto-encoder output is used to suppress the background and define signal regions with a high signal-to-background ratio. The auto-encoder output for data and for a range of potential new physics signatures is shown in Figure~\\\\ref{fig:atlasae}.\\nAs mentioned in the beginning of this section, autoencoders are efficient for event-by-event outlier detection and are not expected to perform well in finding overdensities. This makes them complimentary to the weakly supervised methods. Furthermore, an additional problem that auto-encoders have is discussed in Ref.~\\\\citen{obstructions}. In the aforementioned work it is demonstrated that the connection between large MSE and anomalies is not completely clear: for data sets with a nontrivial topology, there will always be points that wrongly are classified as anomalous. Conventionally, this can be mitigated by using VAEs and classifying anomalous events using the regularized latent space. An alternative method of circumventing this issue is based on the so called normalised AE \\\\cite{yoon2023autoencoding}, which is located at the boundary between self-supervised and unsupervised learning. This newer type of AE architecture uses energy-based models as an alternative to the likelihood ratio or the MSE. Thus, the normalised AE avoids classifying genuinely complex albeit standard events as anomalous. For more details on this last kind of AE and its possible application to HEP, see Ref.\\\\,\\\\citen{dillon2023normalized}. As mentioned earlier, diffusion models are also being explored as an alternative method to perform density estimation, similar to variational autoencoders, utilizing the learned density as a permutation-invariant anomaly detection score ~\\\\cite{mikuni2023highdimensional}.\\nAs mentioned earlier, diffusion models are also increasingly being investigated as an alternative approach for density estimation. This method parallels the use of variational autoencoders, leveraging the learned density to create a permutation-invariant score for anomaly detection, as detailed in Ref.~\\\\citen{mikuni2023highdimensional}.\\n\\\\subsubsection{Unsupervised Methods}\\nUnsupervised anomaly detection methods usually perform some type of data clustering. They include models such as Support Vector Machines~\\\\cite{boser1992training}, Isolation Forests~\\\\cite{isoforest}, and Gaussian Mixture Models\\\\cite{vanBeekveld:2020txa, Kuusela_2012}. An application using SVMs for anomaly detection in particle physics is discussed in Section~\\\\ref{sec:qml}. An example of unsupervised clustering for collider physics is presented in Ref.~\\\\citen{ucluster}. Therein, the Unsupervised Clustering algorithm, or UCluster, uses an attention-based Graph Neural Network known as \"ABC net\"~\\\\cite{abcnet} to create a latent space in which points sharing similar properties are placed close to each other. This is achieved by combining a clustering objective and a classification task during training. The produced embedding is shown to be capable of clustering together events that contain a new physics signal. A benefit of this method is that it naturally provides a way of performing background estimation. For each identified cluster, the nearest cluster within the embedding space can be used as a background model. The anomalous signal remains\\nlocalized in a particular cluster. Therefore, the nearest clusters are signal free, as shown in Figure~\\\\ref{fig:ucluster}. \\nA model-independent search method, based on Gaussian Mixture Models (GMMs), is introduced in Ref.~\\\\citen{Kuusela_2012}. Within this methodology, a GMM is being used to model the background. Then, in order to avoid any dependence on a signal hypothesis, deviations from this model are identified by fitting a mixture of the background model and a number of additional Gaussians to the observed data. This allows to search for any potential deviation from the background expectation without developing a model for the signal a priori.\\nFinally, decision trees have also been explored in anomaly detection for searches. For example, in Ref.~\\\\citen{roche2023nanosecond}, a tree-based autoencoder is trained through a self-supervised paradigm on background data and then evaluated on the ADC challenge data~\\\\cite{adcchallenge}. Their unsupervised counterpart, isolation forests, have been less prominent in particle physics, but they have been applied for accelerator control~\\\\cite{Halilovic:2665985}.\\nA key challenge with outlier detection methods, as discussed in Ref.~\\\\citen{golling2023massive}, is their tendency to generate anomaly scores closely correlated to the variable of interest. This may lead to undesired sculpting effects, complicating bump-hunt like searches. To address this, strategies such as decorrelating the latent space from the variable of interest or tailoring the anomaly metric to be conditional on the jet mass \\\\cite{Cheng_2023} should be explored. However, efforts in these areas remain limited.\\n', \"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\"}},\n",
       "       {'entity_name': 'permutation test', 'entity_type': 'analysis_technique', 'description': 'A non-parametric statistical test that assesses the significance of observed data by comparing it to a distribution generated by rearranging the data points.', 'relevant_passages': {\"\\\\section{Machine Learning in Theoretical/Phenomenological\\nHigh Energy Physics}\\nBuilding upon the sustained successes of the SM in describing the\\nmeasured phenomena in HEP, new hybrid approaches are developed pairing\\nthe strength of cutting-edge machine learning techniques with our\\nknowledge of the underlying physics processes.\\n\\\\subsection{Constraining Effective Field Theories}\\nNew data analysis techniques, aimed at improving the precision of the\\nLHC legacy constraints, are developed in~\\\\cite{Brehmer:2018kdj}.\\nTraditionally in HEP, searches for signatures of new phenomena or\\nlimits on their parameters are produced by selecting the kinematic\\nvariables considered to be most relevant. This can effectively explore\\nparts of the phase space, but leave other parts weakly explored or\\nconstrained. By using the fully differential cross sections at the\\nparton level, approaches like the matrix element method or optimal\\nobservables can improve the sensitivity in the complex cases of\\nmultiple parameters. The weak side of these methods is how to handle\\nthe next steps to reach the experimental data: parton showers and\\ndetector response. Both of these steps are often simulated by\\ncomplicated Monte Carlo programs with notoriously slow convergence of\\nthe underlying integrals. While simulations can be very accurate, they\\nproduce no roadmap how to extract the physics from data, especially\\nfor high dimensional problems with many observables and\\nparameters. Building upon our knowledge of the underlying particle\\nphysics processes and the ability of ML techniques to recognize\\npatterns in the simulated data, it can be effectively summarized for\\nthe next steps in the data analysis. In this way NN can be trained to\\nextract additional information and estimate more precisely the\\nlikelihood of the theory parameters from the MC simulations.\\nThe likelihood $\\\\mathbf{p}(x|\\\\theta)$ of theory parameters $\\\\theta$\\nfor data $x$ can be factorized in HEP as follows:\\n\\\\begin{equation}\\n\\\\mathbf{p}(x|\\\\theta) = \\\\int dz_{detector} \\\\int dz_{shower} \\\\int dz \\\\mathbf{p}(x|z_{detector}) \\\\mathbf{p}(z_{detector}|z_{shower}) \\\\mathbf{p}(z_{shower}|z) \\\\mathbf{p}(z|\\\\theta)\\n\\\\end{equation}\\nwhere\\n$\\\\mathbf{p}(z|\\\\theta)\\\\ =\\\\ \\\\frac{d\\\\sigma(\\\\theta)/dz}{\\\\sigma(\\\\theta)}$\\nis the probability density of the parton-level momenta $z$ on the\\ntheory parameters $\\\\theta$. The other terms in the integral correspond\\nto the path from partons through parton showers, detector and\\nreconstruction effects to the experimental data $x$ used in the\\nanalysis. The steps on this path have the Markov property: each one\\nonly depends on the previous one. A single event can contain millions\\nof variables. Calculating these integrals, and then the likelihood\\nfunction and the likelihood ratios, the preferred test statistic for\\nlimit setting at the LHC, is an intractable problem. On the other\\nhand, at the parton level $\\\\mathbf{p}(z|\\\\theta)$ can be calculated\\nfrom the theory matrix elements and the proton parton distribution\\nfunctions for arbitrary $z$ or $\\\\theta$ values. In this way more\\ninformation can be extracted from the simulation than just generated\\nsamples of observables {$x$}, namely the joint likelihood ratio $r$\\nand the joint score $t(x,z|\\\\theta_0)$ (which describes the relative\\ngradient of the likelihood to $\\\\theta$):\\n\\\\begin{equation}\\nr(x,z|\\\\theta_0,\\\\theta_1)\\\\ =\\\\ \\\\frac{\\\\mathbf{p}(z|\\\\theta_0)}{\\\\mathbf{p}(z|\\\\theta_1)}\\n\\\\end{equation}\\nThe joint quantities $r$ and $t$ depend on the parton level momenta\\n$z$, which for sure are not available in the measured data. Here ML\\nhelps by using suitable loss functions based on data available from\\nthe simulation to train a deep NN with stochastic gradient descent to\\napproximate functionals that can produce the important likelihood\\nratio: $r(x|\\\\theta_0,\\\\theta_1)$ depending only on the data and theory\\nparameters. For technical details we refer interested readers\\nto~\\\\cite{Brehmer:2018kdj} and references therein.\\nAs a case study the weak-boson-fusion Higgs production with decays to\\nfour leptons is taken. The {\\\\tt RASCAL} technique uses the joint\\nlikelihood ratio and the joint score to train an estimator for the\\nlikelihood ratio. In essence this is a ML version of the matrix\\nelement method, replacing very computationally intensive numerical\\nintegrations with a regression training phase. Once the training is\\ncomplete, it takes microseconds to compute the likelihood ratio per\\nevent and parameter point. As a bonus, the parton shower, detector and\\nreconstruction effects are learned from full simulations instead of\\nretorting to simplified, and sometimes crude, smearing functions. At\\nthe cost of a more complex data analysis architecture, the precision\\nof the measurements is improved by tapping the full simulation\\ninformation. For a typical operating point from the case study, aimed\\nat putting limits on dimension-six operators in effective field\\ntheories, a relative gain of 16\\\\observed, corresponding to 90\\\\\\n\\\\subsection{Model-Independent Searches for New Physics}\\nSo far, searches for beyond the SM (BSM), new physics (NP), phenomena\\nat the LHC have been negative, despite herculean efforts by the\\nexperiments. The majority of these searches are inspired and guided by\\nparticular BSM models, like supersymmetry or dark matter (DM). An\\nalternative approach, which could provide a path to NP, potentially\\neven lurking so far {\\\\it unseen} in the already collected data, are\\nmodel-independent searches. They could unravel unpredicted phenomena,\\nfor which no models are available.\\nOne proof-of-concept~\\\\cite{DeSimone:2018efk} strategy along these\\nlines is developed based on unsupervised learning, where the data are\\nnot labeled. The goal is to compare two D (usually high) dimensional\\nsamples: the SM simulated events (background to BSM searches), and the\\nreal data, and to check if the two are drawn from the same probability\\ndensity distribution. If the density distributions are $p_{SM}$ and\\n$p_{data}$, the null hypothesis is $H_0:p_{SM}\\\\ =\\\\ p_{data}$, and the\\nalternative is $H_1:p_{SM} \\\\neq p_{data}$. In statistical terms, this\\nis a two-sample test, and there are many methods to handle it. Here, a\\nmodel-independent (no assumptions about the densities), non-parametric\\n(compare the densities as a whole, not just e.g. means and standard\\ndeviations) and un-binned (use the full multi-dimensional information)\\ntwo-sample test is proposed. As the densities $p_{SM}$ and $p_{data}$\\nare unknown, they are replaced by the estimated densities\\n$\\\\hat{p}_{SM}$ and $\\\\hat{p}_{data}$. A test statistic\\n(TS), based on the Kullback-Leibler KL divergence measure~\\\\cite{KL},\\nis built for the ratio of the two densities, with values close to zero\\nif $H_0$ is true, and far from zero otherwise. The ratio is estimated\\nusing a nearest-neighbors approach. A fixed number of neighbors K is\\nused, and the densities are estimated by the numbers of points within\\nlocal spheres in D dimensional space around each point divided by the\\nsphere volumes and normalized to the total number of points. Then the\\ndistribution of the test statistic $f(TS|H_0)$ is derived by a\\nresampling method known as the permutation test, by randomly sampling\\nwithout replacement from the two samples under the assumption that\\nthey originate from the same distribution, as expected under $H_0$.\\nAccumulating enough permutations to estimate the TS distribution\\nprecisely enough, this allows to select the critical region for\\nrejecting the null hypothesis at a given significance $\\\\alpha$,\\ne.g. 0.05, when the corresponding p-value is smaller than $\\\\alpha$.\\nA proof-of-concept case study for dark matter searches with monojet\\nsignatures at the LHC is performed. The DM mass is 100 GeV, the\\nmediator masses 1200--3000 GeV, detector effects are accounted for by\\nfast simulation, and the input features have D=8: $p_T$ and $\\\\eta$ for\\nthe two leading jets, number of jets, missing energy, hadronic energy\\n$H_T$, and transverse angle between the leading jet and the missing\\nenergy. The comparison is done for K=5 and 3000 permutations. As an\\nadded bonus, regions of discrepancy can be identified for detailed\\nscrutiny in a model-independent way. The results show promise. Before\\napplying them to real data, several refinements are needed: systematic\\nuncertainties and limited MC statistics will weaken the power of the\\nstatistical tests, and the algorithm has to be optimized or made\\ncompletely unsupervised by automatically choosing the optimal\\nparameters like the value of K.\\nA different approach~\\\\cite{DAgnolo:2018cun} for NP searches based on\\nsupervised learning builds upon the same setup. This time, using the\\nsame notation as for the unsupervised approach introduced earlier:\\n\\\\begin{equation}\\np_{data}(x|\\\\mathbf{w}) = p_{SM}(x) \\\\cdot \\\\exp{f(x;\\\\mathbf{w})}\\n\\\\end{equation}\\nwhere $x$ represents the d-dimensional input variables, $\\\\mathcal{F} =\\n\\\\{ f(x;\\\\mathbf{w}), \\\\forall \\\\mathbf{w} \\\\}$ is a set of real functions,\\nand the NP would traditionally depend on a number of free parameters\\n$\\\\mathbf{w}$, introducing model dependence. Here $\\\\mathcal{F}$ is\\nreplaced by a neural network, in effect replacing histograms with NN,\\nbased upon their well known capability~\\\\cite{Cybenko} for smooth\\napproximations to wide classes of functions. The NP parameters are\\nreplaced by the NN parameters, which are obtained from training on the\\ndata and SM samples. The minimization of a suitable loss function\\n(which also maximizes the likelihood) provides the best fit values\\n$\\\\hat{\\\\mathbf{w}}$. Again a t-statistic and p-values are derived for\\nrejecting the same null hypothesis, as well as the log-ratio of the\\ndata and SM probability density distributions.\\nThe method is illustrated on simple numerical experiments for the\\nresonant and non-resonant searches for NP in the 1D invariant mass\\ndistributions, and for a 2D case adding the $\\\\cos{\\\\theta}$ of the\\ndecay products.\\nA limitation of these methods is the precision of the SM\\npredictions. Usually produced by MC full detector simulations, they\\nare computationally costly. In addition, systematic uncertainties of\\nthe predictions reduce the sensitivity to new phenomena. Given the\\nexcellent performance of the LHC and the experiments, by the end of\\nRun2 the data available in many corners of the phase space exceeds\\nthe MC statistics, and the situation could get even more critical in\\nthe future. Certainly approaches driven by data in relatively NP-free\\nregions, e.g. sidebands of distributions, will also have an important\\nrole to play.\\n\\\\subsection{Parton Distribution Functions}\\nThe well known capability of NN for smooth approximations to wide\\nclasses of functions is used in Parton Distribution Function (PDF)\\nfits to the available lower energy and LHC data by the\\nNNPDF~\\\\cite{Ball:2014uwa,Ball:2017nwa} collaboration. The fit is based\\non a genetic algorithm with a larger number of mutants to explore a\\nlarger portion of the phase space, and nodal mutations well suited for\\nthe NN utilized as unbiased interpolators of the various flavors of\\nPDFs. To avoid overfitting, the cross-validation runs over a\\nvalidation set which is never used in the training, but remembers the\\nbest validation $\\\\chi^2$. At the end, not the ``best'' fit on the\\ntraining set, but a ``look-back'' to the best validation fit is\\nretained as the final result. The NNPDF sets are easily accessible\\nthrough the LHAPDF~\\\\cite{Bourilkov:2003kk,Whalley:2005nh,Bourilkov:2006cj,Buckley:2014ana}\\nlibraries.\\nA set of Monte Carlo ``replicas'' is used to estimate the\\nuncertainties by computing the RMSE of predictions for physical\\nobservables over the ensemble. In practice this works well in most\\ncases. Care is needed in corners of the phase space, like searches at\\nhigh invariant masses, where cross sections for some members of the\\nstandard PDF set can become negative, or unphysical. For these cases,\\na special PDF set with reduced number of replicas, but ensuring\\npositivity, is provided. The price to pay is enhanced PDF uncertainty\\ncompared to other PDF families, where the PDF parameterizations\\nextrapolate to such phase space corners with smaller uncertainties. In\\nany case, comparing several families before claiming a discovery is\\nhighly recommended.\\n\"}},\n",
       "       {'entity_name': 'quantum mechanical interference', 'entity_type': 'statistics_concept', 'description': 'A phenomenon where the probability amplitudes of different processes combine, affecting the overall probability distribution of outcomes in particle physics experiments.', 'relevant_passages': {'\\\\section{Introduction}\\nParticle physics is the study of the subatomic constituents of matter: How many are there? What are their properties? How do they interact? There are two basic approaches to answering these questions: a theoretical one and an experimental one. On the theoretical side, we can ask: what possible subatomic particles could there be? Remarkably, there are constraints due to theoretical consistency of the underlying theory that strongly limit the types of particles possible. For example, there is a direct logical path from the requirement that things do not start appearing out of nowhere (``unitarity\\'\\') to the Pauli exclusion principle, which keeps matter from imploding. To the dismay of many theorists, however, there seem to be many more self-consistent theories than the one describing nature, and so experiments are essential. \\nThe state-of-the art particle experiment is the Large Hadron Collider (LHC) at CERN on the border between France and Switzerland. \\nIts major success so far is finding the Higgs boson in 2012. The LHC collides protons together at close to the speed of light. This energy is then converted into mass via $E=mc^2$ thereby forming new particles.\\nUsually these particles last for only fractions of a second (the lifetime of the Higgs boson is $10^{-22}$ s); hence, the art of modern experimental particle physics involves finding indications that a particle was made even though we never actually see it. The experimental challenge is complicated by the fact that particles of interest are usually quite rare and look nearly identical to much more common backgrounds. For example, only one in every billion proton collisions at LHC produces a Higgs boson, and only one in ten thousand of these is easy to see. Finding new particles in modern experiments is like finding a particular piece of hay in a haystack.\\nLuckily, hay-in-a-haystack problems are exactly what modern machine learning excels at solving. \\nThere are two aspects of particle physics that make it unique, or at least highly atypical, as compared to other fields where machine learning is applied. First, particle physics is governed by quantum mechanics. Of course, everything is governed by quantum mechanics, but in particle physics the inherent uncertainty of the quantum mechanical world affects the nature of the truth we might hope to learn. Just like how \\nSchr\\\\\"odinger\\'s cat can be alive and dead at the same time, a collision at the LHC can both produce a Higgs boson and not produce a Higgs boson at the same time. In fact, there is quantum mechanical interference between the signal process (protons collide and a Higgs boson is produced) and a background process (protons collide without producing a Higgs). The question ``Was there a Higgs boson in this event?\\'\\' is unanswerable. To be a little more precise, for a given number of particles $n$ produced, the probability distribution for signal and background, differential in the momenta of the particles produced (the phase space) has the form\\n\\\\begin{equation}\\nd P_{\\\\text{data}}^n = |{\\\\mathcal M}_S +{\\\\mathcal M}_B|^2 dp_1 \\\\cdots dp_n \\n\\\\end{equation}\\nHere, ${\\\\mathcal M}_S$ and ${\\\\mathcal M}_B$ are the quantum-mechanical amplitudes ($S$-matrix elements, which are complex numbers) for producing signal and background, and the cross term ${\\\\mathcal M}_S {\\\\mathcal M}_B^\\\\star + {\\\\mathcal M}_B {\\\\mathcal M}_S^\\\\star$ represents the interference. This interference term can be positive (constructive interference) or negative (destructive interference).\\nAlthough an individual event cannot be assigned a truth label, the probability of finding a certain set of particles showing up in the detector depends on whether the Higgs boson exists:\\nfinding the Higgs boson amounts to excluding the background-only hypothesis (Eq.~\\\\eqref{Pdata1} with ${\\\\mathcal M}_S=0$). \\nIn practice, the probability distribution of signal is often strongly peaked, due to a resonance for example, in some small region of phase space. In such regions, background can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_S$. In complementary regions, signal can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_B$. Thus it is commonplace to approximate the full probability distribution with a mixture model. If we sum over possible numbers $n$ of particles and integrate over the momenta in the observable region, we can then write\\n\\\\begin{equation}\\nP_{\\\\text{data}} = \\\\alpha_S \\\\, P_{\\\\text{S}} + \\\\alpha_B \\\\, P_{\\\\text{B}}\\n\\\\end{equation}\\nwith $\\\\alpha_S + \\\\alpha_B=1$. \\nThat is, we treat the probability distribution of the data as a linear combination of the probability distributions for signal and background. The goal is then to determine the coefficients $\\\\alpha_S$ and $\\\\alpha_B$, or often more succinctly, whether $\\\\alpha_S$ is non-zero. \\nEach measured event gives us a number of particles $n$ and point in $n$-particle phase space $\\\\{p_i\\\\}$, with some uncertainty or binning $dp_1\\\\cdots d p_n$,\\ndrawn from the true probability distribution $d P^n_{\\\\text{data}}$.\\nOnly after many draws can we hope to constrain $\\\\alpha_S$. Even within the mixture model approximation, there is still no truth label for individual events. \\nThis is different from, say, distinguishing cats from dogs (or alive cats from dead cats) in an image database. For cats and dogs, even if the distributions overlap, there is a correct answer ($\\\\alpha_S=1$ or $0$ for each event). For particle physics, where the distributions overlap, a particle is both signal {\\\\it and} background.\\nThe second way in which particle physics differs from typical machine learning applications is that particle physics has remarkably accurate simulation tools for producing synthetic data for training. These tools have been developed by experts over more than 40 years. Together, they describe the evolution of a particle collision over 20 orders of magnitude in length. The smallest scale the LHC probes is around $10^{-18}$ m, one thousandth the size of a proton. Here the physics is described by perturbative quantum field theory; particles interact rather weakly and first-principles calculations are accurate. The Higgs boson has a size (Compton wavelength) of $10^{-17}$ m, so it is only at these small distances that we have any hope of examining it. Between $10^{-18}$ m and $10^{-15}$ m, a semi-classical Markov model is used to turn a handful of primordial particles into hundreds of quarks and gluons. Between $10^{-15}$ m and $10^{-6}$ m, the quarks and gluons turn into a zoo of metastable subatomic particles that subsequently decay into hundreds of ``stable\\'\\' particles: pions, protons, neutrons, electrons and photons. These then start interacting with detector components and propagating through the material, as described by other excellent parameterized models. The detector model is accurate from $10^{-6}$ m to the $100$ m size of the LHC detectors (the ATLAS detector at the LHC is 46 meters long). The result is a progression from an order-10 dimensional phase space at the shortest distances, to an order-$10^3$ dimensional phase space at intermediate scales, to an order-$10^8$ dimensional phase space of electronic detector readouts channels. Combined, these simulation tools give a phenomenally robust (but embarrassingly sparse) sampling from this hundred million dimensional space. Around one trillion events have been recorded at the LHC, and a comparable number of events have been simulated, providing hundreds of petabytes of actual and synthetic data to analyze. The first stage of the simulations, up to the stable particle level (the $10^3$ dimensional space) is relatively fast: one million events can be generated in an hour or so on a laptop.\\nThe second stage of the simulation, through the detector, is much slower, taking seconds or even minutes per event. Conveniently, for many applications, the first stage of the simulation is sufficient. \\nNo human being, and as yet no machine, can visualize a hundred million dimensional distribution. So the typical analysis pipeline is to take all of the low-level outputs and aggregate them into a single composite feature, such as the total energy of the particles in some region. Ideally, a histogram of this feature would exhibit a resonance peak or some other salient indication of signal. We additionally want this feature to have a simple physical interpretation, so that we can cross-check the distribution against our intuition. For the Higgs boson, the ``golden discovery channel\\'\\' was two muons (or electrons) and two antimuons (or positrons). A Feynman diagram describing this process looks like\\n\\\\begin{equation}\\n{\\n\\\\parbox{10mm} {\\n\\\\includegraphics[width=0.6\\\\columnwidth,trim = {100 0 00 0}]{hmmmm.pdf}\\n}\\n}\\n\\\\end{equation}\\nThe invariant mass $m=\\\\sqrt{(E_1+E_2+E_3+E_4)^2 - (\\\\vec{p}_1+\\\\vec{p}_2+\\\\vec{p}_3+\\\\vec{p}_4)^2}$, with $E_i$ the energies and $\\\\vec{p}_i$ the momenta,\\nof the four observed particles is a powerful way to discover the signal.\\nFor the Higgs boson signal, the probability density of this feature has a peak at the Higgs boson mass of 125 GeV where the background is very small. Unfortunately, only one in every $10^{13}$ proton collisions will give such a signal. If we do not demand that our signal be background-free, and we also do not demand having any physical interpretation such as we have for a feature like mass, \\nthen we can ask: what feature is the {\\\\it optimal} way of statistically discriminating a signal from its background? Such questions, when supplemented with the enormous amounts of easily produced synthetic data, are ideally suited to modern machine learning methodology.\\n'}},\n",
       "       {'entity_name': 'resonance', 'entity_type': 'statistics_concept', 'description': 'A peak in the probability distribution of a signal, indicating the presence of a particle at a specific mass, often used in the context of particle discovery.', 'relevant_passages': {'\\\\section{Introduction}\\nParticle physics is the study of the subatomic constituents of matter: How many are there? What are their properties? How do they interact? There are two basic approaches to answering these questions: a theoretical one and an experimental one. On the theoretical side, we can ask: what possible subatomic particles could there be? Remarkably, there are constraints due to theoretical consistency of the underlying theory that strongly limit the types of particles possible. For example, there is a direct logical path from the requirement that things do not start appearing out of nowhere (``unitarity\\'\\') to the Pauli exclusion principle, which keeps matter from imploding. To the dismay of many theorists, however, there seem to be many more self-consistent theories than the one describing nature, and so experiments are essential. \\nThe state-of-the art particle experiment is the Large Hadron Collider (LHC) at CERN on the border between France and Switzerland. \\nIts major success so far is finding the Higgs boson in 2012. The LHC collides protons together at close to the speed of light. This energy is then converted into mass via $E=mc^2$ thereby forming new particles.\\nUsually these particles last for only fractions of a second (the lifetime of the Higgs boson is $10^{-22}$ s); hence, the art of modern experimental particle physics involves finding indications that a particle was made even though we never actually see it. The experimental challenge is complicated by the fact that particles of interest are usually quite rare and look nearly identical to much more common backgrounds. For example, only one in every billion proton collisions at LHC produces a Higgs boson, and only one in ten thousand of these is easy to see. Finding new particles in modern experiments is like finding a particular piece of hay in a haystack.\\nLuckily, hay-in-a-haystack problems are exactly what modern machine learning excels at solving. \\nThere are two aspects of particle physics that make it unique, or at least highly atypical, as compared to other fields where machine learning is applied. First, particle physics is governed by quantum mechanics. Of course, everything is governed by quantum mechanics, but in particle physics the inherent uncertainty of the quantum mechanical world affects the nature of the truth we might hope to learn. Just like how \\nSchr\\\\\"odinger\\'s cat can be alive and dead at the same time, a collision at the LHC can both produce a Higgs boson and not produce a Higgs boson at the same time. In fact, there is quantum mechanical interference between the signal process (protons collide and a Higgs boson is produced) and a background process (protons collide without producing a Higgs). The question ``Was there a Higgs boson in this event?\\'\\' is unanswerable. To be a little more precise, for a given number of particles $n$ produced, the probability distribution for signal and background, differential in the momenta of the particles produced (the phase space) has the form\\n\\\\begin{equation}\\nd P_{\\\\text{data}}^n = |{\\\\mathcal M}_S +{\\\\mathcal M}_B|^2 dp_1 \\\\cdots dp_n \\n\\\\end{equation}\\nHere, ${\\\\mathcal M}_S$ and ${\\\\mathcal M}_B$ are the quantum-mechanical amplitudes ($S$-matrix elements, which are complex numbers) for producing signal and background, and the cross term ${\\\\mathcal M}_S {\\\\mathcal M}_B^\\\\star + {\\\\mathcal M}_B {\\\\mathcal M}_S^\\\\star$ represents the interference. This interference term can be positive (constructive interference) or negative (destructive interference).\\nAlthough an individual event cannot be assigned a truth label, the probability of finding a certain set of particles showing up in the detector depends on whether the Higgs boson exists:\\nfinding the Higgs boson amounts to excluding the background-only hypothesis (Eq.~\\\\eqref{Pdata1} with ${\\\\mathcal M}_S=0$). \\nIn practice, the probability distribution of signal is often strongly peaked, due to a resonance for example, in some small region of phase space. In such regions, background can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_S$. In complementary regions, signal can often be neglected: ${\\\\mathcal M}_S +{\\\\mathcal M}_B\\\\approx {\\\\mathcal M}_B$. Thus it is commonplace to approximate the full probability distribution with a mixture model. If we sum over possible numbers $n$ of particles and integrate over the momenta in the observable region, we can then write\\n\\\\begin{equation}\\nP_{\\\\text{data}} = \\\\alpha_S \\\\, P_{\\\\text{S}} + \\\\alpha_B \\\\, P_{\\\\text{B}}\\n\\\\end{equation}\\nwith $\\\\alpha_S + \\\\alpha_B=1$. \\nThat is, we treat the probability distribution of the data as a linear combination of the probability distributions for signal and background. The goal is then to determine the coefficients $\\\\alpha_S$ and $\\\\alpha_B$, or often more succinctly, whether $\\\\alpha_S$ is non-zero. \\nEach measured event gives us a number of particles $n$ and point in $n$-particle phase space $\\\\{p_i\\\\}$, with some uncertainty or binning $dp_1\\\\cdots d p_n$,\\ndrawn from the true probability distribution $d P^n_{\\\\text{data}}$.\\nOnly after many draws can we hope to constrain $\\\\alpha_S$. Even within the mixture model approximation, there is still no truth label for individual events. \\nThis is different from, say, distinguishing cats from dogs (or alive cats from dead cats) in an image database. For cats and dogs, even if the distributions overlap, there is a correct answer ($\\\\alpha_S=1$ or $0$ for each event). For particle physics, where the distributions overlap, a particle is both signal {\\\\it and} background.\\nThe second way in which particle physics differs from typical machine learning applications is that particle physics has remarkably accurate simulation tools for producing synthetic data for training. These tools have been developed by experts over more than 40 years. Together, they describe the evolution of a particle collision over 20 orders of magnitude in length. The smallest scale the LHC probes is around $10^{-18}$ m, one thousandth the size of a proton. Here the physics is described by perturbative quantum field theory; particles interact rather weakly and first-principles calculations are accurate. The Higgs boson has a size (Compton wavelength) of $10^{-17}$ m, so it is only at these small distances that we have any hope of examining it. Between $10^{-18}$ m and $10^{-15}$ m, a semi-classical Markov model is used to turn a handful of primordial particles into hundreds of quarks and gluons. Between $10^{-15}$ m and $10^{-6}$ m, the quarks and gluons turn into a zoo of metastable subatomic particles that subsequently decay into hundreds of ``stable\\'\\' particles: pions, protons, neutrons, electrons and photons. These then start interacting with detector components and propagating through the material, as described by other excellent parameterized models. The detector model is accurate from $10^{-6}$ m to the $100$ m size of the LHC detectors (the ATLAS detector at the LHC is 46 meters long). The result is a progression from an order-10 dimensional phase space at the shortest distances, to an order-$10^3$ dimensional phase space at intermediate scales, to an order-$10^8$ dimensional phase space of electronic detector readouts channels. Combined, these simulation tools give a phenomenally robust (but embarrassingly sparse) sampling from this hundred million dimensional space. Around one trillion events have been recorded at the LHC, and a comparable number of events have been simulated, providing hundreds of petabytes of actual and synthetic data to analyze. The first stage of the simulations, up to the stable particle level (the $10^3$ dimensional space) is relatively fast: one million events can be generated in an hour or so on a laptop.\\nThe second stage of the simulation, through the detector, is much slower, taking seconds or even minutes per event. Conveniently, for many applications, the first stage of the simulation is sufficient. \\nNo human being, and as yet no machine, can visualize a hundred million dimensional distribution. So the typical analysis pipeline is to take all of the low-level outputs and aggregate them into a single composite feature, such as the total energy of the particles in some region. Ideally, a histogram of this feature would exhibit a resonance peak or some other salient indication of signal. We additionally want this feature to have a simple physical interpretation, so that we can cross-check the distribution against our intuition. For the Higgs boson, the ``golden discovery channel\\'\\' was two muons (or electrons) and two antimuons (or positrons). A Feynman diagram describing this process looks like\\n\\\\begin{equation}\\n{\\n\\\\parbox{10mm} {\\n\\\\includegraphics[width=0.6\\\\columnwidth,trim = {100 0 00 0}]{hmmmm.pdf}\\n}\\n}\\n\\\\end{equation}\\nThe invariant mass $m=\\\\sqrt{(E_1+E_2+E_3+E_4)^2 - (\\\\vec{p}_1+\\\\vec{p}_2+\\\\vec{p}_3+\\\\vec{p}_4)^2}$, with $E_i$ the energies and $\\\\vec{p}_i$ the momenta,\\nof the four observed particles is a powerful way to discover the signal.\\nFor the Higgs boson signal, the probability density of this feature has a peak at the Higgs boson mass of 125 GeV where the background is very small. Unfortunately, only one in every $10^{13}$ proton collisions will give such a signal. If we do not demand that our signal be background-free, and we also do not demand having any physical interpretation such as we have for a feature like mass, \\nthen we can ask: what feature is the {\\\\it optimal} way of statistically discriminating a signal from its background? Such questions, when supplemented with the enormous amounts of easily produced synthetic data, are ideally suited to modern machine learning methodology.\\n'}},\n",
       "       {'entity_name': 'btagging', 'entity_type': 'analysis_technique', 'description': 'A technique used in particle physics to identify and classify jets originating from bottom quarks, typically by analyzing the decay products and their properties.', 'relevant_passages': {\"\\\\section{Supervised learning}\\nMachine learning (ML) has played a role in particle physics for decades. An emblematic use case is in ``$b$-tagging'': determining whether a given set of particles is associated with a primordial bottom quark. Bottom quarks are around four times heavier than a proton and have properties that help distinguish them from other particles. For example, they tend to travel around half a millimeter away from the collision point before decaying. Technically, the $b$ quark binds with other quarks into metastable hadrons, like the $B_d$ meson, which then decay into particles like muons and pions. One cannot directly measure the distance the particles travel, but by measuring things like the number of decay products, distances among charged tracks, whether there was a muon in the decay, etc., one can accumulate a number of highly correlated features that can be combined to estimate\\nthe probability that there was a $b$ quark involved. Traditionally, the various features might be fed into a shallow neural network or a boosted decision tree to determine a $b$-tagging probability.\\n$b$-tagging is characteristic of how ML has traditionally (and very successfully) been used in particle physics: physically motivated classifiers are first understood individually and then combined using a relatively simple multivariate technique.\\nOver the last several years, this paradigm has been replaced by what I like to call {\\\\it modern} machine learning. The modern approach is to feed raw, minimally-processed data, rather than high-level physically-motivated variables, into a deep neural network. The network is then free to find what it thinks is most valuable in the data. For example, with $b$ tagging, a modern machine learning approach is to put all the measured tracks into a recurrent neural network. The network is then trained using labeled simulated data to distinguish signal events ($b$ quarks) and background events (other quarks). This is in contrast to the traditional approach, where the tracks are connected with curves and distilled down to an impact parameter. While the traditional approach works very well, it might for example obtain a factor of 1000 rejection of background quarks while keeping 50\\\\\\nAs a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collected into bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, 100 or more protons may collide each time the bunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e., has quarks within each proton strike each other with enough energy to produce something of interest, like a Higgs boson (only one in a billion collisions produce a Higgs boson). When there is a direct hit, often called a primary collision, there are other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintegrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritus is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is) and azimuthal angle. Area subtraction recalibrates the event based on the amount energy deposited in some region of the detector where products from the primary collision are believed to be absent. Another method, used extensively by the CMS collaboration, is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so that one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction works only on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary collision point, one for charged particles from the secondary collisions points and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is adapted for a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top quark has a mass, the top jet usually has three subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms noticeably outperform traditional physics-motivated algorithms.\\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{ATLAStoptag}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to around 500,000 parameters for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were identified by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'pileup mitigation techniques with machine learning', 'entity_type': 'analysis_technique', 'description': 'A collection of methods aimed at reducing the effects of pileup in particle collision events, particularly through the use of advanced machine learning algorithms. This includes the PileUp Mitigation with Machine Learning (PUMML) algorithm, which employs convolutional neural networks to reconstruct particle distributions from primary collisions while effectively mitigating pileup effects.', 'relevant_passages': {\"\\\\section{Supervised learning}\\nMachine learning (ML) has played a role in particle physics for decades. An emblematic use case is in ``$b$-tagging'': determining whether a given set of particles is associated with a primordial bottom quark. Bottom quarks are around four times heavier than a proton and have properties that help distinguish them from other particles. For example, they tend to travel around half a millimeter away from the collision point before decaying. Technically, the $b$ quark binds with other quarks into metastable hadrons, like the $B_d$ meson, which then decay into particles like muons and pions. One cannot directly measure the distance the particles travel, but by measuring things like the number of decay products, distances among charged tracks, whether there was a muon in the decay, etc., one can accumulate a number of highly correlated features that can be combined to estimate\\nthe probability that there was a $b$ quark involved. Traditionally, the various features might be fed into a shallow neural network or a boosted decision tree to determine a $b$-tagging probability.\\n$b$-tagging is characteristic of how ML has traditionally (and very successfully) been used in particle physics: physically motivated classifiers are first understood individually and then combined using a relatively simple multivariate technique.\\nOver the last several years, this paradigm has been replaced by what I like to call {\\\\it modern} machine learning. The modern approach is to feed raw, minimally-processed data, rather than high-level physically-motivated variables, into a deep neural network. The network is then free to find what it thinks is most valuable in the data. For example, with $b$ tagging, a modern machine learning approach is to put all the measured tracks into a recurrent neural network. The network is then trained using labeled simulated data to distinguish signal events ($b$ quarks) and background events (other quarks). This is in contrast to the traditional approach, where the tracks are connected with curves and distilled down to an impact parameter. While the traditional approach works very well, it might for example obtain a factor of 1000 rejection of background quarks while keeping 50\\\\\\nAs a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collected into bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, 100 or more protons may collide each time the bunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e., has quarks within each proton strike each other with enough energy to produce something of interest, like a Higgs boson (only one in a billion collisions produce a Higgs boson). When there is a direct hit, often called a primary collision, there are other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintegrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritus is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is) and azimuthal angle. Area subtraction recalibrates the event based on the amount energy deposited in some region of the detector where products from the primary collision are believed to be absent. Another method, used extensively by the CMS collaboration, is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so that one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction works only on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary collision point, one for charged particles from the secondary collisions points and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is adapted for a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top quark has a mass, the top jet usually has three subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms noticeably outperform traditional physics-motivated algorithms.\\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{ATLAStoptag}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to around 500,000 parameters for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were identified by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\", \" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'area subtraction', 'entity_type': 'analysis_technique', 'description': 'A traditional method for pileup removal in particle physics that recalibrates events by measuring the energy deposited in regions of the detector where primary collision products are believed to be absent or not expected.', 'relevant_passages': {\"\\\\section{Supervised learning}\\nMachine learning (ML) has played a role in particle physics for decades. An emblematic use case is in ``$b$-tagging'': determining whether a given set of particles is associated with a primordial bottom quark. Bottom quarks are around four times heavier than a proton and have properties that help distinguish them from other particles. For example, they tend to travel around half a millimeter away from the collision point before decaying. Technically, the $b$ quark binds with other quarks into metastable hadrons, like the $B_d$ meson, which then decay into particles like muons and pions. One cannot directly measure the distance the particles travel, but by measuring things like the number of decay products, distances among charged tracks, whether there was a muon in the decay, etc., one can accumulate a number of highly correlated features that can be combined to estimate\\nthe probability that there was a $b$ quark involved. Traditionally, the various features might be fed into a shallow neural network or a boosted decision tree to determine a $b$-tagging probability.\\n$b$-tagging is characteristic of how ML has traditionally (and very successfully) been used in particle physics: physically motivated classifiers are first understood individually and then combined using a relatively simple multivariate technique.\\nOver the last several years, this paradigm has been replaced by what I like to call {\\\\it modern} machine learning. The modern approach is to feed raw, minimally-processed data, rather than high-level physically-motivated variables, into a deep neural network. The network is then free to find what it thinks is most valuable in the data. For example, with $b$ tagging, a modern machine learning approach is to put all the measured tracks into a recurrent neural network. The network is then trained using labeled simulated data to distinguish signal events ($b$ quarks) and background events (other quarks). This is in contrast to the traditional approach, where the tracks are connected with curves and distilled down to an impact parameter. While the traditional approach works very well, it might for example obtain a factor of 1000 rejection of background quarks while keeping 50\\\\\\nAs a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collected into bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, 100 or more protons may collide each time the bunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e., has quarks within each proton strike each other with enough energy to produce something of interest, like a Higgs boson (only one in a billion collisions produce a Higgs boson). When there is a direct hit, often called a primary collision, there are other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintegrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritus is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is) and azimuthal angle. Area subtraction recalibrates the event based on the amount energy deposited in some region of the detector where products from the primary collision are believed to be absent. Another method, used extensively by the CMS collaboration, is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so that one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction works only on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary collision point, one for charged particles from the secondary collisions points and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is adapted for a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top quark has a mass, the top jet usually has three subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms noticeably outperform traditional physics-motivated algorithms.\\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{ATLAStoptag}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to around 500,000 parameters for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were identified by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\", \" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'charged hadron subtraction', 'entity_type': 'analysis_technique', 'description': 'A technique in particle physics used to identify and eliminate tracks from secondary collisions, allowing for the isolation of primary collision products. This method involves matching tracks of charged particles to distinguish between primary and secondary collisions, effectively filtering out unwanted signals.', 'relevant_passages': {\"\\\\section{Supervised learning}\\nMachine learning (ML) has played a role in particle physics for decades. An emblematic use case is in ``$b$-tagging'': determining whether a given set of particles is associated with a primordial bottom quark. Bottom quarks are around four times heavier than a proton and have properties that help distinguish them from other particles. For example, they tend to travel around half a millimeter away from the collision point before decaying. Technically, the $b$ quark binds with other quarks into metastable hadrons, like the $B_d$ meson, which then decay into particles like muons and pions. One cannot directly measure the distance the particles travel, but by measuring things like the number of decay products, distances among charged tracks, whether there was a muon in the decay, etc., one can accumulate a number of highly correlated features that can be combined to estimate\\nthe probability that there was a $b$ quark involved. Traditionally, the various features might be fed into a shallow neural network or a boosted decision tree to determine a $b$-tagging probability.\\n$b$-tagging is characteristic of how ML has traditionally (and very successfully) been used in particle physics: physically motivated classifiers are first understood individually and then combined using a relatively simple multivariate technique.\\nOver the last several years, this paradigm has been replaced by what I like to call {\\\\it modern} machine learning. The modern approach is to feed raw, minimally-processed data, rather than high-level physically-motivated variables, into a deep neural network. The network is then free to find what it thinks is most valuable in the data. For example, with $b$ tagging, a modern machine learning approach is to put all the measured tracks into a recurrent neural network. The network is then trained using labeled simulated data to distinguish signal events ($b$ quarks) and background events (other quarks). This is in contrast to the traditional approach, where the tracks are connected with curves and distilled down to an impact parameter. While the traditional approach works very well, it might for example obtain a factor of 1000 rejection of background quarks while keeping 50\\\\\\nAs a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collected into bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, 100 or more protons may collide each time the bunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e., has quarks within each proton strike each other with enough energy to produce something of interest, like a Higgs boson (only one in a billion collisions produce a Higgs boson). When there is a direct hit, often called a primary collision, there are other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintegrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritus is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is) and azimuthal angle. Area subtraction recalibrates the event based on the amount energy deposited in some region of the detector where products from the primary collision are believed to be absent. Another method, used extensively by the CMS collaboration, is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so that one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction works only on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary collision point, one for charged particles from the secondary collisions points and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is adapted for a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top quark has a mass, the top jet usually has three subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms noticeably outperform traditional physics-motivated algorithms.\\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{ATLAStoptag}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to around 500,000 parameters for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were identified by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\", \" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'tag and probe method', 'entity_type': 'analysis_technique', 'description': 'A technique in experimental particle physics used to gather truth-labeled samples for calibration by identifying clean events where labeling can be done unambiguously.', 'relevant_passages': {\"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that the modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was no real reason to insure that the correlations are all correct. \\nA commonplace, and often implicit, belief is that that although the correlations in the synthetic data may not be exactly the same as the correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using the synthetic data would be to train on real data. Unfortunately, while the data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. And even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. Thus there are two ways to proceed. First, we can try to to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to get truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the faction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, recall that these simulations have different components. The short-distance simulation, which produces of order 1000 particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely exciting, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. An proof-of-principle implementation of this idea is OminFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then the mapping can be inverted to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up my many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background an some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, were some supervised training is used to guide data-driven estimation technique is a very promising area for future development of ML for particle physics. \\n\", \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was not a strong motivation to ensure that the correlations were all correct. \\nA commonplace, and often implicit, belief is that, although correlations in the synthetic data may not be exactly the same as correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using synthetic data would be to train on real data. Unfortunately, while synthetic data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. Even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. There are two ways to proceed. First, we can try to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to gather truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the fraction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments in probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, as discussed above these simulations have different components. The short-distance simulation, which produces hundreds of particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely appealing, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. A proof-of-principle implementation of this idea is OmniFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then one can try to invert the mapping to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up by many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting (refining the event selection) on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background in some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, where some supervised training is used to guide a data-driven estimation technique, is a very promising area for future development of ML for particle physics. \\n\"}},\n",
       "       {'entity_name': 'generative adversarial network', 'entity_type': 'analysis_technique', 'description': 'A class of machine learning frameworks used to generate new data samples by training two networks: a generator that creates data and a discriminator that evaluates its authenticity.', 'relevant_passages': {\"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that the modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was no real reason to insure that the correlations are all correct. \\nA commonplace, and often implicit, belief is that that although the correlations in the synthetic data may not be exactly the same as the correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using the synthetic data would be to train on real data. Unfortunately, while the data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. And even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. Thus there are two ways to proceed. First, we can try to to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to get truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the faction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, recall that these simulations have different components. The short-distance simulation, which produces of order 1000 particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely exciting, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. An proof-of-principle implementation of this idea is OminFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then the mapping can be inverted to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up my many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background an some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, were some supervised training is used to guide data-driven estimation technique is a very promising area for future development of ML for particle physics. \\n\", \"\\\\section{Data-driven approaches}\\nAll the techniques described above heavily exploit our ability to generate synthetic data sets for training.\\nAlthough the simulations are highly sophisticated, and reproduce the data over 20 orders of magnitude in length scale, they are not engineered to reproduce all of the subtle correlations that modern machine-learning methods might be exploiting. Indeed, until the modern machine learning revolution, there was not a strong motivation to ensure that the correlations were all correct. \\nA commonplace, and often implicit, belief is that, although correlations in the synthetic data may not be exactly the same as correlations in real data, the ML methodology should still work. But, until we know for sure, it is hard to assign uncertainties to the output of the ML algorithms on actual data. An alternative to using synthetic data would be to train on real data. Unfortunately, while synthetic data sets have truth labels for training, because we know how we generated them, real data does not. \\nMoreover, as mentioned in the introduction, there is no actual ground truth in the real world: in physics, each data point is both signal {\\\\it and} background, to some extent. Even when quantum mechanical interference is small (as it often is), the data is at best mixed signal and background, so it is not immediately clear how to use data directly for training. There are two ways to proceed. First, we can try to train the network directly on the real data despite its impurity. Second, we can use ML to determine how well the simulations agree with the data, and then try to improve the simulations. Both approaches have already received some attention in particle physics and are currently being explored using LHC data.\\nAn important observation relevant for training directly on data is that although actual data does not come with labels, it is possible to find particularly clean events where labelling can be done unambiguously. For example, top quarks almost always come in pairs (a top and an anti-top). One can restrict to events where, say, the anti-top decays to a muon and a $b$-jet that are cleanly tagged. Then the rest of the event provides a clean top-quark data point. This tag-and-probe method has been a mainstay of experimental particle physics since well before machine learning, and is a useful way to gather truth-labeled samples for calibration.\\nAnother more machine-learning oriented approach is to train directly on mixed samples. For example, one can use a sample of events with one jet and a $Z$ boson, and another sample with two jets. In these samples, it is expected that the fraction of jets coming from a quark is different from the fraction from a gluon (roughly 80\\\\work surprisingly well, either when trained on high-level classifiers like mass~\\\\citep{Metodiev_2017} or when trained with a image CNN~\\\\citep{Komiske:2018oaa}. Such studies foretell a future in which the simulations can be done away with altogether and the data used directly for both training and validation. \\nThere are a number of fully unsupervised approaches also being developed for applications at the LHC. One example is the JUNIPR framework, which attempts to learn the full differential distribution $\\\\frac{d^n P}{dp_1 \\\\cdots d p_n}$ of the data using machine learning~\\\\citep{Andreassen:2019txo}. JUNIPR has a network architecture scaffolded around a binary jet-clustering tree, similar to the highly effective ``Tree NN'' shown in Fig.~\\\\ref{fig:tops}. Using the tag-and-probe method or weakly supervised learning, one can then train JUNIPR on separate samples to get different probability functions. Doing so lets us go beyond the typical likelihood-free inference approach used in supervised learning applications. For example,\\ncomparing these learned functions can discriminate different samples and find features of interest. Alternatively, a method like JUNIPR can be trained on data and then events can be drawn from the learned probability distributions for data augmentation. Thus, JUNIPR can act like a kind of simulation itself, but with all the elements learned rather than built upon microphysical models. Such methodology could dovetail well with developments in probabilistic programming approaches, as in~\\\\citep{Baydin:2019fap}.\\nContinuing on the line of improving the simulations, as discussed above these simulations have different components. The short-distance simulation, which produces hundreds of particles using quantum field theory is relatively fast (of order microseconds per event),\\nwhile simulating the propagation of these particle through the detector can be significantly slower (of order seconds or minutes per event). Indeed, a significant fraction of all LHC computing time is devoted to running detector simulations. To ameliorate this computing problem, one might turn to an unsupervised learning method like CaloGAN~\\\\citep{Paganini_2018}.\\nCaloGAN uses a generative adversarial network to mimic the detector simulator. \\nWith CaloGAN, a first network produces events and a second adversary network tries to tell if those events are from the real detector or the neural network one. Once trained, the NN simulator can be used at a cost of as little as 12 microseconds per event: a five order-of-magnitude speed up compared to the full simulation. Such approaches are extremely appealing, particularly for higher-luminosity future LHC runs where all the computing resources in the world would not be enough to simulate a sufficient number of events.\\nRather than learning to reproduce and generate events similar to the particle-level simulation (like JUNIPR) or the detector simulator (like CaloGAN), one can instead learn just the places where the simulation is inaccurate. For example, one could train an unsupervised model on the synthetic data and the real data, and then when the two differ reweight synthetic data to look like real data. A proof-of-principle implementation of this idea is OmniFold~\\\\citep{Andreassen:2019cjw}. OmniFold learns the mapping from simulation to data. Then one can try to invert the mapping to effectively remove the effects of the detector simulation. The process of removing detector effects in particle physics is called unfolding. Unfolding is typically a laborious process, done for each observable separately. OmniFold uses ML methods to learn how the detector affects each event, so that {\\\\it any} observable can be unfolded using the same trained network. This could be a game-changer for experimental analyses, speeding them up by many orders of magnitude.\\nFinally, it is worth mentioning one more issue that has received some attention in applying ML methods directly to data. A potential problem with ML methods is that they can be so powerful that cutting (refining the event selection) on a learned classifier can sculpt the background to look like the signal. Such sculpting can be misleading if there is no signal actually present, and it can complicate the extraction of signal events from data. To deal with this, one can train the network to learn not the optimal discriminant but an optimum within the class of discriminants that do not sculpt the background in some undesirable way~\\\\citep{Louppe:2016ylz}. Similarly, finding two uncorrelated observables that together optimize a discrimination task can be useful for data-driven sideband background estimation~\\\\citep{Kasieczka:2020pil}. This kind of hybrid approach, where some supervised training is used to guide a data-driven estimation technique, is a very promising area for future development of ML for particle physics. \\n\"}},\n",
       "       {'entity_name': 'tree nn', 'entity_type': 'analysis_technique', 'description': 'A neural network that organizes particle momenta into a binary tree structure, inspired by the semi-classical branching picture of particle decays, used for classification tasks in particle physics.', 'relevant_passages': {\" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'particle net', 'entity_type': 'analysis_technique', 'description': 'An architecture based on unordered point-cloud representations of particle inputs, developed for computer vision and adapted for use in particle physics analysis.', 'relevant_passages': {\" \\n\\\\newgeometry{bottom=1.5in}\\n\\\\volumeheader{0}{0}{00.000}\\nFor a second example, consider the problem of pileup mitigation. To understand pileup, it is important to first understand the way modern particle colliders work. At the LHC for example, in order to collide a billion protons per second, the particles are collimated into in bunches of around $10^{11}$ protons each, with around 3000 bunches circulating in the LHC tunnel at any given time. At these operating parameters, of order 100 protons may collide each time the brunches pass through each other. Of these 100 collisions, only rarely is one a direct hit, i.e. has quarks within each proton collide with enough energy to produce something of interest, like a Higgs boson (only 1 in a billion collisons produce a Higgs boson). When there is a direct hit, often called a primary collision, there are necessarily other protons colliding too, called secondary collisions. The protons involved in the secondary collisions disintigrate into essentially sprays of relatively low-energy pions that permeate the detectors. This uninteresting detritius is called pileup. Pileup makes it difficult to ascertain the exact energy involved in the primary collision and contaminates nearly every measurement at the LHC.\\nThere are a number of traditional approaches to pileup removal. One popular method called area subtraction \\\\citep{Cacciari:2008gn} exploits the fact that pileup comprises mostly low-energy particles that are nearly isotropically distributed in pseudorapidity (pseudorapidity measures how close to a beam a particle is). Area subtraction measures the amount of energy deposited in some region of the detector where the products from the primary collision are not believed to have gone, and uses this to recalibrate the event. Another method, used extensively by the CMS collaboration is called charged hadron subtraction~\\\\citep{CMS:2014ata}. This method uses the fact that charged particles leave tracks, so one can match the tracks to either the primary collision or a secondary collision. The ones that come from the secondary collision are then removed from the event. Both of these methods are effective but rather coarse: area-subtraction only works on average, and charge hadron subtraction cannot account for the neutral particles. Neither method attempts to locate all the pileup radiation in each individual event. \\nA modern machine learning approach to pileup removal is the PUMML (PileUp Mitigation with Machine Learning) algorithm~\\\\citep{Komiske:2017ubm}. PUMML builds upon the idea of a event image: the energy deposited into a given region of the detector is translated into the intensity of a pixel in an image~\\\\citep{Cogan:2014oua}. For PUMML, three images are constructed: one for charged particles from the primary vertex, one for charge particles from the second vertices and a third from the neutral particles. These three images are fed into a convolutional neural network (CNN) which attempts to regress out a fourth image showing neutral particles from the primary interaction only. The algorithm can be trained on synthetic data, where truth information about the origin of the neutral particles is known, and then applied to real data where the truth is not known. A sketch of the algorithm is shown in Fig.~\\\\ref{fig:netarch}. The PUMML algorithm is extraordinarily effective: it succeeds in reconstructing the full distribution of particles from the primary collision on an event-by-event basis independently of the number of synchronous secondary collisions. Although it foregoes some physics knowledge (like the isotropy of pileup radiation exploited by area subtraction), this modern machine learning approach reaps enormous gains in efficacy. \\nWe have seen how a recurrent neutral architecture, originally developed for natural language processing, was useful in $b$-tagging. We have also seen how a convolutional neural network, developed for image recognition, was useful for pileup removal. In a sense, a large part of what has been done so far in supervised learning in particle physics can be characterized as a series of similar exercises: a ML technique developed for an entirely different purpose is shoehorned into a particle physics application. A fair comparison of a variety of these approaches was recently made for the problem of boosted top-tagging. The top quark is the heaviest known quark. When it is produced with energy much in excess of its rest mass, as it commonly is at the LHC, it will decay to a collimated beam of particles, a ``jet'', that is difficult to distinguish from a collimated beam of particles {\\\\it not} coming from a top-quark decay. Indeed, there can be 10,000 times more of these background jets then there are top jets. \\nThe traditional approach to distinguishing top jets from background jets focuses on physically-motivated distinctions: the top-quark has a mass, the top jet usually has 3 subjets, corresponding to the three light quarks into which a top quark decays, etc.~\\\\citep{Kaplan:2008ie}. The modern machine learning approach is to throw the kitchen sink into a neural network and hope it works. \\nFig.~\\\\ref{fig:tops} shows a comparison between traditional approaches (points) and modern machine learning approaches (curves). At the time, the traditional approach was a game-changing advance in particle physics. Before that, people did not even hope to find tops at these energies. Nevertheless, we can see from this figure that the modern machine learning algorithms blow traditional physics-motivated algorithms out of the water. \\nIn more detail, the curve labeled ``Deep Network'', modeled after ~\\\\citep{alvarez2019performance}, takes as input the momenta components of the particles (up to 600 inputs) and passes them through a four-layer densely-connected feed-forward neural network. The ``1D Particle CNN'' network uses similar inputs but pipes them through a 1-dimensional convolutional network. This more sophisticated network architecture leads to noticeable improvement. The ``2D Image CNN'' curve uses an image-based convolutional network, where the input image pixel intensity is proportional to the the energy deposited in a region of the detector~\\\\citep{Kasieczka_2017}, as in the pileup example discussed earlier. The ``ResNeXt Image CNN'' curve uses the much more sophisticated ResNeXt convolutional network architecture~\\\\citep{xie2016aggregated}. While there is performance gain, it is at the expense of 1.5 million trainable network parameters (in contrast to of order 500,000 for the CNN). The ResNeXt curve is unstable at small signal efficiency due to statistical limitations of the testing samples. \\nThe ``Tree NN'' curve uses a little more physics input: it organizes the particles' momenta into a binary tree similar to the semi-classical branching picture of sequential particle decays. With only 34,000 parameters, this network performs comparably to the highly-engineered ResNeXt one, giving hope that physical insight may not entirely be disposable. Finally, the curve labeled ``Point Cloud'' uses unordered point-cloud type representation of the inputs called Particle Net~\\\\citep{Qu_2020}; the architecture is based on a Dynamic Graph CNN developed for computer vision~\\\\citep{DGCNN}. More details of all of these algorithms, and some others, can be found in~\\\\citep{Kasieczka:2019dbj}. The superior performance of these neural networks over traditional classifiers makes it clear that modern machine learning methods are finding many more differences between signal and background than were discovered by physical reasoning. It remains an open question whether any of these differences can be understood and explained in some simple human-interpretable way. I will return to this question in Section \\\\ref{sec:outlook}.\\n\"}},\n",
       "       {'entity_name': 'statistical fluctuation', 'entity_type': 'statistics_concept', 'description': 'Variations in data that occur due to random chance, which can lead to misleading interpretations of statistical significance in experimental results.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\nSearching for physics beyond the Standard Model is one of the most important aspects of the physics program at the Large Hadron Collider (LHC). Since the start of proton-proton collisions at the LHC in 2011, the ATLAS~\\\\cite{ATLAS} and CMS~\\\\cite{CMS} Collaborations have derived stringent bounds on a range of new physics signatures, pushing the allowed mass range for many postulated new particles far into the TeV scale. While it is possible that these particles have yet to be observed because they are too heavy to be produced at the LHC, or have to small cross section to be detected with the current data size, it could also be that new particles are kinematically accessible and produced at observable rates, but our current methods of detection prevent their discovery.\\nSearches for new physics processes at particle colliders are usually performed as \\\\textit{blind searches}. Such searches proceed by defining a region of interest in the parameter space, using simulated data of the signal and the Standard Model background processes in order to enhance the data purity. The data is only looked at in the very end where it is tested for the presence of signal through a simultaneous fit of the signal and background probability distributions, hoping to extract a non-zero signal component.\\nHundreds of such searches have been performed for hundreds of different potential new particles, but thus far none have been discovered. Despite this, there are still regions of the data that have not yet been probed for the presence of a signal. This has led to an increased interest in more \\\\textit{model-agnostic} search strategies. Model-independent searches is nothing new in high energy particle physics, and strategies relying less on a signal hypothesis have been devised and utilized~\\\\cite{D0:2000vuh,H1:2008aak,H1:2004rlm,Cranmer:823591,CDF:2007iou,CDF:2007ykt,CDF:2008voc,CMS-PAS-EXO-14-016,CMS-PAS-EXO-10-021,CMS-PAS-EXO-19-008,CMS:2020zjg,ATLAS:2018zdn,ATLAS-CONF-2014-006,ATLAS-CONF-2012-107,ATLAS:2020iwa}.\\nThese mainly take advantage of Monte Carlo simulation, and use this to compare distributions in the observed data to simulation across several observables and many histogram bins. The drawback of this methodology is that one needs to rely on accurate simulation, and also that, due to the vast size of the parameter space being searched, an observation that appears statistically significant could potentially be the result of a statistical fluctuation.\\nIn the following, we discuss machine learning techniques which mitigate some of these challenges and have the potential to improve and extend model-independent searches.\\n'}},\n",
       "       {'entity_name': 'normalizing flows', 'entity_type': 'analysis_technique', 'description': 'A class of generative models that learn to transform a simple probability distribution into a more complex distribution resembling the target data distribution, enabling tasks such as density estimation and likelihood computation.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Overdensity estimation}\\nIn order to train the most powerful ML-based classifier to discriminate signal from background, one would ideally train a network in a supervised manner with labeled data. This relies on a signal hypothesis that is chosen a-priori. An early attempt at discriminating background from \"everything else\" in order to obtain some degree of model-independence, was demonstrated in Ref.~\\\\citen{antiqcd}. Targeting searches for new physics in hadronic final states, a classifier was trained to discriminate QCD jets from various potential signal jets using Monte Carlo simulation. The disadvantage of such an approach is the dependence on signal simulation and which signals are to be included in the training.\\nAlthough simulated particle physics data is highly accurate over several orders of magnitude in length scale, simulation is known to not fully accurately reproduce collider data and this disagreement affects the tagging performance. Using weakly- or self-supervised (see Section~\\\\ref{sec:selfsupervised}) methods, algorithms can be trained directly on the data itself which has the added benefit of not having to derive transfer factors when training on synthetic data and testing on real data.\\n\\\\subsubsection{Weakly supervised methods}\\nIn weakly supervised learning, impure or noisy data sources can be used to label signal and background data in such a way that models can be trained in a supervised manner. Such methods can be utilized for anomaly detection when the signal is unknown, but there exist datasets where both signal and background are expected to be present in some relative fraction. This can be achieved by placing weak assumptions on the signal and background processes using domain knowledge.\\nThe goal of the weakly supervised methods we will discuss here, is to learn an approximation of the likelihood ratio $R(x)$ between the underlying probability densities of background $p_\\\\text{bg}(x)$ and data (possibly including signal) $p_\\\\text{data}(x)$, as a function of some input variables $x$:\\n\\\\begin{equation} \\nR(x)=\\\\frac{p_\\\\text{data}(x)}{p_\\\\text{bg}(x)}.\\n\\\\end{equation}\\nThis likelihood ratio, if it could be learned exactly, would be the most powerful model-agnostic anomaly detector, as given \\n\\\\begin{equation}\\np_\\\\text{data}(x)=(1-\\\\epsilon)p_\\\\text{bg}(x)+\\\\epsilon p_\\\\text{sig}(x),\\n\\\\end{equation}\\nwhere $p_\\\\text{sig}(x)$ is the probability density of signal, it would be monotonically related to the signal-to-background LR for any signal present in the data. A strategy for learning a good approximation of the likelihood ratio $R(x)$, is to train a classifier between data from a signal enriched region and samples drawn from a (fully data-driven) background model. If the background model is accurate and the classifier is well-trained, this approaches the likelihood ratio $R(x)$ by the Neyman-Pearson Lemma~\\\\cite{neyman1933ix}.\\nHence, the aim is to test whether the signal region data contains a combination of signal and background data. In the event that there are signal events present in the signal region, the classifier can differentiate between the signal region data and the background template. The true signal events are expected to have higher classification scores than the true background data. A cut on this classifier score can then be used to enhance the significance of signal events, making it a useful anomaly detection metric.\\nIn Ref.~\\\\citen{Dery2017WeaklySC} a method referred to as Learning from Label Proportions~\\\\cite{LLP} was utilized to discriminate between quarks and gluons using impure data samples. Despite not having access to the per-instance labels, the class proportions could be derived using domain knowledge. A supervised task was then defined using the class proportions themselves as the target, although operating the algorithm at a per-instance level. This concept has been extended in in the Classification WithOut Labels (CWola)~\\\\cite{cwola} framework. In this setup, the class proportions themselves do not need to be known, and it is enough to have two datasets at hand with an unequal fraction of signal instances in each set. A standard classifier can then be trained to discriminate between the two mixed datasets, and this can be shown to be the optimal classifier to discriminate between signal and background instances. The larger the difference in signal fraction between is dataset, the better the classifier becomes. The challenge is being able to design such mixed datasets, especially for a model-independent setup.\\nThe CWola strategy has been demonstrated and deployed for various model-independent search setups. In Ref.~\\\\citen{cwolabumphunt}, the authors introduce the \\\\textit{CWola bumphunt}. In this setup, one attempts to look for new, heavy generic particles that resonate around the particle mass in the dijet invariant mass spectrum. Starting from the weak assumption that this is a localized, narrow resonance, two mixed samples are created in the following way: The region in the dijet invariant mass close to the particle mass is defined as the signal-enriched mixed sample, and the regions next to it are defined as background-enriched regions. This is illustrated in Fig.~\\\\ref{fig:fig1}. In this way, the dijet invariant mass sideband regions serve as the background samples; these can serve as a good model for the background if the input features are statistically independent from the dijet invariant mass. If there is a signal present in the signal-like region around the particle mass, the classifier learns to identify it, while in the absence of a signal the classifier will likely learn random noise as there would be no difference between the two groups of events. It is crucial that the features being used for classification are not correlated with the dijet invariant mass. Otherwise, the classifier will be able to differentiate background events in the signal region from those in the adjacent dijet invariant mass regions used as the background-enriched mixed sample. Background events within the signal region will then be classified as signal-like, which can introduce artificial sculpting of the dijet invariant mass distribution. Note that the above strategy only works for narrow resonances, if there is a significant amount of signal in both datasets, as would be the case for a broad resonance, the classification performance is reduced. This method was used to analyze data collected by the ATLAS experiment in the search for generic new heavy resonances decaying into jets in Ref.~\\\\citen{ATLAS:2020iwa}, a first of its kind using weak supervision for model-agnostic searches. The power of this analysis can be seen in Figure~\\\\ref{fig:atlascwola}. This plot shows 95\\\\\\nThis methodology can also be applied in other setups than for a dijet bumphunt. In Ref.~\\\\citen{cwola_monojet}, model-agnostic learning using the CWola method is harnessed in order to improve the sensitivity of searches for new physics models with anomalous jet dynamics and a mono-jet signature. Focusing on cases where a heavy new particle decays into two jets which hadronize partially in the dark sector (making them \\\\textit{semi-visible jets}), and where one of the jets become completely invisible and the other partially visible, anomaly detection is utilized to detect the semi-visible anomalous jet. The degree of visibility of this jet can vary, making it difficult to train a supervised algorithm for each visibility fraction. Rather, CWola is deployed to train a generic anomalous jet identification classifier. The dominant background for a mono jet search is the electroweak production of vector bosons and jets, where the vector boson further decays to neutrinos, $Z(\\\\nu\\\\nu)$+jets. The experimental signature is missing transverse energy and a jet, mimicking the signal signature. Taking advantage of the fact that the vector boson also can decay visibly into two leptons and in these cases the jet remains the same, a background enriched control CWola sample can be defined using $Z(\\\\ell\\\\ell)$+jets events. None of the signal should be present in events with a di-lepton and jet signature. A model-independent anomalous jet tagger is then trained supervised to discriminate between jets coming from a $(\\\\ell\\\\ell)+jet$ and a $(\\\\nu\\\\nu)$+jet sample. If the monojet signature is present, CWola guarantees that the best algorithm trained to distinguish between these two regions, is also the best algorithm to discriminate between a normal SM jet from the V+jet background, and a semi-visible anomalous jet.\\nThis illustrates how generally CWola can be used. The only requirement is that one is able to define regions of the data depleted and enriched in signal, and that the signal and background events are statistically equal in the two regions. In terms of model independence, some degree of signal assumption is needed in order to define appropriate mixed samples.\\nMethods can also be used to bootstrap CWola and further improve the classification performance. In \\\\citen{Amram:2020ykb}, a powerful and model-independent anomalous jet tagger is defined starting from the CWola hunting methodology, but defining the mixed samples for training differently. Targeting signals where both jets in the event are anomalous, the key idea is that for a resonance decaying to a pair of anomalous jets, one can use an initial self- or supervised classifier (like an autoencoder, see Section~\\\\ref{sec:selfsupervised}) \\nto tag an event as signal-like or background-like using one jet and then use that information to construct samples for training a classifier using the other jet with weak supervision. By using an autoencoder as an initial classifier, one can group events into a signal-like and background-like sample based on the anomaly score on one of the jets (assuming that if the one jet is anomalous, the other must be too). A classifier can then be trained for the other jet that has not been tagged, where the mixed samples are defined based on the anomaly score of the tagged jet.\\nAnother weakly-supervised method for over-density detection is ANODE~\\\\cite{nachman2020anode}. In ANODE, conditional neural density estimation is used in order to interpolate probability densities from a data sideband into the data signal region. This interpolation is used and compared to the probability density of the actual data observed in the signal region, and used to construct a likelihood ratio as in Eq.~\\\\ref{eq:weak-supervision-likelihood-ratio}. This implies having to learn both the interpolated likelihood of the background in the signal region, as well as the likelihood for data in the signal region (see Fig.~\\\\ref{fig:fig1}). An improvement on this method is CATHODE (Classifying Anomalies Through Outer Density Estimation)~\\\\cite{Hallin:2021wme}. In CATHODE, rather than directly constructing the likelihood ratio, one rather samples events from the trained background estimator after it is interpolated into the signal region. This avoids having to learn the likelihood of data in the signal region. Then, a classifier is trained to discriminate data in the signal region from the data samples from the interpolated density estimator. This algorithm was first demonstrated for searches for heavy particles decaying into two jets. CATHODE proceeds by first training a conditional normalizing flow~\\\\cite{rezende2016variational} on the dijet invariant mass sidebands and then interpolating this into the signal region; samples from this flow are used as the background model and should correctly take into account any correlations between the input features and the dijet invariant mass.\\nNormalizing flows are a type of generative model that learn to transform a simple probability distribution (usually a standard Gaussian distribution) into a more complex distribution that resembles the target distribution of the data. This is achieved by defining a sequence of invertible transformations that map samples from the simple distribution to samples from the target distribution. The resulting model can be used to generate new data samples, perform density estimation, and compute likelihoods. Invertibility is important, as it ensures that the transformation has a well-defined inverse, which is needed for density estimation and likelihood computation. The key challenge in designing normalizing flows is to ensure that the resulting distribution is both complex enough to capture the target distribution and easy to work with, in the sense that likelihood computation and sampling are efficient. Recent work has focused on designing more expressive and flexible transformations, such as coupling layers, which allow for the transformation to depend on only a subset of the input variables~\\\\cite{dinh2017realnvp}. CATHODE utilizes such a normalizing flow to estimate the background density, conditioned on the dijet invariant mass. The density can then be interpolated into the dijet invariant mass signal region, while accounting for all correlations between the input features. Finally, a classifier is trained to distinguish between the artificially generated background samples from the normalizing flow (trained in data sidebands) and actual samples from the data signal region, yielding an estimate of the likelihood ratio as an anomaly metric (following the CWola paradigm).\\nA similar method is CURTAINS~\\\\cite{Raine:2022hht}. This method also takes advantage of a conditioned invertible neural network to learn the distribution of background events in a sideband and then use that to transform datapoints to those of the target distribution in the signal region. CURTAINs use an optimal transport loss to train the network to minimize the distance between the model output and the target data. The goal is to approximate the optimal transport function between two points in feature space when moving along the resonant spectrum. As a result, instead of generating new samples to create a background template, CURTAINs transforms the data in the side-bands to equivalent data points with a mass in the signal region. This approach eliminates the need to match data encodings to an intermediate prior distribution, which is the case of CATHODE, as it can lead to mismodelling of underlying correlations between the observables in the data if the trained posterior is not in perfect agreement with the prior distribution. Additionally, CURTAINs can also be employed to transform side-band data into validation regions, rather than simply constructing the background template in the signal region, making the algorithm easier to validate and test. Once the CURTAINs density estimation algorithm has been trained, a similar approach as in CATHODE is taken. Specifically, the transformed data (from sideband to signal region) is assumed to represent a sample of pure background events, while the signal region data represents a mixture of signal and background. A CWola classifier is trained to discriminate between the two datasets based on this assumption. \\nIn Ref.~\\\\citen{klein2022flows,curtains2}, an improvement of the CURTAINs technique is introduced, where a maximum likelihood estimation is used instead of an optimal transport loss. This improves the fidelity of the transformed data and is significantly faster and easier to train.\\nMore recently, diffusion models~\\\\cite{10.5555/3045118.3045358}, emerging as potent tools for high-dimensional density estimation, have been explored both for overdensity estimation~\\\\cite{sengupta2023improving} and for outlier detection~\\\\cite{mikuni2023highdimensional}.\\nThere are also weakly supervised methods that take advantage of simulation in the training of density estimators. In Simulation Assisted Likelihood-free Anomaly Detection (SALAD), a reweighting function for reweighting simulation to match data in the data sidebands is trained. This (parametrized) reweighting function is then interpolated into the signal region. Finally, a classifier to discriminate between the two is trained to get the likelihood ratio. Another simulation-assisted technique is Flow-enhanced transportation for anomaly detection (FETA)~\\\\cite{feta}, a mixture of SALAD and CURTAINS. A normalizing flow is trained in the sideband to map MC simulation to data. This learned flow is then applied to simulation in the signal region to obtain an approximation of the background.\\nThere are caveats when deploying weakly supervised methods.\\nAsymptotically, a weakly supervised classifier will converge to the performance of a fully supervised one. But in practice, performance typically degrades with smaller samples sizes available for training and lower fractions of signal events in the data sample. However, one can still obtain signal versus background classifiers with reasonable performance even with signal fractions well below 1\\\\\\n'}},\n",
       "       {'entity_name': 'bootstrap method', 'entity_type': 'analysis_technique', 'description': 'A resampling technique used to estimate the distribution of a statistic by repeatedly sampling with replacement from the data, allowing for the assessment of variability and confidence intervals.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Overdensity estimation}\\nIn order to train the most powerful ML-based classifier to discriminate signal from background, one would ideally train a network in a supervised manner with labeled data. This relies on a signal hypothesis that is chosen a-priori. An early attempt at discriminating background from \"everything else\" in order to obtain some degree of model-independence, was demonstrated in Ref.~\\\\citen{antiqcd}. Targeting searches for new physics in hadronic final states, a classifier was trained to discriminate QCD jets from various potential signal jets using Monte Carlo simulation. The disadvantage of such an approach is the dependence on signal simulation and which signals are to be included in the training.\\nAlthough simulated particle physics data is highly accurate over several orders of magnitude in length scale, simulation is known to not fully accurately reproduce collider data and this disagreement affects the tagging performance. Using weakly- or self-supervised (see Section~\\\\ref{sec:selfsupervised}) methods, algorithms can be trained directly on the data itself which has the added benefit of not having to derive transfer factors when training on synthetic data and testing on real data.\\n\\\\subsubsection{Weakly supervised methods}\\nIn weakly supervised learning, impure or noisy data sources can be used to label signal and background data in such a way that models can be trained in a supervised manner. Such methods can be utilized for anomaly detection when the signal is unknown, but there exist datasets where both signal and background are expected to be present in some relative fraction. This can be achieved by placing weak assumptions on the signal and background processes using domain knowledge.\\nThe goal of the weakly supervised methods we will discuss here, is to learn an approximation of the likelihood ratio $R(x)$ between the underlying probability densities of background $p_\\\\text{bg}(x)$ and data (possibly including signal) $p_\\\\text{data}(x)$, as a function of some input variables $x$:\\n\\\\begin{equation} \\nR(x)=\\\\frac{p_\\\\text{data}(x)}{p_\\\\text{bg}(x)}.\\n\\\\end{equation}\\nThis likelihood ratio, if it could be learned exactly, would be the most powerful model-agnostic anomaly detector, as given \\n\\\\begin{equation}\\np_\\\\text{data}(x)=(1-\\\\epsilon)p_\\\\text{bg}(x)+\\\\epsilon p_\\\\text{sig}(x),\\n\\\\end{equation}\\nwhere $p_\\\\text{sig}(x)$ is the probability density of signal, it would be monotonically related to the signal-to-background LR for any signal present in the data. A strategy for learning a good approximation of the likelihood ratio $R(x)$, is to train a classifier between data from a signal enriched region and samples drawn from a (fully data-driven) background model. If the background model is accurate and the classifier is well-trained, this approaches the likelihood ratio $R(x)$ by the Neyman-Pearson Lemma~\\\\cite{neyman1933ix}.\\nHence, the aim is to test whether the signal region data contains a combination of signal and background data. In the event that there are signal events present in the signal region, the classifier can differentiate between the signal region data and the background template. The true signal events are expected to have higher classification scores than the true background data. A cut on this classifier score can then be used to enhance the significance of signal events, making it a useful anomaly detection metric.\\nIn Ref.~\\\\citen{Dery2017WeaklySC} a method referred to as Learning from Label Proportions~\\\\cite{LLP} was utilized to discriminate between quarks and gluons using impure data samples. Despite not having access to the per-instance labels, the class proportions could be derived using domain knowledge. A supervised task was then defined using the class proportions themselves as the target, although operating the algorithm at a per-instance level. This concept has been extended in in the Classification WithOut Labels (CWola)~\\\\cite{cwola} framework. In this setup, the class proportions themselves do not need to be known, and it is enough to have two datasets at hand with an unequal fraction of signal instances in each set. A standard classifier can then be trained to discriminate between the two mixed datasets, and this can be shown to be the optimal classifier to discriminate between signal and background instances. The larger the difference in signal fraction between is dataset, the better the classifier becomes. The challenge is being able to design such mixed datasets, especially for a model-independent setup.\\nThe CWola strategy has been demonstrated and deployed for various model-independent search setups. In Ref.~\\\\citen{cwolabumphunt}, the authors introduce the \\\\textit{CWola bumphunt}. In this setup, one attempts to look for new, heavy generic particles that resonate around the particle mass in the dijet invariant mass spectrum. Starting from the weak assumption that this is a localized, narrow resonance, two mixed samples are created in the following way: The region in the dijet invariant mass close to the particle mass is defined as the signal-enriched mixed sample, and the regions next to it are defined as background-enriched regions. This is illustrated in Fig.~\\\\ref{fig:fig1}. In this way, the dijet invariant mass sideband regions serve as the background samples; these can serve as a good model for the background if the input features are statistically independent from the dijet invariant mass. If there is a signal present in the signal-like region around the particle mass, the classifier learns to identify it, while in the absence of a signal the classifier will likely learn random noise as there would be no difference between the two groups of events. It is crucial that the features being used for classification are not correlated with the dijet invariant mass. Otherwise, the classifier will be able to differentiate background events in the signal region from those in the adjacent dijet invariant mass regions used as the background-enriched mixed sample. Background events within the signal region will then be classified as signal-like, which can introduce artificial sculpting of the dijet invariant mass distribution. Note that the above strategy only works for narrow resonances, if there is a significant amount of signal in both datasets, as would be the case for a broad resonance, the classification performance is reduced. This method was used to analyze data collected by the ATLAS experiment in the search for generic new heavy resonances decaying into jets in Ref.~\\\\citen{ATLAS:2020iwa}, a first of its kind using weak supervision for model-agnostic searches. The power of this analysis can be seen in Figure~\\\\ref{fig:atlascwola}. This plot shows 95\\\\\\nThis methodology can also be applied in other setups than for a dijet bumphunt. In Ref.~\\\\citen{cwola_monojet}, model-agnostic learning using the CWola method is harnessed in order to improve the sensitivity of searches for new physics models with anomalous jet dynamics and a mono-jet signature. Focusing on cases where a heavy new particle decays into two jets which hadronize partially in the dark sector (making them \\\\textit{semi-visible jets}), and where one of the jets become completely invisible and the other partially visible, anomaly detection is utilized to detect the semi-visible anomalous jet. The degree of visibility of this jet can vary, making it difficult to train a supervised algorithm for each visibility fraction. Rather, CWola is deployed to train a generic anomalous jet identification classifier. The dominant background for a mono jet search is the electroweak production of vector bosons and jets, where the vector boson further decays to neutrinos, $Z(\\\\nu\\\\nu)$+jets. The experimental signature is missing transverse energy and a jet, mimicking the signal signature. Taking advantage of the fact that the vector boson also can decay visibly into two leptons and in these cases the jet remains the same, a background enriched control CWola sample can be defined using $Z(\\\\ell\\\\ell)$+jets events. None of the signal should be present in events with a di-lepton and jet signature. A model-independent anomalous jet tagger is then trained supervised to discriminate between jets coming from a $(\\\\ell\\\\ell)+jet$ and a $(\\\\nu\\\\nu)$+jet sample. If the monojet signature is present, CWola guarantees that the best algorithm trained to distinguish between these two regions, is also the best algorithm to discriminate between a normal SM jet from the V+jet background, and a semi-visible anomalous jet.\\nThis illustrates how generally CWola can be used. The only requirement is that one is able to define regions of the data depleted and enriched in signal, and that the signal and background events are statistically equal in the two regions. In terms of model independence, some degree of signal assumption is needed in order to define appropriate mixed samples.\\nMethods can also be used to bootstrap CWola and further improve the classification performance. In \\\\citen{Amram:2020ykb}, a powerful and model-independent anomalous jet tagger is defined starting from the CWola hunting methodology, but defining the mixed samples for training differently. Targeting signals where both jets in the event are anomalous, the key idea is that for a resonance decaying to a pair of anomalous jets, one can use an initial self- or supervised classifier (like an autoencoder, see Section~\\\\ref{sec:selfsupervised}) \\nto tag an event as signal-like or background-like using one jet and then use that information to construct samples for training a classifier using the other jet with weak supervision. By using an autoencoder as an initial classifier, one can group events into a signal-like and background-like sample based on the anomaly score on one of the jets (assuming that if the one jet is anomalous, the other must be too). A classifier can then be trained for the other jet that has not been tagged, where the mixed samples are defined based on the anomaly score of the tagged jet.\\nAnother weakly-supervised method for over-density detection is ANODE~\\\\cite{nachman2020anode}. In ANODE, conditional neural density estimation is used in order to interpolate probability densities from a data sideband into the data signal region. This interpolation is used and compared to the probability density of the actual data observed in the signal region, and used to construct a likelihood ratio as in Eq.~\\\\ref{eq:weak-supervision-likelihood-ratio}. This implies having to learn both the interpolated likelihood of the background in the signal region, as well as the likelihood for data in the signal region (see Fig.~\\\\ref{fig:fig1}). An improvement on this method is CATHODE (Classifying Anomalies Through Outer Density Estimation)~\\\\cite{Hallin:2021wme}. In CATHODE, rather than directly constructing the likelihood ratio, one rather samples events from the trained background estimator after it is interpolated into the signal region. This avoids having to learn the likelihood of data in the signal region. Then, a classifier is trained to discriminate data in the signal region from the data samples from the interpolated density estimator. This algorithm was first demonstrated for searches for heavy particles decaying into two jets. CATHODE proceeds by first training a conditional normalizing flow~\\\\cite{rezende2016variational} on the dijet invariant mass sidebands and then interpolating this into the signal region; samples from this flow are used as the background model and should correctly take into account any correlations between the input features and the dijet invariant mass.\\nNormalizing flows are a type of generative model that learn to transform a simple probability distribution (usually a standard Gaussian distribution) into a more complex distribution that resembles the target distribution of the data. This is achieved by defining a sequence of invertible transformations that map samples from the simple distribution to samples from the target distribution. The resulting model can be used to generate new data samples, perform density estimation, and compute likelihoods. Invertibility is important, as it ensures that the transformation has a well-defined inverse, which is needed for density estimation and likelihood computation. The key challenge in designing normalizing flows is to ensure that the resulting distribution is both complex enough to capture the target distribution and easy to work with, in the sense that likelihood computation and sampling are efficient. Recent work has focused on designing more expressive and flexible transformations, such as coupling layers, which allow for the transformation to depend on only a subset of the input variables~\\\\cite{dinh2017realnvp}. CATHODE utilizes such a normalizing flow to estimate the background density, conditioned on the dijet invariant mass. The density can then be interpolated into the dijet invariant mass signal region, while accounting for all correlations between the input features. Finally, a classifier is trained to distinguish between the artificially generated background samples from the normalizing flow (trained in data sidebands) and actual samples from the data signal region, yielding an estimate of the likelihood ratio as an anomaly metric (following the CWola paradigm).\\nA similar method is CURTAINS~\\\\cite{Raine:2022hht}. This method also takes advantage of a conditioned invertible neural network to learn the distribution of background events in a sideband and then use that to transform datapoints to those of the target distribution in the signal region. CURTAINs use an optimal transport loss to train the network to minimize the distance between the model output and the target data. The goal is to approximate the optimal transport function between two points in feature space when moving along the resonant spectrum. As a result, instead of generating new samples to create a background template, CURTAINs transforms the data in the side-bands to equivalent data points with a mass in the signal region. This approach eliminates the need to match data encodings to an intermediate prior distribution, which is the case of CATHODE, as it can lead to mismodelling of underlying correlations between the observables in the data if the trained posterior is not in perfect agreement with the prior distribution. Additionally, CURTAINs can also be employed to transform side-band data into validation regions, rather than simply constructing the background template in the signal region, making the algorithm easier to validate and test. Once the CURTAINs density estimation algorithm has been trained, a similar approach as in CATHODE is taken. Specifically, the transformed data (from sideband to signal region) is assumed to represent a sample of pure background events, while the signal region data represents a mixture of signal and background. A CWola classifier is trained to discriminate between the two datasets based on this assumption. \\nIn Ref.~\\\\citen{klein2022flows,curtains2}, an improvement of the CURTAINs technique is introduced, where a maximum likelihood estimation is used instead of an optimal transport loss. This improves the fidelity of the transformed data and is significantly faster and easier to train.\\nMore recently, diffusion models~\\\\cite{10.5555/3045118.3045358}, emerging as potent tools for high-dimensional density estimation, have been explored both for overdensity estimation~\\\\cite{sengupta2023improving} and for outlier detection~\\\\cite{mikuni2023highdimensional}.\\nThere are also weakly supervised methods that take advantage of simulation in the training of density estimators. In Simulation Assisted Likelihood-free Anomaly Detection (SALAD), a reweighting function for reweighting simulation to match data in the data sidebands is trained. This (parametrized) reweighting function is then interpolated into the signal region. Finally, a classifier to discriminate between the two is trained to get the likelihood ratio. Another simulation-assisted technique is Flow-enhanced transportation for anomaly detection (FETA)~\\\\cite{feta}, a mixture of SALAD and CURTAINS. A normalizing flow is trained in the sideband to map MC simulation to data. This learned flow is then applied to simulation in the signal region to obtain an approximation of the background.\\nThere are caveats when deploying weakly supervised methods.\\nAsymptotically, a weakly supervised classifier will converge to the performance of a fully supervised one. But in practice, performance typically degrades with smaller samples sizes available for training and lower fractions of signal events in the data sample. However, one can still obtain signal versus background classifiers with reasonable performance even with signal fractions well below 1\\\\\\n'}},\n",
       "       {'entity_name': 'autoencoders', 'entity_type': 'analysis_technique', 'description': 'A class of neural networks used for unsupervised learning that aim to reconstruct input data by compressing it into a lower-dimensional latent space and then reconstructing it back to the original space. This includes traditional autoencoders as well as quantum autoencoders, which utilize quantum circuits for data compression and can be applied to tasks such as anomaly detection in high-energy physics.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Outlier detection}\\nThe above methods focus on detecting new physics as overdensities in very specific regions of the kinematic phase space; this paradigm is similar to a traditional bump hunt, often performed in HEP searches for novel particles. However, new physics signatures are equally likely to manifest themselves as unexpected events in the tail of distributions. This type of events may be identified using out-of-distribution detection algorithms. The prime example of such an algorithm is the auto-encoder\\\\cite{lecun1987phd, ballard1987modular, hinton1993autoencoders}, which is especially popular in high-energy physics applications\\\\cite{Radovic:2018dip, albertsson2019machine, Jawahar:2021vyu, tsan2021particle, Finke_2021, Laguarta:2023evo, Vaslin:2023lig, Anzalone:2023ugq, Bohm:2023ihd}.\\n\\\\subsubsection{Self-supervised methods}\\nSelf-supervised learning\\\\cite{balestriero2023cookbook} is a form of unsupervised learning where the data provides the supervision. In general, a part of the data is initially withheld from the model and the task of the network is to reproduce this data. Consequently, the network learns a meaningful representation of the data to solve this problem. The self-supervised learning workflow usually involves two stages: first, generating a set of supervisory signals from the input data; and second, employing these signals for a supervised~task. Self-supervised learning can be seen as a hybrid approach that lies somewhere between unsupervised and supervised learning. In high-energy physics, the most used type of self-supervised model is by far the auto-encoder.\\nThe standard Auto-Encoder (AE) model consists of two neural networks: the encoder and the decoder. The encoder maps the input data to a \\\\textit{latent space} of a lower dimensionality. For example, a particle that is represented by 64 features (transverse momentum, azimuthal angle, etc.) is reduced to a 16 feature representation. In contrast, the objective of the decoder is to reconstruct the input features from the latent space features. The ultimate goal of the AE training is to minimise the difference between the input and reconstructed data. This difference can be quantified by employing various loss functions. The Mean Squared Error (MSE) loss function is the most basic example of quantifying the input-output discrepancy:\\n\\\\begin{equation}\\nL_\\\\mathrm{MSE} = (x - f(z,\\\\theta))^2 \\n\\\\end{equation}\\nwhere $x$ is the input data, $z$ is the latent space data, and $\\\\theta$ are the weights of the decoder. This reconstruction loss is propagated through both the decoder and the encoder. Thus, the latent space and the reconstructed data evolve simultaneously.\\nThe extent to which the auto-encoder latent space follows a statistical distribution is referred to as the latent space \\\\textit{regularity}. The latent space of the standard auto-encoder does not follow any particular distribution. The regularity of the standard AE depends on the input features, the dimension of the latent space, and the encoder architecture. Thus, the encoder will shape the latent space such that it facilitates the reconstruction task, thus minimising the MSE loss from \\\\autoref{eq:vanillaloss}. In contrast, the Variational Auto-encoder (VAE)\\\\cite{kingma2014autoencoding} is an extension of the conventional auto-encoder described above, which models the latent representation to approximate a given probability distribution. This is typically a Gaussian distribution, described by a mean and a variance; however, many alternatives exist\\\\cite{joo2019dirichlet, patrini2019sinkhorn, Cerri_2019, Dillon_2021, Cheng_2023}, and the choice of latent space distribution ultimately depends on the task. \\nThe main idea of variational inference is to deﬁne a parametrised family of distributions and to search within it for the best approximation of the chosen prior distribution. The ``best approximation\\'\\' is defined as the element of the aforementioned family of distributions that minimises a pre-deﬁned function that measures the dissimilarity between the trial approximation and the prior. The function that is most commonly employed for this task is the Kullback-Leibler\\\\cite{Joyce2011} (KL) divergence, defined as\\n\\\\begin{equation}\\n\\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma}) = -\\\\frac{1}{2}\\\\sum_i \\\\left ( \\\\log(\\\\sigma_i^2) - \\\\sigma_i^2 -\\\\mu_i^2 +1 \\\\right)~,\\n\\\\end{equation}\\nfor the specific case of comparing a parametrised Gaussian distribution $\\\\mathrm{N}(\\\\vec{\\\\mu},\\\\vec{\\\\sigma})$ with $\\\\mathrm{N}(1, 0)$. A broader discussion on the KL divergence is found in Ref.\\\\,\\\\citen{paisley2012variational}. Note that the KL divergence is a somewhat unstable dissimilarity metric. Hence, more robust alternatives exist, such as the Wasserstein distance, which led to the creation of an AE architecture with the same name\\\\cite{tolstikhin2019wasserstein}. Variations on the Wasserstein AE have also been applied in a high-energy physics context\\\\cite{Komiske_2019, Komiske_2020}.\\nThe VAE loss consists of two components: the reconstruction loss, conventionally the MSE, and the KL divergence term. The latter encourages the VAE to produce a latent space that follows a well-defined prior distribution, regularising the model. Thus, the VAE loss can be written schematically as\\n\\\\begin{equation}\\n{\\\\cal L} = (1-\\\\beta) \\\\mathrm{MSE}(\\\\mathrm{Output}, \\\\mathrm{Input}) + \\\\beta \\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma})~,\\n\\\\end{equation}\\nwhere MSE labels the reconstruction loss, $\\\\mathrm{D_\\\\mathrm{{KL}}}$ is the KL regularization term, and $\\\\beta\\\\in[0, 1]$ is a hyperparameter that balances the effect of the two loss components.\\nThe weakly supervised methods from the previous sections aim to learn the likelihood ratio and thus can identify anomalies. In contrast, self-supervised models only learn the probability density of the background. Hence, an event may be labeled as anomalous if its probability to be associated with the learned latent distribution is very low. Additionally, the learned distribution exists in a lower dimensional embedded space. This stops the model from memorizing the input and is a form of lossy compression. Therefore, the model is generally capable of reconstructing events it is frequently exposed to during its training, but it fails at reconstructing events that are rare in the training set. The difference between the input data and its reconstructed counterpart may then be used to define an anomaly score: a high MSE is expected for anomalous data and a low MSE is expected for typical events. An illustration of this paradigm is shown in Figure~\\\\ref{fig:ae}. There exist several studies in HEP where AEs and VAEs are used for detecting new physics as outliers in the data~\\\\cite{Farina:2018fyg, Heimel:2018mkt,Blance:2019ibf,Hajer:2018kqm,Roy:2019jae,Cheng_2023}. For~example, this type of workflow was used to search for new physics in the two-body invariant mass spectrum of two jets or a jet and a lepton with the ATLAS Experiment in Ref.~\\\\citen{ATLAS:2023ixc}. Therein, a selection on an auto-encoder output is used to suppress the background and define signal regions with a high signal-to-background ratio. The auto-encoder output for data and for a range of potential new physics signatures is shown in Figure~\\\\ref{fig:atlasae}.\\nAs mentioned in the beginning of this section, autoencoders are efficient for event-by-event outlier detection and are not expected to perform well in finding overdensities. This makes them complimentary to the weakly supervised methods. Furthermore, an additional problem that auto-encoders have is discussed in Ref.~\\\\citen{obstructions}. In the aforementioned work it is demonstrated that the connection between large MSE and anomalies is not completely clear: for data sets with a nontrivial topology, there will always be points that wrongly are classified as anomalous. Conventionally, this can be mitigated by using VAEs and classifying anomalous events using the regularized latent space. An alternative method of circumventing this issue is based on the so called normalised AE \\\\cite{yoon2023autoencoding}, which is located at the boundary between self-supervised and unsupervised learning. This newer type of AE architecture uses energy-based models as an alternative to the likelihood ratio or the MSE. Thus, the normalised AE avoids classifying genuinely complex albeit standard events as anomalous. For more details on this last kind of AE and its possible application to HEP, see Ref.\\\\,\\\\citen{dillon2023normalized}. As mentioned earlier, diffusion models are also being explored as an alternative method to perform density estimation, similar to variational autoencoders, utilizing the learned density as a permutation-invariant anomaly detection score ~\\\\cite{mikuni2023highdimensional}.\\nAs mentioned earlier, diffusion models are also increasingly being investigated as an alternative approach for density estimation. This method parallels the use of variational autoencoders, leveraging the learned density to create a permutation-invariant score for anomaly detection, as detailed in Ref.~\\\\citen{mikuni2023highdimensional}.\\n\\\\subsubsection{Unsupervised Methods}\\nUnsupervised anomaly detection methods usually perform some type of data clustering. They include models such as Support Vector Machines~\\\\cite{boser1992training}, Isolation Forests~\\\\cite{isoforest}, and Gaussian Mixture Models\\\\cite{vanBeekveld:2020txa, Kuusela_2012}. An application using SVMs for anomaly detection in particle physics is discussed in Section~\\\\ref{sec:qml}. An example of unsupervised clustering for collider physics is presented in Ref.~\\\\citen{ucluster}. Therein, the Unsupervised Clustering algorithm, or UCluster, uses an attention-based Graph Neural Network known as \"ABC net\"~\\\\cite{abcnet} to create a latent space in which points sharing similar properties are placed close to each other. This is achieved by combining a clustering objective and a classification task during training. The produced embedding is shown to be capable of clustering together events that contain a new physics signal. A benefit of this method is that it naturally provides a way of performing background estimation. For each identified cluster, the nearest cluster within the embedding space can be used as a background model. The anomalous signal remains\\nlocalized in a particular cluster. Therefore, the nearest clusters are signal free, as shown in Figure~\\\\ref{fig:ucluster}. \\nA model-independent search method, based on Gaussian Mixture Models (GMMs), is introduced in Ref.~\\\\citen{Kuusela_2012}. Within this methodology, a GMM is being used to model the background. Then, in order to avoid any dependence on a signal hypothesis, deviations from this model are identified by fitting a mixture of the background model and a number of additional Gaussians to the observed data. This allows to search for any potential deviation from the background expectation without developing a model for the signal a priori.\\nFinally, decision trees have also been explored in anomaly detection for searches. For example, in Ref.~\\\\citen{roche2023nanosecond}, a tree-based autoencoder is trained through a self-supervised paradigm on background data and then evaluated on the ADC challenge data~\\\\cite{adcchallenge}. Their unsupervised counterpart, isolation forests, have been less prominent in particle physics, but they have been applied for accelerator control~\\\\cite{Halilovic:2665985}.\\nA key challenge with outlier detection methods, as discussed in Ref.~\\\\citen{golling2023massive}, is their tendency to generate anomaly scores closely correlated to the variable of interest. This may lead to undesired sculpting effects, complicating bump-hunt like searches. To address this, strategies such as decorrelating the latent space from the variable of interest or tailoring the anomaly metric to be conditional on the jet mass \\\\cite{Cheng_2023} should be explored. However, efforts in these areas remain limited.\\n', '\\\\section{Quantum Anomaly Detection}\\n\\\\subsection{Background}\\nIn recent years, \\\\ac{QML} has emerged as a new paradigm for data processing at the intersection of machine learning and quantum information processing. Quantum computing has the potential to address real-world challenges that are difficult or even intractable for classical computers~\\\\cite{arute_quantum_2019, zhong_quantum_2020, madsen_quantum_2022}. \\nSuch problems include prime number factoring~\\\\cite{shor97}, a problem at the basis of classical modern-day encryption, search in unstructured databases~\\\\cite{grover1996fast}, solving systems of equations~\\\\cite{Harrow_Hassidim_Lloyd_2009}, and simulations of quantum systems, enabling first-principle computation of chemical properties in atomic, molecular, and nuclear systems~\\\\cite{Kandala2017, Barkoutsos2018, Kiss2022}\\\\footnote{Please also refer to references therein, references provided are non-exhaustive.}.\\nInitially, applications of quantum computing in \\\\ac{ML} focused on investigating speedups in computationally expensive subroutines of learning algorithms, such as optimization and matrix inversion~\\\\cite{Lloyd_Mohseni_Rebentrost_2014, Wiebe_Braun_Lloyd_2012, Rebentrost_Mohseni_Lloyd_2014}. Through this scope, replacing classical subroutines with quantum algorithms provide provable speedups in terms of runtime complexity of the \\\\ac{ML} training. Nevertheless, such proofs frequently require large and fault tolerant quantum hardware. Namely, quantum computers with error correction schemes that are able to arbitrarily suppress the inherent logical error rates~\\\\cite{Acharya2023}. Such devices do not exist yet. Currently available quantum computers are noisy and have limited number of qubits of small decoherence times~\\\\cite{preskill_nisq_2018}. Hence, the size of the quantum circuits and the number of operations that can be carried out at present are limited. Quantum algorithms with too many operations for the device at hand, can be rendered useless, or at least equivalent to a classical computation, by the inherent hardware noise.\\nLately, studies have also investigated the potential of quantum computing to enhance fundamentally the learning model itself~\\\\cite{Biamonte2017, Schuld2018, Schuld_2018_book, Havlicek2019}. \\\\ac{QML} models have been shown to generalise well with few training data~\\\\cite{caro_generalization_2022}, to provide advantages over classical algorithms for specific types of learning problems~\\\\cite{Liu2021rigorous, Kubler2021, Huang2021, pirnay22_superpol, muser2023}, and are able to identify patterns in data that cannot be recognised efficiently with classical methods~\\\\cite{huangQA2022}. Mirroring classical models for classification tasks, \\\\ac{QML} algorithms can be coarsely grouped into two categories: \\\\textit{kernel-based} methods and \\\\textit{variational} learning approaches\\\\cite{Cerezo2022}.\\nIn the former category a main example are \\\\ac{QSVM}, where a classical \\\\ac{SVM}~\\\\cite{svmVapnik} is equipped with a \\\\textit{quantum kernel}. The values of the kernel are evaluated on a quantum computer through measurements~\\\\cite{Schuld_2019, Havlicek2019}. During training, (Q)SVMs find the hyperplane that separates different classes of data, maximizing the margin between them. These models can create non-linear decision boundaries in the data input space when equipped with kernel functions~\\\\cite{svmVapnik}. The kernels are constructed by feature maps that transform the data into a higher dimensional feature space, in which the classes can be more effectively separated by a linear decision boundary. In the case of a quantum kernel, the data is mapped to the Hilbert space spanned by the qubit states. The dimensionality of this space grows exponentially with the number of qubits, and hence, such models are difficult to simulate classically. After constructing the quantum kernel matrix from the measurements the loss function of the model is minimized on a classical device using quadratic programming techniques. In particle physics, SVMs can be used for supervised classification tasks~\\\\cite{vaiciulis2003, sforza2013, sahin2016}, although their use is not as prevalent as deep learning approaches or ensemble models such as boosted decision trees. Additionally, kernel machines have been extended to an unsupervised setting~\\\\cite{one_class_svm}, where the training data is assumed to contain mostly background events and an upper bound on the expected anomaly contamination is set using a hyperparameter.\\nThe latter category encompasses parametrised quantum circuits, also referred to as \\\\ac{QNN} or variational quantum circuits. These circuits are composed of gates whose parameters can be tuned iteratively to minimize a loss function using classical gradient-based learning techniques~\\\\cite{Benedetti2019, Mitarai2018}. The output of the circuit is an expectation value of an operator, that is sampled from a quantum computer. This approach allows for the training of quantum circuits to perform specific tasks, such as classification and generative modeling. Specific architectures of QNNs have been shown to be universal function approximators~\\\\cite{Pérez-Salinas2020}, and that they can be expressed as a Fourier series expansion~\\\\cite{Schuld_Sweke_Meyer_2021}. Contrary to (quantum) kernel machines, the loss function landscape of QNNs is non-convex, which can lead to trainability issues similar to the ones of the vanishing gradient problem in classical neural networks~\\\\cite{Cerezo2021, Wang2021, Holmes2022}. Nevertheless, the authors of Ref.~\\\\citen{thanasilp2022exponential} argue that under certain conditions, kernel-based models can also manifest similar problems in training. Additionally, some works provide a unified view of \\\\ac{QML}~\\\\cite{Jerbi2023}, while others claim that the kernel-based learning is more natural for \\\\ac{QML}~\\\\cite{Schuld_qml_is_kernels_2021}.\\nIn most current applications, one can treat the quantum computer as a specialized processing unit, that is part of the overall computation. The hybrid algorithms discussed above aim to leverage the different strengths of classical and quantum processing units while mitigating their corresponding weaknesses.\\n\\\\subsection{Applications in High Energy Physics} \\nStudies have assessed the potential of quantum computing and variational algorithms for simulations of lattice field theories~\\\\cite{atas2021, mildenberger2022probing, funcke2023review}, as an alternative to \\\\ac{MC} techniques~\\\\cite{Kiss_MC2022, Delgado_Hamilton_2022, Chang2021, Bravo2022}, and for parton showering simulations~\\\\cite{nachman_qshower2021, bepari2022}. Research along these lines is motivated by the question of whether quantum algorithms can provide a natural platform for simulating fundamental physics~\\\\cite{dimeglio2023quantum}. An additional motive is the prospect of quantum computers providing a more favorable computational complexity than currently available classical methods. Furthermore, \\\\ac{QML} models have been developed for solving reconstruction problems in the context of collider experiments~\\\\cite{Tuysuz_2020, Grossi2020, magano2022, Lerjarza2022, duckett2022}. \\n\\\\subsection{Supervised classification} \\nIn terms of classification tasks in a model-dependent setting, quantum computing was first considered in Ref.~\\\\citen{Mott:2017}. Therein, the training of a classifier for $H\\\\to\\\\gamma\\\\gamma$ events was mapped to a quantum annealing task. \\nSince then, studies have mainly focused on the design and implementation of supervised \\\\ac{QML} algorithms based on different \\\\ac{QSVM} and \\\\ac{QNN} architectures that are able classify \\\\ac{HEP} events by discriminating the signal distribution from the background distribution~\\\\cite{terashi2021, blance_quantum_2021, qmlHiggs2021, Guan_2021, Heredge2021, Chen2020, Chen2021, wu_2021_kernel, wu_2021_qnn}. Such quantum models, are often developed and assessed via computationally expensive quantum simulations on classical processors using limited number of qubits; typically up to 20. In these simulations, the algorithms can be investigated in an ideal noiseless environment. After the architecture has been chosen and its hyperparameters have converged to values that lead to good performance on the learning task at hand, the \\\\ac{QML} algorithms are tested by running experiments on real hardware via cloud-based platforms. \\nThe developed quantum models are typically benchmarked against classical models of similar complexity, that are trained on the same small data sets. So far, the number of training data points is at the order of $10^2$ to $10^4$. \\\\ac{HEP} datasets are frequently high dimensional, with number of features exceeding the order of presently available number qubits, posing a challenge for direct input and processing by data encoding circuits on current quantum devices. To address this challenge, a set of reduced features is typically used as an input to the \\\\ac{QML} models. This representation of reduced dimensionality is obtained by manual selection of physical variables~\\\\cite{terashi2021, blance_quantum_2021}, Principle Component Analysis (PCA)~\\\\cite{wu_2021_qnn, wu_2021_kernel, Schuhmacher23, Peixoto2023}, or autoencoders~\\\\cite{qmlHiggs2021, wozniak_belis_puljak23}. In the case of autoencoders, the compression of \\\\ac{HEP} events can be regarded as more representative since these models can, at least approximately, retain non-linear correlations of the input features in their latent space. Such higher order correlations are removed by definition in the case of PCA, and are potentially lost in manual feature selection or feature extraction based on univariate discrimination metrics~\\\\cite{qmlHiggs2021}.\\nSo far, in most studies regarding supervised models, the performance of the quantum algorithms is competitive and matches the performance of their classical counterparts. Numerical evidence suggests that \\\\ac{QML} algorithms might outperform classical models when the training datasets are small~\\\\cite{terashi2021, wu_2021_kernel, Guan_2021, Gianelle2022}. However, such a property has not been proven in general, yet.\\n\\\\subsection{Unsupervised new-physics searches} \\nRecently, different strategies were proposed for new-physics searches at the \\\\ac{LHC} using \\\\ac{QML} in the context of anomaly detection~\\\\cite{Blance_Spannowsky_2021, Ngairangbam_2022, alvi2023, wozniak_belis_puljak23, Schuhmacher23, Bermot2023, Bordoni:2023lad}. In Ref.~\\\\citen{Blance_Spannowsky_2021}, Gaussian Boson Sampling (GBS) is used to create a lower dimensional representation of \\\\ac{BSM} events where the Higgs boson decays into two pseudoscalar particles. GBS is classically difficult to simulate and can be implemented using continuous variable devices such as photonic quantum computers. This procedure is combined with a quantum version of the K-means clustering algorithm, Q-means, to detect anomalies. \\nK-means is a method that aims to partition an unlabeled dataset into K clusters in the feature space. Each cluster center, also called a centroid, is defined as the mean of the datapoints that belong to that cluster. Each datapoint is assigned to the nearest centroid, according to some distance measure, typically the Euclidean metric, which serves us the loss function of the algorithm. The cluster assignment and the coordinates of the centroids are iteratively updated, according to the loss minimization procedure, until the datapoints have converged to specific stable clusters. In the case of Q-means, the datapoints are embedded in the quantum Hilbert space, where the distance calculation occurs depending on the chosen quantum circuit. Additionally, depending on the design of the quantum model the minimisation of the loss can be accomplished with a quantum or classical algorithm. K-means has been applied to \\\\ac{HEP} for jet clustering~\\\\cite{Chekanov2006, Thaler2012, Stewart2015}, while Q-means and its variants have been applied also for unsupervised detection of new-physics events~\\\\cite{Blance_Spannowsky_2021, wozniak_belis_puljak23}.\\nA quantum autoencoder (QAE) is considered in Ref.~\\\\citen{Ngairangbam_2022} using four physical variables as an input. The authors demonstrate, both in quantum simulation and hardware, that the QAE is a promising approach for \\\\ac{BSM} scenarios involving a resonant heavy Higgs decaying to a pair of top quarks, and a \\\\ac{SM} Higgs decaying to two dark matter particles. The authors of Ref.~\\\\citen{Bordoni:2023lad} employ QAEs for the detection of long-lived particles and adapt the proposed methods for execution on real quantum hardware. Additionally, different architectures of \\\\ac{QNN} have been investigated in Ref.~\\\\citen{alvi2023}, using low dimensional (simulated) datasets involving Higgs-like scalar particles signatures as anomalies in a semi-supervised setting. The authors do not identify any region where the tested \\\\ac{QML} models present an advantage in performance or in terms of the needed size of the trained dataset.\\nRef.~\\\\citen{Schuhmacher23} proposes a simulation-assisted new-physics search where supervised quantum and classical \\\\ac{SVM}s are trained on a dataset that contains \\\\ac{SM} processes as background and an artificial set of anomalous events obtained from the \\\\ac{SM}-distributed features as signal. Specifically, the authors generate the distributions of the signal samples by a so-called scrambling process, in which the feature distributions of the background are smeared by the normal distribution, preserving energy and momentum conservation. Furthermore, it is demonstrated that the considered models are able to generalize to real signals such as Higgs and Graviton production events.\\nIn Ref.~\\\\citen{Bermot2023}, a quantum Generative Adversarial Network is designed to extract an anomaly score for each data input. The authors benchmark the proposed model and verify its efficacy in data sets where they treat the Higgs boson production and Graviton production as anomalies, respectively. Additionally, generative modeling in the context of Hamiltonian learning has been investigated for semi-leptonic top and dijet event production~\\\\cite{araz2023}. The anomaly score in Ref.~\\\\citen{araz2023} is constructed using the different properties of the time evolution of quantum states that represent background and signal data.\\nA new-physics search in dijet topologies is addressed in Ref.~\\\\citen{wozniak_belis_puljak23}, where an unsupervised quantum kernel machine and quantum clustering methods are designed to define a metric of typicality for QCD jets. The dijet events are described by 600 features --100 particle constituents per jet and three features per particle-- and the examined anomalies include two different Graviton scenarios and a \\\\ac{BSM} scalar boson production with the final state. The authors develop a convolutional autoencoder to produce a compressed representation of the \\\\ac{HEP} events, addressing the challenge of directly processing realistic high-dimensional data on current quantum devices. Consequently, the quantum anomaly detection algorithms use as an input the latent representation of the data that is generated by the encoder and are trained using QCD background events. For the proposed kernel-based anomaly detection model, this work identifies an advantage in performance of the quantum model over its classical counterpart.\\n\\\\subsection{Discussion \\\\& Outlook}\\nIn \\\\ac{HEP} applications so far, the quantum models are not designed to explicitly manifest an inductive bias towards the structure of the chosen (simulated) particle physics datasets. In the aforementioned studies, the model architectures, i.e., quantum circuits used for the implementation of \\\\ac{QNN}s and feature maps for the kernel methods, are constructed following ansätze in the \\\\ac{QML} literature that have desired properties such as expressiveness and hardware efficiency. \\nMany \\\\ac{QML} algorithms have been inspired by classical model architectures, such as autoencoders~\\\\cite{Romero_2017}, convolutional neural networks~\\\\cite{cong2019}, equivariant models~\\\\cite{Nguyen:2022lww}, and graph neural networks~\\\\cite{verdon2019quantum}. Despite drawing inspiration from classical models, these quantum counterparts may exhibit distinct properties and inductive biases~\\\\cite{Kubler2021,Bowles2023}. The studies presented in Sections~\\\\ref{sec:quantum_supervised} and~\\\\ref{sec:quantum_unsupervised} compare the performance of their proposed models to their classical counterparts for the task at hand. However, beyond promising results in specific problems and datasets, identifying precisely in which applications QML models could provide consistent benefits such as enhancement in model performance, or computational speed-ups, still remains an open question and an active area of research. Furthermore, due to limitations in current hardware, the behavior of \\\\ac{QML} models in the regime that is comparable to current state-of-the-art deep learning models, i.e., having millions or even billions of training samples and model parameters, is unknown.\\nIn general, the exploration of \\\\ac{QML} strategies for \\\\ac{HEP} data is, at least partly, motivated by the question of whether quantum models can exploit correlations and information existing in particle physics datasets leading to advantages in performance. It is important to note, that no studies, so far, have used quantum models for supervised or unsupervised classification in real data from \\\\ac{HEP} experiments.\\nThe data measured by the detectors and stored for the analysis of \\\\ac{HEP} experiments is classical. However, a quantum field theory framework is essential to predict and properly explain the outcome of such experiments. Furthermore, remnants of the initial quantum mechanical process --particle interaction-- are still present in the data. Specifically, measuring spin correlations between particles~\\\\cite{top_correlations_CMS2019}, observing entanglement between particles produced in proton collisions~\\\\cite{ATLAS:2023_entanglement, Cervera2017, Severi2022, Fabbrichesi2023} and violation of Bell inequalities~\\\\cite{Fabbrichesi2021, Afik_Nova_2022, Ghosh2023} in \\\\ac{LHC} data has been established. Measuring these first-principle quantities highlights that data from particle physics experiments cannot be described by classical local hidden-variable theories. In conclusion, the topics discussed above represent an active field of research and hold promise for classical and quantum data analysis algorithms that can enhance our ability to probe for new-physics.\\n'}},\n",
       "       {'entity_name': 'variational autoencoder vae', 'entity_type': 'analysis_technique', 'description': 'An extension of the conventional auto-encoder that models the latent representation to approximate a given probability distribution, typically a Gaussian distribution, and incorporates a regularization term to ensure the latent space follows this distribution.', 'relevant_passages': {'\\\\section{Anomaly detection for model-agnostic new physics searches}\\n\\\\subsection{Outlier detection}\\nThe above methods focus on detecting new physics as overdensities in very specific regions of the kinematic phase space; this paradigm is similar to a traditional bump hunt, often performed in HEP searches for novel particles. However, new physics signatures are equally likely to manifest themselves as unexpected events in the tail of distributions. This type of events may be identified using out-of-distribution detection algorithms. The prime example of such an algorithm is the auto-encoder\\\\cite{lecun1987phd, ballard1987modular, hinton1993autoencoders}, which is especially popular in high-energy physics applications\\\\cite{Radovic:2018dip, albertsson2019machine, Jawahar:2021vyu, tsan2021particle, Finke_2021, Laguarta:2023evo, Vaslin:2023lig, Anzalone:2023ugq, Bohm:2023ihd}.\\n\\\\subsubsection{Self-supervised methods}\\nSelf-supervised learning\\\\cite{balestriero2023cookbook} is a form of unsupervised learning where the data provides the supervision. In general, a part of the data is initially withheld from the model and the task of the network is to reproduce this data. Consequently, the network learns a meaningful representation of the data to solve this problem. The self-supervised learning workflow usually involves two stages: first, generating a set of supervisory signals from the input data; and second, employing these signals for a supervised~task. Self-supervised learning can be seen as a hybrid approach that lies somewhere between unsupervised and supervised learning. In high-energy physics, the most used type of self-supervised model is by far the auto-encoder.\\nThe standard Auto-Encoder (AE) model consists of two neural networks: the encoder and the decoder. The encoder maps the input data to a \\\\textit{latent space} of a lower dimensionality. For example, a particle that is represented by 64 features (transverse momentum, azimuthal angle, etc.) is reduced to a 16 feature representation. In contrast, the objective of the decoder is to reconstruct the input features from the latent space features. The ultimate goal of the AE training is to minimise the difference between the input and reconstructed data. This difference can be quantified by employing various loss functions. The Mean Squared Error (MSE) loss function is the most basic example of quantifying the input-output discrepancy:\\n\\\\begin{equation}\\nL_\\\\mathrm{MSE} = (x - f(z,\\\\theta))^2 \\n\\\\end{equation}\\nwhere $x$ is the input data, $z$ is the latent space data, and $\\\\theta$ are the weights of the decoder. This reconstruction loss is propagated through both the decoder and the encoder. Thus, the latent space and the reconstructed data evolve simultaneously.\\nThe extent to which the auto-encoder latent space follows a statistical distribution is referred to as the latent space \\\\textit{regularity}. The latent space of the standard auto-encoder does not follow any particular distribution. The regularity of the standard AE depends on the input features, the dimension of the latent space, and the encoder architecture. Thus, the encoder will shape the latent space such that it facilitates the reconstruction task, thus minimising the MSE loss from \\\\autoref{eq:vanillaloss}. In contrast, the Variational Auto-encoder (VAE)\\\\cite{kingma2014autoencoding} is an extension of the conventional auto-encoder described above, which models the latent representation to approximate a given probability distribution. This is typically a Gaussian distribution, described by a mean and a variance; however, many alternatives exist\\\\cite{joo2019dirichlet, patrini2019sinkhorn, Cerri_2019, Dillon_2021, Cheng_2023}, and the choice of latent space distribution ultimately depends on the task. \\nThe main idea of variational inference is to deﬁne a parametrised family of distributions and to search within it for the best approximation of the chosen prior distribution. The ``best approximation\\'\\' is defined as the element of the aforementioned family of distributions that minimises a pre-deﬁned function that measures the dissimilarity between the trial approximation and the prior. The function that is most commonly employed for this task is the Kullback-Leibler\\\\cite{Joyce2011} (KL) divergence, defined as\\n\\\\begin{equation}\\n\\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma}) = -\\\\frac{1}{2}\\\\sum_i \\\\left ( \\\\log(\\\\sigma_i^2) - \\\\sigma_i^2 -\\\\mu_i^2 +1 \\\\right)~,\\n\\\\end{equation}\\nfor the specific case of comparing a parametrised Gaussian distribution $\\\\mathrm{N}(\\\\vec{\\\\mu},\\\\vec{\\\\sigma})$ with $\\\\mathrm{N}(1, 0)$. A broader discussion on the KL divergence is found in Ref.\\\\,\\\\citen{paisley2012variational}. Note that the KL divergence is a somewhat unstable dissimilarity metric. Hence, more robust alternatives exist, such as the Wasserstein distance, which led to the creation of an AE architecture with the same name\\\\cite{tolstikhin2019wasserstein}. Variations on the Wasserstein AE have also been applied in a high-energy physics context\\\\cite{Komiske_2019, Komiske_2020}.\\nThe VAE loss consists of two components: the reconstruction loss, conventionally the MSE, and the KL divergence term. The latter encourages the VAE to produce a latent space that follows a well-defined prior distribution, regularising the model. Thus, the VAE loss can be written schematically as\\n\\\\begin{equation}\\n{\\\\cal L} = (1-\\\\beta) \\\\mathrm{MSE}(\\\\mathrm{Output}, \\\\mathrm{Input}) + \\\\beta \\\\mathrm{D_\\\\mathrm{{KL}}}(\\\\vec{\\\\mu}, \\\\vec{\\\\sigma})~,\\n\\\\end{equation}\\nwhere MSE labels the reconstruction loss, $\\\\mathrm{D_\\\\mathrm{{KL}}}$ is the KL regularization term, and $\\\\beta\\\\in[0, 1]$ is a hyperparameter that balances the effect of the two loss components.\\nThe weakly supervised methods from the previous sections aim to learn the likelihood ratio and thus can identify anomalies. In contrast, self-supervised models only learn the probability density of the background. Hence, an event may be labeled as anomalous if its probability to be associated with the learned latent distribution is very low. Additionally, the learned distribution exists in a lower dimensional embedded space. This stops the model from memorizing the input and is a form of lossy compression. Therefore, the model is generally capable of reconstructing events it is frequently exposed to during its training, but it fails at reconstructing events that are rare in the training set. The difference between the input data and its reconstructed counterpart may then be used to define an anomaly score: a high MSE is expected for anomalous data and a low MSE is expected for typical events. An illustration of this paradigm is shown in Figure~\\\\ref{fig:ae}. There exist several studies in HEP where AEs and VAEs are used for detecting new physics as outliers in the data~\\\\cite{Farina:2018fyg, Heimel:2018mkt,Blance:2019ibf,Hajer:2018kqm,Roy:2019jae,Cheng_2023}. For~example, this type of workflow was used to search for new physics in the two-body invariant mass spectrum of two jets or a jet and a lepton with the ATLAS Experiment in Ref.~\\\\citen{ATLAS:2023ixc}. Therein, a selection on an auto-encoder output is used to suppress the background and define signal regions with a high signal-to-background ratio. The auto-encoder output for data and for a range of potential new physics signatures is shown in Figure~\\\\ref{fig:atlasae}.\\nAs mentioned in the beginning of this section, autoencoders are efficient for event-by-event outlier detection and are not expected to perform well in finding overdensities. This makes them complimentary to the weakly supervised methods. Furthermore, an additional problem that auto-encoders have is discussed in Ref.~\\\\citen{obstructions}. In the aforementioned work it is demonstrated that the connection between large MSE and anomalies is not completely clear: for data sets with a nontrivial topology, there will always be points that wrongly are classified as anomalous. Conventionally, this can be mitigated by using VAEs and classifying anomalous events using the regularized latent space. An alternative method of circumventing this issue is based on the so called normalised AE \\\\cite{yoon2023autoencoding}, which is located at the boundary between self-supervised and unsupervised learning. This newer type of AE architecture uses energy-based models as an alternative to the likelihood ratio or the MSE. Thus, the normalised AE avoids classifying genuinely complex albeit standard events as anomalous. For more details on this last kind of AE and its possible application to HEP, see Ref.\\\\,\\\\citen{dillon2023normalized}. As mentioned earlier, diffusion models are also being explored as an alternative method to perform density estimation, similar to variational autoencoders, utilizing the learned density as a permutation-invariant anomaly detection score ~\\\\cite{mikuni2023highdimensional}.\\nAs mentioned earlier, diffusion models are also increasingly being investigated as an alternative approach for density estimation. This method parallels the use of variational autoencoders, leveraging the learned density to create a permutation-invariant score for anomaly detection, as detailed in Ref.~\\\\citen{mikuni2023highdimensional}.\\n\\\\subsubsection{Unsupervised Methods}\\nUnsupervised anomaly detection methods usually perform some type of data clustering. They include models such as Support Vector Machines~\\\\cite{boser1992training}, Isolation Forests~\\\\cite{isoforest}, and Gaussian Mixture Models\\\\cite{vanBeekveld:2020txa, Kuusela_2012}. An application using SVMs for anomaly detection in particle physics is discussed in Section~\\\\ref{sec:qml}. An example of unsupervised clustering for collider physics is presented in Ref.~\\\\citen{ucluster}. Therein, the Unsupervised Clustering algorithm, or UCluster, uses an attention-based Graph Neural Network known as \"ABC net\"~\\\\cite{abcnet} to create a latent space in which points sharing similar properties are placed close to each other. This is achieved by combining a clustering objective and a classification task during training. The produced embedding is shown to be capable of clustering together events that contain a new physics signal. A benefit of this method is that it naturally provides a way of performing background estimation. For each identified cluster, the nearest cluster within the embedding space can be used as a background model. The anomalous signal remains\\nlocalized in a particular cluster. Therefore, the nearest clusters are signal free, as shown in Figure~\\\\ref{fig:ucluster}. \\nA model-independent search method, based on Gaussian Mixture Models (GMMs), is introduced in Ref.~\\\\citen{Kuusela_2012}. Within this methodology, a GMM is being used to model the background. Then, in order to avoid any dependence on a signal hypothesis, deviations from this model are identified by fitting a mixture of the background model and a number of additional Gaussians to the observed data. This allows to search for any potential deviation from the background expectation without developing a model for the signal a priori.\\nFinally, decision trees have also been explored in anomaly detection for searches. For example, in Ref.~\\\\citen{roche2023nanosecond}, a tree-based autoencoder is trained through a self-supervised paradigm on background data and then evaluated on the ADC challenge data~\\\\cite{adcchallenge}. Their unsupervised counterpart, isolation forests, have been less prominent in particle physics, but they have been applied for accelerator control~\\\\cite{Halilovic:2665985}.\\nA key challenge with outlier detection methods, as discussed in Ref.~\\\\citen{golling2023massive}, is their tendency to generate anomaly scores closely correlated to the variable of interest. This may lead to undesired sculpting effects, complicating bump-hunt like searches. To address this, strategies such as decorrelating the latent space from the variable of interest or tailoring the anomaly metric to be conditional on the jet mass \\\\cite{Cheng_2023} should be explored. However, efforts in these areas remain limited.\\n'}},\n",
       "       {'entity_name': 'outlier detection', 'entity_type': 'analysis_technique', 'description': 'A technique used to identify data points that deviate significantly from the majority of the data, which can enhance acceptance rates for events that are challenging to capture using conventional algorithms in particle physics.', 'relevant_passages': {\"\\\\section{Real-time anomaly detection}\\nThe ultimate limiting factor for many searches for new physics is the event selection system of particle detectors. Tens of terabytes of data per second are produced from proton-proton collisions occurring every 25 ns, an extreme data rate that can not be read out and stored. The rate is reduced by a real-time, two-stage event filter processing system -- the {\\\\em trigger} -- which decides whether each collision event should be kept for further analysis or be discarded. The first stage, the Level-1 trigger, is completely hardware-based running around one thousand large field programmable (FPGA) gate arrays on custom boards. The data is buffered close to the detector while the processing occurs, with a maximum latency of ${\\\\mathcal O}(1)~\\\\mu$s to make the trigger decision. High selection accuracy in the trigger is crucial in order to keep only the most interesting events while keeping the output bandwidth low, reducing the event rate from $40$~MHz to $100$~kHz. The data accepted by the Level-1 trigger, are read out from the detector and sent to the second software-based event filtering system, the High Level Trigger (HLT). Here the data rate is further reduced from $100$~kHz to $1$~kHz. This processing is done on commercially available CPUs and, recently, also on GPU accelerators~\\\\citen{Bocci_2023}. The latency requirement at the HLT is ${\\\\mathcal O}(100)$ms.\\nRecently, it has been proposed to look for Beyond Standard Model physics signatures in a model-agnostic way at the trigger level, both at the HLT~\\\\citen{vaemining} and at the Level-1~\\\\citen{ad_nmi} stage. The existing selection algorithms within the trigger system currently prioritize collisions that generate high-energy outgoing particles. However, these algorithms have reduced sensitivity to e.g. signatures involving a high multiplicity of low-momentum particles. To address this, outlier detection techniques have garnered attention for their potential to enhance acceptance rates for events that are challenging to capture using conventional algorithms. The challenge when designing such models is to adhere to the strict latency, resource and throughput constraints of the trigger.\\nAlgorithm targeting the completely hardware-based Level-1 system are especially difficult to design. Deploying ML algorithms on FPGAs presents a significant challenge due to the specialized engineering expertise required. Unlike traditional software implementations, which can be executed on general-purpose processors, FPGAs demand a deep understanding of hardware design and optimization. The process of mapping complex mathematical operations, like those found in neural networks, onto FPGA circuits is intricate and requires careful consideration of factors such as data flow, parallelism, and memory access patterns. This becomes especially important in the Level-1 trigger, where the maximum latency per algorithm can be as low as 50 ns and only a few percent of the FPGA resources can be allocated to one specific algorithm. Recently, this process has been made easier through the introduction of software libraries that perform an automatic translation of ML models into highly parallel FPGA firmware, hls4ml~\\\\cite{hlsfml} and Finn~\\\\cite{finn}. These libraries are interfaced to libraries that perform \\\\textit{quantization-aware training}, a method for reducing the numerical precision of weights and activations in a neural network during training, hence reducing their memory footprint.\\nUtilizing these tools, Ref.~\\\\citen{ad_nmi} demonstrates that real-time anomaly detection using a variational auto-encoder architecture is feasible within 100 ns and using only a fraction of the FPGA resources. This is made possible through quantization-aware training, and clever architecture choices. For instance, rather than using the mean-squared error between the input and the output of the autoencoder as anomaly score, only the KL divergence term entering the VAE loss is used. The benefits of using the encoder and the KL term only is that one can avoid performing Gaussian sampling on the hardware, saving resources and latency by not having to evaluate the decoder and in addition there is no need to buffer the input data for computation of the MSE.\\nThis demonstration of the capability to perform real-time anomaly detection on FPGAs has generated attention within the community and initiated a challenge similar to those found on Kaggle, the ADC 2021 challenge~\\\\cite{adcchallenge}, as well as a dataset~\\\\cite{ADC} for benchmarking such algorithms.\\nRecently, an outlier detection algorithm similar to the one described in Ref.~\\\\citen{ad_nmi} has been deployed into a copy of the CMS Experiment Global Trigger (GT) board~\\\\cite{axo}. This copy of the GT system receives exactly the same input as the CMS GT, but cannot trigger a full read-out of the CMS detector, making it an excellent test-bench for algorithms targeting the main system. The anomaly detection algorithm, referred to as AXOL1TL, has been trained on unbiased data collected by the CMS experiment and shown to improve the signal efficiency for a range of different BSM signals by up to 46\\\\Figure~\\\\ref{fig:axo} shows an event display of the highest anomaly score event selected by AXOL1TL after analyzing CMS data collected in 2023. The event does not pass and other L1 trigger algorithm and is characterized by a very high number of low to medium momentum jets.\\nWhen using outlier detection as a way of discovering new physics phenomena, the detection of outliers is not sufficient. A full statistical framework for hypothesis testing is also necessary in order to claim discovery. This requires a background estimate with which the observed data in the signal region can be compared to, and this can be either based on simulated data or it can be fully data-driven. This is also true when it comes to analyzing data collected by an \\\\textit{anomaly trigger}. From the discussion above on density estimation, the background estimate was an important part of the anomaly detection method itself, for instance in the CWoLa bumphunt the estimate was taken from the regions adjacent to the signal region and in Cathode it was sampled from the ML-generated density estimate itself. For autoencoder-based anomaly, the background estimate is not provided by the model itself. It only offers a method for achieving high signal sensitivity. In Ref.~\\\\citen{PhysRevD.105.055006}, a statistical method for detecting non-resonant anomalies using auto-encoders is introduced. In this approach, multiple autoencoders are trained with the aim of maximizing their independence from one another. This is achieved by utilizing the distance de-correlation (DisCo) method~\\\\cite{PhysRevLett.125.122001,PhysRevD.103.035021}, where a regularizer term based on the DisCo measure of statistical dependence is included in the training. Events are classified as anomalous if their reconstruction quality is poor across all autoencoders. Instances classified as anomalous by one autoencoder, but not by all, provide the necessary context for estimating the Standard Model background in a manner that is not model dependent. This estimation is carried out using the ABCD method. The ABCD method is commonly used for data-driven background estimation in particle physics. It consists of designing four data regions, A, B C, and D, based on orthogonal selections on two independent variables, $\\\\tau_1$ and $\\\\tau_2$, and then transferring the background prediction from the three signal-free regions into the signal region. In Ref.~\\\\citen{PhysRevD.105.055006}, the two variables are the anomaly scores of the two autoencoders, which are now statistically independent due to the inclusion of the DisCo term at training time. In order to use this to design an anomaly detection method that can run in the trigger system, the authors propose to preserve all events falling within the signal-sensitive region as defined by the two autoencoders. Additionally, a random subset of events in the other three regions would be conserved for subsequent offline background estimation.\\nTrigger algorithms typically focus on achieving a specific signal efficiency. However, in anomaly detection, which aims to be sensitive to a broad spectrum of new signals, there isn't a definite signal efficiency to target. Instead, these algorithms are calibrated for a certain false positive rate (FPR), which is directly related to a predetermined trigger rate. This rate generally falls within the range of around $\\\\mathcal{O}(10-100)$~Hz, necessitating an FPR near $\\\\sim10^{-6}$. During the tuning process, the signal efficiency across various potential new physics scenarios is tracked for guidance, along with the loss on the self-supervisory label (measured as the Mean Squared Error between input and output). Yet, developing improved metrics for refining outlier detection methods, particularly those that include sensitivity to novel, unidentified signals, remains a challenging and unresolved task.\\n\"}},\n",
       "       {'entity_name': 'quantizationaware training', 'entity_type': 'analysis_technique', 'description': 'A method for reducing the numerical precision of weights and activations in a neural network during training, aimed at minimizing memory footprint while maintaining model performance.', 'relevant_passages': {\"\\\\section{Real-time anomaly detection}\\nThe ultimate limiting factor for many searches for new physics is the event selection system of particle detectors. Tens of terabytes of data per second are produced from proton-proton collisions occurring every 25 ns, an extreme data rate that can not be read out and stored. The rate is reduced by a real-time, two-stage event filter processing system -- the {\\\\em trigger} -- which decides whether each collision event should be kept for further analysis or be discarded. The first stage, the Level-1 trigger, is completely hardware-based running around one thousand large field programmable (FPGA) gate arrays on custom boards. The data is buffered close to the detector while the processing occurs, with a maximum latency of ${\\\\mathcal O}(1)~\\\\mu$s to make the trigger decision. High selection accuracy in the trigger is crucial in order to keep only the most interesting events while keeping the output bandwidth low, reducing the event rate from $40$~MHz to $100$~kHz. The data accepted by the Level-1 trigger, are read out from the detector and sent to the second software-based event filtering system, the High Level Trigger (HLT). Here the data rate is further reduced from $100$~kHz to $1$~kHz. This processing is done on commercially available CPUs and, recently, also on GPU accelerators~\\\\citen{Bocci_2023}. The latency requirement at the HLT is ${\\\\mathcal O}(100)$ms.\\nRecently, it has been proposed to look for Beyond Standard Model physics signatures in a model-agnostic way at the trigger level, both at the HLT~\\\\citen{vaemining} and at the Level-1~\\\\citen{ad_nmi} stage. The existing selection algorithms within the trigger system currently prioritize collisions that generate high-energy outgoing particles. However, these algorithms have reduced sensitivity to e.g. signatures involving a high multiplicity of low-momentum particles. To address this, outlier detection techniques have garnered attention for their potential to enhance acceptance rates for events that are challenging to capture using conventional algorithms. The challenge when designing such models is to adhere to the strict latency, resource and throughput constraints of the trigger.\\nAlgorithm targeting the completely hardware-based Level-1 system are especially difficult to design. Deploying ML algorithms on FPGAs presents a significant challenge due to the specialized engineering expertise required. Unlike traditional software implementations, which can be executed on general-purpose processors, FPGAs demand a deep understanding of hardware design and optimization. The process of mapping complex mathematical operations, like those found in neural networks, onto FPGA circuits is intricate and requires careful consideration of factors such as data flow, parallelism, and memory access patterns. This becomes especially important in the Level-1 trigger, where the maximum latency per algorithm can be as low as 50 ns and only a few percent of the FPGA resources can be allocated to one specific algorithm. Recently, this process has been made easier through the introduction of software libraries that perform an automatic translation of ML models into highly parallel FPGA firmware, hls4ml~\\\\cite{hlsfml} and Finn~\\\\cite{finn}. These libraries are interfaced to libraries that perform \\\\textit{quantization-aware training}, a method for reducing the numerical precision of weights and activations in a neural network during training, hence reducing their memory footprint.\\nUtilizing these tools, Ref.~\\\\citen{ad_nmi} demonstrates that real-time anomaly detection using a variational auto-encoder architecture is feasible within 100 ns and using only a fraction of the FPGA resources. This is made possible through quantization-aware training, and clever architecture choices. For instance, rather than using the mean-squared error between the input and the output of the autoencoder as anomaly score, only the KL divergence term entering the VAE loss is used. The benefits of using the encoder and the KL term only is that one can avoid performing Gaussian sampling on the hardware, saving resources and latency by not having to evaluate the decoder and in addition there is no need to buffer the input data for computation of the MSE.\\nThis demonstration of the capability to perform real-time anomaly detection on FPGAs has generated attention within the community and initiated a challenge similar to those found on Kaggle, the ADC 2021 challenge~\\\\cite{adcchallenge}, as well as a dataset~\\\\cite{ADC} for benchmarking such algorithms.\\nRecently, an outlier detection algorithm similar to the one described in Ref.~\\\\citen{ad_nmi} has been deployed into a copy of the CMS Experiment Global Trigger (GT) board~\\\\cite{axo}. This copy of the GT system receives exactly the same input as the CMS GT, but cannot trigger a full read-out of the CMS detector, making it an excellent test-bench for algorithms targeting the main system. The anomaly detection algorithm, referred to as AXOL1TL, has been trained on unbiased data collected by the CMS experiment and shown to improve the signal efficiency for a range of different BSM signals by up to 46\\\\Figure~\\\\ref{fig:axo} shows an event display of the highest anomaly score event selected by AXOL1TL after analyzing CMS data collected in 2023. The event does not pass and other L1 trigger algorithm and is characterized by a very high number of low to medium momentum jets.\\nWhen using outlier detection as a way of discovering new physics phenomena, the detection of outliers is not sufficient. A full statistical framework for hypothesis testing is also necessary in order to claim discovery. This requires a background estimate with which the observed data in the signal region can be compared to, and this can be either based on simulated data or it can be fully data-driven. This is also true when it comes to analyzing data collected by an \\\\textit{anomaly trigger}. From the discussion above on density estimation, the background estimate was an important part of the anomaly detection method itself, for instance in the CWoLa bumphunt the estimate was taken from the regions adjacent to the signal region and in Cathode it was sampled from the ML-generated density estimate itself. For autoencoder-based anomaly, the background estimate is not provided by the model itself. It only offers a method for achieving high signal sensitivity. In Ref.~\\\\citen{PhysRevD.105.055006}, a statistical method for detecting non-resonant anomalies using auto-encoders is introduced. In this approach, multiple autoencoders are trained with the aim of maximizing their independence from one another. This is achieved by utilizing the distance de-correlation (DisCo) method~\\\\cite{PhysRevLett.125.122001,PhysRevD.103.035021}, where a regularizer term based on the DisCo measure of statistical dependence is included in the training. Events are classified as anomalous if their reconstruction quality is poor across all autoencoders. Instances classified as anomalous by one autoencoder, but not by all, provide the necessary context for estimating the Standard Model background in a manner that is not model dependent. This estimation is carried out using the ABCD method. The ABCD method is commonly used for data-driven background estimation in particle physics. It consists of designing four data regions, A, B C, and D, based on orthogonal selections on two independent variables, $\\\\tau_1$ and $\\\\tau_2$, and then transferring the background prediction from the three signal-free regions into the signal region. In Ref.~\\\\citen{PhysRevD.105.055006}, the two variables are the anomaly scores of the two autoencoders, which are now statistically independent due to the inclusion of the DisCo term at training time. In order to use this to design an anomaly detection method that can run in the trigger system, the authors propose to preserve all events falling within the signal-sensitive region as defined by the two autoencoders. Additionally, a random subset of events in the other three regions would be conserved for subsequent offline background estimation.\\nTrigger algorithms typically focus on achieving a specific signal efficiency. However, in anomaly detection, which aims to be sensitive to a broad spectrum of new signals, there isn't a definite signal efficiency to target. Instead, these algorithms are calibrated for a certain false positive rate (FPR), which is directly related to a predetermined trigger rate. This rate generally falls within the range of around $\\\\mathcal{O}(10-100)$~Hz, necessitating an FPR near $\\\\sim10^{-6}$. During the tuning process, the signal efficiency across various potential new physics scenarios is tracked for guidance, along with the loss on the self-supervisory label (measured as the Mean Squared Error between input and output). Yet, developing improved metrics for refining outlier detection methods, particularly those that include sensitivity to novel, unidentified signals, remains a challenging and unresolved task.\\n\"}},\n",
       "       {'entity_name': 'background estimate', 'entity_type': 'statistics_concept', 'description': 'A statistical framework necessary for hypothesis testing in particle physics, which involves estimating the expected background in the signal region to compare with observed data.', 'relevant_passages': {\"\\\\section{Real-time anomaly detection}\\nThe ultimate limiting factor for many searches for new physics is the event selection system of particle detectors. Tens of terabytes of data per second are produced from proton-proton collisions occurring every 25 ns, an extreme data rate that can not be read out and stored. The rate is reduced by a real-time, two-stage event filter processing system -- the {\\\\em trigger} -- which decides whether each collision event should be kept for further analysis or be discarded. The first stage, the Level-1 trigger, is completely hardware-based running around one thousand large field programmable (FPGA) gate arrays on custom boards. The data is buffered close to the detector while the processing occurs, with a maximum latency of ${\\\\mathcal O}(1)~\\\\mu$s to make the trigger decision. High selection accuracy in the trigger is crucial in order to keep only the most interesting events while keeping the output bandwidth low, reducing the event rate from $40$~MHz to $100$~kHz. The data accepted by the Level-1 trigger, are read out from the detector and sent to the second software-based event filtering system, the High Level Trigger (HLT). Here the data rate is further reduced from $100$~kHz to $1$~kHz. This processing is done on commercially available CPUs and, recently, also on GPU accelerators~\\\\citen{Bocci_2023}. The latency requirement at the HLT is ${\\\\mathcal O}(100)$ms.\\nRecently, it has been proposed to look for Beyond Standard Model physics signatures in a model-agnostic way at the trigger level, both at the HLT~\\\\citen{vaemining} and at the Level-1~\\\\citen{ad_nmi} stage. The existing selection algorithms within the trigger system currently prioritize collisions that generate high-energy outgoing particles. However, these algorithms have reduced sensitivity to e.g. signatures involving a high multiplicity of low-momentum particles. To address this, outlier detection techniques have garnered attention for their potential to enhance acceptance rates for events that are challenging to capture using conventional algorithms. The challenge when designing such models is to adhere to the strict latency, resource and throughput constraints of the trigger.\\nAlgorithm targeting the completely hardware-based Level-1 system are especially difficult to design. Deploying ML algorithms on FPGAs presents a significant challenge due to the specialized engineering expertise required. Unlike traditional software implementations, which can be executed on general-purpose processors, FPGAs demand a deep understanding of hardware design and optimization. The process of mapping complex mathematical operations, like those found in neural networks, onto FPGA circuits is intricate and requires careful consideration of factors such as data flow, parallelism, and memory access patterns. This becomes especially important in the Level-1 trigger, where the maximum latency per algorithm can be as low as 50 ns and only a few percent of the FPGA resources can be allocated to one specific algorithm. Recently, this process has been made easier through the introduction of software libraries that perform an automatic translation of ML models into highly parallel FPGA firmware, hls4ml~\\\\cite{hlsfml} and Finn~\\\\cite{finn}. These libraries are interfaced to libraries that perform \\\\textit{quantization-aware training}, a method for reducing the numerical precision of weights and activations in a neural network during training, hence reducing their memory footprint.\\nUtilizing these tools, Ref.~\\\\citen{ad_nmi} demonstrates that real-time anomaly detection using a variational auto-encoder architecture is feasible within 100 ns and using only a fraction of the FPGA resources. This is made possible through quantization-aware training, and clever architecture choices. For instance, rather than using the mean-squared error between the input and the output of the autoencoder as anomaly score, only the KL divergence term entering the VAE loss is used. The benefits of using the encoder and the KL term only is that one can avoid performing Gaussian sampling on the hardware, saving resources and latency by not having to evaluate the decoder and in addition there is no need to buffer the input data for computation of the MSE.\\nThis demonstration of the capability to perform real-time anomaly detection on FPGAs has generated attention within the community and initiated a challenge similar to those found on Kaggle, the ADC 2021 challenge~\\\\cite{adcchallenge}, as well as a dataset~\\\\cite{ADC} for benchmarking such algorithms.\\nRecently, an outlier detection algorithm similar to the one described in Ref.~\\\\citen{ad_nmi} has been deployed into a copy of the CMS Experiment Global Trigger (GT) board~\\\\cite{axo}. This copy of the GT system receives exactly the same input as the CMS GT, but cannot trigger a full read-out of the CMS detector, making it an excellent test-bench for algorithms targeting the main system. The anomaly detection algorithm, referred to as AXOL1TL, has been trained on unbiased data collected by the CMS experiment and shown to improve the signal efficiency for a range of different BSM signals by up to 46\\\\Figure~\\\\ref{fig:axo} shows an event display of the highest anomaly score event selected by AXOL1TL after analyzing CMS data collected in 2023. The event does not pass and other L1 trigger algorithm and is characterized by a very high number of low to medium momentum jets.\\nWhen using outlier detection as a way of discovering new physics phenomena, the detection of outliers is not sufficient. A full statistical framework for hypothesis testing is also necessary in order to claim discovery. This requires a background estimate with which the observed data in the signal region can be compared to, and this can be either based on simulated data or it can be fully data-driven. This is also true when it comes to analyzing data collected by an \\\\textit{anomaly trigger}. From the discussion above on density estimation, the background estimate was an important part of the anomaly detection method itself, for instance in the CWoLa bumphunt the estimate was taken from the regions adjacent to the signal region and in Cathode it was sampled from the ML-generated density estimate itself. For autoencoder-based anomaly, the background estimate is not provided by the model itself. It only offers a method for achieving high signal sensitivity. In Ref.~\\\\citen{PhysRevD.105.055006}, a statistical method for detecting non-resonant anomalies using auto-encoders is introduced. In this approach, multiple autoencoders are trained with the aim of maximizing their independence from one another. This is achieved by utilizing the distance de-correlation (DisCo) method~\\\\cite{PhysRevLett.125.122001,PhysRevD.103.035021}, where a regularizer term based on the DisCo measure of statistical dependence is included in the training. Events are classified as anomalous if their reconstruction quality is poor across all autoencoders. Instances classified as anomalous by one autoencoder, but not by all, provide the necessary context for estimating the Standard Model background in a manner that is not model dependent. This estimation is carried out using the ABCD method. The ABCD method is commonly used for data-driven background estimation in particle physics. It consists of designing four data regions, A, B C, and D, based on orthogonal selections on two independent variables, $\\\\tau_1$ and $\\\\tau_2$, and then transferring the background prediction from the three signal-free regions into the signal region. In Ref.~\\\\citen{PhysRevD.105.055006}, the two variables are the anomaly scores of the two autoencoders, which are now statistically independent due to the inclusion of the DisCo term at training time. In order to use this to design an anomaly detection method that can run in the trigger system, the authors propose to preserve all events falling within the signal-sensitive region as defined by the two autoencoders. Additionally, a random subset of events in the other three regions would be conserved for subsequent offline background estimation.\\nTrigger algorithms typically focus on achieving a specific signal efficiency. However, in anomaly detection, which aims to be sensitive to a broad spectrum of new signals, there isn't a definite signal efficiency to target. Instead, these algorithms are calibrated for a certain false positive rate (FPR), which is directly related to a predetermined trigger rate. This rate generally falls within the range of around $\\\\mathcal{O}(10-100)$~Hz, necessitating an FPR near $\\\\sim10^{-6}$. During the tuning process, the signal efficiency across various potential new physics scenarios is tracked for guidance, along with the loss on the self-supervisory label (measured as the Mean Squared Error between input and output). Yet, developing improved metrics for refining outlier detection methods, particularly those that include sensitivity to novel, unidentified signals, remains a challenging and unresolved task.\\n\"}},\n",
       "       {'entity_name': 'distance decorrelation disco method', 'entity_type': 'analysis_technique', 'description': 'A method used to maximize the independence of multiple autoencoders during training by including a regularizer term based on the DisCo measure of statistical dependence.', 'relevant_passages': {\"\\\\section{Real-time anomaly detection}\\nThe ultimate limiting factor for many searches for new physics is the event selection system of particle detectors. Tens of terabytes of data per second are produced from proton-proton collisions occurring every 25 ns, an extreme data rate that can not be read out and stored. The rate is reduced by a real-time, two-stage event filter processing system -- the {\\\\em trigger} -- which decides whether each collision event should be kept for further analysis or be discarded. The first stage, the Level-1 trigger, is completely hardware-based running around one thousand large field programmable (FPGA) gate arrays on custom boards. The data is buffered close to the detector while the processing occurs, with a maximum latency of ${\\\\mathcal O}(1)~\\\\mu$s to make the trigger decision. High selection accuracy in the trigger is crucial in order to keep only the most interesting events while keeping the output bandwidth low, reducing the event rate from $40$~MHz to $100$~kHz. The data accepted by the Level-1 trigger, are read out from the detector and sent to the second software-based event filtering system, the High Level Trigger (HLT). Here the data rate is further reduced from $100$~kHz to $1$~kHz. This processing is done on commercially available CPUs and, recently, also on GPU accelerators~\\\\citen{Bocci_2023}. The latency requirement at the HLT is ${\\\\mathcal O}(100)$ms.\\nRecently, it has been proposed to look for Beyond Standard Model physics signatures in a model-agnostic way at the trigger level, both at the HLT~\\\\citen{vaemining} and at the Level-1~\\\\citen{ad_nmi} stage. The existing selection algorithms within the trigger system currently prioritize collisions that generate high-energy outgoing particles. However, these algorithms have reduced sensitivity to e.g. signatures involving a high multiplicity of low-momentum particles. To address this, outlier detection techniques have garnered attention for their potential to enhance acceptance rates for events that are challenging to capture using conventional algorithms. The challenge when designing such models is to adhere to the strict latency, resource and throughput constraints of the trigger.\\nAlgorithm targeting the completely hardware-based Level-1 system are especially difficult to design. Deploying ML algorithms on FPGAs presents a significant challenge due to the specialized engineering expertise required. Unlike traditional software implementations, which can be executed on general-purpose processors, FPGAs demand a deep understanding of hardware design and optimization. The process of mapping complex mathematical operations, like those found in neural networks, onto FPGA circuits is intricate and requires careful consideration of factors such as data flow, parallelism, and memory access patterns. This becomes especially important in the Level-1 trigger, where the maximum latency per algorithm can be as low as 50 ns and only a few percent of the FPGA resources can be allocated to one specific algorithm. Recently, this process has been made easier through the introduction of software libraries that perform an automatic translation of ML models into highly parallel FPGA firmware, hls4ml~\\\\cite{hlsfml} and Finn~\\\\cite{finn}. These libraries are interfaced to libraries that perform \\\\textit{quantization-aware training}, a method for reducing the numerical precision of weights and activations in a neural network during training, hence reducing their memory footprint.\\nUtilizing these tools, Ref.~\\\\citen{ad_nmi} demonstrates that real-time anomaly detection using a variational auto-encoder architecture is feasible within 100 ns and using only a fraction of the FPGA resources. This is made possible through quantization-aware training, and clever architecture choices. For instance, rather than using the mean-squared error between the input and the output of the autoencoder as anomaly score, only the KL divergence term entering the VAE loss is used. The benefits of using the encoder and the KL term only is that one can avoid performing Gaussian sampling on the hardware, saving resources and latency by not having to evaluate the decoder and in addition there is no need to buffer the input data for computation of the MSE.\\nThis demonstration of the capability to perform real-time anomaly detection on FPGAs has generated attention within the community and initiated a challenge similar to those found on Kaggle, the ADC 2021 challenge~\\\\cite{adcchallenge}, as well as a dataset~\\\\cite{ADC} for benchmarking such algorithms.\\nRecently, an outlier detection algorithm similar to the one described in Ref.~\\\\citen{ad_nmi} has been deployed into a copy of the CMS Experiment Global Trigger (GT) board~\\\\cite{axo}. This copy of the GT system receives exactly the same input as the CMS GT, but cannot trigger a full read-out of the CMS detector, making it an excellent test-bench for algorithms targeting the main system. The anomaly detection algorithm, referred to as AXOL1TL, has been trained on unbiased data collected by the CMS experiment and shown to improve the signal efficiency for a range of different BSM signals by up to 46\\\\Figure~\\\\ref{fig:axo} shows an event display of the highest anomaly score event selected by AXOL1TL after analyzing CMS data collected in 2023. The event does not pass and other L1 trigger algorithm and is characterized by a very high number of low to medium momentum jets.\\nWhen using outlier detection as a way of discovering new physics phenomena, the detection of outliers is not sufficient. A full statistical framework for hypothesis testing is also necessary in order to claim discovery. This requires a background estimate with which the observed data in the signal region can be compared to, and this can be either based on simulated data or it can be fully data-driven. This is also true when it comes to analyzing data collected by an \\\\textit{anomaly trigger}. From the discussion above on density estimation, the background estimate was an important part of the anomaly detection method itself, for instance in the CWoLa bumphunt the estimate was taken from the regions adjacent to the signal region and in Cathode it was sampled from the ML-generated density estimate itself. For autoencoder-based anomaly, the background estimate is not provided by the model itself. It only offers a method for achieving high signal sensitivity. In Ref.~\\\\citen{PhysRevD.105.055006}, a statistical method for detecting non-resonant anomalies using auto-encoders is introduced. In this approach, multiple autoencoders are trained with the aim of maximizing their independence from one another. This is achieved by utilizing the distance de-correlation (DisCo) method~\\\\cite{PhysRevLett.125.122001,PhysRevD.103.035021}, where a regularizer term based on the DisCo measure of statistical dependence is included in the training. Events are classified as anomalous if their reconstruction quality is poor across all autoencoders. Instances classified as anomalous by one autoencoder, but not by all, provide the necessary context for estimating the Standard Model background in a manner that is not model dependent. This estimation is carried out using the ABCD method. The ABCD method is commonly used for data-driven background estimation in particle physics. It consists of designing four data regions, A, B C, and D, based on orthogonal selections on two independent variables, $\\\\tau_1$ and $\\\\tau_2$, and then transferring the background prediction from the three signal-free regions into the signal region. In Ref.~\\\\citen{PhysRevD.105.055006}, the two variables are the anomaly scores of the two autoencoders, which are now statistically independent due to the inclusion of the DisCo term at training time. In order to use this to design an anomaly detection method that can run in the trigger system, the authors propose to preserve all events falling within the signal-sensitive region as defined by the two autoencoders. Additionally, a random subset of events in the other three regions would be conserved for subsequent offline background estimation.\\nTrigger algorithms typically focus on achieving a specific signal efficiency. However, in anomaly detection, which aims to be sensitive to a broad spectrum of new signals, there isn't a definite signal efficiency to target. Instead, these algorithms are calibrated for a certain false positive rate (FPR), which is directly related to a predetermined trigger rate. This rate generally falls within the range of around $\\\\mathcal{O}(10-100)$~Hz, necessitating an FPR near $\\\\sim10^{-6}$. During the tuning process, the signal efficiency across various potential new physics scenarios is tracked for guidance, along with the loss on the self-supervisory label (measured as the Mean Squared Error between input and output). Yet, developing improved metrics for refining outlier detection methods, particularly those that include sensitivity to novel, unidentified signals, remains a challenging and unresolved task.\\n\"}},\n",
       "       {'entity_name': 'systematic errors', 'entity_type': 'statistics_concept', 'description': 'Errors that arise from factors related to the experimental setup, which can affect the accuracy of measurements and the ability to exclude models in particle physics.', 'relevant_passages': {\"\\\\section{Introduction}\\nIn a particle accelerator, fundamental physical processes unfold, producing a variety of particles that characterize the final state of each event. These particles are reconstructed through detectors, which assign kinematic and dynamic variables to each event, such as the particle's position, energy deposition, and transverse momentum. These physical observables are typically defined by counting events in various observation channels~\\\\cite{read2002presentation,cowan2014statistics}. For example, measuring the invariant mass of a system within the 100-300 GeV range, with a 10 GeV resolution, yields 20 observation channels, each characterized by its event count.\\nFrom a physical perspective, various processes can contribute to the event count in any given observation channel. These processes may include known phenomena, as predicted by the Standard Model, or new physical processes beyond the model's current scope~\\\\cite{feldman1998unified, lista2016practical}. A central aspect of experimental physics analysis is determining which known processes contribute to a specific observable and identifying potential new processes to explain any deviations from established theories.\\nTo evaluate whether reconstructed events align with existing theories or if they indicate the need for alternative explanations, inferential statistical tools, such as parameter estimation and hypothesis testing, are employed~\\\\cite{cowan2014statistics, cranmer2015practical,barlow2019practical}. In this report, we describe and implement, using Python and RooFit~\\\\cite{verkerke2006roofit,schott2012roostats}, a range of statistical methods used in high-energy physics to estimate the sensitivity of new models and to define exclusion limits for these models.\\nThe development of particle physics has led to three main fields: theory, phenomenology, which links theory to experiments, and experimental work. A primary goal of phenomenology is to guide and optimize experimental searches for new particles based on theoretical models~\\\\cite{conway2005calculation, cranmer2015practical}. This strategy generally involves evaluating a model's sensitivity, identifying a kinematic region (signal region) with the highest potential for discovering new physics. Additionally, exclusion limits are established, defining the parameter space where the model is ruled out based on the expected event counts, typically at a 95\\\\\\nIn the experimental phase, the upper limit is calculated based on observed event counts rather than expected ones. After observation, two outcomes are possible: (1) the data conform to the established model, or (2) the data exhibit a discrepancy that cannot be explained by statistical fluctuations alone. In the first scenario, where data align with existing theories~\\\\cite{cowan2014statistics, casadei2011statistical}, the observed and expected limits are similar, providing no substantial evidence to support new theories, thus excluding them within a specific parameter region.\\nIn the second scenario, any significant discrepancy between data and theory is evaluated using the $5\\\\sigma$ threshold (corresponding to a $p$-value of $2.5 \\\\times 10^{-7}$)~\\\\cite{cowan2014statistics,jme2010cms}, which measures the probability of observing such data (or more extreme results) under the assumption that the current theory is correct. If this threshold is exceeded, a discovery can be claimed. Figure[\\\\ref{fig:1}] illustrates the general research framework in high-energy physics, highlighting the role of statistical tools in excluding models or detecting new physics.\\nIn the statistical modeling of new physical theories and their observations, a key source of uncertainty arises from systematic errors. These errors encompass factors related to the characteristics of the accelerator, particle detectors, and intrinsic model parameters, such as parton distribution functions (PDFs)~\\\\cite{junk1999confidence, cranmer2015practical}. Systematic uncertainties typically reduce the ability to exclude models and to detect new physics. Therefore, incorporating systematic effects is essential for achieving more accurate estimates\\\\cite{barlow2002systematic, read2002presentation, lista2016practical}.\\nThis document presents various models for estimating upper limits and significance, organized into two conceptual categories: single-channel and multichannel experiments, both with and without sources of systematic uncertainty. First, for single-channel experiments without systematic effects, we describe the frequentist and Bayesian approaches traditionally applied in experiments such as the former LEP collider and, more recently, the Large Hadron Collider (LHC)~\\\\cite{read2002presentation, atlas2012observation}. Next, we extend these methods to multichannel experiments, where the increase in dimensionality necessitates optimization strategies and Monte Carlo methods. Finally, we examine systematic effects that require a Bayesian approach, as well as the derivation of profile likelihoods through optimization processes\\\\cite{conway2005calculation}.\\n\"}},\n",
       "       {'entity_name': 'discovery threshold', 'entity_type': 'statistics_concept', 'description': 'A predefined level of statistical significance, often set at 5σ in particle physics, that must be reached to claim a discovery, indicating a very low probability of observing the result under the null hypothesis.', 'relevant_passages': {'\\\\section{Upper Limits for one channel experiment}\\n\\nIn scientific research, experiments are designed to collect data, and theories or models are developed to explain those observations. In general, the falsification of theories is based on hypothesis testing. Hypothesis tests determine, with a given confidence level ($CL$), whether the observed data provide sufficient evidence to reject an initial hypothesis, called the null hypothesis ($H_{0}$), in favor of an alternative hypothesis ($H_{1}$). The null hypothesis ($H_{0}$) is considered true until observations indicate otherwise; in such a case, the initial explanation is rejected, and the new theory ($H_{1}$) is accepted~\\\\cite{sinervo2002signal}. Both the frequentist and Bayesian approaches applied here yield robust upper limit estimations, adaptable to various experimental setups, making them vital tools for model testing and exclusion.\\nIn high-energy physics (HEP), the null hypothesis ($H_{0}$) refers to all known physical processes, which are summarized in what is known as the Standard Model. The alternative hypothesis ($H_{1}$) represents potential models that could explain new observations that the accepted model cannot account for, such as supersymmetry, extra dimensions, among others~\\\\cite{cowan2011asymptotic,florez2016probing}.\\nAdditionally, hypothesis testing requires the selection of a confidence level in terms of statistical significance ($\\\\alpha$).\\n\\\\begin{equation}\\nCL = 1 - \\\\alpha.\\n\\\\end{equation}\\nWhere $\\\\alpha$ (type I error) is the probability of rejecting the null hypothesis when it is true. By convention, model exclusion in particle physics is done for a value of $\\\\alpha = 0.05$, which corresponds to a confidence level ($CL$) of $95\\\\\\n\\\\begin{equation}\\n\\\\alpha = \\\\int_{P}^{\\\\infty} \\\\frac{1}{2\\\\pi} e^{-x^{2}/2} dx.\\n\\\\end{equation}\\nWhere $P$ is the percentile of the distribution, for which the type I error is $\\\\alpha = 0.05$. Figure~[\\\\ref{fig:2}] shows the standard normal distribution; the shaded area represents the type I error for the percentile $P_{95} \\\\approx 1.645$, which corresponds to the model exclusion condition at the $3\\\\sigma$ level.\\nThe confidence level for an observation is significantly higher. In general, the discovery threshold is set at $5\\\\sigma$, where the type I error is $\\\\alpha = 2.86 \\\\times 10^{-7}$. Table~[\\\\ref{tb:1}] summarizes different confidence levels and their interpretation in high-energy physics (HEP)~\\\\cite{lista2016practical,cranmer2015practical,cowan2011asymptotic}.\\n\\\\begin{table}[ht]\\n\\\\begin{center}\\n\\\\begin{tabular}{cccl}\\n\\\\hline\\n$\\\\alpha$ & $CL [\\\\ \\\\hline\\n$0.1586$ & $84.13$ & $1\\\\sigma$ & $H_{1}$ no excluded \\\\\\\\\\n$0.05$ & $95.00$ & $1.645\\\\sigma$ & $H_{1}$ excluded (Model exclusion) \\\\\\\\\\n$0.0227$ & $97.72$ & $2\\\\sigma$ & $H_{1}$ excluded \\\\\\\\ \\n$1.349 \\\\times 10^{-3}$ & $99.86$ & $3\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$3.167 \\\\times 10^{-5}$ & $99.99$ & $4\\\\sigma$ & $H_{1}$ excluded \\\\\\\\\\n$2.8665 \\\\times 10^{-7}$ & $99.99997$ & $5\\\\sigma$ & $H_{0}$ excluded (Observation) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Significance at different observation points. The exclusion of the alternative hypothesis requires a result statistical consistent with the background-only hypothesis ($H_{0}$), while confirmation of the observation requires compatibility with the signal + background hypothesis ($H_{1}$).}\\n\\\\end{center}\\n\\\\end{table}\\n'}},\n",
       "       {'entity_name': '5σ criterion', 'entity_type': 'statistics_concept', 'description': 'A standard threshold in particle physics indicating a discovery, where the probability of a false positive is less than 1 in 3.5 million, corresponding to a significance level of 5 standard deviations.', 'relevant_passages': {\"\\\\section{Upper Limits for multi-channel experiments}\\nIn the multi-channel case, the likelihood associated with the signal strength \\\\( \\\\mu \\\\) for the complete observation is determined by the joint likelihood of all channels~\\\\cite{wang2023recent}:\\n\\\\begin{equation}\\n\\\\mathcal{L}(\\\\mu) = \\\\prod_{i}^{Channels} \\\\mathcal{L}_{i}(\\\\mu).\\n\\\\end{equation}\\nWhere it is assumed that the information in each channel is independently and identically distributed. The definitions of the estimators \\\\( \\\\mathcal{Q}(\\\\mu) \\\\) and \\\\( q_{\\\\mu} \\\\) for a single channel naturally extend to the multi-channel case using the properties of logarithms. Thus, the statistical estimators are fully defined as follows, respectively:\\n\\\\begin{equation}\\n\\\\mathcal{Q}(\\\\mu) = \\\\sum_{i}^{Channels} \\\\mathcal{Q}_{i}(\\\\mu). \\n\\\\end{equation}\\n\\\\begin{equation}\\nq_{\\\\mu} = \\\\sum_{i}^{Channels} q_{\\\\mu,i}. \\n\\\\end{equation}\\nAs an example, synthetic data associated with a resonance near the measured mass of the Higgs boson \\\\( m_{H} = 125 \\\\ \\\\text{GeV} \\\\) were simulated, with an exponential background component characteristic of the invariant mass of a diphoton system. A total of 30 channels were simulated to numerically illustrate the discovery reported by the CMS collaboration in 2012~\\\\cite{cms2012observation}. Figure~[\\\\ref{fig:15}] shows the resonance data with a mass similar to the measured Higgs boson mass under an exponential background component (blue shaded area). Additionally, the alternative signal + background hypothesis is shown, which could be consistent with the observation.\\nAs in the 2012 search, several signal models with masses ranging from 100 to 160 GeV in steps of 6 GeV will be assumed. For each signal point, the expected upper limit (\\\\( n = b \\\\)), representing a measurement compatible with the background-only hypothesis, and the observed upper limit using the synthetic data will be calculated. Given the data observation, the upper limits allow for excluding the Higgs model at a specific mass or rejecting the background-only hypothesis in favor of the model with this new particle. Typically, this discrepancy is indicated by the difference between the expected and observed upper limits; when this difference is large, it must be quantified in terms of the \\\\( 5\\\\sigma \\\\) criterion to report a discovery~\\\\cite{lista2016practical}.\\nFigure~[\\\\ref{fig:16}] shows the upper limit values as a function of the hypothetical particle mass using the estimator \\\\( \\\\mathcal{Q} \\\\). The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are calculated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{\\\\mathcal{Q}(\\\\mu_{up})} \\\\)~\\\\cite{conway2005calculation}. Note that low- and high-mass models are excluded at a \\\\( 95\\\\ \\nOn the other hand, Figure~[\\\\ref{fig:17}] shows the upper limit values as a function of the hypothetical particle mass using the \\\\( q_{\\\\mu} \\\\) estimator. In the generation of each random experiment, \\\\( \\\\hat{\\\\mu} \\\\) is found using the \\\\texttt{Scipy.optimize} package. The error bands at \\\\( 1\\\\sigma \\\\) and \\\\( 2\\\\sigma \\\\) are estimated using Wald's asymptotic approximation \\\\( \\\\sigma_{\\\\mu} = \\\\mu_{up}/\\\\sqrt{q_{\\\\mu_{up}}} \\\\). Note the consistency of the results using both estimators. The primary difference lies in the approach to incorporating systematic uncertainties in the estimation of upper limits, significance, and \\\\( 5\\\\sigma \\\\) tension, which is why the profile likelihood is currently used by the CMS and ATLAS collaborations for these estimations~\\\\cite{cms2012observation, atlas2012observation}. Finally, to estimate the discrepancy between the observation and the expected number of events, the p-value of the observation is calculated under the assumption that the background-only hypothesis is correct.\\n\\\\begin{equation}\\nq_{0} = \\n\\\\begin{cases} \\n-2\\\\ln(\\\\lambda(0)) & \\\\hat{\\\\mu} \\\\ge 0 \\\\\\\\\\n0 & \\\\hat{\\\\mu} < 0,\\n\\\\end{cases}\\n\\\\end{equation}\\nwhere \\\\( n \\\\) is the number of observed events.\\n\\\\begin{equation}\\np_{0} = \\\\int_{q_{0,obs}}^{\\\\infty} f(q_{0} / 0) dq_{0}.\\n\\\\end{equation}\\nFigure~[\\\\ref{fig:18}] shows the local p-value as a function of the particle mass. The dashed lines indicate the \\\\( 3\\\\sigma \\\\) evidence region and the \\\\( 5\\\\sigma \\\\) discovery region. This graph illustrates the statistical behavior of the p-value in the search for the Higgs boson in 2012~\\\\cite{cms2012observation}.\\n\"}},\n",
       "       {'entity_name': 'cumulative distribution function cdf', 'entity_type': 'statistics_concept', 'description': 'A function that describes the probability that a random variable takes on a value less than or equal to a specific value, providing a complete description of the probability distribution.', 'relevant_passages': {'\\\\section{Appendix A: Poisson cumulative distribution}\\nThe confidence level $\\\\alpha$ associated with a specific signal strength $\\\\mu$ is determined by the cumulative distribution function (CDF). There is a relation that links the CDF of the Poisson distribution to that of the $\\\\chi^{2}(x; k=2(n+1))$ distribution. By applying the definition of the confidence level, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha = 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)) = \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{k/2+1} e^{-x^{2}/2}}{2^{k/2} \\\\Gamma(k/2)} = \\n\\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{2^{1+n}n!} dx.\\n\\\\end{equation}\\nBy applying integration by parts, where $u=\\\\frac{x^{n}}{n !}$, $du=\\\\frac{x^{n-1}}{(n-1)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n+1}}$, we obtained the following result:\\n\\\\begin{equation}\\n1-\\\\alpha=- \\\\frac{x^{n}e^{-\\\\frac{x}{2}}}{n! 2^{n}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{2^{n}(n-1)!} dx.\\n\\\\end{equation}\\nBy once again applying integration by parts, with $u=\\\\frac{x^{n-1}}{(n-1) !}$, $du=\\\\frac{x^{n-2}}{(n-2)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-1)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n}}$, the following expression is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} -\\\\frac{x^{n-1}e^{-\\\\frac{x}{2}}}{(n-1)! 2^{(n-1)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{2^{(n-1)}(n-2)!} dx.\\n\\\\end{equation}\\nFinally, by integrating once more by parts, with $u=\\\\frac{x^{n-2}}{(n-2) !}$, $du=\\\\frac{x^{n-3}}{(n-3)!} $, $v= -\\\\frac{e^{-\\\\frac{x}{2}}}{2^{(n-2)}}$, and $dv= \\\\frac{e^{-\\\\frac{x}{2}}}{2^{n-1}}$, we obtain:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} -\\\\frac{x^{n-2}e^{-\\\\frac{x}{2}}}{(n-2)! 2^{(n-2)}} \\\\Bigg|_{2 \\\\lambda}^{\\\\infty} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{n!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} + \\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!} + \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{x^{n-3}e^{-\\\\frac{x}{2}}}{2^{(n-2)}(n-3)!} dx.\\n\\\\end{equation}\\nAt this stage, a pattern can be discerned in the results obtained after repeatedly applying integration by parts. Specifically, the evaluation of the integral as $x \\\\to \\\\infty$ equal zero, while for $x = 2\\\\lambda$, a term of the following form is obtained:\\n\\\\begin{equation}\\n\\\\frac{(\\\\lambda)^{n-i}e^{-\\\\lambda}}{(n-i)!}.\\n\\\\end{equation}\\nThus, after performing $n-1$ integration by parts, the following result is obtained:\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+ \\\\int_{2 \\\\lambda}^{\\\\infty} \\\\frac{e^{-\\\\frac{x}{2}}}{2} dx\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\frac{(\\\\lambda)^{n}e^{-\\\\lambda}}{(n)!} + \\\\frac{(\\\\lambda)^{n-1}e^{-\\\\lambda}}{(n-1)!} +\\\\frac{(\\\\lambda)^{n-2}e^{-\\\\lambda}}{(n-2)!}+...+e^{-\\\\lambda}\\n\\\\end{equation}\\n\\\\begin{equation}\\n1-\\\\alpha= \\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}.\\n\\\\end{equation}\\nThis result corresponds to the cumulative distribution function (CDF) of the Poisson distribution. This distribution would then be related to the cumulative $\\\\chi^{2}$ distribution ($F_{\\\\chi^{2}}$) as follows:\\n\\\\begin{equation}\\n\\\\sum_{i=0}^{n} \\\\frac{(\\\\lambda)^{i}e^{-\\\\lambda}}{i!}= 1-F_{\\\\chi^2}(2\\\\lambda;k=2(n+1)).\\n\\\\end{equation}\\n'}}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BeautyInStats)",
   "language": "python",
   "name": "beautyinstats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
